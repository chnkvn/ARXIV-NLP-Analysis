{"title":"Triangle-free $2$-matchings","authors":["Katarzyna Paluch"],"raw_abstract":"We consider the problem of finding a maximum size triangle-free $2$-matching\nin a graph $G$. A $2$-matching is any subset of the edges such that each vertex\nis incident to at most two edges from the subset. We present a fast\ncombinatorial algorithm for the problem. Our algorithm and its analysis are\ndramatically simpler than the very complicated result by Hartvigsen from 1984.\n  In the design of this algorithm we use several new concepts. It has been\nproven before that for any triangle-free $2$-matching $M$ which is not maximum\nthe graph contains an $M$-augmenting path, whose application to $M$ results in\na bigger triangle-free $2$-matching. It was not known how to efficiently find\nsuch a path. A new observation is that the search for an augmenting path $P$\ncan be restricted to so-called {\\em amenable} paths that go through any\ntriangle $t$ contained in $P \\cup M$ a limited number of times. To find an\naugmenting path that is amenable and hence whose application does not create\nany triangle we forbid some edges to be followed by certain others. This\noperation can be thought of as using gadgets, in which some pairs of edges get\ndisconnected. To be able to disconnect two edges we employ {\\em half-edges}. A\n{\\em half-edge} of edge $e$ is, informally speaking, a half of $e$ containing\nexactly one of its endpoints. This is another novel application of half-edges\nthat were already been used for TSP and other matching problems. Additionally,\ngadgets are not fixed during any augmentation phase, but are dynamically\nchanging according to the currently discovered state of reachability by\namenable paths.","publication_date":1700679171,"paper_link":"http://arxiv.org/pdf/2311.13590v1","categories":["Mathematics"],"abstract":"We consider the problem of finding a maximum size triangle-free __FORMULA__-matching in a graph __FORMULA__. A __FORMULA__-matching is any subset of the edges such that each vertex is incident to at most two edges from the subset. We present a fast combinatorial algorithm for the problem. Our algorithm and its analysis are dramatically simpler than the very complicated result by Hartvigsen from 1984.   In the design of this algorithm we use several new concepts. It has been proven before that for any triangle-free __FORMULA__-matching __FORMULA__ which is not maximum the graph contains an __FORMULA__-augmenting path, whose application to __FORMULA__ results in a bigger triangle-free __FORMULA__-matching. It was not known how to efficiently find such a path. A new observation is that the search for an augmenting path __FORMULA__ can be restricted to so-called {\\em amenable} paths that go through any triangle __FORMULA__ contained in __FORMULA__ a limited number of times. To find an augmenting path that is amenable and hence whose application does not create any triangle we forbid some edges to be followed by certain others. This operation can be thought of as using gadgets, in which some pairs of edges get disconnected. To be able to disconnect two edges we employ {\\em half-edges}. A {\\em half-edge} of edge __FORMULA__ is, informally speaking, a half of __FORMULA__ containing exactly one of its endpoints. This is another novel application of half-edges that were already been used for TSP and other matching problems. Additionally, gadgets are not fixed during any augmentation phase, but are dynamically changing according to the currently discovered state of reachability by amenable paths."}
{"title":"Enigma: Privacy-Preserving Execution of QAOA on Untrusted Quantum Computers","authors":["Ramin Ayanzadeh","Ahmad Mousavi","Narges Alavisamani","Moinuddin Qureshi"],"raw_abstract":"Quantum computers can solve problems that are beyond the capabilities of\nconventional computers. As quantum computers are expensive and hard to\nmaintain, the typical model for performing quantum computation is to send the\ncircuit to a quantum cloud provider. This leads to privacy concerns for\ncommercial entities as an untrusted server can learn protected information from\nthe provided circuit. Current proposals for Secure Quantum Computing (SQC)\neither rely on emerging technologies (such as quantum networks) or incur\nprohibitive overheads (for Quantum Homomorphic Encryption). The goal of our\npaper is to enable low-cost privacy-preserving quantum computation that can be\nused with current systems.\n  We propose Enigma, a suite of privacy-preserving schemes specifically\ndesigned for the Quantum Approximate Optimization Algorithm (QAOA). Unlike\nprevious SQC techniques that obfuscate quantum circuits, Enigma transforms the\ninput problem of QAOA, such that the resulting circuit and the outcomes are\nunintelligible to the server. We introduce three variants of Enigma. Enigma-I\nprotects the coefficients of QAOA using random phase flipping and fudging of\nvalues. Enigma-II protects the nodes of the graph by introducing decoy qubits,\nwhich are indistinguishable from primary ones. Enigma-III protects the edge\ninformation of the graph by modifying the graph such that each node has an\nidentical number of connections. For all variants of Enigma, we demonstrate\nthat we can still obtain the solution for the original problem. We evaluate\nEnigma using IBM quantum devices and show that the privacy improvements of\nEnigma come at only a small reduction in fidelity (1%-13%).","publication_date":1700674823,"paper_link":"http://arxiv.org/pdf/2311.13546v1","categories":["Physics"],"abstract":"Quantum computers can solve problems that are beyond the capabilities of conventional computers. As quantum computers are expensive and hard to maintain, the typical model for performing quantum computation is to send the circuit to a quantum cloud provider. This leads to privacy concerns for commercial entities as an untrusted server can learn protected information from the provided circuit. Current proposals for Secure Quantum Computing (SQC) either rely on emerging technologies (such as quantum networks) or incur prohibitive overheads (for Quantum Homomorphic Encryption). The goal of our paper is to enable low-cost privacy-preserving quantum computation that can be used with current systems.   We propose Enigma, a suite of privacy-preserving schemes specifically designed for the Quantum Approximate Optimization Algorithm (QAOA). Unlike previous SQC techniques that obfuscate quantum circuits, Enigma transforms the input problem of QAOA, such that the resulting circuit and the outcomes are unintelligible to the server. We introduce three variants of Enigma. Enigma-I protects the coefficients of QAOA using random phase flipping and fudging of values. Enigma-II protects the nodes of the graph by introducing decoy qubits, which are indistinguishable from primary ones. Enigma-III protects the edge information of the graph by modifying the graph such that each node has an identical number of connections. For all variants of Enigma, we demonstrate that we can still obtain the solution for the original problem. We evaluate Enigma using IBM quantum devices and show that the privacy improvements of Enigma come at only a small reduction in fidelity (1%-13%)."}
{"title":"Euclid preparation TBD. Modelling spectroscopic clustering on mildly nonlinear scales in beyond-$\u039b$CDM models","authors":["Euclid Collaboration","B. Bose","P. Carrilho","M. Marinucci","C. Moretti","M. Pietroni","E. Carella","L. Piga","B. S. Wright","F. Vernizzi","C. Carbone","S. Casas","G. D'Amico","N. Frusciante","K. Koyama","F. Pace","A. Pourtsidou","M. Baldi","L. F. de la Bella","B. Fiorini","C. Giocoli","L. Lombriser","N. Aghanim","A. Amara","S. Andreon","N. Auricchio","S. Bardelli","C. Bodendorf","D. Bonino","E. Branchini","M. Brescia","J. Brinchmann","S. Camera","V. Capobianco","V. F. Cardone","J. Carretero","M. Castellano","S. Cavuoti","A. Cimatti","G. Congedo","C. J. Conselice","L. Conversi","Y. Copin","A. Costille","F. Courbin","H. M. Courtois","A. Da Silva","H. Degaudenzi","A. M. Di Giorgio","F. Dubath","C. A. J. Duncan","X. Dupac","S. Dusini","M. Farina","S. Farrens","S. Ferriol","P. Fosalba","M. Frailis","E. Franceschi","S. Galeotta","B. Garilli","B. Gillis","A. Grazian","F. Grupp","L. Guzzo","S. V. H. Haugan","F. Hormuth","A. Hornstrup","K. Jahnke","B. Joachimi","E. Keih\u00e4nen","S. Kermiche","A. Kiessling","M. Kilbinger","T. Kitching","M. Kunz","H. Kurki-Suonio","S. Ligori","P. B. Lilje","V. Lindholm","I. Lloro","D. Maino","E. Maiorano","O. Mansutti","O. Marggraf","K. Markovic","N. Martinet","F. Marulli","R. Massey","E. Medinaceli","M. Meneghetti","G. Meylan","M. Moresco","L. Moscardini","E. Munari","S. -M. Niemi","C. Padilla","S. Paltani","F. Pasian","K. Pedersen","W. J. Percival","V. Pettorino","S. Pires","G. Polenta","M. Poncet","L. A. Popa","L. Pozzetti","F. Raison","A. Renzi","J. Rhodes","G. Riccio","E. Romelli","M. Roncarelli","R. Saglia","D. Sapone","B. Sartoris","P. Schneider","A. Secroun","G. Seidel","M. Seiffert","S. Serrano","C. Sirignano","G. Sirri","L. Stanco","J. -L. Starck","P. Tallada-Cresp\u00ed","A. N. Taylor","I. Tereno","R. Toledo-Moreo","F. Torradeflot","I. Tutusaus","E. A. Valentijn","L. Valenziano","T. Vassallo","A. Veropalumbo","Y. Wang","J. Weller","G. Zamorani","J. Zoubian","E. Zucca","A. Biviano","E. Bozzo","C. Burigana","C. Colodro-Conde","D. Di Ferdinando","J. Graci\u00e1-Carpio","N. Mauri","C. Neissner","Z. Sakr","V. Scottez","M. Tenti","M. Viel","M. Wiesmann","Y. Akrami","V. Allevato","S. Anselmi","M. Ballardini","F. Bernardeau","S. Borgani","S. Bruton","R. Cabanac","A. Cappi","C. S. Carvalho","G. Castignani","T. Castro","G. Ca\u00f1as-Herrera","K. C. Chambers","A. R. Cooray","J. Coupon","S. Davini","S. de la Torre","G. De Lucia","G. Desprez","S. Di Domizio","H. Dole","A. D\u00edaz-S\u00e1nchez","J. A. Escartin Vigo","S. Escoffier","P. G. Ferreira","I. Ferrero","F. Finelli","L. Gabarra","K. Ganga","J. Garc\u00eda-Bellido","F. Giacomini","G. Gozaliasl","D. Guinet","A. Hall","S. Joudaki","J. J. E. Kajava","V. Kansal","D. Karagiannis","C. C. Kirkpatrick","L. Legrand","A. Loureiro","J. Macias-Perez","M. Magliocchetti","R. Maoli","M. Martinelli","C. J. A. P. Martins","S. Matthew","M. Maturi","L. Maurin","R. B. Metcalf","M. Migliaccio","P. Monaco","G. Morgante","S. Nadathur","Nicholas A. Walton","L. Patrizii","A. Pezzotta","V. Popa","C. Porciani","D. Potter","M. P\u00f6ntinen","P. Reimberg","P. -F. Rocci","A. G. S\u00e1nchez","A. Schneider","E. Sefusatti","M. Sereno","A. Silvestri","A. Spurio Mancini","J. Steinwagner","G. Testera","R. Teyssier","S. Toft","S. Tosi","A. Troja","M. Tucci","J. Valiviita","D. Vergani"],"raw_abstract":"We investigate the approximations needed to efficiently predict the\nlarge-scale clustering of matter and dark matter halos in beyond-$\\Lambda$CDM\nscenarios. We examine the normal branch of the Dvali-Gabadadze-Porrati model,\nthe Hu-Sawicki $f(R)$ model, a slowly evolving dark energy, an interacting dark\nenergy model and massive neutrinos. For each, we test approximations for the\nperturbative kernel calculations, including the omission of screening terms and\nthe use of perturbative kernels based on the Einstein-de Sitter universe; we\nexplore different infrared-resummation schemes, tracer bias models and a linear\ntreatment of massive neutrinos; we employ two models for redshift space\ndistortions, the Taruya-Nishimishi-Saito prescription and the Effective Field\nTheory of Large-Scale Structure. This work further provides a preliminary\nvalidation of the codes being considered by Euclid for the spectroscopic\nclustering probe in beyond-$\\Lambda$CDM scenarios. We calculate and compare the\n$\\chi^2$ statistic to assess the different modelling choices. This is done by\nfitting the spectroscopic clustering predictions to measurements from numerical\nsimulations and perturbation theory-based mock data. We compare the behaviour\nof this statistic in the beyond-$\\Lambda$CDM cases, as a function of the\nmaximum scale included in the fit, to the baseline $\\Lambda$CDM case. We find\nthat the Einstein-de Sitter approximation without screening is surprisingly\naccurate for all cases when comparing to the halo clustering monopole and\nquadrupole obtained from simulations. Our results suggest that the inclusion of\nmultiple redshift bins, higher-order multipoles, higher-order clustering\nstatistics (such as the bispectrum) and photometric probes such as weak\nlensing, will be essential to extract information on massive neutrinos,\nmodified gravity and dark energy.","publication_date":1700672627,"paper_link":"http://arxiv.org/pdf/2311.13529v1","categories":["Physics"],"abstract":"We investigate the approximations needed to efficiently predict the large-scale clustering of matter and dark matter halos in beyond-__FORMULA__CDM scenarios. We examine the normal branch of the Dvali-Gabadadze-Porrati model, the Hu-Sawicki __FORMULA__ model, a slowly evolving dark energy, an interacting dark energy model and massive neutrinos. For each, we test approximations for the perturbative kernel calculations, including the omission of screening terms and the use of perturbative kernels based on the Einstein-de Sitter universe; we explore different infrared-resummation schemes, tracer bias models and a linear treatment of massive neutrinos; we employ two models for redshift space distortions, the Taruya-Nishimishi-Saito prescription and the Effective Field Theory of Large-Scale Structure. This work further provides a preliminary validation of the codes being considered by Euclid for the spectroscopic clustering probe in beyond-__FORMULA__CDM scenarios. We calculate and compare the __FORMULA__ statistic to assess the different modelling choices. This is done by fitting the spectroscopic clustering predictions to measurements from numerical simulations and perturbation theory-based mock data. We compare the behaviour of this statistic in the beyond-__FORMULA__CDM cases, as a function of the maximum scale included in the fit, to the baseline __FORMULA__CDM case. We find that the Einstein-de Sitter approximation without screening is surprisingly accurate for all cases when comparing to the halo clustering monopole and quadrupole obtained from simulations. Our results suggest that the inclusion of multiple redshift bins, higher-order multipoles, higher-order clustering statistics (such as the bispectrum) and photometric probes such as weak lensing, will be essential to extract information on massive neutrinos, modified gravity and dark energy."}
{"title":"On factorization hierarchy of equations for banana Feynman amplitudes","authors":["V. Mishnyakov","A. Morozov","M. Reva"],"raw_abstract":"We present a review of the relations between various equations for maximal\ncut banana Feynman diagrams, i.e. amplitudes with propagators substituted with\n$\\delta$-functions. We consider both equal and generic masses. There are three\ntypes of equation to consider: those in coordinate space, their Fourier\ntransform and Picard-Fuchs equations originating from the parametric\nrepresentation. First, we review the properties of the corresponding\ndifferential operators themselves, mainly their factorization properties at the\nequal mass locus and their form at special values of the dimension. Then we\nstudy the relation between the Fourier transform of the coordinate space\nequations and the Picard-Fuchs equations and show that they are related by\nfactorization as well. The equations in question are the counterparts of the\nVirasoro constraints in the much-better studied theory of eigenvalue matrix\nmodels and are the first step towards building a full-fledged theory of Feynman\nintegrals, which will reveal their hidden integrable structure.","publication_date":1700672108,"paper_link":"http://arxiv.org/pdf/2311.13524v1","categories":["Mathematics","Physics"],"abstract":"We present a review of the relations between various equations for maximal cut banana Feynman diagrams, i.e. amplitudes with propagators substituted with __FORMULA__-functions. We consider both equal and generic masses. There are three types of equation to consider: those in coordinate space, their Fourier transform and Picard-Fuchs equations originating from the parametric representation. First, we review the properties of the corresponding differential operators themselves, mainly their factorization properties at the equal mass locus and their form at special values of the dimension. Then we study the relation between the Fourier transform of the coordinate space equations and the Picard-Fuchs equations and show that they are related by factorization as well. The equations in question are the counterparts of the Virasoro constraints in the much-better studied theory of eigenvalue matrix models and are the first step towards building a full-fledged theory of Feynman integrals, which will reveal their hidden integrable structure."}
{"title":"Outerplanar and Forest Storyplans","authors":["Ji\u0159\u00ed Fiala","Oksana Firman","Giuseppe Liotta","Alexander Wolff","Johannes Zink"],"raw_abstract":"We study the problem of gradually representing a complex graph as a sequence\nof drawings of small subgraphs whose union is the complex graph. The sequence\nof drawings is called \\emph{storyplan}, and each drawing in the sequence is\ncalled a \\emph{frame}. In an outerplanar storyplan, every frame is outerplanar;\nin a forest storyplan, every frame is acyclic. We identify graph families that\nadmit such storyplans and families for which such storyplans do not always\nexist. In the affirmative case, we present efficient algorithms that produce\nstraight-line storyplans.","publication_date":1700672004,"paper_link":"http://arxiv.org/pdf/2311.13523v1","categories":["Mathematics"],"abstract":"We study the problem of gradually representing a complex graph as a sequence of drawings of small subgraphs whose union is the complex graph. The sequence of drawings is called storyplan, and each drawing in the sequence is called a frame. In an outerplanar storyplan, every frame is outerplanar; in a forest storyplan, every frame is acyclic. We identify graph families that admit such storyplans and families for which such storyplans do not always exist. In the affirmative case, we present efficient algorithms that produce straight-line storyplans."}
{"title":"Projective toric designs, difference sets, and quantum state designs","authors":["Joseph T. Iosue","T. C. Mooney","Adam Ehrenberg","Alexey V. Gorshkov"],"raw_abstract":"Trigonometric cubature rules of degree $t$ are sets of points on the torus\nover which sums reproduce integrals of degree $t$ monomials over the full\ntorus. They can be thought of as $t$-designs on the torus. Motivated by the\nprojective structure of quantum mechanics, we develop the notion of $t$-designs\non the projective torus, which, surprisingly, have a much more restricted\nstructure than their counterparts on full tori. We provide various\nconstructions of these projective toric designs and prove some bounds on their\nsize and characterizations of their structure. We draw connections between\nprojective toric designs and a diverse set of mathematical objects, including\ndifference and Sidon sets from the field of additive combinatorics, symmetric,\ninformationally complete positive operator valued measures (SIC-POVMs) and\ncomplete sets of mutually unbiased bases (MUBs) (which are conjectured to\nrelate to finite projective geometry) from quantum information theory, and\ncrystal ball sequences of certain root lattices. Using these connections, we\nprove bounds on the maximal size of dense $B_t \\bmod m$ sets. We also use\nprojective toric designs to construct families of quantum state designs.\nFinally, we discuss many open questions about the properties of these\nprojective toric designs and how they relate to other questions in number\ntheory, geometry, and quantum information.","publication_date":1700668119,"paper_link":"http://arxiv.org/pdf/2311.13479v1","categories":["Mathematics","Physics"],"abstract":"Trigonometric cubature rules of degree __FORMULA__ are sets of points on the torus over which sums reproduce integrals of degree __FORMULA__ monomials over the full torus. They can be thought of as __FORMULA__-designs on the torus. Motivated by the projective structure of quantum mechanics, we develop the notion of __FORMULA__-designs on the projective torus, which, surprisingly, have a much more restricted structure than their counterparts on full tori. We provide various constructions of these projective toric designs and prove some bounds on their size and characterizations of their structure. We draw connections between projective toric designs and a diverse set of mathematical objects, including difference and Sidon sets from the field of additive combinatorics, symmetric, informationally complete positive operator valued measures (SIC-POVMs) and complete sets of mutually unbiased bases (MUBs) (which are conjectured to relate to finite projective geometry) from quantum information theory, and crystal ball sequences of certain root lattices. Using these connections, we prove bounds on the maximal size of dense __FORMULA__ sets. We also use projective toric designs to construct families of quantum state designs. Finally, we discuss many open questions about the properties of these projective toric designs and how they relate to other questions in number theory, geometry, and quantum information."}
{"title":"Solution discovery via reconfiguration for problems in P","authors":["Mario Grobler","Stephanie Maaz","Nicole Megow","Amer E. Mouawad","Vijayaragunathan Ramamoorthi","Daniel Schmand","Sebastian Siebertz"],"raw_abstract":"In the recently introduced framework of solution discovery via\nreconfiguration [Fellows et al., ECAI 2023], we are given an initial\nconfiguration of $k$ tokens on a graph and the question is whether we can\ntransform this configuration into a feasible solution (for some problem) via a\nbounded number $b$ of small modification steps. In this work, we study solution\ndiscovery variants of polynomial-time solvable problems, namely Spanning Tree\nDiscovery, Shortest Path Discovery, Matching Discovery, and Vertex/Edge Cut\nDiscovery in the unrestricted token addition/removal model, the token jumping\nmodel, and the token sliding model. In the unrestricted token addition/removal\nmodel, we show that all four discovery variants remain in P. For the toking\njumping model we also prove containment in P, except for Vertex/Edge Cut\nDiscovery, for which we prove NP-completeness. Finally, in the token sliding\nmodel, almost all considered problems become NP-complete, the exception being\nSpanning Tree Discovery, which remains polynomial-time solvable. We then study\nthe parameterized complexity of the NP-complete problems and provide a full\nclassification of tractability with respect to the parameters solution size\n(number of tokens) $k$ and transformation budget (number of steps) $b$. Along\nthe way, we observe strong connections between the solution discovery variants\nof our base problems and their (weighted) rainbow variants as well as their\nred-blue variants with cardinality constraints.","publication_date":1700668099,"paper_link":"http://arxiv.org/pdf/2311.13478v1","categories":["Mathematics"],"abstract":"In the recently introduced framework of solution discovery via reconfiguration [Fellows et al., ECAI 2023], we are given an initial configuration of __FORMULA__ tokens on a graph and the question is whether we can transform this configuration into a feasible solution (for some problem) via a bounded number __FORMULA__ of small modification steps. In this work, we study solution discovery variants of polynomial-time solvable problems, namely Spanning Tree Discovery, Shortest Path Discovery, Matching Discovery, and Vertex/Edge Cut Discovery in the unrestricted token addition/removal model, the token jumping model, and the token sliding model. In the unrestricted token addition/removal model, we show that all four discovery variants remain in P. For the toking jumping model we also prove containment in P, except for Vertex/Edge Cut Discovery, for which we prove NP-completeness. Finally, in the token sliding model, almost all considered problems become NP-complete, the exception being Spanning Tree Discovery, which remains polynomial-time solvable. We then study the parameterized complexity of the NP-complete problems and provide a full classification of tractability with respect to the parameters solution size (number of tokens) __FORMULA__ and transformation budget (number of steps) __FORMULA__. Along the way, we observe strong connections between the solution discovery variants of our base problems and their (weighted) rainbow variants as well as their red-blue variants with cardinality constraints."}
{"title":"Comments on mathematical aspects of the Bir\u00f3-N\u00e9da model","authors":["Ilda In\u00e1cio","Jos\u00e9 Velhinho"],"raw_abstract":"We address two mathematical aspects of the Bir\\'o--N\\'eda dynamical model,\nrecently applied in the statistical analysis of several and varied complex\nphenomena. First, we show that a given implicit assumption ceases to be valid\noutside the most simple and common cases, and we analyze the consequences\nthereof, in what the formulation of the model and probability conservation is\nconcerned. Second, we revisit the transient behavior in the case of a constant\nreset rate and a constant or linear growth rate, improving on a previous\nanalysis by including more general initial conditions.","publication_date":1700666097,"paper_link":"http://arxiv.org/pdf/2311.13449v1","categories":["Mathematics","Physics"],"abstract":"We address two mathematical aspects of the Bir\\'o--N\\'eda dynamical model, recently applied in the statistical analysis of several and varied complex phenomena. First, we show that a given implicit assumption ceases to be valid outside the most simple and common cases, and we analyze the consequences thereof, in what the formulation of the model and probability conservation is concerned. Second, we revisit the transient behavior in the case of a constant reset rate and a constant or linear growth rate, improving on a previous analysis by including more general initial conditions."}
{"title":"State Diagrams to determine Tree Tensor Network Operators","authors":["Richard M. Milbradt","Qunsheng Huang","Christian B. Mendl"],"raw_abstract":"This work is concerned with tree tensor network operators (TTNOs) for\nrepresenting quantum Hamiltonians. We first establish a mathematical framework\nconnecting tree topologies with state diagrams. Based on these, we devise an\nalgorithm for constructing a TTNO given a Hamiltonian. The algorithm exploits\nthe tensor product structure of the Hamiltonian to add paths to a state\ndiagram, while combining local operators if possible. We test the capabilities\nof our algorithm on random Hamiltonians for a given tree structure.\nAdditionally, we construct explicit TTNOs for nearest neighbour interactions on\na tree topology. Furthermore, we derive a bound on the bond dimension of tensor\noperators representing arbitrary interactions on trees. Finally, we consider an\nopen quantum system in the form of a Heisenberg spin chain coupled to bosonic\nbath sites as a concrete example. We find that tree structures allow for lower\nbond dimensions of the Hamiltonian tensor network representation compared to a\nmatrix product operator structure. This reduction is large enough to reduce the\nnumber of total tensor elements required as soon as the number of baths per\nspin reaches $3$.","publication_date":1700664446,"paper_link":"http://arxiv.org/pdf/2311.13433v1","categories":["Physics"],"abstract":"This work is concerned with tree tensor network operators (TTNOs) for representing quantum Hamiltonians. We first establish a mathematical framework connecting tree topologies with state diagrams. Based on these, we devise an algorithm for constructing a TTNO given a Hamiltonian. The algorithm exploits the tensor product structure of the Hamiltonian to add paths to a state diagram, while combining local operators if possible. We test the capabilities of our algorithm on random Hamiltonians for a given tree structure. Additionally, we construct explicit TTNOs for nearest neighbour interactions on a tree topology. Furthermore, we derive a bound on the bond dimension of tensor operators representing arbitrary interactions on trees. Finally, we consider an open quantum system in the form of a Heisenberg spin chain coupled to bosonic bath sites as a concrete example. We find that tree structures allow for lower bond dimensions of the Hamiltonian tensor network representation compared to a matrix product operator structure. This reduction is large enough to reduce the number of total tensor elements required as soon as the number of baths per spin reaches __FORMULA__."}
{"title":"Simultaneous uniqueness and numerical inversion for an inverse problem in the time-domain diffuse optical tomography with fluorescence","authors":["Zhiyuan Li","Chunlong Sun"],"raw_abstract":"In this work, an inverse problem on the determination of multiple\ncoefficients arising from the time-domain diffuse optical tomography with\nfluorescence (DOT-FDOT) is investigated. We simultaneously recover the\ndistribution of background absorption coefficient, photon diffusion coefficient\nas well as the fluorescence absorption in biological tissue by the\ntime-dependent boundary measurements. We build the uniqueness theorem of this\nmultiple coefficients simultaneous inverse problem. After that, the numerical\ninversions are considered. We introduce an accelerated Landweber iterative\nalgorithm and give several numerical examples illustrating the performance of\nthe proposed inversion schemes.","publication_date":1700660337,"paper_link":"http://arxiv.org/pdf/2311.13391v1","categories":["Mathematics","Physics"],"abstract":"In this work, an inverse problem on the determination of multiple coefficients arising from the time-domain diffuse optical tomography with fluorescence (DOT-FDOT) is investigated. We simultaneously recover the distribution of background absorption coefficient, photon diffusion coefficient as well as the fluorescence absorption in biological tissue by the time-dependent boundary measurements. We build the uniqueness theorem of this multiple coefficients simultaneous inverse problem. After that, the numerical inversions are considered. We introduce an accelerated Landweber iterative algorithm and give several numerical examples illustrating the performance of the proposed inversion schemes."}
{"title":"A highly efficient finite volume method with a diffusion control parameter for hyperbolic problems","authors":["Wassim Aboussi","Moussa Ziggaf","Imad Kissami","Mohamed Boubekeur"],"raw_abstract":"This article proposes a highly accurate and conservative method for\nhyperbolic systems using the finite volume approach. This innovative scheme\nconstructs the intermediate states at the interfaces of the control volumes\nusing the method of characteristics. The approach is simple to implement,\ngenerates entropic solutions, and avoids solving Riemann problems. A diffusion\ncontrol parameter is introduced to increase the accuracy of the scheme.\nNumerical examples are presented for the Euler equation for an ideal gas. The\nresults demonstrate the method's ability to capture contact discontinuity and\nshock wave profiles with high accuracy and low cost as well as its robustness.","publication_date":1700655413,"paper_link":"http://arxiv.org/pdf/2311.13344v1","categories":["Mathematics"],"abstract":"This article proposes a highly accurate and conservative method for hyperbolic systems using the finite volume approach. This innovative scheme constructs the intermediate states at the interfaces of the control volumes using the method of characteristics. The approach is simple to implement, generates entropic solutions, and avoids solving Riemann problems. A diffusion control parameter is introduced to increase the accuracy of the scheme. Numerical examples are presented for the Euler equation for an ideal gas. The results demonstrate the method's ability to capture contact discontinuity and shock wave profiles with high accuracy and low cost as well as its robustness."}
{"title":"Learning principle and mathematical realization of the learning mechanism in the brain","authors":["Taisuke Katayose"],"raw_abstract":"While deep learning has achieved remarkable success, there is no clear\nexplanation about why it works so well. In order to discuss this question\nquantitatively, we need a mathematical framework that explains what learning is\nin the first place. After several considerations, we succeeded in constructing\na mathematical framework that can provide a unified understanding of all types\nof learning, including deep learning and learning in the brain. We call it\nlearning principle, and it follows that all learning is equivalent to\nestimating the probability of input data. We not only derived this principle,\nbut also mentioned its application to actual machine learning models. For\nexample, we found that conventional supervised learning is equivalent to\nestimating conditional probabilities, and succeeded in making supervised\nlearning more effective and generalized. We also proposed a new method of\ndefining the values of estimated probability using differentiation, and showed\nthat unsupervised learning can be performed on arbitrary dataset without any\nprior knowledge. Namely, this method is a general-purpose machine learning in\nthe true sense. Moreover, we succeeded in describing the learning mechanism in\nthe brain by considering the time evolution of a fully or partially connected\nmodel and applying this new method. The learning principle provides solutions\nto many unsolved problems in deep learning and cognitive neuroscience.","publication_date":1700654881,"paper_link":"http://arxiv.org/pdf/2311.13341v1","categories":["Mathematics","Quantitative Biology","Statistics"],"abstract":"While deep learning has achieved remarkable success, there is no clear explanation about why it works so well. In order to discuss this question quantitatively, we need a mathematical framework that explains what learning is in the first place. After several considerations, we succeeded in constructing a mathematical framework that can provide a unified understanding of all types of learning, including deep learning and learning in the brain. We call it learning principle, and it follows that all learning is equivalent to estimating the probability of input data. We not only derived this principle, but also mentioned its application to actual machine learning models. For example, we found that conventional supervised learning is equivalent to estimating conditional probabilities, and succeeded in making supervised learning more effective and generalized. We also proposed a new method of defining the values of estimated probability using differentiation, and showed that unsupervised learning can be performed on arbitrary dataset without any prior knowledge. Namely, this method is a general-purpose machine learning in the true sense. Moreover, we succeeded in describing the learning mechanism in the brain by considering the time evolution of a fully or partially connected model and applying this new method. The learning principle provides solutions to many unsolved problems in deep learning and cognitive neuroscience."}
{"title":"Bounds on spectral gaps of Hyperbolic spin surfaces","authors":["Elliott Gesteau","Sridip Pal","David Simmons-Duffin","Yixin Xu"],"raw_abstract":"We describe a method for constraining Laplacian and Dirac spectra of two\ndimensional compact orientable hyperbolic spin manifolds and orbifolds. The key\ningredient is an infinite family of identities satisfied by the spectra. These\nspectral identities follow from the consistency between 1) the spectral\ndecomposition of functions on the spin bundle into irreducible representations\nof $\\mathrm{SL}(2,\\mathbb{R})$ and 2) associativity of pointwise multiplication\nof functions. Applying semidefinite programming methods to our identities\nproduces rigorous upper bounds on the Laplacian spectral gap as well as on the\nDirac spectral gap conditioned on the former. In several examples, our bounds\nare nearly sharp; a numerical algorithm based on the Selberg trace formula\nshows that the $[0;3,3,5]$ orbifold, a particular surface with signature\n$[1;3]$, and the Bolza surface nearly saturate the bounds at genus $0$, $1$ and\n$2$ respectively. Under additional assumptions on the number of harmonic\nspinors carried by the spin-surface, we obtain more restrictive bounds on the\nLaplacian spectral gap. In particular, these bounds apply to hyperelliptic\nsurfaces. We also determine the set of Laplacian spectral gaps attained by all\ncompact orientable two-dimensional hyperbolic spin orbifolds. We show that this\nset is upper bounded by $12.13798$; this bound is nearly saturated by the\n$[0;3,3,5]$ orbifold, whose first non-zero Laplacian eigenvalue is\n$\\lambda^{(0)}_1\\approx 12.13623$.","publication_date":1700653892,"paper_link":"http://arxiv.org/pdf/2311.13330v1","categories":["Mathematics","Physics"],"abstract":"We describe a method for constraining Laplacian and Dirac spectra of two dimensional compact orientable hyperbolic spin manifolds and orbifolds. The key ingredient is an infinite family of identities satisfied by the spectra. These spectral identities follow from the consistency between 1) the spectral decomposition of functions on the spin bundle into irreducible representations of __FORMULA__ and 2) associativity of pointwise multiplication of functions. Applying semidefinite programming methods to our identities produces rigorous upper bounds on the Laplacian spectral gap as well as on the Dirac spectral gap conditioned on the former. In several examples, our bounds are nearly sharp; a numerical algorithm based on the Selberg trace formula shows that the __FORMULA__ orbifold, a particular surface with signature __FORMULA__, and the Bolza surface nearly saturate the bounds at genus __FORMULA__, __FORMULA__ and __FORMULA__ respectively. Under additional assumptions on the number of harmonic spinors carried by the spin-surface, we obtain more restrictive bounds on the Laplacian spectral gap. In particular, these bounds apply to hyperelliptic surfaces. We also determine the set of Laplacian spectral gaps attained by all compact orientable two-dimensional hyperbolic spin orbifolds. We show that this set is upper bounded by __FORMULA__; this bound is nearly saturated by the __FORMULA__ orbifold, whose first non-zero Laplacian eigenvalue is __FORMULA__."}
{"title":"An $hp$-adaptive strategy based on locally predicted error reductions","authors":["Patrick Bammer","Andreas Schr\u00f6der","Thomas P. Wihler"],"raw_abstract":"We introduce a new $hp$-adaptive strategy for self-adjoint elliptic boundary\nvalue problems that does not rely on using classical a posteriori error\nestimators. Instead, our approach is based on a generally applicable prediction\nstrategy for the reduction of the energy error that can be expressed in terms\nof local modifications of the degrees of freedom in the underlying discrete\napproximation space. The computations related to the proposed prediction\nstrategy involve low-dimensional linear problems that are computationally\ninexpensive and highly parallelizable. The mathematical building blocks for\nthis new concept are first developed on an abstract Hilbert space level, before\nthey are employed within the specific context of $hp$-type finite element\ndiscretizations. For this particular framework, we discuss an explicit\nconstruction of $p$-enrichments and $hp$-refinements by means of an appropriate\nconstraint coefficient technique that can be employed in any dimensions. The\napplicability and effectiveness of the resulting $hp$-adaptive strategy is\nillustrated with some $1$- and $2$-dimensional numerical examples.","publication_date":1700644822,"paper_link":"http://arxiv.org/pdf/2311.13255v1","categories":["Mathematics"],"abstract":"We introduce a new __FORMULA__-adaptive strategy for self-adjoint elliptic boundary value problems that does not rely on using classical a posteriori error estimators. Instead, our approach is based on a generally applicable prediction strategy for the reduction of the energy error that can be expressed in terms of local modifications of the degrees of freedom in the underlying discrete approximation space. The computations related to the proposed prediction strategy involve low-dimensional linear problems that are computationally inexpensive and highly parallelizable. The mathematical building blocks for this new concept are first developed on an abstract Hilbert space level, before they are employed within the specific context of __FORMULA__-type finite element discretizations. For this particular framework, we discuss an explicit construction of __FORMULA__-enrichments and __FORMULA__-refinements by means of an appropriate constraint coefficient technique that can be employed in any dimensions. The applicability and effectiveness of the resulting __FORMULA__-adaptive strategy is illustrated with some __FORMULA__- and __FORMULA__-dimensional numerical examples."}
{"title":"The Bonahon-Wong-Yang volume conjecture for the four-puncture sphere","authors":["Tushar Pandey"],"raw_abstract":"We compute the Bonahon-Wong-Yang quantum invariant for self-diffeomorphisms\nof the four-puncture sphere explicitly, based on the representation theory of\nthe Checkov-Fock algebra. As an application of the computation, we verify the\nvolume conjecture proposed by Bonahon-Wong-Yang for the four puncture sphere\nbundles with some technical conditions. Together with the one-puncture torus\nbundles, this is the first family of examples for which the conjecture is\nverified providing evidence for the conjecture","publication_date":1700627171,"paper_link":"http://arxiv.org/pdf/2311.13151v1","categories":["Mathematics","Physics"],"abstract":"We compute the Bonahon-Wong-Yang quantum invariant for self-diffeomorphisms of the four-puncture sphere explicitly, based on the representation theory of the Checkov-Fock algebra. As an application of the computation, we verify the volume conjecture proposed by Bonahon-Wong-Yang for the four puncture sphere bundles with some technical conditions. Together with the one-puncture torus bundles, this is the first family of examples for which the conjecture is verified providing evidence for the conjecture"}
{"title":"Height of walks with resets, the Moran model, and the discrete Gumbel distribution","authors":["Rafik Aguech","Asma Althagafi","Cyril Banderier"],"raw_abstract":"In this article, we consider several models of random walks in one or several\ndimensions, additionally allowing, at any unit of time, a reset (or\n\"catastrophe\") of the walk with probability $q$. We establish the distribution\nof the final altitude. We prove algebraicity of the generating functions of\nwalks of bounded height $h$ (showing in passing the equivalence between\nLagrange interpolation and the kernel method). To get these generating\nfunctions, our approach offers an algorithm of cost $O(1)$, instead of cost\n$O(h^3)$ if a Markov chain approach would be used. The simplest nontrivial\nmodel corresponds to famous dynamics in population genetics: the Moran model.\n  We prove that the height of these Moran walks asymptotically follows a\ndiscrete Gumbel distribution. For $q=1/2$, this generalizes a model of carry\npropagation over binary numbers considered e.g. by von Neumann and Knuth. For\ngeneric $q$, using a Mellin transform approach, we show that the asymptotic\nheight exhibits fluctuations for which we get an explicit description (and, in\npassing, new bounds for the digamma function). We end by showing how to solve\nmultidimensional generalizations of these walks (where any subset of particles\nis attributed a different probability of dying) and we give an application to\nthe soliton wave model.","publication_date":1700622551,"paper_link":"http://arxiv.org/pdf/2311.13124v1","categories":["Mathematics"],"abstract":"In this article, we consider several models of random walks in one or several dimensions, additionally allowing, at any unit of time, a reset (or \"catastrophe\") of the walk with probability __FORMULA__. We establish the distribution of the final altitude. We prove algebraicity of the generating functions of walks of bounded height __FORMULA__ (showing in passing the equivalence between Lagrange interpolation and the kernel method). To get these generating functions, our approach offers an algorithm of cost __FORMULA__, instead of cost __FORMULA__ if a Markov chain approach would be used. The simplest nontrivial model corresponds to famous dynamics in population genetics: the Moran model.   We prove that the height of these Moran walks asymptotically follows a discrete Gumbel distribution. For __FORMULA__, this generalizes a model of carry propagation over binary numbers considered e.g. by von Neumann and Knuth. For generic __FORMULA__, using a Mellin transform approach, we show that the asymptotic height exhibits fluctuations for which we get an explicit description (and, in passing, new bounds for the digamma function). We end by showing how to solve multidimensional generalizations of these walks (where any subset of particles is attributed a different probability of dying) and we give an application to the soliton wave model."}
{"title":"White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?","authors":["Yaodong Yu","Sam Buchanan","Druv Pai","Tianzhe Chu","Ziyang Wu","Shengbang Tong","Hao Bai","Yuexiang Zhai","Benjamin D. Haeffele","Yi Ma"],"raw_abstract":"In this paper, we contend that a natural objective of representation learning\nis to compress and transform the distribution of the data, say sets of tokens,\ntowards a low-dimensional Gaussian mixture supported on incoherent subspaces.\nThe goodness of such a representation can be evaluated by a principled measure,\ncalled sparse rate reduction, that simultaneously maximizes the intrinsic\ninformation gain and extrinsic sparsity of the learned representation. From\nthis perspective, popular deep network architectures, including transformers,\ncan be viewed as realizing iterative schemes to optimize this measure.\nParticularly, we derive a transformer block from alternating optimization on\nparts of this objective: the multi-head self-attention operator compresses the\nrepresentation by implementing an approximate gradient descent step on the\ncoding rate of the features, and the subsequent multi-layer perceptron\nsparsifies the features. This leads to a family of white-box transformer-like\ndeep network architectures, named CRATE, which are mathematically fully\ninterpretable. We show, by way of a novel connection between denoising and\ncompression, that the inverse to the aforementioned compressive encoding can be\nrealized by the same class of CRATE architectures. Thus, the so-derived\nwhite-box architectures are universal to both encoders and decoders.\nExperiments show that these networks, despite their simplicity, indeed learn to\ncompress and sparsify representations of large-scale real-world image and text\ndatasets, and achieve performance very close to highly engineered\ntransformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the\nproposed computational framework demonstrates great potential in bridging the\ngap between theory and practice of deep learning, from a unified perspective of\ndata compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .","publication_date":1700619812,"paper_link":"http://arxiv.org/pdf/2311.13110v1","categories":["Mathematics"],"abstract":"In this paper, we contend that a natural objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a low-dimensional Gaussian mixture supported on incoherent subspaces. The goodness of such a representation can be evaluated by a principled measure, called sparse rate reduction, that simultaneously maximizes the intrinsic information gain and extrinsic sparsity of the learned representation. From this perspective, popular deep network architectures, including transformers, can be viewed as realizing iterative schemes to optimize this measure. Particularly, we derive a transformer block from alternating optimization on parts of this objective: the multi-head self-attention operator compresses the representation by implementing an approximate gradient descent step on the coding rate of the features, and the subsequent multi-layer perceptron sparsifies the features. This leads to a family of white-box transformer-like deep network architectures, named CRATE, which are mathematically fully interpretable. We show, by way of a novel connection between denoising and compression, that the inverse to the aforementioned compressive encoding can be realized by the same class of CRATE architectures. Thus, the so-derived white-box architectures are universal to both encoders and decoders. Experiments show that these networks, despite their simplicity, indeed learn to compress and sparsify representations of large-scale real-world image and text datasets, and achieve performance very close to highly engineered transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the proposed computational framework demonstrates great potential in bridging the gap between theory and practice of deep learning, from a unified perspective of data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE ."}
{"title":"Gauged compact Q-balls and Q-shells in a multi-component $CP^N$ model","authors":["P. Klimas","L. C. Kubaski","N. Sawado","S. Yanai"],"raw_abstract":"We study a multicomponent $CP^N$ model's scalar electrodynamics. The model\ncontains Q-balls/shells, which are non-topological compact solitons with time\ndependency $e^{i\\omega t}$. Two coupled $CP^N$ models can decouple locally if\none of their $CP^N$ fields takes the vacuum value. Because of the compacton\nnature of solutions, Q-shells can shelter another compact Q-ball or Q-shell\nwithin their hollow region. Even if compactons do not overlap, they can\ninteract through the electromagnetic field. We investigate how the size of\nmulti-compacton formations is affected by electric charge. We are interested in\nstructures with non-zero or zero total net charge.","publication_date":1700613527,"paper_link":"http://arxiv.org/pdf/2311.13076v1","categories":["Mathematics","Physics"],"abstract":"We study a multicomponent __FORMULA__ model's scalar electrodynamics. The model contains Q-balls/shells, which are non-topological compact solitons with time dependency __FORMULA__. Two coupled __FORMULA__ models can decouple locally if one of their __FORMULA__ fields takes the vacuum value. Because of the compacton nature of solutions, Q-shells can shelter another compact Q-ball or Q-shell within their hollow region. Even if compactons do not overlap, they can interact through the electromagnetic field. We investigate how the size of multi-compacton formations is affected by electric charge. We are interested in structures with non-zero or zero total net charge."}
{"title":"Multi-fidelity Bayesian Optimization in Engineering Design","authors":["Bach Do","Ruda Zhang"],"raw_abstract":"Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian\noptimization (BO), MF BO has found a niche in solving expensive engineering\ndesign optimization problems, thanks to its advantages in incorporating\nphysical and mathematical understandings of the problems, saving resources,\naddressing exploitation-exploration trade-off, considering uncertainty, and\nprocessing parallel computing. The increasing number of works dedicated to MF\nBO suggests the need for a comprehensive review of this advanced optimization\ntechnique. In this paper, we survey recent developments of two essential\ningredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition\nfunctions. We first categorize the existing MF modeling methods and MFO\nstrategies to locate MF BO in a large family of surrogate-based optimization\nand MFO algorithms. We then exploit the common properties shared between the\nmethods from each ingredient of MF BO to describe important GP-based MF\nsurrogate models and review various acquisition functions. By doing so, we\nexpect to provide a structured understanding of MF BO. Finally, we attempt to\nreveal important aspects that require further research for applications of MF\nBO in solving intricate yet important design optimization problems, including\nconstrained optimization, high-dimensional optimization, optimization under\nuncertainty, and multi-objective optimization.","publication_date":1700608931,"paper_link":"http://arxiv.org/pdf/2311.13050v1","categories":["Mathematics","Statistics"],"abstract":"Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian optimization (BO), MF BO has found a niche in solving expensive engineering design optimization problems, thanks to its advantages in incorporating physical and mathematical understandings of the problems, saving resources, addressing exploitation-exploration trade-off, considering uncertainty, and processing parallel computing. The increasing number of works dedicated to MF BO suggests the need for a comprehensive review of this advanced optimization technique. In this paper, we survey recent developments of two essential ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition functions. We first categorize the existing MF modeling methods and MFO strategies to locate MF BO in a large family of surrogate-based optimization and MFO algorithms. We then exploit the common properties shared between the methods from each ingredient of MF BO to describe important GP-based MF surrogate models and review various acquisition functions. By doing so, we expect to provide a structured understanding of MF BO. Finally, we attempt to reveal important aspects that require further research for applications of MF BO in solving intricate yet important design optimization problems, including constrained optimization, high-dimensional optimization, optimization under uncertainty, and multi-objective optimization."}
{"title":"The Penrose Tiling is a Quantum Error-Correcting Code","authors":["Zhi Li","Latham Boyle"],"raw_abstract":"The Penrose tiling (PT) is an intrinsically non-periodic way of tiling the\nplane, with many remarkable properties. A quantum error-correcting code (QECC)\nis a clever way of protecting quantum information from noise, by encoding the\ninformation with a sophisticated type of redundancy. Although PTs and QECCs\nmight seem completely unrelated, in this paper we point out that PTs give rise\nto (or, in a sense, are) a remarkable new type of QECC in which any local\nerrors or erasures in any finite region, no matter how large, may be diagnosed\nand corrected. We also construct variants of this code (based on the\nAmmann-Beenker and Fibonacci tilings) that can live on finite spatial tori, in\ndiscrete spin systems, or in an arbitrary number of spatial dimensions. We\ndiscuss connections to quantum computing, condensed matter physics, and quantum\ngravity.","publication_date":1700607822,"paper_link":"http://arxiv.org/pdf/2311.13040v1","categories":["Mathematics","Physics"],"abstract":"The Penrose tiling (PT) is an intrinsically non-periodic way of tiling the plane, with many remarkable properties. A quantum error-correcting code (QECC) is a clever way of protecting quantum information from noise, by encoding the information with a sophisticated type of redundancy. Although PTs and QECCs might seem completely unrelated, in this paper we point out that PTs give rise to (or, in a sense, are) a remarkable new type of QECC in which any local errors or erasures in any finite region, no matter how large, may be diagnosed and corrected. We also construct variants of this code (based on the Ammann-Beenker and Fibonacci tilings) that can live on finite spatial tori, in discrete spin systems, or in an arbitrary number of spatial dimensions. We discuss connections to quantum computing, condensed matter physics, and quantum gravity."}
{"title":"Critical exponent of binary words with few distinct palindromes","authors":["L'ubom\u00edra Dvo\u0159\u00e1kov\u00e1","Pascal Ochem","Daniela Opo\u010densk\u00e1"],"raw_abstract":"We study infinite binary words that contain few distinct palindromes. In\nparticular, we classify such words according to their critical exponents. This\nextends results by Fici and Zamboni [TCS 2013]. Interestingly, the words with\n18 and 20 palindromes happen to be morphic images of the fixed point of the\nmorphism $\\texttt{0}\\mapsto\\texttt{01}$, $\\texttt{1}\\mapsto\\texttt{21}$,\n$\\texttt{2}\\mapsto\\texttt{0}$.","publication_date":1700602070,"paper_link":"http://arxiv.org/pdf/2311.13003v1","categories":["Mathematics"],"abstract":"We study infinite binary words that contain few distinct palindromes. In particular, we classify such words according to their critical exponents. This extends results by Fici and Zamboni [TCS 2013]. Interestingly, the words with 18 and 20 palindromes happen to be morphic images of the fixed point of the morphism __FORMULA__, __FORMULA__, __FORMULA__."}
{"title":"Information Diffusion, Word-of-mouth effects, and Mutual Funds Performance: A Mathematical Modelling Approach","authors":["Leonard Mushunje"],"raw_abstract":"This paper puts forward that money managers and prospective investors\nexchange information and ideas about assets directly through word-of-mouth\ncommunication. This, in turn, affects the performance of the mutual funds and\ntheir respective managers. We develop a novel \"Epi-Finance\" model that connects\nepidemiology and finance to provide a solid explanation behind information\ntransmission in the fund industry. By considering factors such as location,\nrace, gender, and education concerning the equity market, several claims and\nconnections are observed and modeled using an epidemic model. For example,\nbased on location, a mutual fund manager is likelier to buy (or sell) a\nparticular equity at any time if other managers in the\n{\\epsilon}-neighborhoods({\\epsilon}>0) are buying (or selling) that same\nequity. If the information about that particular equity spreads fast and the\ncommunication network is well connected, the same pattern shows up even when\nthe fund managers are far apart. Also, a male fund manager is likelier to\nrelate his investment style to other male managers, which is true otherwise.\nThe same fashionable results are expected on race education, among others with\nspecial contextual differences.","publication_date":1700599057,"paper_link":"http://arxiv.org/pdf/2311.12984v1","categories":["Mathematics"],"abstract":"This paper puts forward that money managers and prospective investors exchange information and ideas about assets directly through word-of-mouth communication. This, in turn, affects the performance of the mutual funds and their respective managers. We develop a novel \"Epi-Finance\" model that connects epidemiology and finance to provide a solid explanation behind information transmission in the fund industry. By considering factors such as location, race, gender, and education concerning the equity market, several claims and connections are observed and modeled using an epidemic model. For example, based on location, a mutual fund manager is likelier to buy (or sell) a particular equity at any time if other managers in the {\\epsilon}-neighborhoods({\\epsilon}>0) are buying (or selling) that same equity. If the information about that particular equity spreads fast and the communication network is well connected, the same pattern shows up even when the fund managers are far apart. Also, a male fund manager is likelier to relate his investment style to other male managers, which is true otherwise. The same fashionable results are expected on race education, among others with special contextual differences."}
{"title":"A Fermionic Grunsky operator","authors":["Peter Kristel","Eric Schippers","Wolfgang Staubach"],"raw_abstract":"To a conformal map $f$ from the disk $\\mathbb{D}$ into the complex plane onto\na domain with rectifiable Ahlfors-regular boundary, we associate a new kind of\nGrunsky operator on the Hardy space of the unit disk. This is analogous to the\nclassical Grunsky operator, which itself can be viewed as an operator on\nBergman or Dirichlet space. We show that the pull-back of the Smirnov space of\nthe complement of $f(\\mathbb{D})$ by $f$ is the graph of the Grunsky operator.\nWe also characterize those domains with rectifiable Ahlfors-regular boundaries\nsuch that the Grunsky operator is Hilbert-Schmidt. In particular, we show that\nif the Grunsky operator is Hilbert-Schmidt, then $f(\\mathbb{D})$ is a\nWeil-Petersson quasidisk. The formulations of the results and proofs make\nessential use of a geometric treatment of Smirnov space as a space of\nhalf-order differentials.","publication_date":1700598136,"paper_link":"http://arxiv.org/pdf/2311.12972v1","categories":["Mathematics","Physics"],"abstract":"To a conformal map __FORMULA__ from the disk __FORMULA__ into the complex plane onto a domain with rectifiable Ahlfors-regular boundary, we associate a new kind of Grunsky operator on the Hardy space of the unit disk. This is analogous to the classical Grunsky operator, which itself can be viewed as an operator on Bergman or Dirichlet space. We show that the pull-back of the Smirnov space of the complement of __FORMULA__ by __FORMULA__ is the graph of the Grunsky operator. We also characterize those domains with rectifiable Ahlfors-regular boundaries such that the Grunsky operator is Hilbert-Schmidt. In particular, we show that if the Grunsky operator is Hilbert-Schmidt, then __FORMULA__ is a Weil-Petersson quasidisk. The formulations of the results and proofs make essential use of a geometric treatment of Smirnov space as a space of half-order differentials."}
{"title":"A Cosheaf Theory of Reciprocal Figures: Planar and Higher Genus Graphic Statics","authors":["Zoe Cooperband","Robert Ghrist","Jakob Hansen"],"raw_abstract":"This paper introduces cellular sheaf theory to graphical methods and\nreciprocal constructions in structural engineering. The elementary mechanics\nand statics of trusses are derived from the linear algebra of sheaves and\ncosheaves. Further, the homological algebra of these mathematical constructions\ncleanly and concisely describes the formation of 2D reciprocal diagrams and 3D\npolyhedral lifts. Additional relationships between geometric quantities of\nthese dual diagrams are developed, including systems of impossible edge\nrotations. These constructions generalize to non-planar graphs. When a truss\nembedded in a torus or higher genus surface has a sufficient degree of axial\nself stress, we show non-trivial reciprocal figures and non-simply connected\npolyhedral lifts are guaranteed to exist.","publication_date":1700594359,"paper_link":"http://arxiv.org/pdf/2311.12946v1","categories":["Mathematics"],"abstract":"This paper introduces cellular sheaf theory to graphical methods and reciprocal constructions in structural engineering. The elementary mechanics and statics of trusses are derived from the linear algebra of sheaves and cosheaves. Further, the homological algebra of these mathematical constructions cleanly and concisely describes the formation of 2D reciprocal diagrams and 3D polyhedral lifts. Additional relationships between geometric quantities of these dual diagrams are developed, including systems of impossible edge rotations. These constructions generalize to non-planar graphs. When a truss embedded in a torus or higher genus surface has a sufficient degree of axial self stress, we show non-trivial reciprocal figures and non-simply connected polyhedral lifts are guaranteed to exist."}
{"title":"Argyres-Douglas Theories, IR N-ality and Complete Graphs","authors":["Anindya Dey"],"raw_abstract":"We show that for a large subclass of Argyres-Douglas-type theories, the Higgs\nbranch admits multiple hyperkahler quotient realizations as Higgs branches of\nthree dimensional $\\mathcal{N}=4$ quiver gauge theories, which are related by a\nsequence of Seiberg-like IR dualities. We refer to this phenomenon as the\nHyperkahler Quotient N-ality of the four dimensional Higgs branch. The\nassociated set of 3d theories contains a special subset of maximal unitary\nquivers: quiver gauge theories for which the resolution/deformation parameters\nof the Higgs branch are manifest in the Lagrangian as Fayet-Iliopoulas\nparameters. Starting from the Type IIB description for a given SCFT, we present\nan explicit construction to determine the aforementioned set of 3d quivers,\nincluding the subset of maximal unitary quivers. As a byproduct, we find a\nsimple method for constructing the three dimensional mirror associated with the\nSCFT. We demonstrate the construction for the $(A_k, A_k)$ theories of Cecotti,\nNeitzke and Vafa, focusing on the cases $k=3$ and $k=4$. The associated maximal\nunitary quiver is unique up to field redefinitions and turns out to be an\nAbelian quiver gauge theory. The three dimensional mirror obtained in this\nfashion reproduces the well-known complete graph. In the appendices to the main\npaper, we study the quotient N-ality in the closely related family of $D^b_p\n(SU(N))$ SCFTs, for which both the maximal unitary quiver as well as the 3d\nmirror turn out to be non-Abelian gauge theories generically","publication_date":1700593205,"paper_link":"http://arxiv.org/pdf/2311.12931v1","categories":["Mathematics","Physics"],"abstract":"We show that for a large subclass of Argyres-Douglas-type theories, the Higgs branch admits multiple hyperkahler quotient realizations as Higgs branches of three dimensional __FORMULA__ quiver gauge theories, which are related by a sequence of Seiberg-like IR dualities. We refer to this phenomenon as the Hyperkahler Quotient N-ality of the four dimensional Higgs branch. The associated set of 3d theories contains a special subset of maximal unitary quivers: quiver gauge theories for which the resolution/deformation parameters of the Higgs branch are manifest in the Lagrangian as Fayet-Iliopoulas parameters. Starting from the Type IIB description for a given SCFT, we present an explicit construction to determine the aforementioned set of 3d quivers, including the subset of maximal unitary quivers. As a byproduct, we find a simple method for constructing the three dimensional mirror associated with the SCFT. We demonstrate the construction for the __FORMULA__ theories of Cecotti, Neitzke and Vafa, focusing on the cases __FORMULA__ and __FORMULA__. The associated maximal unitary quiver is unique up to field redefinitions and turns out to be an Abelian quiver gauge theory. The three dimensional mirror obtained in this fashion reproduces the well-known complete graph. In the appendices to the main paper, we study the quotient N-ality in the closely related family of __FORMULA__ SCFTs, for which both the maximal unitary quiver as well as the 3d mirror turn out to be non-Abelian gauge theories generically"}
{"title":"Exact cube-root fluctuations in an area-constrained random walk model","authors":["Lucas D'Alimonte","Romain Panis"],"raw_abstract":"This article is devoted to the study of the behaviour of a (1+1)-dimensional\nmodel of random walk conditioned to enclose an area of order $N^2$. Such a\nconditioning enforces a globally concave trajectory. We study the local\ndeviations of the walk from its convex hull. To this end, we introduce two\nquantities -- the mean facet length $\\mathsf{MeanFL}$ and the mean local\nroughness $\\mathsf{MeanLR}$ -- measuring the typical longitudinal and\ntransversal fluctuations around the boundary of the convex hull of the random\nwalk. Our main result is that $\\mathsf{MeanFL}$ is of order $N^{2/3}$ and\n$\\mathsf{MeanLR}$ is of order $N^{1/3}$. Moreover, following the strategy of\nHammond (Ann. Prob., 2012), we identify the polylogarithmic corrections in the\nscaling of the maximal facet length and of the maximal local roughness, showing\nthat the former one scales as $N^{2/3}(\\log N)^{1/3}$, while the latter scales\nas $N^{1/3}(\\log N)^{2/3}$. The object of study is intended to be a toy model\nfor the interface of a two-dimensional statistical mechanics model (such as the\nIsing model) in the phase separation regime -- we discuss this issue at the end\nof this work.","publication_date":1700592348,"paper_link":"http://arxiv.org/pdf/2311.12780v1","categories":["Mathematics","Physics"],"abstract":"This article is devoted to the study of the behaviour of a (1+1)-dimensional model of random walk conditioned to enclose an area of order __FORMULA__. Such a conditioning enforces a globally concave trajectory. We study the local deviations of the walk from its convex hull. To this end, we introduce two quantities -- the mean facet length __FORMULA__ and the mean local roughness __FORMULA__ -- measuring the typical longitudinal and transversal fluctuations around the boundary of the convex hull of the random walk. Our main result is that __FORMULA__ is of order __FORMULA__ and __FORMULA__ is of order __FORMULA__. Moreover, following the strategy of Hammond (Ann. Prob., 2012), we identify the polylogarithmic corrections in the scaling of the maximal facet length and of the maximal local roughness, showing that the former one scales as __FORMULA__, while the latter scales as __FORMULA__. The object of study is intended to be a toy model for the interface of a two-dimensional statistical mechanics model (such as the Ising model) in the phase separation regime -- we discuss this issue at the end of this work."}
{"title":"Wick-type deformation quantization of contact metric manifolds","authors":["Boris M. Elfimov","Alexey A. Sharapov"],"raw_abstract":"We construct a Wick-type deformation quantization of contact metric\nmanifolds. The construction is fully canonical and involves no arbitrary\nchoice. Unlike the case of symplectic or Poisson manifolds, not every classical\nobservable on a general contact metric manifold can be promoted to a quantum\none due to possible obstructions to quantization. We prove, however, that all\nthese obstructions disappear for Sasakian manifolds.","publication_date":1700591104,"paper_link":"http://arxiv.org/pdf/2311.12767v1","categories":["Mathematics","Physics"],"abstract":"We construct a Wick-type deformation quantization of contact metric manifolds. The construction is fully canonical and involves no arbitrary choice. Unlike the case of symplectic or Poisson manifolds, not every classical observable on a general contact metric manifold can be promoted to a quantum one due to possible obstructions to quantization. We prove, however, that all these obstructions disappear for Sasakian manifolds."}
{"title":"Lecture hall graphs and the Askey scheme","authors":["Sylvie Corteel","Bhargavi Jonnadula","Jonathan P. Keating","Jang Soo Kim"],"raw_abstract":"We establish, for every family of orthogonal polynomials in the $q$-Askey\nscheme and the Askey scheme, a combinatorial model for mixed moments and\ncoefficients in terms of paths on the lecture hall graph. This generalizes the\nprevious results of Corteel and Kim for the little $q$-Jacobi polynomials. We\nbuild these combinatorial models by bootstrapping, beginning with polynomials\nat the bottom and working towards Askey-Wilson polynomials which sit at the top\nof the $q$-Askey scheme. As an application of the theory, we provide the first\ncombinatorial proof of the symmetries in the parameters of the Askey-Wilson\npolynomials.","publication_date":1700590530,"paper_link":"http://arxiv.org/pdf/2311.12761v2","categories":["Mathematics","Physics"],"abstract":"We establish, for every family of orthogonal polynomials in the __FORMULA__-Askey scheme and the Askey scheme, a combinatorial model for mixed moments and coefficients in terms of paths on the lecture hall graph. This generalizes the previous results of Corteel and Kim for the little __FORMULA__-Jacobi polynomials. We build these combinatorial models by bootstrapping, beginning with polynomials at the bottom and working towards Askey-Wilson polynomials which sit at the top of the __FORMULA__-Askey scheme. As an application of the theory, we provide the first combinatorial proof of the symmetries in the parameters of the Askey-Wilson polynomials."}
{"title":"Thermodynamic Matrix Exponentials and Thermodynamic Parallelism","authors":["Samuel Duffield","Maxwell Aifer","Gavin Crooks","Thomas Ahle","Patrick J. Coles"],"raw_abstract":"Thermodynamic computing exploits fluctuations and dissipation in physical\nsystems to efficiently solve various mathematical problems. For example, it was\nrecently shown that certain linear algebra problems can be solved\nthermodynamically, leading to an asymptotic speedup scaling with the matrix\ndimension. The origin of this \"thermodynamic advantage\" has not yet been fully\nexplained, and it is not clear what other problems might benefit from it. Here\nwe provide a new thermodynamic algorithm for exponentiating a real matrix, with\napplications in simulating linear dynamical systems. We describe a simple\nelectrical circuit involving coupled oscillators, whose thermal equilibration\ncan implement our algorithm. We also show that this algorithm also provides an\nasymptotic speedup that is linear in the dimension. Finally, we introduce the\nconcept of thermodynamic parallelism to explain this speedup, stating that\nthermodynamic noise provides a resource leading to effective parallelization of\ncomputations, and we hypothesize this as a mechanism to explain thermodynamic\nadvantage more generally.","publication_date":1700590233,"paper_link":"http://arxiv.org/pdf/2311.12759v1","categories":["Physics"],"abstract":"Thermodynamic computing exploits fluctuations and dissipation in physical systems to efficiently solve various mathematical problems. For example, it was recently shown that certain linear algebra problems can be solved thermodynamically, leading to an asymptotic speedup scaling with the matrix dimension. The origin of this \"thermodynamic advantage\" has not yet been fully explained, and it is not clear what other problems might benefit from it. Here we provide a new thermodynamic algorithm for exponentiating a real matrix, with applications in simulating linear dynamical systems. We describe a simple electrical circuit involving coupled oscillators, whose thermal equilibration can implement our algorithm. We also show that this algorithm also provides an asymptotic speedup that is linear in the dimension. Finally, we introduce the concept of thermodynamic parallelism to explain this speedup, stating that thermodynamic noise provides a resource leading to effective parallelization of computations, and we hypothesize this as a mechanism to explain thermodynamic advantage more generally."}
{"title":"Insider trading with penalties, entropy and quadratic BSDEs","authors":["Umut \u00c7etin"],"raw_abstract":"Kyle model in continuous time where the insider may be subject to legal\npenalties is considered. In equilibrium the insider internalises this legal\nrisk by trading less aggressively. The equilibrium is characterised via the\nsolution of a backward stochastic differential equation (BSDE) whose terminal\ncondition is determined as the fixed point of a non-linear operator in\nequilibrium. The insider's expected penalties in equilibrium is non-monotone in\nthe fee structure and is given by the relative entropy of the law of a\nparticular h-transformation of Brownian motion.","publication_date":1700587882,"paper_link":"http://arxiv.org/pdf/2311.12743v1","categories":["Mathematics","Quantitative Finance"],"abstract":"Kyle model in continuous time where the insider may be subject to legal penalties is considered. In equilibrium the insider internalises this legal risk by trading less aggressively. The equilibrium is characterised via the solution of a backward stochastic differential equation (BSDE) whose terminal condition is determined as the fixed point of a non-linear operator in equilibrium. The insider's expected penalties in equilibrium is non-monotone in the fee structure and is given by the relative entropy of the law of a particular h-transformation of Brownian motion."}
{"title":"Geometric Construction of non-linear Sigma models with Q-ball/Q-kink solutions","authors":["A. Alonso-Izquierdo","D. Canillas Martinez","C. Garzon Sanchez","M. A. Gonzalez Leon"],"raw_abstract":"Non-linear Sigma models involving U(1) symmetry group are studied using a\ngeometrical formalism. In this type of models, Q-balls and Q-Kinks solutions\nare found. The geometrical framework described in this article allows the\nidentification of the necessary conditions on the metric and the potential to\nguarantee the existence of these Q-balls and Q-Kinks. Using this procedure,\nSigma models where both types of solutions coexist, have been identified. Only\nthe internal rotational frequency distinguishes which one of these defects will\narise.","publication_date":1700586365,"paper_link":"http://arxiv.org/pdf/2311.12728v1","categories":["Mathematics","Physics"],"abstract":"Non-linear Sigma models involving U(1) symmetry group are studied using a geometrical formalism. In this type of models, Q-balls and Q-Kinks solutions are found. The geometrical framework described in this article allows the identification of the necessary conditions on the metric and the potential to guarantee the existence of these Q-balls and Q-Kinks. Using this procedure, Sigma models where both types of solutions coexist, have been identified. Only the internal rotational frequency distinguishes which one of these defects will arise."}
{"title":"Casimir energy for elliptic fixed points","authors":["J. S. Dowker"],"raw_abstract":"The contribution of elliptic fixed points to the scalar Casimir energy on\ncompact quotients of the upper half hyperbolic plane is computed for a\npropagation operator conformal in three dimensions. The expression involves\nderivatives of two-dimensional Barnes zeta-functions which are reduced to\nHurwitz zeta-functions for numerical purposes. The values are all positive for\nany elliptic order.","publication_date":1700583682,"paper_link":"http://arxiv.org/pdf/2311.12708v1","categories":["Mathematics","Physics"],"abstract":"The contribution of elliptic fixed points to the scalar Casimir energy on compact quotients of the upper half hyperbolic plane is computed for a propagation operator conformal in three dimensions. The expression involves derivatives of two-dimensional Barnes zeta-functions which are reduced to Hurwitz zeta-functions for numerical purposes. The values are all positive for any elliptic order."}
{"title":"A Data-Driven Integrated Framework for Fast-Charging Facility Planning using Multi-Period Bi-Objective Optimization","authors":["Mingjia He","Panchamy Krishnakumari","Ding Luo","Jiaqi Chen"],"raw_abstract":"With the electrification in freight transportation, the availability of\nfast-charging facilities becomes essential to facilitate en-route charging for\nfreight electric vehicles. Most studies focus on planning charging facilities\nbased on mathematical modeling and hypothetical scenarios. This study aims to\ndevelop a data-driven integrated framework for fast-charging facility planning.\nBy leveraging the highway traffic data, we extracted, analyzed, and compared\nspatial and temporal flow patterns of general traffic and freight traffic.\nFurthermore, graph theory-based network evaluation methods are employed to\nidentify traffic nodes within the highway network that play a significant role\nin accommodating charging infrastructure. A candidate selection method is\nproposed to obtain potential deployment locations for charging stations and\nto-go chargers. Based on this, we present a multi-period bi-objective\noptimization model to provide optimal solutions for the placement of charging\nfacilities, with the objectives of minimizing investment cost and maximizing\ndemand coverage. The case study on the Amsterdam highway network shows how\nexisting traffic data can be used to generate more realistic charging demand\nscenarios and how it can be integrated and evaluated within the optimization\nframework for facility planning. The study also shows that the proposed model\ncan leverage the potential of early investment in improving the charging demand\ncoverage.","publication_date":1700583114,"paper_link":"http://arxiv.org/pdf/2311.12700v1","categories":["Electrical Engineering and Systems Science"],"abstract":"With the electrification in freight transportation, the availability of fast-charging facilities becomes essential to facilitate en-route charging for freight electric vehicles. Most studies focus on planning charging facilities based on mathematical modeling and hypothetical scenarios. This study aims to develop a data-driven integrated framework for fast-charging facility planning. By leveraging the highway traffic data, we extracted, analyzed, and compared spatial and temporal flow patterns of general traffic and freight traffic. Furthermore, graph theory-based network evaluation methods are employed to identify traffic nodes within the highway network that play a significant role in accommodating charging infrastructure. A candidate selection method is proposed to obtain potential deployment locations for charging stations and to-go chargers. Based on this, we present a multi-period bi-objective optimization model to provide optimal solutions for the placement of charging facilities, with the objectives of minimizing investment cost and maximizing demand coverage. The case study on the Amsterdam highway network shows how existing traffic data can be used to generate more realistic charging demand scenarios and how it can be integrated and evaluated within the optimization framework for facility planning. The study also shows that the proposed model can leverage the potential of early investment in improving the charging demand coverage."}
{"title":"Non-radial NLS equation with competing inhomogeneous nonlinearities: Ground states, Blow-up and scattering","authors":["Tianxiang Gou","Mohamed Majdoub","Tarek Saanouni"],"raw_abstract":"We investigate a class of nonlinear Schr\\\"odinger equations with competing\ninhomogeneous nonlinearities in the non-radial inter-critical regime.\n  First, we establish the existence/nonexistence, symmetry, decay, uniqueness,\nnon-degeneracy and instability of ground states. Then, we prove scattering\nversus blowup below the ground state energy threshold. We follow the approach\nof Dodson-Murphy (A new proof of scattering below the ground state for the 3D\nradial focusing cubic NLS, {Proc. Am. Math. Soc.} (2017)) which is based on\nTao's scattering criteria and Morawetz estimates. We also obtain an upper bound\nof the blowup rate.\n  The novelty here is to handle two main difficulties. First, due to the\ncompeting nonlinearties, the equation doesn't enjoy any scaling invariance.\nSecond, the singular weights prevent the invariance by translation in the space\nvariable.\n  To the best of authors knowledge, this is the first time when inhomegenous\nNLS equation with a focusing leading order nonlinearity and a defocusing\nperturbation is investigated.","publication_date":1700582271,"paper_link":"http://arxiv.org/pdf/2311.12693v1","categories":["Mathematics","Physics"],"abstract":"We investigate a class of nonlinear Schr\\\"odinger equations with competing inhomogeneous nonlinearities in the non-radial inter-critical regime.   First, we establish the existence/nonexistence, symmetry, decay, uniqueness, non-degeneracy and instability of ground states. Then, we prove scattering versus blowup below the ground state energy threshold. We follow the approach of Dodson-Murphy (A new proof of scattering below the ground state for the 3D radial focusing cubic NLS, {Proc. Am. Math. Soc.} (2017)) which is based on Tao's scattering criteria and Morawetz estimates. We also obtain an upper bound of the blowup rate.   The novelty here is to handle two main difficulties. First, due to the competing nonlinearties, the equation doesn't enjoy any scaling invariance. Second, the singular weights prevent the invariance by translation in the space variable.   To the best of authors knowledge, this is the first time when inhomegenous NLS equation with a focusing leading order nonlinearity and a defocusing perturbation is investigated."}
{"title":"On Neumann-Poincar\u00e9 operators and self-adjoint transmission problems","authors":["Badreddine Benhellal","Konstantin Pankrashkin"],"raw_abstract":"We discuss the self-adjointness in $L^2$-setting of the operators acting as\n$-\\nabla\\cdot h\\nabla$, with piecewise constant functions $h$ having a jump\nalong a Lipschitz hypersurface $\\Sigma$, without explicit assumptions on the\nsign of $h$. We establish a number of sufficient conditions for the\nself-adjointness of the operator with $H^{\\frac{3}{2}}$-regularity in terms of\nthe jump value and the regularity and geometric properties of $\\Sigma$. An\nimportant intermediate step is a link with Fredholm properties of the\nNeumann-Poincar\\'e operator on $\\Sigma$, which is new for the Lipschitz\nsetting.","publication_date":1700580551,"paper_link":"http://arxiv.org/pdf/2311.12672v1","categories":["Mathematics","Physics"],"abstract":"We discuss the self-adjointness in __FORMULA__-setting of the operators acting as __FORMULA__, with piecewise constant functions __FORMULA__ having a jump along a Lipschitz hypersurface __FORMULA__, without explicit assumptions on the sign of __FORMULA__. We establish a number of sufficient conditions for the self-adjointness of the operator with __FORMULA__-regularity in terms of the jump value and the regularity and geometric properties of __FORMULA__. An important intermediate step is a link with Fredholm properties of the Neumann-Poincar\\'e operator on __FORMULA__, which is new for the Lipschitz setting."}
{"title":"Hand-Eye Calibration","authors":["Radu Horaud","Fadi Dornaika"],"raw_abstract":"Whenever a sensor is mounted on a robot hand it is important to know the\nrelationship between the sensor and the hand. The problem of determining this\nrelationship is referred to as hand-eye calibration, which is important in at\nleast two types of tasks: (i) map sensor centered measurements into the robot\nworkspace and (ii) allow the robot to precisely move the sensor. In the past\nsome solutions were proposed in the particular case of a camera. With almost no\nexception, all existing solutions attempt to solve the homogeneous matrix\nequation AX=XB. First we show that there are two possible formulations of the\nhand-eye calibration problem. One formulation is the classical one that we just\nmentioned. A second formulation takes the form of the following homogeneous\nmatrix equation: MY=M'YB. The advantage of the latter is that the extrinsic and\nintrinsic camera parameters need not be made explicit. Indeed, this formulation\ndirectly uses the 3 by 4 perspective matrices (M and M') associated with two\npositions of the camera. Moreover, this formulation together with the classical\none cover a wider range of camera-based sensors to be calibrated with respect\nto the robot hand. Second, we develop a common mathematical framework to solve\nfor the hand-eye calibration problem using either of the two formulations. We\npresent two methods, (i) a rotation then translation and (ii) a non-linear\nsolver for rotation and translation. Third, we perform a stability analysis\nboth for our two methods and for the classical linear method of Tsai and Lenz\n(1989). In the light of this comparison, the non-linear optimization method,\nthat solves for rotation and translation simultaneously, seems to be the most\nrobust one with respect to noise and to measurement errors.","publication_date":1700578644,"paper_link":"http://arxiv.org/pdf/2311.12655v2","categories":["Mathematics"],"abstract":"Whenever a sensor is mounted on a robot hand it is important to know the relationship between the sensor and the hand. The problem of determining this relationship is referred to as hand-eye calibration, which is important in at least two types of tasks: (i) map sensor centered measurements into the robot workspace and (ii) allow the robot to precisely move the sensor. In the past some solutions were proposed in the particular case of a camera. With almost no exception, all existing solutions attempt to solve the homogeneous matrix equation AX=XB. First we show that there are two possible formulations of the hand-eye calibration problem. One formulation is the classical one that we just mentioned. A second formulation takes the form of the following homogeneous matrix equation: MY=M'YB. The advantage of the latter is that the extrinsic and intrinsic camera parameters need not be made explicit. Indeed, this formulation directly uses the 3 by 4 perspective matrices (M and M') associated with two positions of the camera. Moreover, this formulation together with the classical one cover a wider range of camera-based sensors to be calibrated with respect to the robot hand. Second, we develop a common mathematical framework to solve for the hand-eye calibration problem using either of the two formulations. We present two methods, (i) a rotation then translation and (ii) a non-linear solver for rotation and translation. Third, we perform a stability analysis both for our two methods and for the classical linear method of Tsai and Lenz (1989). In the light of this comparison, the non-linear optimization method, that solves for rotation and translation simultaneously, seems to be the most robust one with respect to noise and to measurement errors."}
{"title":"MathGloss: Building mathematical glossaries from text","authors":["Lucy Horowitz","Valeria de Paiva"],"raw_abstract":"MathGloss is a project to create a knowledge graph (KG) for undergraduate\nmathematics from text, automatically, using modern natural language processing\n(NLP) tools and resources already available on the web. MathGloss is a linked\ndatabase of undergraduate concepts in mathematics. So far, it combines five\nresources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph\nhosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses\nat the University of Chicago, (iii) the syllabus of the French undergraduate\nmathematics curriculum which includes hyperlinks to the automated theorem\nprover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by\nmathematicians, and (v) the nLab, a wiki for category theory also curated by\nmathematicians. MathGloss's goal is to bring together resources for learning\nmathematics and to allow every mathematician to tailor their learning to their\nown preferences. Moreover, by organizing different resources for learning\nundergraduate mathematics alongside those for learning formal mathematics, we\nhope to make it easier for mathematicians and formal tools (theorem provers,\ncomputer algebra systems, etc) experts to \"understand\" each other and break\ndown some of the barriers to formal math.","publication_date":1700578140,"paper_link":"http://arxiv.org/pdf/2311.12649v1","categories":["Mathematics"],"abstract":"MathGloss is a project to create a knowledge graph (KG) for undergraduate mathematics from text, automatically, using modern natural language processing (NLP) tools and resources already available on the web. MathGloss is a linked database of undergraduate concepts in mathematics. So far, it combines five resources: (i) Wikidata, a collaboratively edited, multilingual knowledge graph hosted by the Wikimedia Foundation, (ii) terms covered in mathematics courses at the University of Chicago, (iii) the syllabus of the French undergraduate mathematics curriculum which includes hyperlinks to the automated theorem prover Lean 4, (iv) MuLiMa, a multilingual dictionary of mathematics curated by mathematicians, and (v) the nLab, a wiki for category theory also curated by mathematicians. MathGloss's goal is to bring together resources for learning mathematics and to allow every mathematician to tailor their learning to their own preferences. Moreover, by organizing different resources for learning undergraduate mathematics alongside those for learning formal mathematics, we hope to make it easier for mathematicians and formal tools (theorem provers, computer algebra systems, etc) experts to \"understand\" each other and break down some of the barriers to formal math."}
{"title":"Online landmark replacement for out-of-sample dimensionality reduction methods","authors":["Chanon Thongprayoon","Naoki Masuda"],"raw_abstract":"A strategy to assist visualization and analysis of large and complex data\nsets is dimensionality reduction, with which one maps each data point into a\nlow-dimensional manifold. However, various dimensionality reduction techniques\nare computationally infeasible for large data. Out-of-sample techniques aim to\nresolve this difficulty; they only apply the dimensionality reduction technique\non a small portion of data, referred to as landmarks, and determine the\nembedding coordinates of the other points using landmarks as references.\nOut-of-sample techniques have been applied to online settings, or when data\narrive as time series. However, existing online out-of-sample techniques use\neither all the previous data points as landmarks or the fixed set of landmarks\nand therefore are potentially not good at capturing the geometry of the entire\ndata set when the time series is non-stationary. To address this problem, we\npropose an online landmark replacement algorithm for out-of-sample techniques\nusing geometric graphs and the minimal dominating set on them. We\nmathematically analyze some properties of the proposed algorithm, particularly\nfocusing on the case of landmark multidimensional scaling as the out-of-sample\ntechnique, and test its performance on synthetic and empirical time series\ndata.","publication_date":1700578081,"paper_link":"http://arxiv.org/pdf/2311.12646v1","categories":["Physics"],"abstract":"A strategy to assist visualization and analysis of large and complex data sets is dimensionality reduction, with which one maps each data point into a low-dimensional manifold. However, various dimensionality reduction techniques are computationally infeasible for large data. Out-of-sample techniques aim to resolve this difficulty; they only apply the dimensionality reduction technique on a small portion of data, referred to as landmarks, and determine the embedding coordinates of the other points using landmarks as references. Out-of-sample techniques have been applied to online settings, or when data arrive as time series. However, existing online out-of-sample techniques use either all the previous data points as landmarks or the fixed set of landmarks and therefore are potentially not good at capturing the geometry of the entire data set when the time series is non-stationary. To address this problem, we propose an online landmark replacement algorithm for out-of-sample techniques using geometric graphs and the minimal dominating set on them. We mathematically analyze some properties of the proposed algorithm, particularly focusing on the case of landmark multidimensional scaling as the out-of-sample technique, and test its performance on synthetic and empirical time series data."}
{"title":"Nonlocal PDEs and quantum optics: band structure of periodic atomic sytems","authors":["Erik Orvehed Hiltunen","Joseph Kraisler","John C. Schotland","Michael I. Weinstein"],"raw_abstract":"We continue our study of the quantum optics of a single photon interacting\nwith a system of two level atoms. In this work we investigate the case of a\nperiodic arrangement of atoms. We provide a general structure theorem\ncharacterizing the band functions of this problem, which comprise the spectrum\nof the associated Hamiltonian. Additionally, we study atomic densities arising\nas periodically arranged scaled inclusions. For this family of examples, we\nobtain explicit asymptotic formulas for the band functions.","publication_date":1700576817,"paper_link":"http://arxiv.org/pdf/2311.12632v1","categories":["Mathematics","Physics"],"abstract":"We continue our study of the quantum optics of a single photon interacting with a system of two level atoms. In this work we investigate the case of a periodic arrangement of atoms. We provide a general structure theorem characterizing the band functions of this problem, which comprise the spectrum of the associated Hamiltonian. Additionally, we study atomic densities arising as periodically arranged scaled inclusions. For this family of examples, we obtain explicit asymptotic formulas for the band functions."}
{"title":"The Quantum Rabi model: Towards Braak's conjecture","authors":["Ze\u00e9v Rudnick"],"raw_abstract":"We establish a density one version of Braak's conjecture on the fine\nstructure of the spectrum of the quantum Rabi model, as well as a recent\nconjecture of Braak, Nguyen, Reyes-Bustos and Wakayama on the nearest neighbor\nspacings of the spectrum. The proof uses a three-term asymptotics for large\neigenvalues due to Boutet de Monvel and Zielinski, and a number theoretic\nargument from uniform distribution theory.","publication_date":1700576061,"paper_link":"http://arxiv.org/pdf/2311.12622v1","categories":["Mathematics","Physics"],"abstract":"We establish a density one version of Braak's conjecture on the fine structure of the spectrum of the quantum Rabi model, as well as a recent conjecture of Braak, Nguyen, Reyes-Bustos and Wakayama on the nearest neighbor spacings of the spectrum. The proof uses a three-term asymptotics for large eigenvalues due to Boutet de Monvel and Zielinski, and a number theoretic argument from uniform distribution theory."}
{"title":"Complete separation of variables in the geodesic Hamilton--Jacobi equation in four dimensions","authors":["M. O. Katanaev"],"raw_abstract":"We list all metrics of arbitrary signature in four dimensions which admit\ncomplete separation of variables in the Hamilton--Jacobi equation for geodesic\nHamiltonians. There are only ten classes of separable metrics admitting\ncommuting Killing vector fields, indecomposable quadratic conservation laws,\nand coisotropic coordinates. Canonical separable metrics parameterized by\nseveral (up to twelve) arbitrary functions of single coordinates are written\nexplicitly. The full set of independent conservation laws in involution for\neach canonical metrics is also found.","publication_date":1700574906,"paper_link":"http://arxiv.org/pdf/2311.12907v1","categories":["Mathematics","Physics"],"abstract":"We list all metrics of arbitrary signature in four dimensions which admit complete separation of variables in the Hamilton--Jacobi equation for geodesic Hamiltonians. There are only ten classes of separable metrics admitting commuting Killing vector fields, indecomposable quadratic conservation laws, and coisotropic coordinates. Canonical separable metrics parameterized by several (up to twelve) arbitrary functions of single coordinates are written explicitly. The full set of independent conservation laws in involution for each canonical metrics is also found."}
{"title":"Start-up and Transient Response Characteristics of Natural Draft Direct Dry Cooling Systems at Various Scales","authors":["Wian Strydom","Johannes Pretorius","Ryno Laubscher"],"raw_abstract":"Natural draft direct dry cooling systems (NDDDCSs) combine the benefits of\ntwo traditional dry cooling systems, air-cooled condensers (ACCs) and indirect\nnatural draft dry cooling systems, to offer advantages such as reduced system\ncomplexity, high thermal efficiency, and low parasitic power losses and\noperational costs. These scalable systems can potentially operate in both large\ncoal-fired power plants (900 MWt) and medium-scale concentrated solar power\n(CSP) plants (100 MWt). While prior research focused on NDDDCS steady-state\nperformance, this study delves into the transient behaviour and start-up\ncharacteristics of this cooling system, crucial for power plants with rapid\nstart-up requirements or load ramps during electricity demand peaks. The paper\nemploys a transient 1-D mathematical model to characterize the NDDDCSs'\nresponse to start-up and load ramp conditions under no-wind, presenting results\nfor both large- and medium scales while identifying limiting factors.","publication_date":1700573518,"paper_link":"http://arxiv.org/pdf/2311.12595v1","categories":["Physics"],"abstract":"Natural draft direct dry cooling systems (NDDDCSs) combine the benefits of two traditional dry cooling systems, air-cooled condensers (ACCs) and indirect natural draft dry cooling systems, to offer advantages such as reduced system complexity, high thermal efficiency, and low parasitic power losses and operational costs. These scalable systems can potentially operate in both large coal-fired power plants (900 MWt) and medium-scale concentrated solar power (CSP) plants (100 MWt). While prior research focused on NDDDCS steady-state performance, this study delves into the transient behaviour and start-up characteristics of this cooling system, crucial for power plants with rapid start-up requirements or load ramps during electricity demand peaks. The paper employs a transient 1-D mathematical model to characterize the NDDDCSs' response to start-up and load ramp conditions under no-wind, presenting results for both large- and medium scales while identifying limiting factors."}
{"title":"A Renunciation of Axiom, A New Conception of Logic","authors":["Eitan Wander"],"raw_abstract":"Mathematicians invented Mathematics to escape from words, but at last they\ndepend on them just as much as everybody else. At the end, all basic\ndefinitions will be reliant on words, yet the mathematician believes that he's\nelevated from them by use of axioms, only that just postpones the problem and\nlater becomes more serious G\\\"odelian problems due to the countability of the\naxiomatic array. I think we should do our best to rid ourselves of axioms, and\nin this paper, I revisit what's called \"Na\\\"ive Set Theory\", which is set\ntheory that is fully reliant on verbal definitions, and resolve the problems\nthat were once found with it in the form of paradoxes, most notably, Russell's\nParadox. I believe that by this, a new approach to mathematics as a whole is\npresented, an approach that refers to mathematics as the science of\ndefinitions. The question \"how do you define definition?\" will not be a part of\nMathematics but of Metamathematics and to it many of the problems known about\nNa\\\"ive Set Theory will be funneled. A thorough yet concise treatment of the\nmatter will be given in this paper.","publication_date":1700572372,"paper_link":"http://arxiv.org/pdf/2311.12587v1","categories":["Mathematics"],"abstract":"Mathematicians invented Mathematics to escape from words, but at last they depend on them just as much as everybody else. At the end, all basic definitions will be reliant on words, yet the mathematician believes that he's elevated from them by use of axioms, only that just postpones the problem and later becomes more serious G\\\"odelian problems due to the countability of the axiomatic array. I think we should do our best to rid ourselves of axioms, and in this paper, I revisit what's called \"Na\\\"ive Set Theory\", which is set theory that is fully reliant on verbal definitions, and resolve the problems that were once found with it in the form of paradoxes, most notably, Russell's Paradox. I believe that by this, a new approach to mathematics as a whole is presented, an approach that refers to mathematics as the science of definitions. The question \"how do you define definition?\" will not be a part of Mathematics but of Metamathematics and to it many of the problems known about Na\\\"ive Set Theory will be funneled. A thorough yet concise treatment of the matter will be given in this paper."}
{"title":"$\u03ba$-Minkowski as tangent space I: quantum partition of unity","authors":["Kilian Hersent","Jean-Christophe Wallet"],"raw_abstract":"We define a quantum (noncommutative) analogue of locally trivial tangent\nbundle based on three main elements: the definition of local algebras through\nquotients of ideals of the global algebra as introduced in [20], the triviality\nof the local tangent space as being the $\\kappa$-Minkowski space inspirated\nfrom [2], and the gluing of local objects to global ones through the\nintroduction of a notion of quantum (noncommutative partition) of unity.","publication_date":1700571990,"paper_link":"http://arxiv.org/pdf/2311.12584v1","categories":["Mathematics","Physics"],"abstract":"We define a quantum (noncommutative) analogue of locally trivial tangent bundle based on three main elements: the definition of local algebras through quotients of ideals of the global algebra as introduced in [20], the triviality of the local tangent space as being the __FORMULA__-Minkowski space inspirated from [2], and the gluing of local objects to global ones through the introduction of a notion of quantum (noncommutative partition) of unity."}
{"title":"Machine-Guided Discovery of a Real-World Rogue Wave Model","authors":["Dion H\u00e4fner","Johannes Gemmrich","Markus Jochum"],"raw_abstract":"Big data and large-scale machine learning have had a profound impact on\nscience and engineering, particularly in fields focused on forecasting and\nprediction. Yet, it is still not clear how we can use the superior pattern\nmatching abilities of machine learning models for scientific discovery. This is\nbecause the goals of machine learning and science are generally not aligned. In\naddition to being accurate, scientific theories must also be causally\nconsistent with the underlying physical process and allow for human analysis,\nreasoning, and manipulation to advance the field. In this paper, we present a\ncase study on discovering a new symbolic model for oceanic rogue waves from\ndata using causal analysis, deep learning, parsimony-guided model selection,\nand symbolic regression. We train an artificial neural network on causal\nfeatures from an extensive dataset of observations from wave buoys, while\nselecting for predictive performance and causal invariance. We apply symbolic\nregression to distill this black-box model into a mathematical equation that\nretains the neural network's predictive capabilities, while allowing for\ninterpretation in the context of existing wave theory. The resulting model\nreproduces known behavior, generates well-calibrated probabilities, and\nachieves better predictive scores on unseen data than current theory. This\nshowcases how machine learning can facilitate inductive scientific discovery,\nand paves the way for more accurate rogue wave forecasting.","publication_date":1700571024,"paper_link":"http://arxiv.org/pdf/2311.12579v1","categories":["Physics"],"abstract":"Big data and large-scale machine learning have had a profound impact on science and engineering, particularly in fields focused on forecasting and prediction. Yet, it is still not clear how we can use the superior pattern matching abilities of machine learning models for scientific discovery. This is because the goals of machine learning and science are generally not aligned. In addition to being accurate, scientific theories must also be causally consistent with the underlying physical process and allow for human analysis, reasoning, and manipulation to advance the field. In this paper, we present a case study on discovering a new symbolic model for oceanic rogue waves from data using causal analysis, deep learning, parsimony-guided model selection, and symbolic regression. We train an artificial neural network on causal features from an extensive dataset of observations from wave buoys, while selecting for predictive performance and causal invariance. We apply symbolic regression to distill this black-box model into a mathematical equation that retains the neural network's predictive capabilities, while allowing for interpretation in the context of existing wave theory. The resulting model reproduces known behavior, generates well-calibrated probabilities, and achieves better predictive scores on unseen data than current theory. This showcases how machine learning can facilitate inductive scientific discovery, and paves the way for more accurate rogue wave forecasting."}
{"title":"The $\u03b2$ maps and the strong clustering at the complex unit circle","authors":["Alec Schiavoni Piazza","David Meadon","Stefano Serra-Capizzano"],"raw_abstract":"In this work, we study eigenvalue distribution results of a class of highly\nnon-normal matrix-sequences, which can be viewed as a low rank perturbation\ndepending on a parameter $\\beta>1$ of the basic Toeplitz matrix-sequence\n$\\{T_n(e^{\\mathbf{i}\\theta})\\}_{n\\in\\N}$, $\\mathbf{i}^2=-1$. The latter has\nobviously all eigenvalues equal to zero for any matrix order $n$, while for the\nmatrix-sequence under consideration we will show a strong clustering at the\nrange of the generating function $e^{\\mathbf{i}\\theta}$ i.e. at the complex\nunit circle. For $\\beta\\ge 2$ no outliers show up, while for $\\beta \\in (1,2)$\nonly two outliers are present, which are both real, positive and have finite\nlimits equal to $\\beta-1$ and $(\\beta-1)^{-1}$, respectively. The problem looks\nmathematically innocent, but indeed it is quite challenging since all the\nsophisticated machinery for deducing the eigenvalue clustering is not easy to\napply in the current setting and at most we may hope for weak clustering\nresults. In the derivations, we resort to a trick already used for the spectral\nanalysis of the Google matrix plus several tools from complex analysis. We only\nmention that the problem is not an academical curiosity and in fact it stems\nfrom problems in dynamical systems and number theory. Numerical experiments in\nhigh precision are provided and more results are sketched for limit case of\n$\\beta=1$.","publication_date":1700569885,"paper_link":"http://arxiv.org/pdf/2311.12568v1","categories":["Mathematics"],"abstract":"In this work, we study eigenvalue distribution results of a class of highly non-normal matrix-sequences, which can be viewed as a low rank perturbation depending on a parameter __FORMULA__ of the basic Toeplitz matrix-sequence __FORMULA__, __FORMULA__. The latter has obviously all eigenvalues equal to zero for any matrix order __FORMULA__, while for the matrix-sequence under consideration we will show a strong clustering at the range of the generating function __FORMULA__ i.e. at the complex unit circle. For __FORMULA__ no outliers show up, while for __FORMULA__ only two outliers are present, which are both real, positive and have finite limits equal to __FORMULA__ and __FORMULA__, respectively. The problem looks mathematically innocent, but indeed it is quite challenging since all the sophisticated machinery for deducing the eigenvalue clustering is not easy to apply in the current setting and at most we may hope for weak clustering results. In the derivations, we resort to a trick already used for the spectral analysis of the Google matrix plus several tools from complex analysis. We only mention that the problem is not an academical curiosity and in fact it stems from problems in dynamical systems and number theory. Numerical experiments in high precision are provided and more results are sketched for limit case of __FORMULA__."}
{"title":"Cutting-plane methods witout nested approximating sets","authors":["Igor Zabotin","Rashid Yarullin"],"raw_abstract":"In connection with the needs of solving optimization problems, the\ndevelopment of conditional minimization methods with convenient numerical\nimplementation continues to attract the attention of mathematicians. In this\nmonograph we propose methods for solving mathematical programming problems that\nbelong to the class of cutting methods. The proposed methods are characterized,\nin particular, by the fact that they contain the possibility of periodically\nupdating approximating sets by discarding accumulating cutting planes, which\nmakes these methods convenient from a practical point of view. The monograph\ncan be used by specialists in the field of mathematical programming, as well as\nby students studying in the specialties \"Applied Mathematics\", \"Applied\nMathematics and Computer Science\", etc.","publication_date":1700564092,"paper_link":"http://arxiv.org/pdf/2311.12520v1","categories":["Mathematics"],"abstract":"In connection with the needs of solving optimization problems, the development of conditional minimization methods with convenient numerical implementation continues to attract the attention of mathematicians. In this monograph we propose methods for solving mathematical programming problems that belong to the class of cutting methods. The proposed methods are characterized, in particular, by the fact that they contain the possibility of periodically updating approximating sets by discarding accumulating cutting planes, which makes these methods convenient from a practical point of view. The monograph can be used by specialists in the field of mathematical programming, as well as by students studying in the specialties \"Applied Mathematics\", \"Applied Mathematics and Computer Science\", etc."}
{"title":"Quantum Mechanics on a background modulo observation","authors":["Jose A. Pereira Frugone"],"raw_abstract":"In this work we will answer the following question: What remains of Quantum\nMechanics when we transform the background space-time into a space modularized\nby observation or measurement regions ? This new moduli space is constructed by\nidentifying regions of space-time where quantum phase comparison (observation,\nmeasurement) is implied. We call it Observation Modular space (OM-space). In\naddition we replace in QM statements the Plank constant (h) by the quantity\n$\\zeta_0 4 \\pi^2$ (where $\\zeta_0$ is the Plank Length) or otherwise, replacing\n$P_0$ (the Planck Momentum) by $4 \\pi^2$. This maps Quantum Mechanics into a\nvery rich dual Number Theory which we call Observation Modular Quantum\nMechanics (OM-QM). We find the OM-dual to the Dirac Equation, the quantum Wave\nFunction and a free particle's mass. The OM-QM counterparts of the Energy turns\nout to be a simple function of the zeroes of the Riemann zeta function. We also\nfind the OM-QM correspondents to the electron spin, the electron charge, the\nElectric Field and the Fine Structure Constant. We also find the OM-QM\ncorrespondents of the Heisemberg uncertainty relation and Einstein's General\nRelativity Field equation emerging as certain limits of a unique OM-QM\nequation. We also get the OM-QM correspondents of the Gravitational Constant\nand the Cosmological Constant. We find the analog of holography in the OM-QM\nside and we get an interpretation of spin as a high dimensional curvature. An\ninterpretation of the OM-QM correspondence is proposed as giving the part of QM\ninformation which is not measurement or observation dependent. Some potential\nfuture applications of this correspondence are discussed.","publication_date":1700561338,"paper_link":"http://arxiv.org/pdf/2311.12493v1","categories":["Mathematics","Physics"],"abstract":"In this work we will answer the following question: What remains of Quantum Mechanics when we transform the background space-time into a space modularized by observation or measurement regions ? This new moduli space is constructed by identifying regions of space-time where quantum phase comparison (observation, measurement) is implied. We call it Observation Modular space (OM-space). In addition we replace in QM statements the Plank constant (h) by the quantity __FORMULA__ (where __FORMULA__ is the Plank Length) or otherwise, replacing __FORMULA__ (the Planck Momentum) by __FORMULA__. This maps Quantum Mechanics into a very rich dual Number Theory which we call Observation Modular Quantum Mechanics (OM-QM). We find the OM-dual to the Dirac Equation, the quantum Wave Function and a free particle's mass. The OM-QM counterparts of the Energy turns out to be a simple function of the zeroes of the Riemann zeta function. We also find the OM-QM correspondents to the electron spin, the electron charge, the Electric Field and the Fine Structure Constant. We also find the OM-QM correspondents of the Heisemberg uncertainty relation and Einstein's General Relativity Field equation emerging as certain limits of a unique OM-QM equation. We also get the OM-QM correspondents of the Gravitational Constant and the Cosmological Constant. We find the analog of holography in the OM-QM side and we get an interpretation of spin as a high dimensional curvature. An interpretation of the OM-QM correspondence is proposed as giving the part of QM information which is not measurement or observation dependent. Some potential future applications of this correspondence are discussed."}
{"title":"Representations of the super-Yangian of type $B(n,m)$","authors":["Alexander Molev","Eric Ragoucy"],"raw_abstract":"We are concerned with finite-dimensional irreducible representations of the\nYangians associated with the orthosymplectic Lie superalgebras ${\\frak\nosp}_{2n+1|2m}$. Every such representation is highest weight and we use\nembedding theorems and odd reflections of Yangian type to derive necessary\nconditions for an irreducible highest weight representation to be\nfinite-dimensional. We conjecture that these conditions are also sufficient. We\nprove the conjecture in the case $n=1$ and arbitrary $m\\geqslant 1$.","publication_date":1700559788,"paper_link":"http://arxiv.org/pdf/2311.12479v1","categories":["Mathematics","Physics"],"abstract":"We are concerned with finite-dimensional irreducible representations of the Yangians associated with the orthosymplectic Lie superalgebras __FORMULA__. Every such representation is highest weight and we use embedding theorems and odd reflections of Yangian type to derive necessary conditions for an irreducible highest weight representation to be finite-dimensional. We conjecture that these conditions are also sufficient. We prove the conjecture in the case __FORMULA__ and arbitrary __FORMULA__."}
{"title":"Is mathematics a game?","authors":["Klaas Landsman","Kirti Singh"],"raw_abstract":"We re-examine the old question to what extent mathematics may be compared to\na game. Under the spell of Wittgenstein, we propose that the more refined\nobject of comparison is a \"motley of language games\", the nature of which was\n(implicitly) clarified by Hilbert: via different language games, axiomatization\nlies at the basis of both the rigour and the applicability of mathematics. In\nthe \"formalist\" game, mathematics resembles chess via a clear conceptual\ndictionary. Accepting this resemblance: like positions in chess, mathematical\nsentences cannot be true or false; true statements in mathematics are about\nsentences, namely that they are theorems (if they are). In principle, the\ncertainty of mathematics resides in proofs, but to this end, in practice these\nmust be \"surveyable\". Hilbert and Wittgenstein proposed almost oppositie\ncriteria for surveyability; we try to overcome their difference by invoking\ncomputer-verified proofs. The \"applied\"' language game is based on Hilbert's\naxiomatization program for physics (and other scientific disciplines), refined\nby Wittgenstein's idea that theorems are yardsticks to which empirical\nphenomena may be compared, and further improved by invoking elements of van\nFraassen's constructive empiricism. From this perspective, in an appendix we\nalso briefly review the varying roles and structures of axioms, definitions,\nand proofs in mathematics. Our view is not meant as a philosophy of mathematics\nby itself, but as a coat rack analogous to category theory, onto which various\n(traditional and new) philosophies of mathematics (such as formalism,\nintuitionism, structuralism, deductivism, and the philosophy of mathematical\npractice) may be attached and may even peacefully support each other.","publication_date":1700559698,"paper_link":"http://arxiv.org/pdf/2311.12478v1","categories":["Mathematics","Physics"],"abstract":"We re-examine the old question to what extent mathematics may be compared to a game. Under the spell of Wittgenstein, we propose that the more refined object of comparison is a \"motley of language games\", the nature of which was (implicitly) clarified by Hilbert: via different language games, axiomatization lies at the basis of both the rigour and the applicability of mathematics. In the \"formalist\" game, mathematics resembles chess via a clear conceptual dictionary. Accepting this resemblance: like positions in chess, mathematical sentences cannot be true or false; true statements in mathematics are about sentences, namely that they are theorems (if they are). In principle, the certainty of mathematics resides in proofs, but to this end, in practice these must be \"surveyable\". Hilbert and Wittgenstein proposed almost oppositie criteria for surveyability; we try to overcome their difference by invoking computer-verified proofs. The \"applied\"' language game is based on Hilbert's axiomatization program for physics (and other scientific disciplines), refined by Wittgenstein's idea that theorems are yardsticks to which empirical phenomena may be compared, and further improved by invoking elements of van Fraassen's constructive empiricism. From this perspective, in an appendix we also briefly review the varying roles and structures of axioms, definitions, and proofs in mathematics. Our view is not meant as a philosophy of mathematics by itself, but as a coat rack analogous to category theory, onto which various (traditional and new) philosophies of mathematics (such as formalism, intuitionism, structuralism, deductivism, and the philosophy of mathematical practice) may be attached and may even peacefully support each other."}
{"title":"An Assessment of PC-mer's Performance in Alignment-Free Phylogenetic Tree Construction","authors":["Saeedeh Akbari Rokn Abadi","Melika Honarmand","Ali Hajialinaghi","Somayyeh Koohi"],"raw_abstract":"Background: Sequence comparison is essential in bioinformatics, serving\nvarious purposes such as taxonomy, functional inference, and drug discovery.\nThe traditional method of aligning sequences for comparison is time-consuming,\nespecially with large datasets. To overcome this, alignment-free methods have\nemerged as an alternative approach, prioritizing comparison scores over\nalignment itself. These methods directly compare sequences without the need for\nalignment. However, accurately representing the relationships between sequences\nis a significant challenge in the design of these tools. Methods:One of the\nalignment-free comparison approaches utilizes the frequency of fixed-length\nsubstrings, known as K-mers, which serves as the foundation for many sequence\ncomparison methods. However, a challenge arises in these methods when\nincreasing the length of the substring (K), as it leads to an exponential\ngrowth in the number of possible states. In this work, we explore the PC-mer\nmethod, which utilizes a more limited set of words that experience slower\ngrowth 2^k instead of 4^k compared to K. We conducted a comparison of sequences\nand evaluated how the reduced input vector size influenced the performance of\nthe PC-mer method. Results: For the evaluation, we selected the Clustal Omega\nmethod as our reference approach, alongside three alignment-free methods:\nkmacs, FFP, and alfpy (word count). These methods also leverage the frequency\nof K-mers. We applied all five methods to 9 datasets for comprehensive\nanalysis. The results were compared using phylogenetic trees and metrics such\nas Robinson-Foulds and normalized quartet distance (nQD). Conclusion: Our\nfindings indicate that, unlike reducing the input features in other\nalignment-independent methods, the PC-mer method exhibits competitive\nperformance when compared to the aforementioned methods especially when input\nsequences are very varied.","publication_date":1700558385,"paper_link":"http://arxiv.org/pdf/2311.12898v1","categories":["Quantitative Biology"],"abstract":"Background: Sequence comparison is essential in bioinformatics, serving various purposes such as taxonomy, functional inference, and drug discovery. The traditional method of aligning sequences for comparison is time-consuming, especially with large datasets. To overcome this, alignment-free methods have emerged as an alternative approach, prioritizing comparison scores over alignment itself. These methods directly compare sequences without the need for alignment. However, accurately representing the relationships between sequences is a significant challenge in the design of these tools. Methods:One of the alignment-free comparison approaches utilizes the frequency of fixed-length substrings, known as K-mers, which serves as the foundation for many sequence comparison methods. However, a challenge arises in these methods when increasing the length of the substring (K), as it leads to an exponential growth in the number of possible states. In this work, we explore the PC-mer method, which utilizes a more limited set of words that experience slower growth 2^k instead of 4^k compared to K. We conducted a comparison of sequences and evaluated how the reduced input vector size influenced the performance of the PC-mer method. Results: For the evaluation, we selected the Clustal Omega method as our reference approach, alongside three alignment-free methods: kmacs, FFP, and alfpy (word count). These methods also leverage the frequency of K-mers. We applied all five methods to 9 datasets for comprehensive analysis. The results were compared using phylogenetic trees and metrics such as Robinson-Foulds and normalized quartet distance (nQD). Conclusion: Our findings indicate that, unlike reducing the input features in other alignment-independent methods, the PC-mer method exhibits competitive performance when compared to the aforementioned methods especially when input sequences are very varied."}
{"title":"Extracting Definienda in Mathematical Scholarly Articles with Transformers","authors":["Shufan Jiang","Pierre Senellart"],"raw_abstract":"We consider automatically identifying the defined term within a mathematical\ndefinition from the text of an academic article. Inspired by the development of\ntransformer-based natural language processing applications, we pose the problem\nas (a) a token-level classification task using fine-tuned pre-trained\ntransformers; and (b) a question-answering task using a generalist large\nlanguage model (GPT). We also propose a rule-based approach to build a labeled\ndataset from the LATEX source of papers. Experimental results show that it is\npossible to reach high levels of precision and recall using either recent (and\nexpensive) GPT 4 or simpler pre-trained models fine-tuned on our task.","publication_date":1700557137,"paper_link":"http://arxiv.org/pdf/2311.12448v1","categories":["Mathematics"],"abstract":"We consider automatically identifying the defined term within a mathematical definition from the text of an academic article. Inspired by the development of transformer-based natural language processing applications, we pose the problem as (a) a token-level classification task using fine-tuned pre-trained transformers; and (b) a question-answering task using a generalist large language model (GPT). We also propose a rule-based approach to build a labeled dataset from the LATEX source of papers. Experimental results show that it is possible to reach high levels of precision and recall using either recent (and expensive) GPT 4 or simpler pre-trained models fine-tuned on our task."}
{"title":"IMJENSE: Scan-specific Implicit Representation for Joint Coil Sensitivity and Image Estimation in Parallel MRI","authors":["Ruimin Feng","Qing Wu","Jie Feng","Huajun She","Chunlei Liu","Yuyao Zhang","Hongjiang Wei"],"raw_abstract":"Parallel imaging is a commonly used technique to accelerate magnetic\nresonance imaging (MRI) data acquisition. Mathematically, parallel MRI\nreconstruction can be formulated as an inverse problem relating the sparsely\nsampled k-space measurements to the desired MRI image. Despite the success of\nmany existing reconstruction algorithms, it remains a challenge to reliably\nreconstruct a high-quality image from highly reduced k-space measurements.\nRecently, implicit neural representation has emerged as a powerful paradigm to\nexploit the internal information and the physics of partially acquired data to\ngenerate the desired object. In this study, we introduced IMJENSE, a\nscan-specific implicit neural representation-based method for improving\nparallel MRI reconstruction. Specifically, the underlying MRI image and coil\nsensitivities were modeled as continuous functions of spatial coordinates,\nparameterized by neural networks and polynomials, respectively. The weights in\nthe networks and coefficients in the polynomials were simultaneously learned\ndirectly from sparsely acquired k-space measurements, without fully sampled\nground truth data for training. Benefiting from the powerful continuous\nrepresentation and joint estimation of the MRI image and coil sensitivities,\nIMJENSE outperforms conventional image or k-space domain reconstruction\nalgorithms. With extremely limited calibration data, IMJENSE is more stable\nthan supervised calibrationless and calibration-based deep-learning methods.\nResults show that IMJENSE robustly reconstructs the images acquired at\n5$\\mathbf{\\times}$ and 6$\\mathbf{\\times}$ accelerations with only 4 or 8\ncalibration lines in 2D Cartesian acquisitions, corresponding to 22.0% and\n19.5% undersampling rates. The high-quality results and scanning specificity\nmake the proposed method hold the potential for further accelerating the data\nacquisition of parallel MRI.","publication_date":1700551451,"paper_link":"http://arxiv.org/pdf/2311.12892v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"Parallel imaging is a commonly used technique to accelerate magnetic resonance imaging (MRI) data acquisition. Mathematically, parallel MRI reconstruction can be formulated as an inverse problem relating the sparsely sampled k-space measurements to the desired MRI image. Despite the success of many existing reconstruction algorithms, it remains a challenge to reliably reconstruct a high-quality image from highly reduced k-space measurements. Recently, implicit neural representation has emerged as a powerful paradigm to exploit the internal information and the physics of partially acquired data to generate the desired object. In this study, we introduced IMJENSE, a scan-specific implicit neural representation-based method for improving parallel MRI reconstruction. Specifically, the underlying MRI image and coil sensitivities were modeled as continuous functions of spatial coordinates, parameterized by neural networks and polynomials, respectively. The weights in the networks and coefficients in the polynomials were simultaneously learned directly from sparsely acquired k-space measurements, without fully sampled ground truth data for training. Benefiting from the powerful continuous representation and joint estimation of the MRI image and coil sensitivities, IMJENSE outperforms conventional image or k-space domain reconstruction algorithms. With extremely limited calibration data, IMJENSE is more stable than supervised calibrationless and calibration-based deep-learning methods. Results show that IMJENSE robustly reconstructs the images acquired at 5__FORMULA__ and 6__FORMULA__ accelerations with only 4 or 8 calibration lines in 2D Cartesian acquisitions, corresponding to 22.0% and 19.5% undersampling rates. The high-quality results and scanning specificity make the proposed method hold the potential for further accelerating the data acquisition of parallel MRI."}
{"title":"Limiting spectral distribution of random self-adjoint quantum channels","authors":["C\u00e9cilia Lancien","Patrick Oliveira Santos","Pierre Youssef"],"raw_abstract":"We study the limiting spectral distribution of quantum channels whose Kraus\noperators are sampled as $n\\times n$ random Hermitian matrices satisfying\ncertain assumptions. We show that when the Kraus rank goes to infinity with n,\nthe limiting spectral distribution (suitably rescaled) of the corresponding\nquantum channel coincides with the semi-circle distribution. When the Kraus\nrank is fixed, the limiting spectral distribution is no longer the semi-circle\ndistribution. It corresponds to an explicit law, which can also be described\nusing tools from free probability.","publication_date":1700547329,"paper_link":"http://arxiv.org/pdf/2311.12368v1","categories":["Mathematics","Physics"],"abstract":"We study the limiting spectral distribution of quantum channels whose Kraus operators are sampled as __FORMULA__ random Hermitian matrices satisfying certain assumptions. We show that when the Kraus rank goes to infinity with n, the limiting spectral distribution (suitably rescaled) of the corresponding quantum channel coincides with the semi-circle distribution. When the Kraus rank is fixed, the limiting spectral distribution is no longer the semi-circle distribution. It corresponds to an explicit law, which can also be described using tools from free probability."}
{"title":"Strongly Coupled Two-scale System with Nonlinear Dispersion: Weak Solvability and Numerical Simulation","authors":["Vishnu Raveendran","Surendra Nepal","Rainey Lyons","Michael Eden","Adrian Muntean"],"raw_abstract":"We investigate a two-scale system featuring an upscaled parabolic\ndispersion-reaction equation intimately linked to a family of elliptic cell\nproblems. The system is strongly coupled through a dispersion tensor, which\ndepends on the solutions to the cell problems, and via the cell problems\nthemselves, where the solution of the parabolic problem interacts nonlinearly\nwith the drift term. This particular mathematical structure is motivated by a\nrigorously derived upscaled reaction-diffusion-convection model that describes\nthe evolution of a population of interacting particles pushed by a large drift\nthrough an array of periodically placed obstacles (i.e., through a regular\nporous medium).\n  We prove the existence of weak solutions to our system by means of an\niterative scheme, where particular care is needed to ensure the uniform\npositivity of the dispersion tensor. Additionally, we use finite element-based\napproximations for the same iteration scheme to perform multiple simulation\nstudies. Finally, we highlight how the choice of micro-geometry (building the\nregular porous medium) and of the nonlinear drift coupling affects the\nmacroscopic dispersion of particles.","publication_date":1700525877,"paper_link":"http://arxiv.org/pdf/2311.12251v1","categories":["Mathematics"],"abstract":"We investigate a two-scale system featuring an upscaled parabolic dispersion-reaction equation intimately linked to a family of elliptic cell problems. The system is strongly coupled through a dispersion tensor, which depends on the solutions to the cell problems, and via the cell problems themselves, where the solution of the parabolic problem interacts nonlinearly with the drift term. This particular mathematical structure is motivated by a rigorously derived upscaled reaction-diffusion-convection model that describes the evolution of a population of interacting particles pushed by a large drift through an array of periodically placed obstacles (i.e., through a regular porous medium).   We prove the existence of weak solutions to our system by means of an iterative scheme, where particular care is needed to ensure the uniform positivity of the dispersion tensor. Additionally, we use finite element-based approximations for the same iteration scheme to perform multiple simulation studies. Finally, we highlight how the choice of micro-geometry (building the regular porous medium) and of the nonlinear drift coupling affects the macroscopic dispersion of particles."}
{"title":"Evolution of viscous vortex filaments and desingularization of the Biot-Savart integral","authors":["Marco A. Fontelos","Luis Vega"],"raw_abstract":"We consider a viscous fluid with kinematic viscosity $\\nu $ and initial data\nconsisting of a smooth closed vortex filament with circulation $\\Gamma $. We\nshow that, for short enough time, the solution consists of a deformed\nLamb-Oseen vortex whose center (a filament) follows the binormal flow dynamics\nplus leading order corrections that depend locally on the filament curvature\nand the nonlocal interactions with distant parts of the filament. In order to\nachieve this scale separation we require $\\Gamma /\\nu $ to be sufficiently\nsmall.","publication_date":1700525071,"paper_link":"http://arxiv.org/pdf/2311.12246v1","categories":["Mathematics","Physics"],"abstract":"We consider a viscous fluid with kinematic viscosity __FORMULA__ and initial data consisting of a smooth closed vortex filament with circulation __FORMULA__. We show that, for short enough time, the solution consists of a deformed Lamb-Oseen vortex whose center (a filament) follows the binormal flow dynamics plus leading order corrections that depend locally on the filament curvature and the nonlocal interactions with distant parts of the filament. In order to achieve this scale separation we require __FORMULA__ to be sufficiently small."}
{"title":"Sturm-Liouville systems for the survival probability in first-passage time problems","authors":["M. Dahlenburg","G. Pagnini"],"raw_abstract":"We derive a Sturm-Liouville system of equations for the exact calculation of\nthe survival probability in first-passage time problems. This system is the one\nassociated with the Wiener-Hopf integral equation obtained from the theory of\nrandom walks. The derived approach is an alternative to the existing literature\nand we tested it against direct calculations from both discrete- and\ncontinuous-time random walks in a manageable, but meaningful, example. Within\nthis framework, the Sparre Andersen theorem results to be a boundary condition\nfor the system.","publication_date":1700523163,"paper_link":"http://arxiv.org/pdf/2311.12240v1","categories":["Mathematics","Physics"],"abstract":"We derive a Sturm-Liouville system of equations for the exact calculation of the survival probability in first-passage time problems. This system is the one associated with the Wiener-Hopf integral equation obtained from the theory of random walks. The derived approach is an alternative to the existing literature and we tested it against direct calculations from both discrete- and continuous-time random walks in a manageable, but meaningful, example. Within this framework, the Sparre Andersen theorem results to be a boundary condition for the system."}
{"title":"Foundations of Quantum Information for Physical Chemistry","authors":["Weijun Wu","Gregory D. Scholes"],"raw_abstract":"Quantum information, a field in which great advances have been made in the\npast decades, now presents opportunities for chemistry. One roadblock to\nprogress, especially for experimental chemical science, is that new concepts\nand technical definitions need to be learned. In this paper, we review some\nbasic, but sometimes misunderstood, concepts of quantum information based on\nthe mathematical formulation of quantum mechanics that will be useful for\nchemists interested in discovering ways that chemistry can contribute to the\nquantum information field. We cover topics including qubits and their density\nmatrix formalism, quantum measurement as a quantum operation, information\ntheory, and entanglement. We focus on the difference between the concepts in\nthe quantum context and the classic context. We also discuss the relation and\ndistinction among entanglement, correlation, and coherence. We aim to clarify\nthe rigorous definition of these concepts, and then indicate some examples in\nphysical chemistry.","publication_date":1700522944,"paper_link":"http://arxiv.org/pdf/2311.12238v1","categories":["Physics"],"abstract":"Quantum information, a field in which great advances have been made in the past decades, now presents opportunities for chemistry. One roadblock to progress, especially for experimental chemical science, is that new concepts and technical definitions need to be learned. In this paper, we review some basic, but sometimes misunderstood, concepts of quantum information based on the mathematical formulation of quantum mechanics that will be useful for chemists interested in discovering ways that chemistry can contribute to the quantum information field. We cover topics including qubits and their density matrix formalism, quantum measurement as a quantum operation, information theory, and entanglement. We focus on the difference between the concepts in the quantum context and the classic context. We also discuss the relation and distinction among entanglement, correlation, and coherence. We aim to clarify the rigorous definition of these concepts, and then indicate some examples in physical chemistry."}
{"title":"Characterizing traces of processes defined by precedence and response constraints: an order theory approach","authors":["Mark Dukes","Anton Sohn"],"raw_abstract":"In this paper we consider a general system of activities that can, but do not\nhave to, occur. This system is governed by a set of two types of constraints:\nprecedence and response. A precedence constraint dictates that an activity can\nonly occur if it has been preceded by some other specified activity. Response\nconstraints are similarly defined. An execution of the system is a listing of\nactivities in the order they occur and which satisfies all constraints. Such\nsystems naturally arise in areas of theoretical computer science and decision\nscience. An outcome of the freedom with which activities can occur is that\nthere are many different possible executions, and gaining a combinatorial\ninsight into these is a non-trivial problem.\n  We characterize all of the ways in which such a system can be executed. Our\napproach uses order theory to provide a classification in terms of the linear\nextensions of posets constructed from the constraint sets. This\ncharacterization is essential in calculating the stakeholder utility metrics\nthat have been developed by the first author that allow for quantitative\ncomparisons of such systems/processes. It also allows for a better\nunderstanding of the theoretical backbone to these processes and their\ndeconstruction as a shuffle product of smaller systems.","publication_date":1700518749,"paper_link":"http://arxiv.org/pdf/2311.12218v1","categories":["Mathematics"],"abstract":"In this paper we consider a general system of activities that can, but do not have to, occur. This system is governed by a set of two types of constraints: precedence and response. A precedence constraint dictates that an activity can only occur if it has been preceded by some other specified activity. Response constraints are similarly defined. An execution of the system is a listing of activities in the order they occur and which satisfies all constraints. Such systems naturally arise in areas of theoretical computer science and decision science. An outcome of the freedom with which activities can occur is that there are many different possible executions, and gaining a combinatorial insight into these is a non-trivial problem.   We characterize all of the ways in which such a system can be executed. Our approach uses order theory to provide a classification in terms of the linear extensions of posets constructed from the constraint sets. This characterization is essential in calculating the stakeholder utility metrics that have been developed by the first author that allow for quantitative comparisons of such systems/processes. It also allows for a better understanding of the theoretical backbone to these processes and their deconstruction as a shuffle product of smaller systems."}
{"title":"Evolutionary Equations are $G$-compact","authors":["Kre\u0161imir Burazin","Marko Erceg","Marcus Waurick"],"raw_abstract":"We prove a compactness result related to $G$-convergence for autonomous\nevolutionary equations in the sense of Picard. Compared to previous work\nrelated to applications, we do not require any boundedness or regularity of the\nunderlying spatial domain; nor do we assume any periodicity or ergodicity\nassumption on the potentially oscillatory part. In terms of abstract\nevolutionary equations, we remove any compactness assumptions of the resolvent\nmodulo kernel of the spatial operator. To achieve the results, we introduced a\nslightly more general class of material laws. As a by-product, we also provide\na criterion for $G$-convergence for time-dependent equations solely in terms of\nstatic equations.","publication_date":1700518088,"paper_link":"http://arxiv.org/pdf/2311.12213v1","categories":["Mathematics","Physics"],"abstract":"We prove a compactness result related to __FORMULA__-convergence for autonomous evolutionary equations in the sense of Picard. Compared to previous work related to applications, we do not require any boundedness or regularity of the underlying spatial domain; nor do we assume any periodicity or ergodicity assumption on the potentially oscillatory part. In terms of abstract evolutionary equations, we remove any compactness assumptions of the resolvent modulo kernel of the spatial operator. To achieve the results, we introduced a slightly more general class of material laws. As a by-product, we also provide a criterion for __FORMULA__-convergence for time-dependent equations solely in terms of static equations."}
{"title":"Optimal Transport Divergences induced by Scoring Functions","authors":["Silvana M. Pesenti","Steven Vanduffel"],"raw_abstract":"We employ scoring functions, used in statistics for eliciting risk\nfunctionals, as cost functions in the Monge-Kantorovich (MK) optimal transport\nproblem. This gives raise to a rich variety of novel asymmetric MK divergences,\nwhich subsume the family of Bregman-Wasserstein divergences. We show that for\ndistributions on the real line, the comonotonic coupling is optimal for the\nmajority the new divergences. Specifically, we derive the optimal coupling of\nthe MK divergences induced by functionals including the mean, generalised\nquantiles, expectiles, and shortfall measures. Furthermore, we show that while\nany elicitable law-invariant convex risk measure gives raise to infinitely many\nMK divergences, the comonotonic coupling is simultaneously optimal.\n  The novel MK divergences, which can be efficiently calculated, open an array\nof applications in robust stochastic optimisation. We derive sharp bounds on\ndistortion risk measures under a Bregman-Wasserstein divergence constraint, and\nsolve for cost-efficient portfolio strategies under benchmark constraints.","publication_date":1700514046,"paper_link":"http://arxiv.org/pdf/2311.12183v1","categories":["Quantitative Finance","Statistics"],"abstract":"We employ scoring functions, used in statistics for eliciting risk functionals, as cost functions in the Monge-Kantorovich (MK) optimal transport problem. This gives raise to a rich variety of novel asymmetric MK divergences, which subsume the family of Bregman-Wasserstein divergences. We show that for distributions on the real line, the comonotonic coupling is optimal for the majority the new divergences. Specifically, we derive the optimal coupling of the MK divergences induced by functionals including the mean, generalised quantiles, expectiles, and shortfall measures. Furthermore, we show that while any elicitable law-invariant convex risk measure gives raise to infinitely many MK divergences, the comonotonic coupling is simultaneously optimal.   The novel MK divergences, which can be efficiently calculated, open an array of applications in robust stochastic optimisation. We derive sharp bounds on distortion risk measures under a Bregman-Wasserstein divergence constraint, and solve for cost-efficient portfolio strategies under benchmark constraints."}
{"title":"Novel exact solutions for PDEs with mixed boundary conditions","authors":["Mark Craddock","Martino Grasselli","Andrea Mazzoran"],"raw_abstract":"We develop methods for the solution of inhomogeneous Robin type boundary\nvalue problems (BVPs) that arise for certain linear parabolic Partial\nDifferential Equations (PDEs) on a half line, as well as a second order\ngeneralisation. We are able to obtain non-standard solutions to equations\narising in a range of areas, including mathematical finance, stochastic\nanalysis, hyperbolic geometry and mathematical physics. Our approach uses the\nodd and even Hilbert transforms. The solutions we obtain and the method itself\nseem to be new.","publication_date":1700513034,"paper_link":"http://arxiv.org/pdf/2311.12177v1","categories":["Mathematics","Quantitative Finance"],"abstract":"We develop methods for the solution of inhomogeneous Robin type boundary value problems (BVPs) that arise for certain linear parabolic Partial Differential Equations (PDEs) on a half line, as well as a second order generalisation. We are able to obtain non-standard solutions to equations arising in a range of areas, including mathematical finance, stochastic analysis, hyperbolic geometry and mathematical physics. Our approach uses the odd and even Hilbert transforms. The solutions we obtain and the method itself seem to be new."}
{"title":"Optimal Retirement Choice under Age-dependent Force of Mortality","authors":["Giorgio Ferrari","Shihao Zhu"],"raw_abstract":"This paper examines the retirement decision, optimal investment, and\nconsumption strategies under an age-dependent force of mortality. We formulate\nthe optimization problem as a combined stochastic control and optimal stopping\nproblem with a random time horizon, featuring three state variables: wealth,\nlabor income, and force of mortality. To address this problem, we transform it\ninto its dual form, which is a finite time horizon, three-dimensional\ndegenerate optimal stopping problem with interconnected dynamics. We establish\nthe existence of an optimal retirement boundary that splits the state space\ninto continuation and stopping regions. Regularity of the optimal stopping\nvalue function is derived and the boundary is proved to be Lipschitz\ncontinuous, and it is characterized as the unique solution to a nonlinear\nintegral equation, which we compute numerically. In the original coordinates,\nthe agent thus retires whenever her wealth exceeds an age-, labor income- and\nmortality-dependent transformed version of the optimal stopping boundary. We\nalso provide numerical illustrations of the optimal strategies, including the\nsensitivities of the optimal retirement boundary concerning the relevant\nmodel's parameters.","publication_date":1700512617,"paper_link":"http://arxiv.org/pdf/2311.12169v1","categories":["Mathematics","Quantitative Finance"],"abstract":"This paper examines the retirement decision, optimal investment, and consumption strategies under an age-dependent force of mortality. We formulate the optimization problem as a combined stochastic control and optimal stopping problem with a random time horizon, featuring three state variables: wealth, labor income, and force of mortality. To address this problem, we transform it into its dual form, which is a finite time horizon, three-dimensional degenerate optimal stopping problem with interconnected dynamics. We establish the existence of an optimal retirement boundary that splits the state space into continuation and stopping regions. Regularity of the optimal stopping value function is derived and the boundary is proved to be Lipschitz continuous, and it is characterized as the unique solution to a nonlinear integral equation, which we compute numerically. In the original coordinates, the agent thus retires whenever her wealth exceeds an age-, labor income- and mortality-dependent transformed version of the optimal stopping boundary. We also provide numerical illustrations of the optimal strategies, including the sensitivities of the optimal retirement boundary concerning the relevant model's parameters."}
{"title":"On anomalous diffusion in the Kraichnan model and correlated-in-time variants","authors":["Keefer Rowan"],"raw_abstract":"We provide a concise PDE-based proof of anomalous diffusion in the Kraichan\nmodel -- a stochastic, white-in-time model of passive scalar turbulence. That\nis, we show an exponential rate of $L^2$ decay in expectation of a passive\nscalar advected by a certain white-in-time, correlated-in-space,\ndivergence-free Gaussian field, uniform in the initial data and the diffusivity\nof the passive scalar. Additionally, we provide examples of correlated-in-time\nversions of the Kraichnan model which fail to exhibit anomalous diffusion\ndespite their (formal) white-in-time limits exhibiting anomalous diffusion. As\npart of this analysis, we prove that anomalous diffusion of a scalar advected\nby some flow implies non-uniqueness of the ODE trajectories of that flow.","publication_date":1700510294,"paper_link":"http://arxiv.org/pdf/2311.12147v1","categories":["Mathematics","Physics"],"abstract":"We provide a concise PDE-based proof of anomalous diffusion in the Kraichan model -- a stochastic, white-in-time model of passive scalar turbulence. That is, we show an exponential rate of __FORMULA__ decay in expectation of a passive scalar advected by a certain white-in-time, correlated-in-space, divergence-free Gaussian field, uniform in the initial data and the diffusivity of the passive scalar. Additionally, we provide examples of correlated-in-time versions of the Kraichnan model which fail to exhibit anomalous diffusion despite their (formal) white-in-time limits exhibiting anomalous diffusion. As part of this analysis, we prove that anomalous diffusion of a scalar advected by some flow implies non-uniqueness of the ODE trajectories of that flow."}
{"title":"Simplifying the Type $A$ Argyres-Douglas Landscape","authors":["Christopher Beem","Mario Martone","Matteo Sacchi","Palash Singh","Jake Stedman"],"raw_abstract":"A well-established organisational principle for Argyres--Douglas-type\n$\\mathcal{N}=2$ superconformal field theories in four dimensions is to\ncharacterise such theories by the data defining a(n irregular) Hitchin system\non $\\mathbb{CP}^1$. The dictionary between Hitchin system data and various\nfeatures of the corresponding SCFT has been studied extensively, but the\noverall structure of the resulting space of SCFTs still appears quite\ncomplicated. In this work, we systematically delineate a variety of\nsimplifications that arise within this class of constructions due to several\nlarge classes of isomorphisms between SCFTs associated with inequivalent\nHitchin system data (and their exactly marginal gaugings). We restrict to the\nmost studied class of theories, namely the type $A$ theories without outer\nautomorphism twists.","publication_date":1700507025,"paper_link":"http://arxiv.org/pdf/2311.12123v1","categories":["Mathematics","Physics"],"abstract":"A well-established organisational principle for Argyres--Douglas-type __FORMULA__ superconformal field theories in four dimensions is to characterise such theories by the data defining a(n irregular) Hitchin system on __FORMULA__. The dictionary between Hitchin system data and various features of the corresponding SCFT has been studied extensively, but the overall structure of the resulting space of SCFTs still appears quite complicated. In this work, we systematically delineate a variety of simplifications that arise within this class of constructions due to several large classes of isomorphisms between SCFTs associated with inequivalent Hitchin system data (and their exactly marginal gaugings). We restrict to the most studied class of theories, namely the type __FORMULA__ theories without outer automorphism twists."}
{"title":"A tensor network view of multiconfiguration time-dependent Hartree methods","authors":["Henrik R. Larsson"],"raw_abstract":"The multilayer multiconfiguration time-dependent Hartree (ML-MCTDH) method\nand the density matrix renormalization group (DMRG) are powerful workhorses\napplied mostly in different scientific fields. Although both methods are based\non tensor network states, very different mathematical languages are used for\ndescribing them. This severely limits knowledge transfer and sometimes leads to\nre-inventions of ideas well known in the other field. Here, we review ML-MCTDH\nand DMRG theory using both MCTDH expressions and tensor network diagrams. We\nderive the ML-MCTDH equations of motions using diagrams and compare them with\ntime-dependent and time-independent DMRG algorithms. We further review two\nselected recent advancements. The first advancement is related to optimizing\nunoccupied single-particle functions in MCTDH, which corresponds to subspace\nenrichment in the DMRG. The second one is related to finding optimal tree\nstructures and on highlighting similarities and differences of MCTDH and DMRG\ntheories. We hope that this contribution will foster more fruitful\ncross-fertilization of ideas between ML-MCTDH and DMRG.","publication_date":1700506802,"paper_link":"http://arxiv.org/pdf/2311.12103v1","categories":["Physics"],"abstract":"The multilayer multiconfiguration time-dependent Hartree (ML-MCTDH) method and the density matrix renormalization group (DMRG) are powerful workhorses applied mostly in different scientific fields. Although both methods are based on tensor network states, very different mathematical languages are used for describing them. This severely limits knowledge transfer and sometimes leads to re-inventions of ideas well known in the other field. Here, we review ML-MCTDH and DMRG theory using both MCTDH expressions and tensor network diagrams. We derive the ML-MCTDH equations of motions using diagrams and compare them with time-dependent and time-independent DMRG algorithms. We further review two selected recent advancements. The first advancement is related to optimizing unoccupied single-particle functions in MCTDH, which corresponds to subspace enrichment in the DMRG. The second one is related to finding optimal tree structures and on highlighting similarities and differences of MCTDH and DMRG theories. We hope that this contribution will foster more fruitful cross-fertilization of ideas between ML-MCTDH and DMRG."}
{"title":"The Hierarchy of Curvatures in Exceptional Geometry","authors":["Falk Hassler","Yuho Sakatani"],"raw_abstract":"Despite remarkable success in describing supergravity reductions and\nbackgrounds, generalized geometry and the closely related exceptional field\ntheory are still lacking a fundamental object of differential geometry, the\nRiemann tensor. We explain that to construct such a tensor, an as of yet\noverlooked hierarchy of connections is required. They complement the spin\nconnection with higher representations known from the tensor hierarchy of\ngauged supergravities. In addition to solving an important conceptual problem,\nthis idea allows to define and explicitly construct generalized homogeneous\nspaces. They are the underlying structures of generalized U-duality, admit\nconsistent truncations and provide a huge class of new backgrounds for flux\ncompactifications with non-trivial generalized structure groups.","publication_date":1700506800,"paper_link":"http://arxiv.org/pdf/2311.12095v1","categories":["Mathematics","Physics"],"abstract":"Despite remarkable success in describing supergravity reductions and backgrounds, generalized geometry and the closely related exceptional field theory are still lacking a fundamental object of differential geometry, the Riemann tensor. We explain that to construct such a tensor, an as of yet overlooked hierarchy of connections is required. They complement the spin connection with higher representations known from the tensor hierarchy of gauged supergravities. In addition to solving an important conceptual problem, this idea allows to define and explicitly construct generalized homogeneous spaces. They are the underlying structures of generalized U-duality, admit consistent truncations and provide a huge class of new backgrounds for flux compactifications with non-trivial generalized structure groups."}
{"title":"Series over fat partitions: matrix models and discrete ensembles","authors":["A. Yu. Orlov"],"raw_abstract":"We consider series over Young diagrams of products of Schur functions\n$s_{\\lambda\\cup\\lambda}$, marked with ``fat partitions'' $\\lambda\\cup\\lambda$,\nwhich appear in matrix models associated with ensembles of symplectic and\northogonal matrices and quaternion ensemble Ginibre. We consider mixed matrix\nmodels that also contain complex Ginibre ensembles labeled by graphs and the\nthree ensembles mentioned above. Cases are identified when the series of\nperturbations in coupling constants turn out to be tau functions of the DKP\nhierarchy introduced by the Kyoto school. This topic relates matrix models to\nrandom partitions - discrete symplectic ensemble and its modifications.","publication_date":1700506768,"paper_link":"http://arxiv.org/pdf/2311.12027v2","categories":["Mathematics","Physics"],"abstract":"We consider series over Young diagrams of products of Schur functions __FORMULA__, marked with ``fat partitions'' __FORMULA__, which appear in matrix models associated with ensembles of symplectic and orthogonal matrices and quaternion ensemble Ginibre. We consider mixed matrix models that also contain complex Ginibre ensembles labeled by graphs and the three ensembles mentioned above. Cases are identified when the series of perturbations in coupling constants turn out to be tau functions of the DKP hierarchy introduced by the Kyoto school. This topic relates matrix models to random partitions - discrete symplectic ensemble and its modifications."}
{"title":"Brownian bridge limit of path measures in the upper tail of KPZ models","authors":["Shirshendu Ganguly","Milind Hegde","Lingfu Zhang"],"raw_abstract":"For models in the KPZ universality class, such as the zero temperature model\nof planar last passage-percolation (LPP) and the positive temperature model of\ndirected polymers, its upper tail behavior has been a topic of recent interest,\nwith particular focus on the associated path measures (i.e., geodesics or\npolymers). For Exponential LPP, diffusive fluctuation had been established in\nBasu-Ganguly. In the directed landscape, the continuum limit of LPP, the\nlimiting Gaussianity at one point, as well as of related finite-dimensional\ndistributions of the KPZ fixed point, were established, using exact formulas in\nLiu and Wang-Liu. It was further conjectured in these works that the limit of\nthe corresponding geodesic should be a Brownian bridge. We prove it in both\nzero and positive temperatures; for the latter, neither the one-point limit nor\nthe scale of fluctuations was previously known. Instead of relying on formulas\n(which are still missing in the positive temperature literature), our arguments\nare geometric and probabilistic, using the results on the shape of the weight\nand free energy profiles under the upper tail from Ganguly-Hegde as a starting\npoint. Another key ingredient involves novel coalescence estimates, developed\nusing the recently discovered shift-invariance Borodin-Gorin-Wheeler in these\nmodels. Finally, our proof also yields insight into the structure of the\npolymer measure under the upper tail conditioning, establishing a quenched\nlocalization exponent around a random backbone.","publication_date":1700505775,"paper_link":"http://arxiv.org/pdf/2311.12009v1","categories":["Mathematics","Physics"],"abstract":"For models in the KPZ universality class, such as the zero temperature model of planar last passage-percolation (LPP) and the positive temperature model of directed polymers, its upper tail behavior has been a topic of recent interest, with particular focus on the associated path measures (i.e., geodesics or polymers). For Exponential LPP, diffusive fluctuation had been established in Basu-Ganguly. In the directed landscape, the continuum limit of LPP, the limiting Gaussianity at one point, as well as of related finite-dimensional distributions of the KPZ fixed point, were established, using exact formulas in Liu and Wang-Liu. It was further conjectured in these works that the limit of the corresponding geodesic should be a Brownian bridge. We prove it in both zero and positive temperatures; for the latter, neither the one-point limit nor the scale of fluctuations was previously known. Instead of relying on formulas (which are still missing in the positive temperature literature), our arguments are geometric and probabilistic, using the results on the shape of the weight and free energy profiles under the upper tail from Ganguly-Hegde as a starting point. Another key ingredient involves novel coalescence estimates, developed using the recently discovered shift-invariance Borodin-Gorin-Wheeler in these models. Finally, our proof also yields insight into the structure of the polymer measure under the upper tail conditioning, establishing a quenched localization exponent around a random backbone."}
{"title":"Differentiating by prime numbers","authors":["Jack Jeffries"],"raw_abstract":"We introduce $p$-derivations and give a few basic ways in which they act like\nderivatives by numbers.","publication_date":1700505539,"paper_link":"http://arxiv.org/pdf/2311.13551v1","categories":["Mathematics"],"abstract":"We introduce __FORMULA__-derivations and give a few basic ways in which they act like derivatives by numbers."}
{"title":"Algebras of graph functions","authors":["F\u00fcl\u00f6p Bazs\u00f3"],"raw_abstract":"Differential operators acting on functions defined on graphs by different\nstudies do not form a consistent framework for the analysis of real or complex\nfunctions in the sense that they do not satisfy the Leibniz rule of any order.\nIn this paper we propose a new family of operators that satisfy the Leibniz\nrule, and as special cases, produce the specific operators defined in the\nliterature, such as the graph difference and the graph Laplacian. We propose a\nframework to define the order of a differential operator consistently using the\nLeibniz rule in Lie algebraic setting.\n  Furthermore by identifying the space of functions defined on graph edges with\nthe tensor product of node functions we construct a Lie bialgebra of graph\nfunctions and reinterpret the difference operator as a co-bracket. As an\napplication, some explicit solutions of Schr\\\"odinger and Fokker-Planck\nequations are given.","publication_date":1700503939,"paper_link":"http://arxiv.org/pdf/2311.11978v1","categories":["Mathematics","Physics"],"abstract":"Differential operators acting on functions defined on graphs by different studies do not form a consistent framework for the analysis of real or complex functions in the sense that they do not satisfy the Leibniz rule of any order. In this paper we propose a new family of operators that satisfy the Leibniz rule, and as special cases, produce the specific operators defined in the literature, such as the graph difference and the graph Laplacian. We propose a framework to define the order of a differential operator consistently using the Leibniz rule in Lie algebraic setting.   Furthermore by identifying the space of functions defined on graph edges with the tensor product of node functions we construct a Lie bialgebra of graph functions and reinterpret the difference operator as a co-bracket. As an application, some explicit solutions of Schr\\\"odinger and Fokker-Planck equations are given."}
{"title":"Bozonized Momentum Distribution of a Fermi Gas via Friedrichs Diagrams","authors":["Sascha Lill"],"raw_abstract":"Recently, Benedikter and the author proved an approximate formula for the\nmomentum distribution of a 3d fermionic gas interacting by a short-range pair\npotential in the mean-field regime, within a trial state close to the ground\nstate. Here, we derive an exact formula for the momentum distribution in this\ntrial state, using a diagrammatic formalism due to Friedrichs. We further\ndemonstrate how the formula of Benedikter and the author arises from a\nrestriction of the contributing diagrams to those corresponding to a\nbosonization approximation.","publication_date":1700501282,"paper_link":"http://arxiv.org/pdf/2311.11945v1","categories":["Mathematics","Physics"],"abstract":"Recently, Benedikter and the author proved an approximate formula for the momentum distribution of a 3d fermionic gas interacting by a short-range pair potential in the mean-field regime, within a trial state close to the ground state. Here, we derive an exact formula for the momentum distribution in this trial state, using a diagrammatic formalism due to Friedrichs. We further demonstrate how the formula of Benedikter and the author arises from a restriction of the contributing diagrams to those corresponding to a bosonization approximation."}
{"title":"Collineation groups of octonionic and split-octonionic planes","authors":["Daniele Corradetti","Alessio Marrani","Francesco Zucconi"],"raw_abstract":"We present a Veronese formulation of the octonionic and split-octonionic\nprojective and hyperbolic planes. This formulation of the incidence planes\nhighlights the relationship between the Veronese vectors and the rank-1\nelements of the Albert algebras over octonions and split-octonions, yielding to\na clear formulation of the relationship with the real forms of the Lie groups\narising as collineation groups of these planes. The Veronesean representation\nalso provides a novel and minimal construction of the same octonionic and\nsplit-octonionic planes, by exploiting two symmetric composition algebras: the\nOkubo algebra and the paraoctonionic algebra. Besides the intrinsic\nmathematical relevance of this construction of the real forms of the\nCayley-Moufang plane, we expect this approach to have implications in all\nmathematical physics related with exceptional Lie Groups of type $G_{2},F_{4}$\nand $E_{6}$.","publication_date":1700498411,"paper_link":"http://arxiv.org/pdf/2311.11907v1","categories":["Mathematics","Physics"],"abstract":"We present a Veronese formulation of the octonionic and split-octonionic projective and hyperbolic planes. This formulation of the incidence planes highlights the relationship between the Veronese vectors and the rank-1 elements of the Albert algebras over octonions and split-octonions, yielding to a clear formulation of the relationship with the real forms of the Lie groups arising as collineation groups of these planes. The Veronesean representation also provides a novel and minimal construction of the same octonionic and split-octonionic planes, by exploiting two symmetric composition algebras: the Okubo algebra and the paraoctonionic algebra. Besides the intrinsic mathematical relevance of this construction of the real forms of the Cayley-Moufang plane, we expect this approach to have implications in all mathematical physics related with exceptional Lie Groups of type __FORMULA__ and __FORMULA__."}
{"title":"Deep learning complete intersection Calabi-Yau manifolds","authors":["Harold Erbin","Riccardo Finotello"],"raw_abstract":"We review advancements in deep learning techniques for complete intersection\nCalabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how to\nhandle algebraic topological data with machine learning. We first discuss\nmethodological aspects and data analysis, before describing neural networks\narchitectures. Then, we describe the state-of-the art accuracy in predicting\nHodge numbers. We include new results on extrapolating predictions from low to\nhigh Hodge numbers, and conversely.","publication_date":1700494659,"paper_link":"http://arxiv.org/pdf/2311.11847v1","categories":["Mathematics","Physics"],"abstract":"We review advancements in deep learning techniques for complete intersection Calabi-Yau (CICY) 3- and 4-folds, with the aim of understanding better how to handle algebraic topological data with machine learning. We first discuss methodological aspects and data analysis, before describing neural networks architectures. Then, we describe the state-of-the art accuracy in predicting Hodge numbers. We include new results on extrapolating predictions from low to high Hodge numbers, and conversely."}
{"title":"Few-shot Multispectral Segmentation with Representations Generated by Reinforcement Learning","authors":["Dilith Jayakody","Thanuja Ambegoda"],"raw_abstract":"The task of multispectral image segmentation (segmentation of images with\nnumerous channels/bands, each capturing a specific range of wavelengths of\nelectromagnetic radiation) has been previously explored in contexts with large\namounts of labeled data. However, these models tend not to generalize well to\ndatasets of smaller size. In this paper, we propose a novel approach for\nimproving few-shot segmentation performance on multispectral images using\nreinforcement learning to generate representations. These representations are\ngenerated in the form of mathematical expressions between channels and are\ntailored to the specific class being segmented. Our methodology involves\ntraining an agent to identify the most informative expressions, updating the\ndataset using these expressions, and then using the updated dataset to perform\nsegmentation. Due to the limited length of the expressions, the model receives\nuseful representations without any added risk of overfitting. We evaluate the\neffectiveness of our approach on several multispectral datasets and demonstrate\nits effectiveness in boosting the performance of segmentation algorithms.","publication_date":1700492656,"paper_link":"http://arxiv.org/pdf/2311.11827v1","categories":["Mathematics"],"abstract":"The task of multispectral image segmentation (segmentation of images with numerous channels/bands, each capturing a specific range of wavelengths of electromagnetic radiation) has been previously explored in contexts with large amounts of labeled data. However, these models tend not to generalize well to datasets of smaller size. In this paper, we propose a novel approach for improving few-shot segmentation performance on multispectral images using reinforcement learning to generate representations. These representations are generated in the form of mathematical expressions between channels and are tailored to the specific class being segmented. Our methodology involves training an agent to identify the most informative expressions, updating the dataset using these expressions, and then using the updated dataset to perform segmentation. Due to the limited length of the expressions, the model receives useful representations without any added risk of overfitting. We evaluate the effectiveness of our approach on several multispectral datasets and demonstrate its effectiveness in boosting the performance of segmentation algorithms."}
{"title":"Simultaneous Robot-World and Hand-Eye Calibration","authors":["Fadi Dornaika","Radu Horaud"],"raw_abstract":"Recently, Zhuang, Roth, \\& Sudhakar [1] proposed a method that allows\nsimultaneous computation of the rigid transformations from world frame to robot\nbase frame and from hand frame to camera frame. Their method attempts to solve\na homogeneous matrix equation of the form AX=ZB. They use quaternions to derive\nexplicit linear solutions for X and Z. In this short paper, we present two new\nsolutions that attempt to solve the homogeneous matrix equation mentioned\nabove: (i) a closed-form method which uses quaternion algebra and a positive\nquadratic error function associated with this representation and (ii) a method\nbased on non-linear constrained minimization and which simultaneously solves\nfor rotations and translations. These results may be useful to other problems\nthat can be formulated in the same mathematical form. We perform a sensitivity\nanalysis for both our two methods and the linear method developed by Zhuang et\nal. This analysis allows the comparison of the three methods. In the light of\nthis comparison the non-linear optimization method, which solves for rotations\nand translations simultaneously, seems to be the most stable one with respect\nto noise and to measurement errors.","publication_date":1700492122,"paper_link":"http://arxiv.org/pdf/2311.11818v1","categories":["Mathematics"],"abstract":"Recently, Zhuang, Roth, \\& Sudhakar [1] proposed a method that allows simultaneous computation of the rigid transformations from world frame to robot base frame and from hand frame to camera frame. Their method attempts to solve a homogeneous matrix equation of the form AX=ZB. They use quaternions to derive explicit linear solutions for X and Z. In this short paper, we present two new solutions that attempt to solve the homogeneous matrix equation mentioned above: (i) a closed-form method which uses quaternion algebra and a positive quadratic error function associated with this representation and (ii) a method based on non-linear constrained minimization and which simultaneously solves for rotations and translations. These results may be useful to other problems that can be formulated in the same mathematical form. We perform a sensitivity analysis for both our two methods and the linear method developed by Zhuang et al. This analysis allows the comparison of the three methods. In the light of this comparison the non-linear optimization method, which solves for rotations and translations simultaneously, seems to be the most stable one with respect to noise and to measurement errors."}
{"title":"Asymptotics for $d$-fold partition diamonds and related infinite products","authors":["Kathrin Bringmann","William Craig","Joshua Males"],"raw_abstract":"We prove an asymptotic formula for the number of $d$-fold partition diamonds\nof $n$ and their Schmidt-type counterparts. In order to do so, we study the\nasymptotic behavior of certain infinite products. We also remark on interesting\npotential connections with mathematical physics and Bloch groups.","publication_date":1700491114,"paper_link":"http://arxiv.org/pdf/2311.11805v1","categories":["Mathematics"],"abstract":"We prove an asymptotic formula for the number of __FORMULA__-fold partition diamonds of __FORMULA__ and their Schmidt-type counterparts. In order to do so, we study the asymptotic behavior of certain infinite products. We also remark on interesting potential connections with mathematical physics and Bloch groups."}
{"title":"Black hole thermodynamic free energy as $A$-discriminants","authors":["Mounir Nisse","Yen-Kheng Lim","Linus Chang"],"raw_abstract":"We show that the free energy $\\mathcal{F}$ and temperature $T$ of black\nholes, considered as a thermodynamic system, can be viewed as an\n$A$-discriminant of an appropriately-defined polynomial. As such, mathematical\nresults about $A$-discriminants may lead to implications about black hole\nthermodynamics. In particular, for static spacetimes with spherical, planar, or\nhyperbolic symmetry, the number of distinct thermodynamic phases depend on the\nnumber of distinct terms in the metric component $g_{tt}$. We prove that if\n$g_{tt}$ consists of $N_f$ distinct terms, then the $\\mathcal{F}$-$T$ curve\nconsists of $N_f-2$ cusps, which in turn leads to $N_f-1$ distinct\nthermodynamic phases. This result is applied to explicit examples of the\nSchwarzschild-AdS, Reissner--Nordstr\\\"{o}m, power-law Maxwell, and\nEuler--Heisenberg black holes.","publication_date":1700490868,"paper_link":"http://arxiv.org/pdf/2311.11801v1","categories":["Mathematics","Physics"],"abstract":"We show that the free energy __FORMULA__ and temperature __FORMULA__ of black holes, considered as a thermodynamic system, can be viewed as an __FORMULA__-discriminant of an appropriately-defined polynomial. As such, mathematical results about __FORMULA__-discriminants may lead to implications about black hole thermodynamics. In particular, for static spacetimes with spherical, planar, or hyperbolic symmetry, the number of distinct thermodynamic phases depend on the number of distinct terms in the metric component __FORMULA__. We prove that if __FORMULA__ consists of __FORMULA__ distinct terms, then the __FORMULA__-__FORMULA__ curve consists of __FORMULA__ cusps, which in turn leads to __FORMULA__ distinct thermodynamic phases. This result is applied to explicit examples of the Schwarzschild-AdS, Reissner--Nordstr\\\"{o}m, power-law Maxwell, and Euler--Heisenberg black holes."}
{"title":"The point insertion technique and open $r$-spin theories II: intersection theories in genus-zero","authors":["Ran J. Tessler","Yizhen Zhao"],"raw_abstract":"The papers [5, 3, 6, 19, 20] initiated the study of open $r$-spin and open\nFJRW intersection theories, and related them to integrable hierarchies and\nmirror symmetry. This paper uses a new technique, the point insertion\ntechnique, developed in the prequel [36], to define new open r-spin and open\nFJRW intersection theories. These new constructions provide potential\ncandidates for theories whose existence was conjectured before:\n  $\\bullet$ K. Hori [23] predicted the existence of open $r$-spin theory with\n$\\lfloor\\frac{r}{2}\\rfloor$ types of boundary states. The one constructed in\n[5, 3] has only one type of boundary state. In this work we describe\n$\\lfloor\\frac{r}{2}\\rfloor$ open $r$-spin theories, labelled by\n$\\mathfrak{h}\\in\\{0,\\ldots,\\lfloor\\frac{r}{2}\\rfloor-1\\},$ where the\n$\\mathfrak{h}$-th one has $\\mathfrak{h}+1$ boundary states. We prove that the\n$\\mathfrak{h}=0$ theory is equivalent to the [5, 3] construction, and calculate\nall intersection numbers for all these theories.\n  $\\bullet$ In [1] K. Aleshkin and C.C.M. Liu conjectured the existence of a\nquintic Fermat FJRW theory. We construct such an FJRW theory, and provide\nevidence that this is the conjectured theory.\n  We also explain how the point insertion technique can be used for\nconstructing other open enumerative theories, satisfying the same universal\nrecursions.","publication_date":1700489110,"paper_link":"http://arxiv.org/pdf/2311.11779v1","categories":["Mathematics","Physics"],"abstract":"The papers [5, 3, 6, 19, 20] initiated the study of open __FORMULA__-spin and open FJRW intersection theories, and related them to integrable hierarchies and mirror symmetry. This paper uses a new technique, the point insertion technique, developed in the prequel [36], to define new open r-spin and open FJRW intersection theories. These new constructions provide potential candidates for theories whose existence was conjectured before:   __FORMULA__ K. Hori [23] predicted the existence of open __FORMULA__-spin theory with __FORMULA__ types of boundary states. The one constructed in [5, 3] has only one type of boundary state. In this work we describe __FORMULA__ open __FORMULA__-spin theories, labelled by __FORMULA__ where the __FORMULA__-th one has __FORMULA__ boundary states. We prove that the __FORMULA__ theory is equivalent to the [5, 3] construction, and calculate all intersection numbers for all these theories.   __FORMULA__ In [1] K. Aleshkin and C.C.M. Liu conjectured the existence of a quintic Fermat FJRW theory. We construct such an FJRW theory, and provide evidence that this is the conjectured theory.   We also explain how the point insertion technique can be used for constructing other open enumerative theories, satisfying the same universal recursions."}
{"title":"Global Existence to a Class of Triangular Block Matrix Cross Diffusion Systems and the Spectral Gap Condition","authors":["Dung Le"],"raw_abstract":"We study the global existence of classical solutions to cross diffusion\nsystems of $m$ equations on $N$-dimensional domains ($m,N\\ge2$). The diffusion\nmatrix is a triangular block matrix with coupled entries. We establish that the\n$W^{1,p}$ norm of solutions for some $p>N$ does not blow up in finite time so\nthat the results in \\cite{Am2} is applicable. We will also show that the\nspectral gap condition in \\cite{dlebook,dlebook1} can be relaxed via a new\nresult on BMO norms in \\cite{dleBMO}.","publication_date":1700488499,"paper_link":"http://arxiv.org/pdf/2311.12087v1","categories":["Mathematics","Physics"],"abstract":"We study the global existence of classical solutions to cross diffusion systems of __FORMULA__ equations on __FORMULA__-dimensional domains (__FORMULA__). The diffusion matrix is a triangular block matrix with coupled entries. We establish that the __FORMULA__ norm of solutions for some __FORMULA__ does not blow up in finite time so that the results in Am2 is applicable. We will also show that the spectral gap condition in dlebook,dlebook1 can be relaxed via a new result on BMO norms in dleBMO."}
{"title":"$x-y$ duality in Topological Recursion for exponential variables via Quantum Dilogarithm","authors":["Alexander Hock"],"raw_abstract":"For a given spectral curve, the theory of topological recursion generates two\ndifferent families $\\omega_{g,n}$ and $\\omega_{g,n}^\\vee$ of\nmulti-differentials, which are for algebraic spectral curves related via the\nuniversal $x-y$ duality formula. We propose a formalism to extend the validity\nof the $x-y$ duality formula of topological recursion from algebraic curves to\nspectral curves with exponential variables of the form $e^x=F(e^y)$ or\n$e^x=F(y)e^{a y}$ with $F$ rational and $a$ some complex number, which was in\nprinciple already observed in \\cite{Dunin-Barkowski:2017zsd,Bychkov:2020yzy}.\nFrom topological recursion perspective the family $\\omega_{g,n}^\\vee$ would be\ntrivial for these curves. However, we propose changing the $n=1$ sector of\n$\\omega_{g,n}^\\vee$ via a version of the Faddeev's quantum dilogarithm which\nwill lead to the correct two families $\\omega_{g,n}$ and $\\omega_{g,n}^\\vee$\nrelated by the same $x-y$ duality formula as for algebraic curves. As a\nconsequence, the $x-y$ symplectic transformation formula extends further to\nimportant examples governed by topological recursion including, for instance,\nthe topological vertex curve which computes Gromov-Witten invariants of\n$\\mathbb{C}^3$, equivalently triple Hodge integrals on the moduli space of\ncomplex curves, orbifold Hurwitz numbers, or stationary Gromov-Witten\ninvariants of $\\mathbb{P}^1$. The proposed formalism is related to the issue\ntopological recursion encounters for specific choices of framings for the\ntopological vertex curve.","publication_date":1700487638,"paper_link":"http://arxiv.org/pdf/2311.11761v1","categories":["Mathematics","Physics"],"abstract":"For a given spectral curve, the theory of topological recursion generates two different families __FORMULA__ and __FORMULA__ of multi-differentials, which are for algebraic spectral curves related via the universal __FORMULA__ duality formula. We propose a formalism to extend the validity of the __FORMULA__ duality formula of topological recursion from algebraic curves to spectral curves with exponential variables of the form __FORMULA__ or __FORMULA__ with __FORMULA__ rational and __FORMULA__ some complex number, which was in principle already observed in Dunin-Barkowski:2017zsd,Bychkov:2020yzy. From topological recursion perspective the family __FORMULA__ would be trivial for these curves. However, we propose changing the __FORMULA__ sector of __FORMULA__ via a version of the Faddeev's quantum dilogarithm which will lead to the correct two families __FORMULA__ and __FORMULA__ related by the same __FORMULA__ duality formula as for algebraic curves. As a consequence, the __FORMULA__ symplectic transformation formula extends further to important examples governed by topological recursion including, for instance, the topological vertex curve which computes Gromov-Witten invariants of __FORMULA__, equivalently triple Hodge integrals on the moduli space of complex curves, orbifold Hurwitz numbers, or stationary Gromov-Witten invariants of __FORMULA__. The proposed formalism is related to the issue topological recursion encounters for specific choices of framings for the topological vertex curve."}
{"title":"On the Congruency-Constrained Matroid Base","authors":["Siyue Liu","Chao Xu"],"raw_abstract":"Consider a matroid where all elements are labeled with an element from\n$\\mathbb{Z}_m$. We are interested in finding a base where the sum of the labels\nis congruent to $g \\pmod m$. We show that this problem can be solved in\n$O(m^{2m} n r \\log r)$ time for a matroid with $n$ elements and rank $r$, when\n$m$ is either the product of two primes or a prime power. The algorithm\ngeneralizes to all moduli, and in fact, to all abelian groups, if a classic\nadditive combinatorics conjecture of Schrijver and Seymour is true. We also\ndiscuss the optimization version of the problem.","publication_date":1700485132,"paper_link":"http://arxiv.org/pdf/2311.11737v1","categories":["Mathematics"],"abstract":"Consider a matroid where all elements are labeled with an element from __FORMULA__. We are interested in finding a base where the sum of the labels is congruent to __FORMULA__. We show that this problem can be solved in __FORMULA__ time for a matroid with __FORMULA__ elements and rank __FORMULA__, when __FORMULA__ is either the product of two primes or a prime power. The algorithm generalizes to all moduli, and in fact, to all abelian groups, if a classic additive combinatorics conjecture of Schrijver and Seymour is true. We also discuss the optimization version of the problem."}
{"title":"Exploring the benefits of DNA-target search with antenna","authors":["Ludvig Lizana"],"raw_abstract":"The most common gene regulation mechanism is when a protein binds to a\nregulatory sequence to change RNA transcription. However, these sequences are\nshort relative to the genome length, so finding them poses a challenging search\nproblem. This paper presents two mathematical frameworks capturing different\naspects of this problem. First, we study the interplay between diffusional flux\nthrough a target where the searching proteins get sequestered on DNA far from\nthe target because of non-specific interactions. From this model, we derive a\nsimple formula for the optimal protein-DNA unbinding rate, maximizing the\nparticle flux. Second, we study how the flux flows through a target on a single\nantenna with variable length. Here, we identify a non-trivial logarithmic\ncorrection to the linear behavior relative to the target size proposed by\nSmoluchowski's flux formula.","publication_date":1700484425,"paper_link":"http://arxiv.org/pdf/2311.11727v1","categories":["Physics"],"abstract":"The most common gene regulation mechanism is when a protein binds to a regulatory sequence to change RNA transcription. However, these sequences are short relative to the genome length, so finding them poses a challenging search problem. This paper presents two mathematical frameworks capturing different aspects of this problem. First, we study the interplay between diffusional flux through a target where the searching proteins get sequestered on DNA far from the target because of non-specific interactions. From this model, we derive a simple formula for the optimal protein-DNA unbinding rate, maximizing the particle flux. Second, we study how the flux flows through a target on a single antenna with variable length. Here, we identify a non-trivial logarithmic correction to the linear behavior relative to the target size proposed by Smoluchowski's flux formula."}
{"title":"Fast and Stable Credit Gamma of CVA","authors":["Roberto Daluiso"],"raw_abstract":"Credit Valuation Adjustment is a balance sheet item which is nowadays subject\nto active risk management by specialized traders. However, one of the most\nimportant risk factors, which is the vector of default intensities of the\ncounterparty, affects in a non-differentiable way the most general Monte Carlo\nestimator of the adjustment, through simulation of default times. Thus the\ncomputation of first and second order (pure and mixed) sensitivities involving\nthese inputs cannot rely on direct path-wise differentiation, while any\napproach involving finite differences shows very high statistical noise. We\npresent ad hoc analytical estimators which overcome these issues while offering\nvery low runtime overheads over the baseline computation of the price\nadjustment. We also discuss the conversion of the so-obtained sensitivities to\nmodel parameters (e.g. default intensities) into sensitivities to market quotes\n(e.g. Credit Default Swap spreads).","publication_date":1700478762,"paper_link":"http://arxiv.org/pdf/2311.11672v1","categories":["Quantitative Finance"],"abstract":"Credit Valuation Adjustment is a balance sheet item which is nowadays subject to active risk management by specialized traders. However, one of the most important risk factors, which is the vector of default intensities of the counterparty, affects in a non-differentiable way the most general Monte Carlo estimator of the adjustment, through simulation of default times. Thus the computation of first and second order (pure and mixed) sensitivities involving these inputs cannot rely on direct path-wise differentiation, while any approach involving finite differences shows very high statistical noise. We present ad hoc analytical estimators which overcome these issues while offering very low runtime overheads over the baseline computation of the price adjustment. We also discuss the conversion of the so-obtained sensitivities to model parameters (e.g. default intensities) into sensitivities to market quotes (e.g. Credit Default Swap spreads)."}
{"title":"Higgs-like (pseudo)Scalars in AdS$_4$, Marginal and Irrelevant Deformations in CFT_3 and Instantons on S^3","authors":["M. Naghhdi"],"raw_abstract":"With a 4-form ansatz of 11-dimensional supergravity over non-dynamical AdS$_4\n\\times S^7/Z_k$ background, with the internal space as a $S^1$ Hopf fibration\non CP$^3$, we get a consistent truncation. The (pseudo)scalars, in the\nresulting scalar equations in Euclidean AdS_4 space, may be viewed as arising\nfrom (anti)M-branes wrapping around internal directions in the (Wick-rotated)\nskew-whiffed M2-branes background (as the resulting theory is for\nanti-M2-branes) and so, realizing the modes after swapping the three\nfundamental representations $8_s, 8_c, 8_v$ of SO(8). Taking the backreaction\non the external and internal spaces, we get massless and massive modes,\ncorresponding to exactly marginal and marginally irrelevant deformations on the\nboundary CFT$_3$, and write a closed solution for the bulk equation and compute\nits correction to the background action. Next, considering the Higgs-like\n(breathing) mode $m^2=18$, having all supersymmetries, parity and\nscale-invariance broken, by solving the associated bulk equation with\nmathematical methods, especially the Adomian decomposition method, and\nanalyzing the behavior near the boundary of the solutions, we realize the\nboundary duals in SU(4) x U(1)-singlet sectors of the ABJM model. Then,\nintroducing new dual deformation $\\Delta_+$ = 3, 6 operators made of\nbi-fundamental scalars, fermions and U(1) gauge fields, we obtain\nSO(4)-invariant solutions as small instantons on a three-sphere with radius at\ninfinity, which actually correspond to collapsing bulk bubbles leading to\nbig-crunch singularities.","publication_date":1700478719,"paper_link":"http://arxiv.org/pdf/2311.11671v1","categories":["Physics"],"abstract":"With a 4-form ansatz of 11-dimensional supergravity over non-dynamical AdS__FORMULA__ background, with the internal space as a __FORMULA__ Hopf fibration on CP__FORMULA__, we get a consistent truncation. The (pseudo)scalars, in the resulting scalar equations in Euclidean AdS_4 space, may be viewed as arising from (anti)M-branes wrapping around internal directions in the (Wick-rotated) skew-whiffed M2-branes background (as the resulting theory is for anti-M2-branes) and so, realizing the modes after swapping the three fundamental representations __FORMULA__ of SO(8). Taking the backreaction on the external and internal spaces, we get massless and massive modes, corresponding to exactly marginal and marginally irrelevant deformations on the boundary CFT__FORMULA__, and write a closed solution for the bulk equation and compute its correction to the background action. Next, considering the Higgs-like (breathing) mode __FORMULA__, having all supersymmetries, parity and scale-invariance broken, by solving the associated bulk equation with mathematical methods, especially the Adomian decomposition method, and analyzing the behavior near the boundary of the solutions, we realize the boundary duals in SU(4) x U(1)-singlet sectors of the ABJM model. Then, introducing new dual deformation __FORMULA__ = 3, 6 operators made of bi-fundamental scalars, fermions and U(1) gauge fields, we obtain SO(4)-invariant solutions as small instantons on a three-sphere with radius at infinity, which actually correspond to collapsing bulk bubbles leading to big-crunch singularities."}
{"title":"Efficient learning of Sparse Pauli Lindblad models for fully connected qubit topology","authors":["Jose Este Jaloveckas","Minh Tham Pham Nguyen","Lilly Palackal","Jeanette Miriam Lorenz","Hans Ehm"],"raw_abstract":"The challenge to achieve practical quantum computing considering current\nhardware size and gate fidelity is the sensitivity to errors and noise. Recent\nwork has shown that by learning the underlying noise model capturing qubit\ncross-talk, error mitigation can push the boundary of practical quantum\ncomputing. This has been accomplished using Sparse Pauli-Lindblad models only\non devices with a linear topology connectivity (i.e. superconducting qubit\ndevices). In this work we extend the theoretical requirement for learning such\nnoise models on hardware with full connectivity (i.e. ion trap devices).","publication_date":1700474124,"paper_link":"http://arxiv.org/pdf/2311.11639v1","categories":["Mathematics","Physics"],"abstract":"The challenge to achieve practical quantum computing considering current hardware size and gate fidelity is the sensitivity to errors and noise. Recent work has shown that by learning the underlying noise model capturing qubit cross-talk, error mitigation can push the boundary of practical quantum computing. This has been accomplished using Sparse Pauli-Lindblad models only on devices with a linear topology connectivity (i.e. superconducting qubit devices). In this work we extend the theoretical requirement for learning such noise models on hardware with full connectivity (i.e. ion trap devices)."}
{"title":"The homology growth for finite abelian covers of smooth quasi-projective varieties","authors":["Fenglin Li","Yongqiang Liu"],"raw_abstract":"Let $X$ be a complex smooth quasi-projective variety with a fixed epimorphism\n$\\nu\\colon\\pi_1(X)\\twoheadrightarrow H$, where $H$ is a finitely generated\nabelian group with $\\mathrm{rank}H\\geq 1$. In this paper, we study the\nasymptotic behaviour of Betti numbers with all possible field coefficients and\nthe order of the torsion subgroup of singular homology associated to $\\nu$,\nknown as the $L^2$-type invariants. When $\\nu$ is orbifold effective, we give\nexplicit formulas of these invariants at degree 1. This generalizes the\nauthors' previous work for $H\\cong \\Z$.","publication_date":1700467795,"paper_link":"http://arxiv.org/pdf/2311.11593v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be a complex smooth quasi-projective variety with a fixed epimorphism __FORMULA__, where __FORMULA__ is a finitely generated abelian group with __FORMULA__. In this paper, we study the asymptotic behaviour of Betti numbers with all possible field coefficients and the order of the torsion subgroup of singular homology associated to __FORMULA__, known as the __FORMULA__-type invariants. When __FORMULA__ is orbifold effective, we give explicit formulas of these invariants at degree 1. This generalizes the authors' previous work for __FORMULA__."}
{"title":"Traveling phase interfaces in viscous forward-backward diffusion equations","authors":["Carina Geldhauser","Michael Herrmann","Dirk Jan\u00dfen"],"raw_abstract":"The viscous regularization of an ill-posed diffusion equation with bistable\nnonlinearity predicts a hysteretic behavior of dynamical phase transitions but\na complete mathematical \\mbox{understanding} of the intricate multiscale\nevolution is still missing. We shed light on the fine structure of\n\\mbox{propagating} phase boundaries by carefully examining traveling wave\nsolutions in a special case. Assuming a trilinear constitutive relation we\ncharacterize all waves that possess a monotone \\mbox{profile} and connect the\ntwo phases by a single interface of positive width. We further study the two\nsharp-interface regimes related to either vanishing viscosity or the bilinear\nlimit.","publication_date":1700464579,"paper_link":"http://arxiv.org/pdf/2311.11573v1","categories":["Mathematics"],"abstract":"The viscous regularization of an ill-posed diffusion equation with bistable nonlinearity predicts a hysteretic behavior of dynamical phase transitions but a complete mathematical understanding of the intricate multiscale evolution is still missing. We shed light on the fine structure of propagating phase boundaries by carefully examining traveling wave solutions in a special case. Assuming a trilinear constitutive relation we characterize all waves that possess a monotone profile and connect the two phases by a single interface of positive width. We further study the two sharp-interface regimes related to either vanishing viscosity or the bilinear limit."}
{"title":"VyZX: Formal Verification of a Graphical Quantum Language","authors":["Adrian Lehmann","Ben Caldwell","Bhakti Shah","Robert Rand"],"raw_abstract":"Mathematical representations of graphs often resemble adjacency matrices or\nlists, representations that facilitate whiteboard reasoning and algorithm\ndesign. In the realm of proof assistants, inductive representations effectively\ndefine semantics for formal reasoning. This highlights a gap where algorithm\ndesign and proof assistants require a fundamentally different structure of\ngraphs, particularly for process theories which represent programs using\ngraphs. To address this gap, we present VyZX, a verified library for reasoning\nabout inductively defined graphical languages. These inductive constructs arise\nnaturally from category theory definitions. A key goal for VyZX is to Verify\nthe ZX-calculus, a graphical language for reasoning about quantum computation.\nThe ZX-calculus comes with a collection of diagrammatic rewrite rules that\npreserve the graph's semantic interpretation. We show how inductive graphs in\nVyZX are used to prove the correctness of the ZX-calculus rewrite rules and\napply them in practice using standard proof assistant techniques. VyZX\nintegrates easily with the proof engineer's workflow through visualization and\nautomation.","publication_date":1700464342,"paper_link":"http://arxiv.org/pdf/2311.11571v1","categories":["Physics"],"abstract":"Mathematical representations of graphs often resemble adjacency matrices or lists, representations that facilitate whiteboard reasoning and algorithm design. In the realm of proof assistants, inductive representations effectively define semantics for formal reasoning. This highlights a gap where algorithm design and proof assistants require a fundamentally different structure of graphs, particularly for process theories which represent programs using graphs. To address this gap, we present VyZX, a verified library for reasoning about inductively defined graphical languages. These inductive constructs arise naturally from category theory definitions. A key goal for VyZX is to Verify the ZX-calculus, a graphical language for reasoning about quantum computation. The ZX-calculus comes with a collection of diagrammatic rewrite rules that preserve the graph's semantic interpretation. We show how inductive graphs in VyZX are used to prove the correctness of the ZX-calculus rewrite rules and apply them in practice using standard proof assistant techniques. VyZX integrates easily with the proof engineer's workflow through visualization and automation."}
{"title":"A Deep-Genetic Algorithm (Deep-GA) Approach for High-Dimensional Nonlinear Parabolic Partial Differential Equations","authors":["Endah Rokhmati Merdika Putri","Muhammad Luthfi Shahab","Mohammad Iqbal","Imam Mukhlash","Amirul Hakam","Lutfi Mardianto","Hadi Susanto"],"raw_abstract":"We propose a new method, called a deep-genetic algorithm (deep-GA), to\naccelerate the performance of the so-called deep-BSDE method, which is a deep\nlearning algorithm to solve high dimensional partial differential equations\nthrough their corresponding backward stochastic differential equations (BSDEs).\nRecognizing the sensitivity of the solver to the initial guess selection, we\nembed a genetic algorithm (GA) into the solver to optimize the selection. We\naim to achieve faster convergence for the nonlinear PDEs on a broader interval\nthan deep-BSDE. Our proposed method is applied to two nonlinear parabolic PDEs,\ni.e., the Black-Scholes (BS) equation with default risk and the\nHamilton-Jacobi-Bellman (HJB) equation. We compare the results of our method\nwith those of the deep-BSDE and show that our method provides comparable\naccuracy with significantly improved computational efficiency.","publication_date":1700462123,"paper_link":"http://arxiv.org/pdf/2311.11558v1","categories":["Mathematics"],"abstract":"We propose a new method, called a deep-genetic algorithm (deep-GA), to accelerate the performance of the so-called deep-BSDE method, which is a deep learning algorithm to solve high dimensional partial differential equations through their corresponding backward stochastic differential equations (BSDEs). Recognizing the sensitivity of the solver to the initial guess selection, we embed a genetic algorithm (GA) into the solver to optimize the selection. We aim to achieve faster convergence for the nonlinear PDEs on a broader interval than deep-BSDE. Our proposed method is applied to two nonlinear parabolic PDEs, i.e., the Black-Scholes (BS) equation with default risk and the Hamilton-Jacobi-Bellman (HJB) equation. We compare the results of our method with those of the deep-BSDE and show that our method provides comparable accuracy with significantly improved computational efficiency."}
{"title":"Generalised eigenfunction expansion and singularity expansion methods for two-dimensional acoustic time-domain wave scattering problems","authors":["Ben Wilks","Michael H. Meylan","Fabien Montiel","Sarah Wakes"],"raw_abstract":"Time-domain wave scattering in an unbounded two-dimensional acoustic medium\nby sound-hard scatterers is considered. Two canonical geometries, namely a\nsplit-ring resonator (SRR) and an array of cylinders, are used to highlight the\ntheory, which generalises to arbitrary scatterer geometries. The problem is\nsolved using the generalised eigenfunction expansion method (GEM), which\nexpresses the time-domain solution in terms of the frequency-domain solutions.\nA discrete GEM is proposed to numerically approximate the time-domain solution.\nIt relies on quadrature approximations of continuous integrals and can be\nthought of as a generalisation of the discrete Fourier transform. The solution\nthen takes a simple form in terms of direct matrix multiplications. In parallel\nto the GEM, the singularity expansion method (SEM) is also presented and\napplied to the two aforementioned geometries. It expands the time-domain\nsolution over a discrete set of unforced, complex resonant modes of the\nscatterer. Although the coefficients of this expansion are divergent integrals,\nwe introduce a method of regularising them using analytic continuation. The\nresults show that while the SEM is usually inaccurate at $t=0$, it converges\nrapidly to the GEM solution at all spatial points in the computational domain,\nwith the most rapid convergence occurring inside the resonant cavity.","publication_date":1700453035,"paper_link":"http://arxiv.org/pdf/2311.11524v1","categories":["Mathematics","Physics"],"abstract":"Time-domain wave scattering in an unbounded two-dimensional acoustic medium by sound-hard scatterers is considered. Two canonical geometries, namely a split-ring resonator (SRR) and an array of cylinders, are used to highlight the theory, which generalises to arbitrary scatterer geometries. The problem is solved using the generalised eigenfunction expansion method (GEM), which expresses the time-domain solution in terms of the frequency-domain solutions. A discrete GEM is proposed to numerically approximate the time-domain solution. It relies on quadrature approximations of continuous integrals and can be thought of as a generalisation of the discrete Fourier transform. The solution then takes a simple form in terms of direct matrix multiplications. In parallel to the GEM, the singularity expansion method (SEM) is also presented and applied to the two aforementioned geometries. It expands the time-domain solution over a discrete set of unforced, complex resonant modes of the scatterer. Although the coefficients of this expansion are divergent integrals, we introduce a method of regularising them using analytic continuation. The results show that while the SEM is usually inaccurate at __FORMULA__, it converges rapidly to the GEM solution at all spatial points in the computational domain, with the most rapid convergence occurring inside the resonant cavity."}
{"title":"Opportunities for Gas-Phase Science at Short-Wavelength Free-Electron Lasers with Undulator-Based Polarization Control","authors":["Markus Ilchen","Enrico Allaria","Primo\u017e Rebernik Ribi\u010d","Heinz-Dieter Nuhn","Alberto Lutman","Evgeny Schneidmiller","Markus Tischer","Mikail Yurkov","Marco Calvi","Eduard Prat","Sven Reiche","Thomas Schmidt","Gianluca Aldo Geloni","Suren Karabekyan","Jiawei Yan","Svitozar Serkez","Zhangfeng Gao","Bangjie Deng","Chao Feng","Haixiao Deng","Wolfram Helml","Lars Funke","Mats Larsson","Vitali","Zhaunerchyk","Michael Meyer","Tommaso Mazza","Till Jahnke","Reinhard Doerner","Francesca Calegari","Olga Smirnova","Caterina Vozzi","Giovanni De Ninno","Jonas Waetzel","Jamal Berakdar","Sadia","Bari","Lucas Schwob","J\u00e9r\u00e9my R. Rouxel","Shaul Mukamel","Klaus Bartschat","Kathryn Hamilton","Luca Argenti","Nicolas Douguet","Nikolay M. Novikovskiy","Philipp V. Demekhin","Peter Walter"],"raw_abstract":"Free-electron lasers (FELs) are the world's most brilliant light sources with\nrapidly evolving technological capabilities in terms of ultrabright and\nultrashort pulses over a large range of accessible photon energies. Their\nrevolutionary and innovative developments have opened new fields of science\nregarding nonlinear light-matter interaction, the investigation of ultrafast\nprocesses from specific observer sites, and approaches to imaging matter with\natomic resolution. A core aspect of FEL science is the study of isolated and\nprototypical systems in the gas phase with the possibility of addressing\nwell-defined electronic transitions or particular atomic sites in molecules.\nNotably for polarization-controlled short-wavelength FELs, the gas phase offers\nnew avenues for investigations of nonlinear and ultrafast phenomena in spin\norientated systems, for decoding the function of the chiral building blocks of\nlife as well as steering reactions and particle emission dynamics in otherwise\ninaccessible ways. This roadmap comprises descriptions of technological\ncapabilities of facilities worldwide, innovative diagnostics and\ninstrumentation, as well as recent scientific highlights, novel methodology and\nmathematical modeling. The experimental and theoretical landscape of using\npolarization controllable FELs for dichroic light-matter interaction in the gas\nphase will be discussed and comprehensively outlined to stimulate and\nstrengthen global collaborative efforts of all disciplines.","publication_date":1700452125,"paper_link":"http://arxiv.org/pdf/2311.11519v1","categories":["Physics"],"abstract":"Free-electron lasers (FELs) are the world's most brilliant light sources with rapidly evolving technological capabilities in terms of ultrabright and ultrashort pulses over a large range of accessible photon energies. Their revolutionary and innovative developments have opened new fields of science regarding nonlinear light-matter interaction, the investigation of ultrafast processes from specific observer sites, and approaches to imaging matter with atomic resolution. A core aspect of FEL science is the study of isolated and prototypical systems in the gas phase with the possibility of addressing well-defined electronic transitions or particular atomic sites in molecules. Notably for polarization-controlled short-wavelength FELs, the gas phase offers new avenues for investigations of nonlinear and ultrafast phenomena in spin orientated systems, for decoding the function of the chiral building blocks of life as well as steering reactions and particle emission dynamics in otherwise inaccessible ways. This roadmap comprises descriptions of technological capabilities of facilities worldwide, innovative diagnostics and instrumentation, as well as recent scientific highlights, novel methodology and mathematical modeling. The experimental and theoretical landscape of using polarization controllable FELs for dichroic light-matter interaction in the gas phase will be discussed and comprehensively outlined to stimulate and strengthen global collaborative efforts of all disciplines."}
{"title":"An Analysis of Triangulation in Geometrically Noisy Environments using Mathematics","authors":["Noah M. Kenney"],"raw_abstract":"This paper uses mathematics to analyze the challenges of geometrically noisy\nenvironments on triangulation. Given widely accepted algorithmic triangulation\nmethods, such as O (n ln n) or a simpler O (n^3) method, we can mathematically\nprove that triangulation of any two dimensional polygonal region is possible,\nalbeit impractical in some cases. Further, we consider the implications of\nenvironments in which a z-axis is present, as seen in cellular triangulation.\nIn many of the cases where consideration of the z-axis is necessary, we\nrecognize the absence of a fixed or known point of origin and consider methods\nof addressing this challenge.","publication_date":1700444979,"paper_link":"http://arxiv.org/pdf/2311.11480v1","categories":["Mathematics"],"abstract":"This paper uses mathematics to analyze the challenges of geometrically noisy environments on triangulation. Given widely accepted algorithmic triangulation methods, such as O (n ln n) or a simpler O (n^3) method, we can mathematically prove that triangulation of any two dimensional polygonal region is possible, albeit impractical in some cases. Further, we consider the implications of environments in which a z-axis is present, as seen in cellular triangulation. In many of the cases where consideration of the z-axis is necessary, we recognize the absence of a fixed or known point of origin and consider methods of addressing this challenge."}
{"title":"Conformally compact and higher conformal Yang-Mills equations","authors":["A. Rod Gover","Emanuele Latini","Andrew Waldron","Yongbing Zhang"],"raw_abstract":"For the Yang-Mills equations on the interior of a conformally compact\nmanifold we determine the appropriate boundary conditions, formal asymptotics,\nand describe Dirichlet-to-Neumann type maps. Given the \"magnetic\" Dirichlet\ndata of a boundary connection, by constructing a recursion for asymptotic\nsolutions, we find that smooth solutions are obstructed by a conformally\ninvariant, higher order boundary current. We study the asymptotics of the\ninterior Yang-Mills energy functional and show that the obstructing current is\nthe variation of the conformally invariant coefficient of the first log term in\nthis expansion. This provides higher derivative conformally invariant analogs\nof Yang-Mills equations and energies. The invariant energy is the anomaly for\nthe interior renormalized Yang-Mills energy. Global solutions to the magnetic\nboundary problem determine higher order \"electric\" Neumann data. This yields\nthe Dirichlet-to-Neumann map. We construct conformally invariant, higher\ntransverse derivative boundary operators. Acting on interior connections, they\n(i) give obstructions to solving the Yang-Mills boundary problem, (ii)\ndetermine the asymptotics of formal solutions, and (iii) yield conformally\ninvariant tensors capturing the (non-local) electric Neumann data. We also\ncharacterize a renormalized Yang-Mills action functional that encodes global\nfeatures analogously to the renormalized volume for Poincar\\'e-Einstein\nstructures.","publication_date":1700437769,"paper_link":"http://arxiv.org/pdf/2311.11458v1","categories":["Mathematics","Physics"],"abstract":"For the Yang-Mills equations on the interior of a conformally compact manifold we determine the appropriate boundary conditions, formal asymptotics, and describe Dirichlet-to-Neumann type maps. Given the \"magnetic\" Dirichlet data of a boundary connection, by constructing a recursion for asymptotic solutions, we find that smooth solutions are obstructed by a conformally invariant, higher order boundary current. We study the asymptotics of the interior Yang-Mills energy functional and show that the obstructing current is the variation of the conformally invariant coefficient of the first log term in this expansion. This provides higher derivative conformally invariant analogs of Yang-Mills equations and energies. The invariant energy is the anomaly for the interior renormalized Yang-Mills energy. Global solutions to the magnetic boundary problem determine higher order \"electric\" Neumann data. This yields the Dirichlet-to-Neumann map. We construct conformally invariant, higher transverse derivative boundary operators. Acting on interior connections, they (i) give obstructions to solving the Yang-Mills boundary problem, (ii) determine the asymptotics of formal solutions, and (iii) yield conformally invariant tensors capturing the (non-local) electric Neumann data. We also characterize a renormalized Yang-Mills action functional that encodes global features analogously to the renormalized volume for Poincar\\'e-Einstein structures."}
{"title":"Optimal control of thermal and mechanical loads in activation processes of mechanical components","authors":["Nicolai Friedlich","Hanno Gottschalk","Georg Vossen"],"raw_abstract":"This paper develops a mathematical framework that aims to control the\ntemperature and rotational speed in the activation process of a gas turbine in\nan optimal way. These controls influence the deformation and the stress in the\ncomponent due to centripetal loads and transient thermal stress. An optimal\ncontrol problem is formulated as the minimization of maximal von Mises stress\nover a given time and over the whole component. To find a solution for this, we\nneed to solve the linear thermoelasticity and the heat equations using the\nfinite element method. The results for the optimal control as functions of the\nrotation speed and external gas temperature over time are computed by\nsequential quadratic programming, where gradients are computed using finite\ndifferences.The overall outcome reveals a significant reduction of\napproximately 10\\%, from $830 \\frac{N}{mm^2}$ to $750 \\frac{N}{mm^2}$, in von\nMises stress by controlling two parameters, along with the temporal separation\nof physical control phenomena.","publication_date":1700430948,"paper_link":"http://arxiv.org/pdf/2311.11432v1","categories":["Mathematics"],"abstract":"This paper develops a mathematical framework that aims to control the temperature and rotational speed in the activation process of a gas turbine in an optimal way. These controls influence the deformation and the stress in the component due to centripetal loads and transient thermal stress. An optimal control problem is formulated as the minimization of maximal von Mises stress over a given time and over the whole component. To find a solution for this, we need to solve the linear thermoelasticity and the heat equations using the finite element method. The results for the optimal control as functions of the rotation speed and external gas temperature over time are computed by sequential quadratic programming, where gradients are computed using finite differences.The overall outcome reveals a significant reduction of approximately 10\\%, from __FORMULA__ to __FORMULA__, in von Mises stress by controlling two parameters, along with the temporal separation of physical control phenomena."}
{"title":"Make me an Offer: Forward and Reverse Auctioning Problems in the Tourism Industry","authors":["Ioannis T. Christou","Dimitris Doukas","Konstantina Skouri","Gerasimos Meletiou"],"raw_abstract":"Most tourist destinations are facing regular and consistent seasonality with\nsignificant economic and social impacts. This phenomenon is more pronounced in\nthe post-covid era, where demand for travel has increased but unevenly among\ndifferent geographic areas. To counter these problems that both customers and\nhoteliers are facing, we have developed two auctioning systems that allow\nhoteliers of lower popularity tier areas or during low season periods to\nauction their rooms in what we call a forward auction model, and also allows\ncustomers to initiate a bidding process whereby hoteliers in an area may make\noffers to the customer for their rooms, in what constitutes a reverse auction\nmodel initiated by the customer, similar to the bidding concept of\npriceline.com. We develop mathematical programming models that define\nexplicitly both types of auctions, and show that in each type, there are\nsignificant benefits to be gained both on the side of the hotelier as well as\non the side of the customer. We discuss algorithmic techniques for the\napproximate solution of these optimization problems, and present results using\nexact optimization solvers to solve them to guaranteed optimality. These\ntechniques could be beneficial to both customer and hotelier reducing\nseasonality during middle and low season and providing the customer with\nattractive offers.","publication_date":1700420664,"paper_link":"http://arxiv.org/pdf/2311.11400v1","categories":["Mathematics"],"abstract":"Most tourist destinations are facing regular and consistent seasonality with significant economic and social impacts. This phenomenon is more pronounced in the post-covid era, where demand for travel has increased but unevenly among different geographic areas. To counter these problems that both customers and hoteliers are facing, we have developed two auctioning systems that allow hoteliers of lower popularity tier areas or during low season periods to auction their rooms in what we call a forward auction model, and also allows customers to initiate a bidding process whereby hoteliers in an area may make offers to the customer for their rooms, in what constitutes a reverse auction model initiated by the customer, similar to the bidding concept of priceline.com. We develop mathematical programming models that define explicitly both types of auctions, and show that in each type, there are significant benefits to be gained both on the side of the hotelier as well as on the side of the customer. We discuss algorithmic techniques for the approximate solution of these optimization problems, and present results using exact optimization solvers to solve them to guaranteed optimality. These techniques could be beneficial to both customer and hotelier reducing seasonality during middle and low season and providing the customer with attractive offers."}
{"title":"Compatible structures of operads by polarization, their Koszul duality and Manin products","authors":["Huhu Zhang","Xing Gao","Li Guo"],"raw_abstract":"Algebraic structures with multiple copies of a given type of operations\ninterrelated by various compatibility conditions have long being studied in\nmathematics and mathematical physics. They are broadly referred as linearly\ncompatible, matching, and totally compatible structures. This paper gives a\nunified approach to these structures in the context of operads. We first\ngeneralize the process of polarization for polynomials in invariant theory to\nthe one for operads, leading to the general notion of linearly compatible\noperads. Refining the polarization by partitioning it into foliations, we\nobtain a notion of matching operads consolidating those appeared recently from\napplications of regularity structures, and Volterra integral equations.\nDistinguished among them is the leveled matching compatibility, which is unique\nwith respect to a fix ordering of the vertices in tree monomials. Equating all\nmatching compatibilities of a given operad leads to the totally compatible\noperad of this operad.\n  For unary/binary quadratic operads, the linear compatibility and the total\ncompatibility are in Koszul dual to each other, and there is a Koszul\nself-duality among the matching compatibilities. In particular, the leveled\nmatching compatibility is Koszul self-dual. For binary quadratic operads, these\nthree compatible operads can also be obtained by taking Manin black and white\nproducts. For finitely generated binary quadratic operad, we prove that the\nKoszulity is preserved under taking each of the linear and total\ncompatibilities, as well as the leveled matching compatibility.","publication_date":1700419186,"paper_link":"http://arxiv.org/pdf/2311.11394v1","categories":["Mathematics"],"abstract":"Algebraic structures with multiple copies of a given type of operations interrelated by various compatibility conditions have long being studied in mathematics and mathematical physics. They are broadly referred as linearly compatible, matching, and totally compatible structures. This paper gives a unified approach to these structures in the context of operads. We first generalize the process of polarization for polynomials in invariant theory to the one for operads, leading to the general notion of linearly compatible operads. Refining the polarization by partitioning it into foliations, we obtain a notion of matching operads consolidating those appeared recently from applications of regularity structures, and Volterra integral equations. Distinguished among them is the leveled matching compatibility, which is unique with respect to a fix ordering of the vertices in tree monomials. Equating all matching compatibilities of a given operad leads to the totally compatible operad of this operad.   For unary/binary quadratic operads, the linear compatibility and the total compatibility are in Koszul dual to each other, and there is a Koszul self-duality among the matching compatibilities. In particular, the leveled matching compatibility is Koszul self-dual. For binary quadratic operads, these three compatible operads can also be obtained by taking Manin black and white products. For finitely generated binary quadratic operad, we prove that the Koszulity is preserved under taking each of the linear and total compatibilities, as well as the leveled matching compatibility."}
{"title":"An Efficient Elliptic Curve Cryptography Arithmetic Using Nikhilam Multiplication","authors":["Prokash Barman","Banani Saha"],"raw_abstract":"Multiplication is one of the most important operation in Elliptic Curve\nCryptography (ECC) arithmetic. For point addition and point doubling in ECC\nscalar (integer) multiplication is required. In higher order classical\n(standard) multiplication many intermediate operations are required. Reduced\noperation in multiplication will increase the functional speed of ECC\narithmetic. These goals can be achieved using ancient multiplication algorithm\nnamely Nikhilam Sutra. Nikhilam Sutra is one of the Sutra (algorithm) within 16\nVedic mathematics Sutras (algorithms). Nikhilam Sutra is efficient for\nmultiplying two large decimal numbers. The Sutra reduces multiplication of two\nlarge numbers into two smaller numbers multiplication. The functional speed of\nElliptic Curve Cryptography can be increased using Nikhilam method for scalar\nmultiplication.","publication_date":1700418591,"paper_link":"http://arxiv.org/pdf/2311.11392v1","categories":["Mathematics"],"abstract":"Multiplication is one of the most important operation in Elliptic Curve Cryptography (ECC) arithmetic. For point addition and point doubling in ECC scalar (integer) multiplication is required. In higher order classical (standard) multiplication many intermediate operations are required. Reduced operation in multiplication will increase the functional speed of ECC arithmetic. These goals can be achieved using ancient multiplication algorithm namely Nikhilam Sutra. Nikhilam Sutra is one of the Sutra (algorithm) within 16 Vedic mathematics Sutras (algorithms). Nikhilam Sutra is efficient for multiplying two large decimal numbers. The Sutra reduces multiplication of two large numbers into two smaller numbers multiplication. The functional speed of Elliptic Curve Cryptography can be increased using Nikhilam method for scalar multiplication."}
{"title":"Quasi-invariant theorem on the Gaussian path space","authors":["Qinpin Chen","Jian Sun","Bo Wu"],"raw_abstract":"In this article, we will first introduce a class of Gaussian processes, and\nprove the quasi-invariant theorem with respect to the Gaussian Wiener measure,\nwhich is the law of the associated Gaussian process. In particular, it includes\nthe case of the fractional Brownian motion. As applications, we will establish\nthe integration by parts formula and Bismut-Elworthy-Li formula on the Gaussian\npath space, and by which some logarithmic Sobolev inequalities will be\npresented. Moreover, we will also provides some applications in the field of\nfinancial mathematics.","publication_date":1700408708,"paper_link":"http://arxiv.org/pdf/2311.11358v1","categories":["Mathematics"],"abstract":"In this article, we will first introduce a class of Gaussian processes, and prove the quasi-invariant theorem with respect to the Gaussian Wiener measure, which is the law of the associated Gaussian process. In particular, it includes the case of the fractional Brownian motion. As applications, we will establish the integration by parts formula and Bismut-Elworthy-Li formula on the Gaussian path space, and by which some logarithmic Sobolev inequalities will be presented. Moreover, we will also provides some applications in the field of financial mathematics."}
{"title":"p-adaptive discontinuous Galerkin method for the shallow water equations on heterogeneous computing architectures","authors":["Sara Faghih-Naini","Vadym Aizinger","Sebastian Kuckuk","Richard Angersbach","Harald K\u00f6stler"],"raw_abstract":"Heterogeneous computing and exploiting integrated CPU-GPU architectures has\nbecome a clear current trend since the flattening of Moore's Law. In this work,\nwe propose a numerical and algorithmic re-design of a p-adaptive\nquadrature-free discontinuous Galerkin method (DG) for the shallow water\nequations (SWE). Our new approach separates the computations of the\nnon-adaptive (lower-order) and adaptive (higher-order) parts of the\ndiscretization form each other. Thereby, we can overlap computations of the\nlower-order and the higher-order DG solution components. Furthermore, we\ninvestigate execution times of main computational kernels and use automatic\ncode generation to optimize their distribution between the CPU and GPU. Several\nsetups, including a prototype of a tsunami simulation in a tide-driven flow\nscenario, are investigated, and the results show that significant performance\nimprovements can be achieved in suitable setups.","publication_date":1700407199,"paper_link":"http://arxiv.org/pdf/2311.11348v1","categories":["Mathematics"],"abstract":"Heterogeneous computing and exploiting integrated CPU-GPU architectures has become a clear current trend since the flattening of Moore's Law. In this work, we propose a numerical and algorithmic re-design of a p-adaptive quadrature-free discontinuous Galerkin method (DG) for the shallow water equations (SWE). Our new approach separates the computations of the non-adaptive (lower-order) and adaptive (higher-order) parts of the discretization form each other. Thereby, we can overlap computations of the lower-order and the higher-order DG solution components. Furthermore, we investigate execution times of main computational kernels and use automatic code generation to optimize their distribution between the CPU and GPU. Several setups, including a prototype of a tsunami simulation in a tide-driven flow scenario, are investigated, and the results show that significant performance improvements can be achieved in suitable setups."}
{"title":"Equivalence of lattice operators and graph matrices","authors":["Jun Yumoto","Tatsuhiro Misumi"],"raw_abstract":"We explore the relationship between lattice field theory and graph theory,\nplacing special emphasis on the interplay between Dirac and scalar lattice\noperators and matrices within the realm of spectral graph theory. Beyond\ndelving into fundamental concepts of spectral graph theory, such as adjacency\nand Laplacian matrices, we introduce a novel matrix named as \"anti-symmetrized\nadjacency matrix\", specifically tailored for cycle digraphs ($T^1$ lattice) and\nsimple directed paths ($B^1$ lattice). The nontrivial relation between graph\ntheory matrices and lattice operators shows that the graph Laplacian matrix\nmirrors the lattice scalar operator and the Wilson term in lattice fermions,\nwhile the anti-symmetrized adjacency matrix, along with its extensions to\nhigher dimensions, are equivalent to naive lattice Dirac operators. Building\nupon these connections, we provide rigorous proofs for two key assertions: (i)\nThe count of zero-modes in a free lattice scalar operator coincides with the\nzeroth Betti number of the underlying graph (lattice). (ii) The maximum count\nof Dirac zero-modes in a free lattice fermion operator is equivalent to the\ncumulative sum of all Betti numbers when the $D$-dimensional graph results from\na cartesian product of cycle digraphs ($T^1$ lattice) and simple directed paths\n($B^1$ lattice).","publication_date":1700400653,"paper_link":"http://arxiv.org/pdf/2311.11320v1","categories":["Mathematics","Physics"],"abstract":"We explore the relationship between lattice field theory and graph theory, placing special emphasis on the interplay between Dirac and scalar lattice operators and matrices within the realm of spectral graph theory. Beyond delving into fundamental concepts of spectral graph theory, such as adjacency and Laplacian matrices, we introduce a novel matrix named as \"anti-symmetrized adjacency matrix\", specifically tailored for cycle digraphs (__FORMULA__ lattice) and simple directed paths (__FORMULA__ lattice). The nontrivial relation between graph theory matrices and lattice operators shows that the graph Laplacian matrix mirrors the lattice scalar operator and the Wilson term in lattice fermions, while the anti-symmetrized adjacency matrix, along with its extensions to higher dimensions, are equivalent to naive lattice Dirac operators. Building upon these connections, we provide rigorous proofs for two key assertions: (i) The count of zero-modes in a free lattice scalar operator coincides with the zeroth Betti number of the underlying graph (lattice). (ii) The maximum count of Dirac zero-modes in a free lattice fermion operator is equivalent to the cumulative sum of all Betti numbers when the __FORMULA__-dimensional graph results from a cartesian product of cycle digraphs (__FORMULA__ lattice) and simple directed paths (__FORMULA__ lattice)."}
{"title":"Canonical Quantization of the Scalar Field: The Measure Theoretic Perspective","authors":["Jos\u00e9 Velhinho"],"raw_abstract":"This review is devoted to measure theoretical methods in the canonical\nquantization of scalar field theories. We present in some detail the canonical\nquantization of the free scalar field. We study the measures associated with\nthe free fields and present two characterizations of the support of these\nmeasures. The first characterization concerns local properties of the quantum\nfields, whereas for the second one we introduce a sequence of variables that\ntest the field behaviour at large distances, thus allowing to distinguish\nbetween the typical quantum fields associated with different values of the\nmass.","publication_date":1700393926,"paper_link":"http://arxiv.org/pdf/2311.11304v1","categories":["Mathematics","Physics"],"abstract":"This review is devoted to measure theoretical methods in the canonical quantization of scalar field theories. We present in some detail the canonical quantization of the free scalar field. We study the measures associated with the free fields and present two characterizations of the support of these measures. The first characterization concerns local properties of the quantum fields, whereas for the second one we introduce a sequence of variables that test the field behaviour at large distances, thus allowing to distinguish between the typical quantum fields associated with different values of the mass."}
{"title":"Large Learning Rates Improve Generalization: But How Large Are We Talking About?","authors":["Ekaterina Lobacheva","Eduard Pockonechnyy","Maxim Kodryan","Dmitry Vetrov"],"raw_abstract":"Inspired by recent research that recommends starting neural networks training\nwith large learning rates (LRs) to achieve the best generalization, we explore\nthis hypothesis in detail. Our study clarifies the initial LR ranges that\nprovide optimal results for subsequent training with a small LR or weight\naveraging. We find that these ranges are in fact significantly narrower than\ngenerally assumed. We conduct our main experiments in a simplified setup that\nallows precise control of the learning rate hyperparameter and validate our key\nfindings in a more practical setting.","publication_date":1700393795,"paper_link":"http://arxiv.org/pdf/2311.11303v1","categories":["Statistics"],"abstract":"Inspired by recent research that recommends starting neural networks training with large learning rates (LRs) to achieve the best generalization, we explore this hypothesis in detail. Our study clarifies the initial LR ranges that provide optimal results for subsequent training with a small LR or weight averaging. We find that these ranges are in fact significantly narrower than generally assumed. We conduct our main experiments in a simplified setup that allows precise control of the learning rate hyperparameter and validate our key findings in a more practical setting."}
{"title":"Square-root filtering via covariance SVD factors in the accurate continuous-discrete extended-cubature Kalman filter","authors":["Maria V. Kulikova","Gennady Yu. Kulikov"],"raw_abstract":"This paper continues our research devoted to an accurate nonlinear Bayesian\nfilters' design. Our solution implies numerical methods for solving ordinary\ndifferential equations (ODE) when propagating the mean and error covariance of\nthe dynamic state. The key idea is that an accurate implementation strategy\nimplies the methods with a discretization error control involved. This means\nthat the filters' moment differential equations are to be solved accurately,\ni.e. with negligible error. In this paper, we explore the continuous-discrete\nextended-cubature Kalman filter that is a hybrid method between Extended and\nCubature Kalman filters (CKF). Motivated by recent results obtained for the\ncontinuous-discrete CKF in Bayesian filtering realm, we propose the numerically\nstable (to roundoff) square-root approach within a singular value decomposition\n(SVD) for the hybrid filter. The new method is extensively tested on a few\napplication examples including stiff systems.","publication_date":1700392735,"paper_link":"http://arxiv.org/pdf/2311.11299v1","categories":["Mathematics"],"abstract":"This paper continues our research devoted to an accurate nonlinear Bayesian filters' design. Our solution implies numerical methods for solving ordinary differential equations (ODE) when propagating the mean and error covariance of the dynamic state. The key idea is that an accurate implementation strategy implies the methods with a discretization error control involved. This means that the filters' moment differential equations are to be solved accurately, i.e. with negligible error. In this paper, we explore the continuous-discrete extended-cubature Kalman filter that is a hybrid method between Extended and Cubature Kalman filters (CKF). Motivated by recent results obtained for the continuous-discrete CKF in Bayesian filtering realm, we propose the numerically stable (to roundoff) square-root approach within a singular value decomposition (SVD) for the hybrid filter. The new method is extensively tested on a few application examples including stiff systems."}
{"title":"Asymptotic behaviour for convection with anomalous diffusion","authors":["Brian Straughan","Antonio Barletta"],"raw_abstract":"We investigate the fully nonlinear model for convection in a Darcy porous\nmaterial where the diffusion is of anomalous type as recently proposed by\nBarletta. The fully nonlinear model is analysed but we allow for variable\ngravity or penetrative convection effects which result in spatially dependent\ncoefficients. This spatial dependence usually requires numerical solution even\nin the linearized case. In this work we demonstrate that regardless of the size\nof the Rayleigh number the perturbation solution will decay exponentially in\ntime for the superdiffusion case. In addition we establish a similar result for\nconvection in a bidisperse porous medium where both macro and micro porosity\neffects are present. Moreover, we demonstrate a similar result for\nthermosolutal convection.","publication_date":1700381928,"paper_link":"http://arxiv.org/pdf/2311.11264v1","categories":["Mathematics","Physics"],"abstract":"We investigate the fully nonlinear model for convection in a Darcy porous material where the diffusion is of anomalous type as recently proposed by Barletta. The fully nonlinear model is analysed but we allow for variable gravity or penetrative convection effects which result in spatially dependent coefficients. This spatial dependence usually requires numerical solution even in the linearized case. In this work we demonstrate that regardless of the size of the Rayleigh number the perturbation solution will decay exponentially in time for the superdiffusion case. In addition we establish a similar result for convection in a bidisperse porous medium where both macro and micro porosity effects are present. Moreover, we demonstrate a similar result for thermosolutal convection."}
{"title":"Tensor networks for interpretable and efficient quantum-inspired machine learning","authors":["Shi-Ju Ran","Gang Su"],"raw_abstract":"It is a critical challenge to simultaneously gain high interpretability and\nefficiency with the current schemes of deep machine learning (ML). Tensor\nnetwork (TN), which is a well-established mathematical tool originating from\nquantum mechanics, has shown its unique advantages on developing efficient\n``white-box'' ML schemes. Here, we give a brief review on the inspiring\nprogresses made in TN-based ML. On one hand, interpretability of TN ML is\naccommodated with the solid theoretical foundation based on quantum information\nand many-body physics. On the other hand, high efficiency can be rendered from\nthe powerful TN representations and the advanced computational techniques\ndeveloped in quantum many-body physics. With the fast development on quantum\ncomputers, TN is expected to conceive novel schemes runnable on quantum\nhardware, heading towards the ``quantum artificial intelligence'' in the\nforthcoming future.","publication_date":1700379458,"paper_link":"http://arxiv.org/pdf/2311.11258v1","categories":["Physics"],"abstract":"It is a critical challenge to simultaneously gain high interpretability and efficiency with the current schemes of deep machine learning (ML). Tensor network (TN), which is a well-established mathematical tool originating from quantum mechanics, has shown its unique advantages on developing efficient ``white-box'' ML schemes. Here, we give a brief review on the inspiring progresses made in TN-based ML. On one hand, interpretability of TN ML is accommodated with the solid theoretical foundation based on quantum information and many-body physics. On the other hand, high efficiency can be rendered from the powerful TN representations and the advanced computational techniques developed in quantum many-body physics. With the fast development on quantum computers, TN is expected to conceive novel schemes runnable on quantum hardware, heading towards the ``quantum artificial intelligence'' in the forthcoming future."}
{"title":"Sensitivity of robust optimization problems under drift and volatility uncertainty","authors":["Daniel Bartl","Ariel Neufeld","Kyunghyun Park"],"raw_abstract":"We examine optimization problems in which an investor has the opportunity to\ntrade in $d$ stocks with the goal of maximizing her worst-case cost of\ncumulative gains and losses. Here, worst-case refers to taking into account all\npossible drift and volatility processes for the stocks that fall within a\n$\\varepsilon$-neighborhood of predefined fixed baseline processes. Although\nsolving the worst-case problem for a fixed $\\varepsilon>0$ is known to be very\nchallenging in general, we show that it can be approximated as $\\varepsilon\\to\n0$ by the baseline problem (computed using the baseline processes) in the\nfollowing sense: Firstly, the value of the worst-case problem is equal to the\nvalue of the baseline problem plus $\\varepsilon$ times a correction term. This\ncorrection term can be computed explicitly and quantifies how sensitive a given\noptimization problem is to model uncertainty. Moreover, approximately optimal\ntrading strategies for the worst-case problem can be obtained using optimal\nstrategies from the corresponding baseline problem.","publication_date":1700375218,"paper_link":"http://arxiv.org/pdf/2311.11248v1","categories":["Mathematics","Quantitative Finance"],"abstract":"We examine optimization problems in which an investor has the opportunity to trade in __FORMULA__ stocks with the goal of maximizing her worst-case cost of cumulative gains and losses. Here, worst-case refers to taking into account all possible drift and volatility processes for the stocks that fall within a __FORMULA__-neighborhood of predefined fixed baseline processes. Although solving the worst-case problem for a fixed __FORMULA__ is known to be very challenging in general, we show that it can be approximated as __FORMULA__ by the baseline problem (computed using the baseline processes) in the following sense: Firstly, the value of the worst-case problem is equal to the value of the baseline problem plus __FORMULA__ times a correction term. This correction term can be computed explicitly and quantifies how sensitive a given optimization problem is to model uncertainty. Moreover, approximately optimal trading strategies for the worst-case problem can be obtained using optimal strategies from the corresponding baseline problem."}
{"title":"Basic concepts for the Kermack and McKendrick model with static heterogeneity","authors":["Hisashi Inaba"],"raw_abstract":"In this paper, we consider the infection-age-dependent Kermack--McKendrick\nmodel in which host individuals are distributed in a continuous state space. To\nprovide a mathematical foundation for the heterogeneous model, we develop a\n$L^1$-framework to formulate basic epidemiological concepts.\n  First, we show the mathematical well-posedness of the basic model under\nappropriate conditions allowing the unbounded parameters with non-compact\ndomain. Next we define the basic reproduction number and prove the pandemic\nthreshold theorem. We then present a systematic procedure to compute the\neffective reproduction number and the herd immunity threshold. Finally we give\nsome illustrative examples and concrete results by using the separable mixing\nassumption.","publication_date":1700375098,"paper_link":"http://arxiv.org/pdf/2311.11247v1","categories":["Quantitative Biology"],"abstract":"In this paper, we consider the infection-age-dependent Kermack--McKendrick model in which host individuals are distributed in a continuous state space. To provide a mathematical foundation for the heterogeneous model, we develop a __FORMULA__-framework to formulate basic epidemiological concepts.   First, we show the mathematical well-posedness of the basic model under appropriate conditions allowing the unbounded parameters with non-compact domain. Next we define the basic reproduction number and prove the pandemic threshold theorem. We then present a systematic procedure to compute the effective reproduction number and the herd immunity threshold. Finally we give some illustrative examples and concrete results by using the separable mixing assumption."}
{"title":"State-independent all-versus-nothing arguments","authors":["Boseong Kim","Samson Abramsky"],"raw_abstract":"Contextuality is a key feature of quantum information that challenges\nclassical intuitions, providing the basis for constructing explicit proofs of\nquantum advantage. While a number of evidences of quantum advantage are based\non the contextuality argument, the definition of contextuality is different in\neach research, causing incoherence in the establishment of instant connection\nbetween their results. In this report, we review the mathematical structure of\nsheaf-theoretic contextuality and extend this framework to explain\nKochen-Specker type contextuality. We first cover the definitions in\ncontextuality with detailed examples. Then, we state the all-versus-nothing\n(AvN) argument and define a state-independent AvN class. It is shown that\nKochen-Specker type contextuality, or contextuality in a partial closure, can\nbe translated into this framework by the partial closure of observables under\nthe multiplication of commuting measurements. Finally, we compare each case of\ncontextuality in an operator-side view, where the strict hierarchy of\ncontextuality class in a state-side view seems to merge into the\nstate-independent AvN class together with the partial closure formalism.\nOverall, this report provides a unified interpretation of contextuality by\nintegrating Kochen-Specker type notions into the state-independent AvN\nargument. The results present novel insights into contextuality, which pave the\nway for a coherent approach to constructing proofs of quantum advantage.","publication_date":1700366930,"paper_link":"http://arxiv.org/pdf/2311.11218v1","categories":["Mathematics","Physics"],"abstract":"Contextuality is a key feature of quantum information that challenges classical intuitions, providing the basis for constructing explicit proofs of quantum advantage. While a number of evidences of quantum advantage are based on the contextuality argument, the definition of contextuality is different in each research, causing incoherence in the establishment of instant connection between their results. In this report, we review the mathematical structure of sheaf-theoretic contextuality and extend this framework to explain Kochen-Specker type contextuality. We first cover the definitions in contextuality with detailed examples. Then, we state the all-versus-nothing (AvN) argument and define a state-independent AvN class. It is shown that Kochen-Specker type contextuality, or contextuality in a partial closure, can be translated into this framework by the partial closure of observables under the multiplication of commuting measurements. Finally, we compare each case of contextuality in an operator-side view, where the strict hierarchy of contextuality class in a state-side view seems to merge into the state-independent AvN class together with the partial closure formalism. Overall, this report provides a unified interpretation of contextuality by integrating Kochen-Specker type notions into the state-independent AvN argument. The results present novel insights into contextuality, which pave the way for a coherent approach to constructing proofs of quantum advantage."}
{"title":"The general solutions for a non-isospectral integrable TD hierarchy via the inverse scattering transform","authors":["Hongyi Zhang","Yufeng Zhang","Binlu Feng"],"raw_abstract":"A non-isospectral Lax pair is first introduced from which a kind of\nnon-isospectral integrable TD hierarchy is derived, whose reduction is an\nintegrable system called the non-isospectral integrable TD system. Then by\nusing the inverse scattering transform (IST) method, new general soliton\nsolutions for the non-isospectral integrable TD hierarchy are obtained. Because\nwe investigate soliton solutions of non-isospectral integrable systems by the\nIST method, a new Gel'fand-Levitan-Marchenko (GLM) equation needs to be\nconstructed. Finally, we explicitly obtain the exact solutions of the\nnon-isospectral integrable TD system. The method presented in the paper can be\nextensively applied to other integrable equations.","publication_date":1700362414,"paper_link":"http://arxiv.org/pdf/2311.11203v1","categories":["Mathematics","Physics"],"abstract":"A non-isospectral Lax pair is first introduced from which a kind of non-isospectral integrable TD hierarchy is derived, whose reduction is an integrable system called the non-isospectral integrable TD system. Then by using the inverse scattering transform (IST) method, new general soliton solutions for the non-isospectral integrable TD hierarchy are obtained. Because we investigate soliton solutions of non-isospectral integrable systems by the IST method, a new Gel'fand-Levitan-Marchenko (GLM) equation needs to be constructed. Finally, we explicitly obtain the exact solutions of the non-isospectral integrable TD system. The method presented in the paper can be extensively applied to other integrable equations."}
{"title":"Low-dimensional controllability of brain networks","authors":["Remy Ben Messaoud","Vincent Le Du","Brigitte Charlotte Kaufmann","Baptiste Couvy-Duchesne","Lara Migliaccio","Paolo Bartolomeo","Mario Chavez","Fabrizio De Vico Fallani"],"raw_abstract":"Network controllability is a powerful tool to study causal relationships in\ncomplex systems and identify the driver nodes for steering the network dynamics\ninto desired states. However, due to ill-posed conditions, results become\nunreliable when the number of drivers becomes too small compared to the network\nsize. This is a very common situation, particularly in real-world applications,\nwhere the possibility to access multiple nodes at the same time is limited by\ntechnological constraints, such as in the human brain. Although targeting\nsmaller network parts might improve accuracy, challenges may remain for\nextremely unbalanced situations, when for example there is one single driver.\nTo address this problem, we developed a mathematical framework that combines\nconcepts from spectral graph theory and modern network science. Instead of\ncontrolling the original network dynamics, we aimed to control its\nlow-dimensional embedding into the topological space derived from the network\nLaplacian. By performing extensive simulations on synthetic networks, we showed\nthat a relatively low number of projected components is enough to improve the\noverall control accuracy, notably when dealing with very few drivers. Based on\nthese findings, we introduced alternative low-dimensional controllability\nmetrics and used them to identify the main driver areas of the human connectome\nobtained from N=6134 healthy individuals in the UK-biobank cohort. Results\nrevealed previously unappreciated influential regions compared to standard\napproaches, enabled to draw control maps between distinct specialized\nlarge-scale brain systems, and yielded an anatomically-based understanding of\nhemispheric functional lateralization. Taken together, our results offered a\ntheoretically-grounded solution to deal with network controllability in\nreal-life applications and provided insights into the causal interactions of\nthe human brain.","publication_date":1700329592,"paper_link":"http://arxiv.org/pdf/2311.11132v2","categories":["Quantitative Biology"],"abstract":"Network controllability is a powerful tool to study causal relationships in complex systems and identify the driver nodes for steering the network dynamics into desired states. However, due to ill-posed conditions, results become unreliable when the number of drivers becomes too small compared to the network size. This is a very common situation, particularly in real-world applications, where the possibility to access multiple nodes at the same time is limited by technological constraints, such as in the human brain. Although targeting smaller network parts might improve accuracy, challenges may remain for extremely unbalanced situations, when for example there is one single driver. To address this problem, we developed a mathematical framework that combines concepts from spectral graph theory and modern network science. Instead of controlling the original network dynamics, we aimed to control its low-dimensional embedding into the topological space derived from the network Laplacian. By performing extensive simulations on synthetic networks, we showed that a relatively low number of projected components is enough to improve the overall control accuracy, notably when dealing with very few drivers. Based on these findings, we introduced alternative low-dimensional controllability metrics and used them to identify the main driver areas of the human connectome obtained from N=6134 healthy individuals in the UK-biobank cohort. Results revealed previously unappreciated influential regions compared to standard approaches, enabled to draw control maps between distinct specialized large-scale brain systems, and yielded an anatomically-based understanding of hemispheric functional lateralization. Taken together, our results offered a theoretically-grounded solution to deal with network controllability in real-life applications and provided insights into the causal interactions of the human brain."}
{"title":"Non-Abelian discrete Toda chains and related lattices","authors":["Irina Bobrova","Vladimir Retakh","Vladimir Rubtsov","Georgy Sharygin"],"raw_abstract":"We have derived a non-abelian analog for the two-dimensional discrete Toda\nlattice which possesses solutions in terms of quasideterminants and admits Lax\npairs of different forms. Its connection with non-abelian analogs for several\nwell-known (1+1) and one-dimensional lattices is discussed. In particular, we\nconsider a non-commutative analog of the scheme: discrete Toda equations\n$\\rightarrow$ Somos-$N$ sequences $\\rightarrow$ discrete Painlev\\'e equations.","publication_date":1700327500,"paper_link":"http://arxiv.org/pdf/2311.11124v1","categories":["Mathematics","Physics"],"abstract":"We have derived a non-abelian analog for the two-dimensional discrete Toda lattice which possesses solutions in terms of quasideterminants and admits Lax pairs of different forms. Its connection with non-abelian analogs for several well-known (1+1) and one-dimensional lattices is discussed. In particular, we consider a non-commutative analog of the scheme: discrete Toda equations __FORMULA__ Somos-__FORMULA__ sequences __FORMULA__ discrete Painlev\\'e equations."}
{"title":"Testing Intersecting and Union-Closed Families","authors":["Xi Chen","Anindya De","Yuhao Li","Shivam Nadimpalli","Rocco A. Servedio"],"raw_abstract":"Inspired by the classic problem of Boolean function monotonicity testing, we\ninvestigate the testability of other well-studied properties of combinatorial\nfinite set systems, specifically \\emph{intersecting} families and\n\\emph{union-closed} families. A function $f: \\{0,1\\}^n \\to \\{0,1\\}$ is\nintersecting (respectively, union-closed) if its set of satisfying assignments\ncorresponds to an intersecting family (respectively, a union-closed family) of\nsubsets of $[n]$.\n  Our main results are that -- in sharp contrast with the property of being a\nmonotone set system -- the property of being an intersecting set system, and\nthe property of being a union-closed set system, both turn out to be\ninformation-theoretically difficult to test. We show that:\n  $\\bullet$ For $\\epsilon \\geq \\Omega(1/\\sqrt{n})$, any non-adaptive two-sided\n$\\epsilon$-tester for intersectingness must make\n$2^{\\Omega(n^{1/4}/\\sqrt{\\epsilon})}$ queries. We also give a\n$2^{\\Omega(\\sqrt{n \\log(1/\\epsilon)})}$-query lower bound for non-adaptive\none-sided $\\epsilon$-testers for intersectingness.\n  $\\bullet$ For $\\epsilon \\geq 1/2^{\\Omega(n^{0.49})}$, any non-adaptive\ntwo-sided $\\epsilon$-tester for union-closedness must make\n$n^{\\Omega(\\log(1/\\epsilon))}$ queries.\n  Thus, neither intersectingness nor union-closedness shares the\n$\\mathrm{poly}(n,1/\\epsilon)$-query non-adaptive testability that is enjoyed by\nmonotonicity.\n  To complement our lower bounds, we also give a simple\n$\\mathrm{poly}(n^{\\sqrt{n\\log(1/\\epsilon)}},1/\\epsilon)$-query, one-sided,\nnon-adaptive algorithm for $\\epsilon$-testing each of these properties\n(intersectingness and union-closedness). We thus achieve nearly tight upper and\nlower bounds for two-sided testing of intersectingness when $\\epsilon =\n\\Theta(1/\\sqrt{n})$, and for one-sided testing of intersectingness when\n$\\epsilon=\\Theta(1).$","publication_date":1700326897,"paper_link":"http://arxiv.org/pdf/2311.11119v1","categories":["Mathematics"],"abstract":"Inspired by the classic problem of Boolean function monotonicity testing, we investigate the testability of other well-studied properties of combinatorial finite set systems, specifically intersecting families and union-closed families. A function __FORMULA__ is intersecting (respectively, union-closed) if its set of satisfying assignments corresponds to an intersecting family (respectively, a union-closed family) of subsets of __FORMULA__.   Our main results are that -- in sharp contrast with the property of being a monotone set system -- the property of being an intersecting set system, and the property of being a union-closed set system, both turn out to be information-theoretically difficult to test. We show that:   __FORMULA__ For __FORMULA__, any non-adaptive two-sided __FORMULA__-tester for intersectingness must make __FORMULA__ queries. We also give a __FORMULA__-query lower bound for non-adaptive one-sided __FORMULA__-testers for intersectingness.   __FORMULA__ For __FORMULA__, any non-adaptive two-sided __FORMULA__-tester for union-closedness must make __FORMULA__ queries.   Thus, neither intersectingness nor union-closedness shares the __FORMULA__-query non-adaptive testability that is enjoyed by monotonicity.   To complement our lower bounds, we also give a simple __FORMULA__-query, one-sided, non-adaptive algorithm for __FORMULA__-testing each of these properties (intersectingness and union-closedness). We thus achieve nearly tight upper and lower bounds for two-sided testing of intersectingness when __FORMULA__, and for one-sided testing of intersectingness when __FORMULA__"}
{"title":"On the inverse scattering transform to the discrete Hirota equation with nonzero boundary conditions","authors":["Guixian Wang","Xiu-Bin Wang","Bo Han"],"raw_abstract":"Under investigation in this work is the robust inverse scattering transform\nof the discrete Hirota equation with nonzero boundary conditions, which is\napplied to solve simultaneously arbitrary-order poles on the branch points and\nspectral singularities. Using the inverse scattering transform method, we\nconstruct the Darboux transformation but not with the limit progress, which is\nmore convenient than before. Several kinds of rational solutions are derived in\ndetail. These solutions contain W-shape solitons, breathers, high-order rogue\nwaves, and various interactions between solitons and breathers. Moreover, we\nanalyze some remarkable characteristics of rational solutions through graphics.\nOur results are useful to explain the related nonlinear wave phenomena.","publication_date":1700309289,"paper_link":"http://arxiv.org/pdf/2311.11048v1","categories":["Mathematics","Physics"],"abstract":"Under investigation in this work is the robust inverse scattering transform of the discrete Hirota equation with nonzero boundary conditions, which is applied to solve simultaneously arbitrary-order poles on the branch points and spectral singularities. Using the inverse scattering transform method, we construct the Darboux transformation but not with the limit progress, which is more convenient than before. Several kinds of rational solutions are derived in detail. These solutions contain W-shape solitons, breathers, high-order rogue waves, and various interactions between solitons and breathers. Moreover, we analyze some remarkable characteristics of rational solutions through graphics. Our results are useful to explain the related nonlinear wave phenomena."}
{"title":"Hadamard states for linearized gravity on spacetimes with compact Cauchy surfaces","authors":["Christian G\u00e9rard"],"raw_abstract":"We consider the quantization of linearized Einstein equations. We prove the\nexistence of Hadamard states in the harmonic gauge on any Einstein spacetime\nwith compact Cauchy surfaces.","publication_date":1700307039,"paper_link":"http://arxiv.org/pdf/2311.11043v1","categories":["Mathematics","Physics"],"abstract":"We consider the quantization of linearized Einstein equations. We prove the existence of Hadamard states in the harmonic gauge on any Einstein spacetime with compact Cauchy surfaces."}
{"title":"Bernstein polynomials throughout Reverse Mathematics","authors":["Sam Sanders"],"raw_abstract":"Reverse Mathematics (RM for short) is a program in the foundations of\nmathematics where the aim is to find the minimal axioms needed to prove a given\ntheorem of ordinary mathematics. Generally, the minimal axioms are equivalent\nto the theorem at hand, assuming a weak logical system called the base theory.\nMoreover, many (most?) theorems are either provable in the base theory or\nequivalent to one of four logical systems, together called the Big Five. For\ninstance, the Weierstrass approximation theorem, i.e. that a continuous\nfunction can be approximated uniformly by a sequence of polynomials, has been\nclassified in RM as being equivalent to weak Koenig's lemma, the second Big\nFive system. In this paper, we study approximation theorems for discontinuous\nfunctions via Bernstein polynomials from the literature. We obtain (many)\nequivalences between the latter and weak Koenig's lemma. We also show that\nslight variations of these approximation theorems fall far outside of the Big\nFive but fit in the recently developed RM of new 'big' systems, namely the\nuncountability of R, the enumeration principle for countable sets, the\npigeon-hole principle for measure, and the Baire category theorem. In\nconclusion, one equivalence in second-order RM, namely for the Weierstrass\napproximation theorem, gives rise to many equivalences in higher-order RM, and\nwe hope the case study in this paper can serve as a kind of template.","publication_date":1700305310,"paper_link":"http://arxiv.org/pdf/2311.11036v1","categories":["Mathematics"],"abstract":"Reverse Mathematics (RM for short) is a program in the foundations of mathematics where the aim is to find the minimal axioms needed to prove a given theorem of ordinary mathematics. Generally, the minimal axioms are equivalent to the theorem at hand, assuming a weak logical system called the base theory. Moreover, many (most?) theorems are either provable in the base theory or equivalent to one of four logical systems, together called the Big Five. For instance, the Weierstrass approximation theorem, i.e. that a continuous function can be approximated uniformly by a sequence of polynomials, has been classified in RM as being equivalent to weak Koenig's lemma, the second Big Five system. In this paper, we study approximation theorems for discontinuous functions via Bernstein polynomials from the literature. We obtain (many) equivalences between the latter and weak Koenig's lemma. We also show that slight variations of these approximation theorems fall far outside of the Big Five but fit in the recently developed RM of new 'big' systems, namely the uncountability of R, the enumeration principle for countable sets, the pigeon-hole principle for measure, and the Baire category theorem. In conclusion, one equivalence in second-order RM, namely for the Weierstrass approximation theorem, gives rise to many equivalences in higher-order RM, and we hope the case study in this paper can serve as a kind of template."}
{"title":"Quantum and Reality","authors":["Hisham Sati","Urs Schreiber"],"raw_abstract":"Formalizations of quantum information theory in category theory and type\ntheory, for the design of verifiable quantum programming languages, need to\nexpress its two fundamental characteristics: (1) parameterized linearity and\n(2) metricity. The first is naturally addressed by dependent-linearly typed\nlanguages such as Proto-Quipper or, following our recent observations: Linear\nHomotopy Type Theory (LHoTT). The second point has received much attention\n(only) in the form of semantics in \"dagger-categories\", where operator adjoints\nare axiomatized but their specification to Hermitian adjoints still needs to be\nimposed by hand.\n  We describe a natural emergence of Hermiticity which is rooted in principles\nof equivariant homotopy theory, lends itself to homotopically-typed languages\nand naturally connects to topological quantum states classified by twisted\nequivariant KR-theory. Namely, we observe that when the complex numbers are\nconsidered as a monoid internal to Z/2-equivariant real linear types, via\ncomplex conjugation, then (finite-dimensional) Hilbert spaces do become\nself-dual objects among internally-complex Real modules.\n  The point is that this construction of Hermitian forms requires of the\nambient linear type theory nothing further than a negative unit term of tensor\nunit type. We observe that just such a term is constructible in LHoTT, where it\ninterprets as an element of the infinity-group of units of the sphere spectrum,\ntying the foundations of quantum theory to homotopy theory. We close by\nindicating how this allows for encoding (and verifying) the unitarity of\nquantum gates and of quantum channels in quantum languages embedded into LHoTT.","publication_date":1700305212,"paper_link":"http://arxiv.org/pdf/2311.11035v1","categories":["Mathematics","Physics"],"abstract":"Formalizations of quantum information theory in category theory and type theory, for the design of verifiable quantum programming languages, need to express its two fundamental characteristics: (1) parameterized linearity and (2) metricity. The first is naturally addressed by dependent-linearly typed languages such as Proto-Quipper or, following our recent observations: Linear Homotopy Type Theory (LHoTT). The second point has received much attention (only) in the form of semantics in \"dagger-categories\", where operator adjoints are axiomatized but their specification to Hermitian adjoints still needs to be imposed by hand.   We describe a natural emergence of Hermiticity which is rooted in principles of equivariant homotopy theory, lends itself to homotopically-typed languages and naturally connects to topological quantum states classified by twisted equivariant KR-theory. Namely, we observe that when the complex numbers are considered as a monoid internal to Z/2-equivariant real linear types, via complex conjugation, then (finite-dimensional) Hilbert spaces do become self-dual objects among internally-complex Real modules.   The point is that this construction of Hermitian forms requires of the ambient linear type theory nothing further than a negative unit term of tensor unit type. We observe that just such a term is constructible in LHoTT, where it interprets as an element of the infinity-group of units of the sphere spectrum, tying the foundations of quantum theory to homotopy theory. We close by indicating how this allows for encoding (and verifying) the unitarity of quantum gates and of quantum channels in quantum languages embedded into LHoTT."}
{"title":"Which came first, set theory or logic?","authors":["J. Julian Pulgar\u00edn","Andr\u00e9s F. Uribe-Zapata"],"raw_abstract":"The construction of first-order logic and set theory gives rise to apparent\ncircularities of mutual dependence, making it unclear which can act as a\nself-contained starting point in the foundation of mathematics. In this paper,\nwe carry out a metatheoretic and finitistic development of a first-order\nlogical system in which we define formal set theory (ZFC). We contend that the\ntechniques employed in constructing this system offer a philosophically sound\nbasis for introducing ZFC and proceeding with the conventional formal\ndevelopment of mathematics. Lastly, by examining the chronology of the\naforementioned construction, we attempt to answer the titular question.","publication_date":1700304221,"paper_link":"http://arxiv.org/pdf/2311.11032v1","categories":["Mathematics"],"abstract":"The construction of first-order logic and set theory gives rise to apparent circularities of mutual dependence, making it unclear which can act as a self-contained starting point in the foundation of mathematics. In this paper, we carry out a metatheoretic and finitistic development of a first-order logical system in which we define formal set theory (ZFC). We contend that the techniques employed in constructing this system offer a philosophically sound basis for introducing ZFC and proceeding with the conventional formal development of mathematics. Lastly, by examining the chronology of the aforementioned construction, we attempt to answer the titular question."}
{"title":"Higher Topos Theory in Physics","authors":["Urs Schreiber"],"raw_abstract":"A brief exposition of the point of higher topos theory in (mathematical)\nphysics, commissioned for the Encyclopedia of Mathematical Physics 2nd ed.","publication_date":1700302620,"paper_link":"http://arxiv.org/pdf/2311.11026v1","categories":["Mathematics","Physics"],"abstract":"A brief exposition of the point of higher topos theory in (mathematical) physics, commissioned for the Encyclopedia of Mathematical Physics 2nd ed."}
{"title":"Real symmetric $\u03a6^4$-matrix model as Calogero-Moser model","authors":["Harald Grosse","Naoyuki Kanomata","Akifumi Sako","Raimar Wulkenhaar"],"raw_abstract":"We study a real symmetric $\\Phi^4$-matrix model whose kinetic term is given\nby $\\mathrm{Tr}( E \\Phi^2)$, where $E$ is a positive diagonal matrix without\ndegenerate eigenvalues. We show that the partition function of this matrix\nmodel corresponds to a zero-energy solution of a Sch\\\"odinger type equation\nwith Calogero-Moser Hamiltonian. A family of differential equations satisfied\nby the partition function is also obtained from the Virasoro algebra.","publication_date":1700282766,"paper_link":"http://arxiv.org/pdf/2311.10974v1","categories":["Mathematics","Physics"],"abstract":"We study a real symmetric __FORMULA__-matrix model whose kinetic term is given by __FORMULA__, where __FORMULA__ is a positive diagonal matrix without degenerate eigenvalues. We show that the partition function of this matrix model corresponds to a zero-energy solution of a Sch\\\"odinger type equation with Calogero-Moser Hamiltonian. A family of differential equations satisfied by the partition function is also obtained from the Virasoro algebra."}
{"title":"Robust Condition-Based Operations & Maintenance: Synergizing Multi-Asset Degradation Rate Interactions and Operations-Induced Degradation","authors":["Deniz Altinpulluk","Farnaz Fallahi","Murat Yildirim","Javad Feizollahi"],"raw_abstract":"Effective operations and maintenance (O&M) in modern production systems\nhinges on careful orchestration of economic and degradation dependencies across\na multitude of assets. While the economic dependencies are well studied,\ndegradation dependencies and their impact on system operations remain an open\nchallenge. To address this challenge, we model condition-based production and\nmaintenance decisions for multi-asset systems with degradation interactions.\nThere is a rich literature on condition-based O&M policies for single-asset\nsystems. These models fail to represent modern systems composed of multiple\ninteracting assets. We are providing the first O&M model to optimize O&M in\nmulti-asset systems with embedded decision-dependent degradation interactions.\nWe formulate robust optimization models that inherently capture degradation and\nfailure risks by embedding degradation signals via a set of constraints, and\nbuilding condition-based uncertainty sets to model probable degradation\nscenarios. We offer multiple reformulations and a solution algorithm to ensure\ncomputational scalability. Performance of the proposed O&M model is evaluated\nthrough extensive experiments, where the degradation is either emulated or\ntaken from vibration-based readings from a rotating machinery system. The\nproposed model provides significant improvements in terms of operation,\nmaintenance, and reliability metrics. Due to a myriad of dependencies across\nassets and decisions, it is often difficult to translate asset-level failure\npredictions to system-level O&M decisions. This challenge puts a significant\nbarrier to the return on investment in condition monitoring and smart\nmaintenance systems. Our approach offers a seamless integration of data-driven\nfailure modeling and mathematical programming to bridge the gap across\npredictive and prescriptive models.","publication_date":1700281820,"paper_link":"http://arxiv.org/pdf/2311.10966v1","categories":["Mathematics"],"abstract":"Effective operations and maintenance (O&M) in modern production systems hinges on careful orchestration of economic and degradation dependencies across a multitude of assets. While the economic dependencies are well studied, degradation dependencies and their impact on system operations remain an open challenge. To address this challenge, we model condition-based production and maintenance decisions for multi-asset systems with degradation interactions. There is a rich literature on condition-based O&M policies for single-asset systems. These models fail to represent modern systems composed of multiple interacting assets. We are providing the first O&M model to optimize O&M in multi-asset systems with embedded decision-dependent degradation interactions. We formulate robust optimization models that inherently capture degradation and failure risks by embedding degradation signals via a set of constraints, and building condition-based uncertainty sets to model probable degradation scenarios. We offer multiple reformulations and a solution algorithm to ensure computational scalability. Performance of the proposed O&M model is evaluated through extensive experiments, where the degradation is either emulated or taken from vibration-based readings from a rotating machinery system. The proposed model provides significant improvements in terms of operation, maintenance, and reliability metrics. Due to a myriad of dependencies across assets and decisions, it is often difficult to translate asset-level failure predictions to system-level O&M decisions. This challenge puts a significant barrier to the return on investment in condition monitoring and smart maintenance systems. Our approach offers a seamless integration of data-driven failure modeling and mathematical programming to bridge the gap across predictive and prescriptive models."}
{"title":"Journey of Hallucination-minimized Generative AI Solutions for Financial Decision Makers","authors":["Sohini Roychowdhury"],"raw_abstract":"Generative AI has significantly reduced the entry barrier to the domain of AI\nowing to the ease of use and core capabilities of automation, translation, and\nintelligent actions in our day to day lives. Currently, Large language models\n(LLMs) that power such chatbots are being utilized primarily for their\nautomation capabilities for software monitoring, report generation etc. and for\nspecific personalized question answering capabilities, on a limited scope and\nscale. One major limitation of the currently evolving family of LLMs is\n'hallucinations', wherein inaccurate responses are reported as factual.\nHallucinations are primarily caused by biased training data, ambiguous prompts\nand inaccurate LLM parameters, and they majorly occur while combining\nmathematical facts with language-based context. Thus, monitoring and\ncontrolling for hallucinations becomes necessary when designing solutions that\nare meant for decision makers. In this work we present the three major stages\nin the journey of designing hallucination-minimized LLM-based solutions that\nare specialized for the decision makers of the financial domain, namely:\nprototyping, scaling and LLM evolution using human feedback. These three stages\nand the novel data to answer generation modules presented in this work are\nnecessary to ensure that the Generative AI chatbots, autonomous reports and\nalerts are reliable and high-quality to aid key decision-making processes.","publication_date":1700279759,"paper_link":"http://arxiv.org/pdf/2311.10961v1","categories":["Mathematics"],"abstract":"Generative AI has significantly reduced the entry barrier to the domain of AI owing to the ease of use and core capabilities of automation, translation, and intelligent actions in our day to day lives. Currently, Large language models (LLMs) that power such chatbots are being utilized primarily for their automation capabilities for software monitoring, report generation etc. and for specific personalized question answering capabilities, on a limited scope and scale. One major limitation of the currently evolving family of LLMs is 'hallucinations', wherein inaccurate responses are reported as factual. Hallucinations are primarily caused by biased training data, ambiguous prompts and inaccurate LLM parameters, and they majorly occur while combining mathematical facts with language-based context. Thus, monitoring and controlling for hallucinations becomes necessary when designing solutions that are meant for decision makers. In this work we present the three major stages in the journey of designing hallucination-minimized LLM-based solutions that are specialized for the decision makers of the financial domain, namely: prototyping, scaling and LLM evolution using human feedback. These three stages and the novel data to answer generation modules presented in this work are necessary to ensure that the Generative AI chatbots, autonomous reports and alerts are reliable and high-quality to aid key decision-making processes."}
{"title":"Reveal the Mathematical Structures of Honeyword Security Metrics","authors":["Pengcheng Su","Haibo Cheng","Wenting Li","Ping Wang"],"raw_abstract":"Honeyword is a representative ``honey\" technique to detect intruders by\nluring them with decoy data. This kind of honey technique blends a primary\nobject (from distribution $P$) with decoy samples (from distribution $Q$). In\nthis research, we focus on two key Honeyword security metrics: the flatness\nfunction and the success-number function. Previous researchers are engaged in\ndesigning experimental methods to estimate their values. We've derived\ntheoretical formulas on both metrics of the strongest $\\mathcal{A}$ using the\noptimal guessing strategy, marking a first in the field.\n  The mathematical structures of these metrics are intriguing: the flatness\nfunction has an expression as\n$\\epsilon(i)=\\sum_{j=1}^{i}\\int_{0}^{+\\infty}\\tbinom{k-1}{j-1}\nf(x)G^{k-j}(x)(1-G(x))^{j-1}dx$. In particular, the most important one,\n$\\epsilon(1)$ is $\\frac{1}{k}(M-\\int_{0}^{M}G^k(x)dx)+b$, where $M=\\max_{x:\nQ(x)\\neq 0}\\frac{P(x)}{Q(x)}$, $b=\\sum_{x: Q(x)=0}P(x)$, and $G$ is a\ncumulative distribution function derived from $P$ and $Q$. This formula\nprovides a criterion to compare different honey distributions: the one with\nsmaller $M$ and $b$ is more satisfactory. The mathematical structure of the\nsuccess-number function is a series of convolutions with beta distribution\nkernels: $\\lambda_U(i)=U\\sum_{j=1}^{i}\\int_{\\frac{1}{k}}^{1}\n\\frac{\\phi(x)}{1-\\phi(x)} \\tbinom{U-1}{j-1} x^{U-j}(1-x)^{j-1}dx$, where $U$ is\nthe number of users in the system and $\\phi(x)$ is a monotonically increasing\nfunction. For further elaboration, we made some representative calculations.\nOur findings offer insights into security assessments for Honeyword and similar\nhoney techniques, contributing to enhanced security measures in these systems.","publication_date":1700279677,"paper_link":"http://arxiv.org/pdf/2311.10960v1","categories":["Mathematics"],"abstract":"Honeyword is a representative ``honey\" technique to detect intruders by luring them with decoy data. This kind of honey technique blends a primary object (from distribution __FORMULA__) with decoy samples (from distribution __FORMULA__). In this research, we focus on two key Honeyword security metrics: the flatness function and the success-number function. Previous researchers are engaged in designing experimental methods to estimate their values. We've derived theoretical formulas on both metrics of the strongest __FORMULA__ using the optimal guessing strategy, marking a first in the field.   The mathematical structures of these metrics are intriguing: the flatness function has an expression as __FORMULA__. In particular, the most important one, __FORMULA__ is __FORMULA__, where __FORMULA__, __FORMULA__, and __FORMULA__ is a cumulative distribution function derived from __FORMULA__ and __FORMULA__. This formula provides a criterion to compare different honey distributions: the one with smaller __FORMULA__ and __FORMULA__ is more satisfactory. The mathematical structure of the success-number function is a series of convolutions with beta distribution kernels: __FORMULA__, where __FORMULA__ is the number of users in the system and __FORMULA__ is a monotonically increasing function. For further elaboration, we made some representative calculations. Our findings offer insights into security assessments for Honeyword and similar honey techniques, contributing to enhanced security measures in these systems."}
{"title":"Approximate Boltzmann Distributions for Nonreversible Markov Chains","authors":["Jacob Calvert","Dana Randall"],"raw_abstract":"While powerful theories for the analysis of reversible Markov chains have\nenabled significant mathematical advances, nonequilibrium phenomena dominate\nthe sciences and nonequilibrium chains do not enjoy the same formal\nfoundations. For instance, the stationary distributions of reversible chains\nare fundamentally simpler than those of nonreversible chains because they are\nBoltzmann distributions -- they can be expressed in terms of a purely local\n\"free energy\" landscape, in analogy with equilibrium statistical physics. In\ngeneral, it is impossible to similarly represent the steady states of\nnonequilibrium physical systems in a purely local way. However, a series of\nrecent works on rattling theory (e.g., Chvykov et al., Science (2021)) provides\nstrong evidence that a broad class of such systems nevertheless exhibit\n\"approximate Boltzmann distributions,\" which allow some aspects of the global\ndistributions to be inferred, at least approximately, from local information.\n  We formalize the main claims of this physical theory to identify its hidden\nassumptions and demonstrate its basis in the theory of continuous-time Markov\nchains. To do so, we decompose an arbitrary stationary distribution $\\pi$ into\nits \"local\" part -- the exit rates $q$ out of each state -- and its \"global\"\npart -- the stationary distribution $\\psi$ of the embedded \"jump\" chain. We\nexplain a variety of experimental results by showing that, for a random state,\n$\\log \\pi$ and $-\\log q$ are correlated to the extent that $\\log \\psi$ and\n$-\\log q$ are correlated or the ratio of their variances is small. In\nparticular, the predictions of rattling theory apply when the global part of\n$\\pi$ varies over fewer scales than its local part. We use this fact to\ndemonstrate classes of nonreversible chains with stationary distributions that\nare exactly of Boltzmann type.","publication_date":1700278168,"paper_link":"http://arxiv.org/pdf/2311.10957v1","categories":["Mathematics","Physics"],"abstract":"While powerful theories for the analysis of reversible Markov chains have enabled significant mathematical advances, nonequilibrium phenomena dominate the sciences and nonequilibrium chains do not enjoy the same formal foundations. For instance, the stationary distributions of reversible chains are fundamentally simpler than those of nonreversible chains because they are Boltzmann distributions -- they can be expressed in terms of a purely local \"free energy\" landscape, in analogy with equilibrium statistical physics. In general, it is impossible to similarly represent the steady states of nonequilibrium physical systems in a purely local way. However, a series of recent works on rattling theory (e.g., Chvykov et al., Science (2021)) provides strong evidence that a broad class of such systems nevertheless exhibit \"approximate Boltzmann distributions,\" which allow some aspects of the global distributions to be inferred, at least approximately, from local information.   We formalize the main claims of this physical theory to identify its hidden assumptions and demonstrate its basis in the theory of continuous-time Markov chains. To do so, we decompose an arbitrary stationary distribution __FORMULA__ into its \"local\" part -- the exit rates __FORMULA__ out of each state -- and its \"global\" part -- the stationary distribution __FORMULA__ of the embedded \"jump\" chain. We explain a variety of experimental results by showing that, for a random state, __FORMULA__ and __FORMULA__ are correlated to the extent that __FORMULA__ and __FORMULA__ are correlated or the ratio of their variances is small. In particular, the predictions of rattling theory apply when the global part of __FORMULA__ varies over fewer scales than its local part. We use this fact to demonstrate classes of nonreversible chains with stationary distributions that are exactly of Boltzmann type."}
{"title":"Canonical Group Quantization of Noncommutative Graphene with Symmetric and Landau Dual Magnetic Fields","authors":["M. F. Umar","M. S. Nurisya"],"raw_abstract":"The canonical group quantization approach has been used to study\nnoncommutative graphene in the presence of dual magnetic fields. The canonical\ngroup for the phase space $\\mathbb{R}^2\\times \\mathbb{R}^2$ with both symmetric\nand Landau dual gauges is shown to be equivalent to $\\mathtt{H}^2\\rtimes\n\\mathbb{R}$. The representations of both symmetric and Landau dual gauges lead\nto similar canonical commutation relations, and we observe that the energy\nspectrum is corrected by both dual magnetic fields, yielding the same result.","publication_date":1700273909,"paper_link":"http://arxiv.org/pdf/2311.10939v1","categories":["Mathematics","Physics"],"abstract":"The canonical group quantization approach has been used to study noncommutative graphene in the presence of dual magnetic fields. The canonical group for the phase space __FORMULA__ with both symmetric and Landau dual gauges is shown to be equivalent to __FORMULA__. The representations of both symmetric and Landau dual gauges lead to similar canonical commutation relations, and we observe that the energy spectrum is corrected by both dual magnetic fields, yielding the same result."}
{"title":"Extreme quantum states and processes, and extreme points of general spectrahedra in finite dimensional algebras","authors":["Giulio Chiribella"],"raw_abstract":"Convex sets of quantum states and processes play a central role in quantum\ntheory and quantum information. Many important examples of convex sets in\nquantum theory are spectrahedra, that is, sets of positive operators subject to\naffine constraints. These examples include sets of quantum states with given\nexpectation values of some observables of interest, sets of multipartite\nquantum states with given marginals, sets of quantum measurements, channels,\nand multitime quantum processes, as well as sets of higher-order quantum maps\nand quantum causal structures. This contribution provides a characterisation of\nthe extreme points of general spectrahedra, and bounds on the ranks of the\ncorresponding operators. The general results are applied to several special\ncases, and are used to retrieve classic results such as Choi's characterisation\nof the extreme quantum channels, Parhasarathy's characterisation of the extreme\nquantum states with given marginals, and the quantum version of Birkhoff's\ntheorem for qubit unital channels. Finally, we propose a notion of positive\noperator valued measures (POVMs) with general affine constraints for their\nnormalisation, and we characterise the extremal POVMs.","publication_date":1700271076,"paper_link":"http://arxiv.org/pdf/2311.10929v1","categories":["Mathematics","Physics"],"abstract":"Convex sets of quantum states and processes play a central role in quantum theory and quantum information. Many important examples of convex sets in quantum theory are spectrahedra, that is, sets of positive operators subject to affine constraints. These examples include sets of quantum states with given expectation values of some observables of interest, sets of multipartite quantum states with given marginals, sets of quantum measurements, channels, and multitime quantum processes, as well as sets of higher-order quantum maps and quantum causal structures. This contribution provides a characterisation of the extreme points of general spectrahedra, and bounds on the ranks of the corresponding operators. The general results are applied to several special cases, and are used to retrieve classic results such as Choi's characterisation of the extreme quantum channels, Parhasarathy's characterisation of the extreme quantum states with given marginals, and the quantum version of Birkhoff's theorem for qubit unital channels. Finally, we propose a notion of positive operator valued measures (POVMs) with general affine constraints for their normalisation, and we characterise the extremal POVMs."}
{"title":"SDDPM: Speckle Denoising Diffusion Probabilistic Models","authors":["Soumee Guha","Scott T. Acton"],"raw_abstract":"Coherent imaging systems, such as medical ultrasound and synthetic aperture\nradar (SAR), are subject to corruption from speckle due to sub-resolution\nscatterers. Since speckle is multiplicative in nature, the constituent image\nregions become corrupted to different extents. The task of denoising such\nimages requires algorithms specifically designed for removing signal-dependent\nnoise. This paper proposes a novel image denoising algorithm for removing\nsignal-dependent multiplicative noise with diffusion models, called Speckle\nDenoising Diffusion Probabilistic Models (SDDPM). We derive the mathematical\nformulations for the forward process, the reverse process, and the training\nobjective. In the forward process, we apply multiplicative noise to a given\nimage and prove that the forward process is Gaussian. We show that the reverse\nprocess is also Gaussian and the final training objective can be expressed as\nthe Kullback Leibler (KL) divergence between the forward and reverse processes.\nAs derived in the paper, the final denoising task is a single step process,\nthereby reducing the denoising time significantly. We have trained our model\nwith natural land-use images and ultrasound images for different noise levels.\nExtensive experiments centered around two different applications show that\nSDDPM is robust and performs significantly better than the comparative models\neven when the images are severely corrupted.","publication_date":1700254744,"paper_link":"http://arxiv.org/pdf/2311.10868v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Coherent imaging systems, such as medical ultrasound and synthetic aperture radar (SAR), are subject to corruption from speckle due to sub-resolution scatterers. Since speckle is multiplicative in nature, the constituent image regions become corrupted to different extents. The task of denoising such images requires algorithms specifically designed for removing signal-dependent noise. This paper proposes a novel image denoising algorithm for removing signal-dependent multiplicative noise with diffusion models, called Speckle Denoising Diffusion Probabilistic Models (SDDPM). We derive the mathematical formulations for the forward process, the reverse process, and the training objective. In the forward process, we apply multiplicative noise to a given image and prove that the forward process is Gaussian. We show that the reverse process is also Gaussian and the final training objective can be expressed as the Kullback Leibler (KL) divergence between the forward and reverse processes. As derived in the paper, the final denoising task is a single step process, thereby reducing the denoising time significantly. We have trained our model with natural land-use images and ultrasound images for different noise levels. Extensive experiments centered around two different applications show that SDDPM is robust and performs significantly better than the comparative models even when the images are severely corrupted."}
{"title":"Token-level Adaptation of LoRA Adapters for Downstream Task Generalization","authors":["Joshua Belofsky"],"raw_abstract":"This paper introduces a method for adapting LoRA adapters in smaller-sized\nlanguage models to arbitrary downstream tasks. Unlike standard\nmixture-of-expert architectures, our method employs a gradient-free routing\nfunction to choose a weighted combination of experts without increasing the\ncompute requirements for training or inference. The results show that\ntoken-level adaptation of LoRA adapters outperforms the base Llama-2-7b model\nacross mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension\n(SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that\nthe average performance of token-level adaptation outperforms individual models\nfine-tuned for each of the tasks with the best performance observed in\nadaptation of every-other token during inference. The code for this study is\nmade available through a public repository.","publication_date":1700251674,"paper_link":"http://arxiv.org/pdf/2311.10847v1","categories":["Mathematics"],"abstract":"This paper introduces a method for adapting LoRA adapters in smaller-sized language models to arbitrary downstream tasks. Unlike standard mixture-of-expert architectures, our method employs a gradient-free routing function to choose a weighted combination of experts without increasing the compute requirements for training or inference. The results show that token-level adaptation of LoRA adapters outperforms the base Llama-2-7b model across mathematical (GSM8K), scientific (ARC-Challenge), reading comprehension (SQuAD), and coding (CodeAlpaca-20k) tasks. Further evaluations also show that the average performance of token-level adaptation outperforms individual models fine-tuned for each of the tasks with the best performance observed in adaptation of every-other token during inference. The code for this study is made available through a public repository."}
{"title":"A well-balanced lattice Boltzmann model for binary fluids based on the incompressible phase-field theory","authors":["Long Ju","Peiyao Liu","Bicheng Yan","Jin Bao","Shuyu Sun","Zhaoli Guo"],"raw_abstract":"Spurious velocities arising from the imperfect offset of the undesired term\nat the discrete level are frequently observed in numerical simulations of\nequilibrium multiphase flow systems using the lattice Boltzmann equation (LBE)\nmethod. To capture the physical equilibrium state of two-phase fluid systems\nand eliminate spurious velocities, a well-balanced LBE model based on the\nincompressible phase-field theory is developed. In this model, the equilibrium\ndistribution function for the Cahn-Hilliard (CH) equation is designed by\ntreating the convection term as a source to avoid the introduction of undesired\nterms, enabling achievement of possible discrete force balance. Furthermore,\nthis approach allows for the attainment of a divergence-free velocity field,\neffectively mitigating the impact of artificial compression effects and\nenhancing numerical stability. Numerical tests, including a flat interface\nproblem, a stationary droplet, and the coalescence of two droplets, demonstrate\nthe well-balanced properties and improvements in the stability of the present\nmodel.","publication_date":1700248424,"paper_link":"http://arxiv.org/pdf/2311.10827v1","categories":["Mathematics","Physics"],"abstract":"Spurious velocities arising from the imperfect offset of the undesired term at the discrete level are frequently observed in numerical simulations of equilibrium multiphase flow systems using the lattice Boltzmann equation (LBE) method. To capture the physical equilibrium state of two-phase fluid systems and eliminate spurious velocities, a well-balanced LBE model based on the incompressible phase-field theory is developed. In this model, the equilibrium distribution function for the Cahn-Hilliard (CH) equation is designed by treating the convection term as a source to avoid the introduction of undesired terms, enabling achievement of possible discrete force balance. Furthermore, this approach allows for the attainment of a divergence-free velocity field, effectively mitigating the impact of artificial compression effects and enhancing numerical stability. Numerical tests, including a flat interface problem, a stationary droplet, and the coalescence of two droplets, demonstrate the well-balanced properties and improvements in the stability of the present model."}
{"title":"Deriving Algorithms for Triangular Tridiagonalization a (Skew-)Symmetric Matrix","authors":["Robert van de Geijn","Maggie Myers","RuQing G. Xu","Devin Matthews"],"raw_abstract":"We apply the FLAME methodology to derive algorithms hand in hand with their\nproofs of correctness for the computation of the $ L T L^T $ decomposition\n(with and without pivoting) of a skew-symmetric matrix. The approach yields\nknown as well as new algorithms, presented using the FLAME notation. A number\nof BLAS-like primitives are exposed at the core of blocked algorithms that can\nattain high performance. The insights can be easily extended to yield\nalgorithms for computing the $ L T L^T $ decomposition of a symmetric matrix.","publication_date":1700246680,"paper_link":"http://arxiv.org/pdf/2311.10700v1","categories":["Mathematics"],"abstract":"We apply the FLAME methodology to derive algorithms hand in hand with their proofs of correctness for the computation of the __FORMULA__ decomposition (with and without pivoting) of a skew-symmetric matrix. The approach yields known as well as new algorithms, presented using the FLAME notation. A number of BLAS-like primitives are exposed at the core of blocked algorithms that can attain high performance. The insights can be easily extended to yield algorithms for computing the __FORMULA__ decomposition of a symmetric matrix."}
{"title":"Generalized products and Lorentzian length spaces","authors":["Elefterios Soultanis"],"raw_abstract":"We construct a Lorentzian length space with an orthogonal splitting on a\nproduct $I\\times X$ of an interval and a metric space, and use this framework\nto consider the relationship between metric and causal geometry, as well as\nsynthetic time-like Ricci curvature bounds.\n  The generalized Lorentzian product naturally has a Lorentzian length\nstructure but can fail the push-up condition in general. We recover the push-up\nproperty under a log-Lipschitz condition on the time variable and establish\nsufficient conditions for global hyperbolicity. Moreover we formulate time-like\nRicci curvature bounds without push-up and regularity assumptions, and obtain a\npartial rigidity of the splitting under a strong energy condition.","publication_date":1700245246,"paper_link":"http://arxiv.org/pdf/2311.10691v1","categories":["Mathematics","Physics"],"abstract":"We construct a Lorentzian length space with an orthogonal splitting on a product __FORMULA__ of an interval and a metric space, and use this framework to consider the relationship between metric and causal geometry, as well as synthetic time-like Ricci curvature bounds.   The generalized Lorentzian product naturally has a Lorentzian length structure but can fail the push-up condition in general. We recover the push-up property under a log-Lipschitz condition on the time variable and establish sufficient conditions for global hyperbolicity. Moreover we formulate time-like Ricci curvature bounds without push-up and regularity assumptions, and obtain a partial rigidity of the splitting under a strong energy condition."}
{"title":"On the degrees of freedom count on singular phase space submanifolds","authors":["Alexey Golovnev"],"raw_abstract":"It has recently been noticed that a metric $f(R)$ gravity, with $f(R)=R^2$\nand no linear term, has no dynamical degrees of freedom when linearised around\nMinkowski. This is indeed true, in the sense of maximally using the emergent\ngauge freedom of that limit. In this note, I would like to show that it can\neasily be seen directly from the equations of motion, with no need of playing\ngames with Lagrangians. Moreover, the scalar part of this \"strong coupling\"\nbehaviour is actually more interpretational than dynamical, as it is not\nbecause of disappearance of a kinetic energy term but rather due to losing a\nconstraint. On the other hand, the different answer reported from a procedure\n\"a la Stueckelberg\" has nothing to do with whatever remnant symmetries in the\nequations one might imagine. In the way the procedure had been applied, it does\nradically change the model at hand.","publication_date":1700245223,"paper_link":"http://arxiv.org/pdf/2311.10690v1","categories":["Mathematics","Physics"],"abstract":"It has recently been noticed that a metric __FORMULA__ gravity, with __FORMULA__ and no linear term, has no dynamical degrees of freedom when linearised around Minkowski. This is indeed true, in the sense of maximally using the emergent gauge freedom of that limit. In this note, I would like to show that it can easily be seen directly from the equations of motion, with no need of playing games with Lagrangians. Moreover, the scalar part of this \"strong coupling\" behaviour is actually more interpretational than dynamical, as it is not because of disappearance of a kinetic energy term but rather due to losing a constraint. On the other hand, the different answer reported from a procedure \"a la Stueckelberg\" has nothing to do with whatever remnant symmetries in the equations one might imagine. In the way the procedure had been applied, it does radically change the model at hand."}
{"title":"Non-Zero Mean Quantum Wishart Distribution Of Random Quantum States","authors":["Shrobona Bagchi"],"raw_abstract":"Random quantum states are useful in various areas of quantum information\nscience. Distributions of random quantum states using Gaussian distributions\nhave been used in various scenarios in quantum information science. One of this\nis the distribution of random quantum states derived using the Wishart\ndistibution usually used in statistics. This distribution of random quantum\nstates using the Wishart distribution has recently been named as the quantum\nWishart distribution. The quantum Wishart distribution has been found for\nnon-central distribution with a general covariance matrix and zero mean matrix\nin an earlier work. Here, we find out the closed form expression for the\ndistribution of random quantum states pertaining to non-central Wishart\ndistribution with any general rank one mean matrix and a general covariance\nmatrix for arbitrary dimensions in both real and complex Hilbert space. We term\nthis as the non-zero mean quantum Wishart distribution.","publication_date":1700243137,"paper_link":"http://arxiv.org/pdf/2311.10672v1","categories":["Mathematics","Physics"],"abstract":"Random quantum states are useful in various areas of quantum information science. Distributions of random quantum states using Gaussian distributions have been used in various scenarios in quantum information science. One of this is the distribution of random quantum states derived using the Wishart distibution usually used in statistics. This distribution of random quantum states using the Wishart distribution has recently been named as the quantum Wishart distribution. The quantum Wishart distribution has been found for non-central distribution with a general covariance matrix and zero mean matrix in an earlier work. Here, we find out the closed form expression for the distribution of random quantum states pertaining to non-central Wishart distribution with any general rank one mean matrix and a general covariance matrix for arbitrary dimensions in both real and complex Hilbert space. We term this as the non-zero mean quantum Wishart distribution."}
{"title":"Fuse It or Lose It: Deep Fusion for Multimodal Simulation-Based Inference","authors":["Marvin Schmitt","Stefan T. Radev","Paul-Christian B\u00fcrkner"],"raw_abstract":"We present multimodal neural posterior estimation (MultiNPE), a method to\nintegrate heterogeneous data from different sources in simulation-based\ninference with neural networks. Inspired by advances in attention-based deep\nfusion learning, it empowers researchers to analyze data from different domains\nand infer the parameters of complex mathematical models with increased\naccuracy. We formulate different multimodal fusion approaches for MultiNPE\n(early, late, and hybrid) and evaluate their performance in three challenging\nnumerical experiments. MultiNPE not only outperforms na\\\"ive baselines on a\nbenchmark model, but also achieves superior inference on representative\nscientific models from neuroscience and cardiology. In addition, we\nsystematically investigate the impact of partially missing data on the\ndifferent fusion strategies. Across our different experiments, late and hybrid\nfusion techniques emerge as the methods of choice for practical applications of\nmultimodal simulation-based inference.","publication_date":1700242991,"paper_link":"http://arxiv.org/pdf/2311.10671v1","categories":["Mathematics"],"abstract":"We present multimodal neural posterior estimation (MultiNPE), a method to integrate heterogeneous data from different sources in simulation-based inference with neural networks. Inspired by advances in attention-based deep fusion learning, it empowers researchers to analyze data from different domains and infer the parameters of complex mathematical models with increased accuracy. We formulate different multimodal fusion approaches for MultiNPE (early, late, and hybrid) and evaluate their performance in three challenging numerical experiments. MultiNPE not only outperforms na\\\"ive baselines on a benchmark model, but also achieves superior inference on representative scientific models from neuroscience and cardiology. In addition, we systematically investigate the impact of partially missing data on the different fusion strategies. Across our different experiments, late and hybrid fusion techniques emerge as the methods of choice for practical applications of multimodal simulation-based inference."}
{"title":"Computable and Faithful Lower Bound for Entanglement Cost","authors":["Xin Wang","Mingrui Jing","Chengkai Zhu"],"raw_abstract":"Quantum entanglement is a crucial resource in quantum information processing.\nHowever, quantifying the entanglement required to prepare quantum states and\nimplement quantum processes remains challenging. This paper proposes computable\nand faithful lower bounds for the entanglement cost of general quantum states\nand quantum channels. We introduce the concept of logarithmic $k$-negativity, a\ngeneralization of logarithmic negativity, to establish a general lower bound\nfor the entanglement cost of quantum states under quantum operations that\ncompletely preserve the positivity of partial transpose (PPT). This bound is\nefficiently computable via semidefinite programming and is non-zero for any\nentangled state that is not PPT, making it faithful in the entanglement theory\nwith non-positive partial transpose. Furthermore, we delve into specific and\ngeneral examples to demonstrate the advantages of our proposed bounds compared\nwith previously known computable ones. Notably, we affirm the irreversibility\nof asymptotic entanglement manipulation under PPT operations for full-rank\nentangled states and the irreversibility of channel manipulation for amplitude\ndamping channels. We also establish the best-known lower bound for the\nentanglement cost of arbitrary dimensional isotropic states. These findings\npush the boundaries of understanding the structure of entanglement and the\nfundamental limits of entanglement manipulation.","publication_date":1700240846,"paper_link":"http://arxiv.org/pdf/2311.10649v1","categories":["Mathematics","Physics"],"abstract":"Quantum entanglement is a crucial resource in quantum information processing. However, quantifying the entanglement required to prepare quantum states and implement quantum processes remains challenging. This paper proposes computable and faithful lower bounds for the entanglement cost of general quantum states and quantum channels. We introduce the concept of logarithmic __FORMULA__-negativity, a generalization of logarithmic negativity, to establish a general lower bound for the entanglement cost of quantum states under quantum operations that completely preserve the positivity of partial transpose (PPT). This bound is efficiently computable via semidefinite programming and is non-zero for any entangled state that is not PPT, making it faithful in the entanglement theory with non-positive partial transpose. Furthermore, we delve into specific and general examples to demonstrate the advantages of our proposed bounds compared with previously known computable ones. Notably, we affirm the irreversibility of asymptotic entanglement manipulation under PPT operations for full-rank entangled states and the irreversibility of channel manipulation for amplitude damping channels. We also establish the best-known lower bound for the entanglement cost of arbitrary dimensional isotropic states. These findings push the boundaries of understanding the structure of entanglement and the fundamental limits of entanglement manipulation."}
{"title":"Mathematical morphology on directional data","authors":["Konstantin Hauch","Claudia Redenbach"],"raw_abstract":"We define morphological operators and filters for directional images whose\npixel values are unit vectors. This requires an ordering relation for unit\nvectors which is obtained by using depth functions. They provide a\ncentre-outward ordering with respect to a specified centre vector. We apply our\noperators on synthetic directional images and compare them with classical\nmorphological operators for grey-scale images. As application examples, we\nenhance the fault region in a compressed glass foam and segment misaligned\nfibre regions of glass fibre reinforced polymers.","publication_date":1700239864,"paper_link":"http://arxiv.org/pdf/2311.10639v1","categories":["Mathematics","Statistics"],"abstract":"We define morphological operators and filters for directional images whose pixel values are unit vectors. This requires an ordering relation for unit vectors which is obtained by using depth functions. They provide a centre-outward ordering with respect to a specified centre vector. We apply our operators on synthetic directional images and compare them with classical morphological operators for grey-scale images. As application examples, we enhance the fault region in a compressed glass foam and segment misaligned fibre regions of glass fibre reinforced polymers."}
{"title":"Counting Answers to Unions of Conjunctive Queries: Natural Tractability Criteria and Meta-Complexity","authors":["Jacob Focke","Leslie Ann Goldberg","Marc Roth","Stanislav \u017divn\u00fd"],"raw_abstract":"We study the problem of counting answers to unions of conjunctive queries\n(UCQs) under structural restrictions on the input query. Concretely, given a\nclass C of UCQs, the problem #UCQ(C) provides as input a UCQ Q in C and a\ndatabase D and the problem is to compute the number of answers of Q in D.\n  Chen and Mengel [PODS'16] have shown that for any recursively enumerable\nclass C, the problem #UCQ(C) is either fixed-parameter tractable or hard for\none of the parameterised complexity classes W[1] or #W[1]. However, their\ntractability criterion is unwieldy in the sense that, given any concrete class\nC of UCQs, it is not easy to determine how hard it is to count answers to\nqueries in C. Moreover, given a single specific UCQ Q, it is not easy to\ndetermine how hard it is to count answers to Q.\n  In this work, we address the question of finding a natural tractability\ncriterion: The combined conjunctive query of a UCQ $\\varphi_1 \\vee \\dots \\vee\n\\varphi_\\ell$ is the conjunctive query $\\varphi_1 \\wedge \\dots \\wedge\n\\varphi_\\ell$. We show that under natural closure properties of C, the problem\n#UCQ(C) is fixed-parameter tractable if and only if the combined conjunctive\nqueries of UCQs in C, and their contracts, have bounded treewidth. A contract\nof a conjunctive query is an augmented structure, taking into account how the\nquantified variables are connected to the free variables. If all variables are\nfree, then a conjunctive query is equal to its contract; in this special case\nthe criterion for fixed-parameter tractability of #UCQ(C) thus simplifies to\nthe combined queries having bounded treewidth.\n  Finally, we give evidence that a closure property on C is necessary for\nobtaining a natural tractability criterion: We show that even for a single UCQ\nQ, the meta problem of deciding whether #UCQ({Q}) can be solved in time\n$O(|D|^d)$ is NP-hard for any fixed $d\\geq 1$.","publication_date":1700239349,"paper_link":"http://arxiv.org/pdf/2311.10634v1","categories":["Mathematics"],"abstract":"We study the problem of counting answers to unions of conjunctive queries (UCQs) under structural restrictions on the input query. Concretely, given a class C of UCQs, the problem #UCQ(C) provides as input a UCQ Q in C and a database D and the problem is to compute the number of answers of Q in D.   Chen and Mengel [PODS'16] have shown that for any recursively enumerable class C, the problem #UCQ(C) is either fixed-parameter tractable or hard for one of the parameterised complexity classes W[1] or #W[1]. However, their tractability criterion is unwieldy in the sense that, given any concrete class C of UCQs, it is not easy to determine how hard it is to count answers to queries in C. Moreover, given a single specific UCQ Q, it is not easy to determine how hard it is to count answers to Q.   In this work, we address the question of finding a natural tractability criterion: The combined conjunctive query of a UCQ __FORMULA__ is the conjunctive query __FORMULA__. We show that under natural closure properties of C, the problem #UCQ(C) is fixed-parameter tractable if and only if the combined conjunctive queries of UCQs in C, and their contracts, have bounded treewidth. A contract of a conjunctive query is an augmented structure, taking into account how the quantified variables are connected to the free variables. If all variables are free, then a conjunctive query is equal to its contract; in this special case the criterion for fixed-parameter tractability of #UCQ(C) thus simplifies to the combined queries having bounded treewidth.   Finally, we give evidence that a closure property on C is necessary for obtaining a natural tractability criterion: We show that even for a single UCQ Q, the meta problem of deciding whether #UCQ({Q}) can be solved in time __FORMULA__ is NP-hard for any fixed __FORMULA__."}
{"title":"DisCoPy: the Hierarchy of Graphical Languages in Python","authors":["Alexis Toumi","Richie Yeung","Boldizs\u00e1r Po\u00f3r","Giovanni de Felice"],"raw_abstract":"DisCoPy is a Python toolkit for computing with monoidal categories. It comes\nwith two flexible data structures for string diagrams: the first one for planar\nmonoidal categories based on lists of layers, the second one for symmetric\nmonoidal categories based on cospans of hypergraphs. Algorithms for functor\napplication then allow to translate string diagrams into code for numerical\ncomputation, be it differentiable, probabilistic or quantum. This report gives\nan overview of the library and the new developments released in its version\n1.0. In particular, we showcase the implementation of diagram equality for a\nlarge fragment of the hierarchy of graphical languages for monoidal categories,\nas well as a new syntax for defining string diagrams as Python functions.","publication_date":1700236988,"paper_link":"http://arxiv.org/pdf/2311.10608v1","categories":["Mathematics"],"abstract":"DisCoPy is a Python toolkit for computing with monoidal categories. It comes with two flexible data structures for string diagrams: the first one for planar monoidal categories based on lists of layers, the second one for symmetric monoidal categories based on cospans of hypergraphs. Algorithms for functor application then allow to translate string diagrams into code for numerical computation, be it differentiable, probabilistic or quantum. This report gives an overview of the library and the new developments released in its version 1.0. In particular, we showcase the implementation of diagram equality for a large fragment of the hierarchy of graphical languages for monoidal categories, as well as a new syntax for defining string diagrams as Python functions."}
{"title":"Generation and New Infinite Families of $K_2$-hypohamiltonian Graphs","authors":["Jan Goedgebeur","Jarne Renders","Carol T. Zamfirescu"],"raw_abstract":"We present an algorithm which can generate all pairwise non-isomorphic\n$K_2$-hypohamiltonian graphs, i.e. non-hamiltonian graphs in which the removal\nof any pair of adjacent vertices yields a hamiltonian graph, of a given order.\nWe introduce new bounding criteria specifically designed for\n$K_2$-hypohamiltonian graphs, allowing us to improve upon earlier computational\nresults. Specifically, we characterise the orders for which\n$K_2$-hypohamiltonian graphs exist and improve existing lower bounds on the\norders of the smallest planar and the smallest bipartite $K_2$-hypohamiltonian\ngraphs. Furthermore, we describe a new operation for creating\n$K_2$-hypohamiltonian graphs that preserves planarity under certain conditions\nand use it to prove the existence of a planar $K_2$-hypohamiltonian graph of\norder $n$ for every integer $n\\geq 134$. Additionally, motivated by a theorem\nof Thomassen on hypohamiltonian graphs, we show the existence\n$K_2$-hypohamiltonian graphs with large maximum degree and size.","publication_date":1700236022,"paper_link":"http://arxiv.org/pdf/2311.10593v1","categories":["Mathematics"],"abstract":"We present an algorithm which can generate all pairwise non-isomorphic __FORMULA__-hypohamiltonian graphs, i.e. non-hamiltonian graphs in which the removal of any pair of adjacent vertices yields a hamiltonian graph, of a given order. We introduce new bounding criteria specifically designed for __FORMULA__-hypohamiltonian graphs, allowing us to improve upon earlier computational results. Specifically, we characterise the orders for which __FORMULA__-hypohamiltonian graphs exist and improve existing lower bounds on the orders of the smallest planar and the smallest bipartite __FORMULA__-hypohamiltonian graphs. Furthermore, we describe a new operation for creating __FORMULA__-hypohamiltonian graphs that preserves planarity under certain conditions and use it to prove the existence of a planar __FORMULA__-hypohamiltonian graph of order __FORMULA__ for every integer __FORMULA__. Additionally, motivated by a theorem of Thomassen on hypohamiltonian graphs, we show the existence __FORMULA__-hypohamiltonian graphs with large maximum degree and size."}
{"title":"On CCGG, the De Donder-Weyl Hamiltonian formulation of canonical gauge gravity","authors":["D. Vasak","J. Kirsch","A. van de Venn","V. Denk","J. Struckmeier"],"raw_abstract":"This paper gives a brief overview of the manifestly covariant canonical gauge\ngravity (CCGG) that is rooted in the De Donder-Weyl Hamiltonian formulation of\nrelativistic field theories, and the proven methodology of the canonical\ntransformation theory. That framework derives, from a few basic physical and\nmathematical assumptions, equations describing generic matter and gravity\ndynamics with the spin connection emerging as a Yang Mills-type gauge field.\nWhile the interaction of any matter field with spacetime is fixed just by the\ntransformation property of that field, a concrete gravity ansatz is introduced\nby the choice of the free (kinetic) gravity Hamiltonian. The key elements of\nthis approach are discussed and its implications for particle dynamics and\ncosmology presented. Among the results are especially: - Anomalous Pauli\ncoupling of spinors to curvature and torsion of spacetime, - spacetime with\n(A)dS ground state, inertia, torsion and geometrical vacuum energy, -\nZero-energy balance of the Universe leading to a vanishing cosmological\nconstant and torsional dark energy.","publication_date":1700235745,"paper_link":"http://arxiv.org/pdf/2311.10589v1","categories":["Mathematics","Physics"],"abstract":"This paper gives a brief overview of the manifestly covariant canonical gauge gravity (CCGG) that is rooted in the De Donder-Weyl Hamiltonian formulation of relativistic field theories, and the proven methodology of the canonical transformation theory. That framework derives, from a few basic physical and mathematical assumptions, equations describing generic matter and gravity dynamics with the spin connection emerging as a Yang Mills-type gauge field. While the interaction of any matter field with spacetime is fixed just by the transformation property of that field, a concrete gravity ansatz is introduced by the choice of the free (kinetic) gravity Hamiltonian. The key elements of this approach are discussed and its implications for particle dynamics and cosmology presented. Among the results are especially: - Anomalous Pauli coupling of spinors to curvature and torsion of spacetime, - spacetime with (A)dS ground state, inertia, torsion and geometrical vacuum energy, - Zero-energy balance of the Universe leading to a vanishing cosmological constant and torsional dark energy."}
{"title":"LUNA-CIM: Lookup Table based Programmable Neural Processing in Memory","authors":["Peyman Dehghanzadeh","Baibhab Chatterjee","Swarup Bhunia"],"raw_abstract":"This paper presents a novel approach for performing computations using\nLook-Up Tables (LUTs) tailored specifically for Compute-in-Memory applications.\nThe aim is to address the scalability challenges associated with LUT-based\ncomputation by reducing storage requirements and energy consumption while\ncapitalizing on the faster and more energy-efficient nature of look-up methods\ncompared to conventional mathematical computations. The proposed method\nleverages a divide and conquer (D&C) strategy to enhance the scalability of\nLUT-based computation. By breaking down high-precision multiplications into\nlower-precision operations, the technique achieves significantly lower area\noverheads, up to approximately 3.7 times less than conventional LUT-based\napproaches, without compromising accuracy. To validate the effectiveness of the\nproposed method, extensive simulations using TSMC 65 nm technology were\nconducted. The experimental analysis reveals that the proposed approach\naccounts for less than 0.1\\% of the total energy consumption, with only a 32\\%\nincrease in area overhead. These results demonstrate considerable improvements\nachieved in energy efficiency and area utilization through the novel\nlow-energy, low-area-overhead LUT-based computation in an SRAM array.","publication_date":1700235131,"paper_link":"http://arxiv.org/pdf/2311.10581v1","categories":["Mathematics"],"abstract":"This paper presents a novel approach for performing computations using Look-Up Tables (LUTs) tailored specifically for Compute-in-Memory applications. The aim is to address the scalability challenges associated with LUT-based computation by reducing storage requirements and energy consumption while capitalizing on the faster and more energy-efficient nature of look-up methods compared to conventional mathematical computations. The proposed method leverages a divide and conquer (D&C) strategy to enhance the scalability of LUT-based computation. By breaking down high-precision multiplications into lower-precision operations, the technique achieves significantly lower area overheads, up to approximately 3.7 times less than conventional LUT-based approaches, without compromising accuracy. To validate the effectiveness of the proposed method, extensive simulations using TSMC 65 nm technology were conducted. The experimental analysis reveals that the proposed approach accounts for less than 0.1\\% of the total energy consumption, with only a 32\\% increase in area overhead. These results demonstrate considerable improvements achieved in energy efficiency and area utilization through the novel low-energy, low-area-overhead LUT-based computation in an SRAM array."}
{"title":"Graph Neural Networks for Pressure Estimation in Water Distribution Systems","authors":["Huy Truong","Andr\u00e9s Tello","Alexander Lazovik","Victoria Degeler"],"raw_abstract":"Pressure and flow estimation in Water Distribution Networks (WDN) allows\nwater management companies to optimize their control operations. For many\nyears, mathematical simulation tools have been the most common approach to\nreconstructing an estimate of the WDN hydraulics. However, pure physics-based\nsimulations involve several challenges, e.g. partially observable data, high\nuncertainty, and extensive manual configuration. Thus, data-driven approaches\nhave gained traction to overcome such limitations. In this work, we combine\nphysics-based modeling and Graph Neural Networks (GNN), a data-driven approach,\nto address the pressure estimation problem. First, we propose a new data\ngeneration method using a mathematical simulation but not considering temporal\npatterns and including some control parameters that remain untouched in\nprevious works; this contributes to a more diverse training data. Second, our\ntraining strategy relies on random sensor placement making our GNN-based\nestimation model robust to unexpected sensor location changes. Third, a\nrealistic evaluation protocol considers real temporal patterns and additionally\ninjects the uncertainties intrinsic to real-world scenarios. Finally, a\nmulti-graph pre-training strategy allows the model to be reused for pressure\nestimation in unseen target WDNs. Our GNN-based model estimates the pressure of\na large-scale WDN in The Netherlands with a MAE of 1.94mH$_2$O and a MAPE of\n7%, surpassing the performance of previous studies. Likewise, it outperformed\nprevious approaches on other WDN benchmarks, showing a reduction of absolute\nerror up to approximately 52% in the best cases.","publication_date":1700235012,"paper_link":"http://arxiv.org/pdf/2311.10579v1","categories":["Mathematics"],"abstract":"Pressure and flow estimation in Water Distribution Networks (WDN) allows water management companies to optimize their control operations. For many years, mathematical simulation tools have been the most common approach to reconstructing an estimate of the WDN hydraulics. However, pure physics-based simulations involve several challenges, e.g. partially observable data, high uncertainty, and extensive manual configuration. Thus, data-driven approaches have gained traction to overcome such limitations. In this work, we combine physics-based modeling and Graph Neural Networks (GNN), a data-driven approach, to address the pressure estimation problem. First, we propose a new data generation method using a mathematical simulation but not considering temporal patterns and including some control parameters that remain untouched in previous works; this contributes to a more diverse training data. Second, our training strategy relies on random sensor placement making our GNN-based estimation model robust to unexpected sensor location changes. Third, a realistic evaluation protocol considers real temporal patterns and additionally injects the uncertainties intrinsic to real-world scenarios. Finally, a multi-graph pre-training strategy allows the model to be reused for pressure estimation in unseen target WDNs. Our GNN-based model estimates the pressure of a large-scale WDN in The Netherlands with a MAE of 1.94mH__FORMULA__O and a MAPE of 7%, surpassing the performance of previous studies. Likewise, it outperformed previous approaches on other WDN benchmarks, showing a reduction of absolute error up to approximately 52% in the best cases."}
{"title":"Kondo effect in the isotropic Heisenberg spin chain","authors":["Pradip Kattel","Parameshwar R. Pasnoori","J. H. Pixley","Patrick Azaria","Natan Andrei"],"raw_abstract":"We investigate the boundary effects that arise when spin-$\\frac{1}{2}$\nimpurities interact with the edges of the antiferromagnetic spin-$\\frac{1}{2}$\nHeisenberg chain through spin exchange interactions. We consider both cases\nwhen the couplings are ferromagnetic or anti-ferromagnetic. We find that in the\ncase of antiferromagnetic interaction, when the impurity coupling strength is\nmuch weaker than that in the bulk, the impurity is screened in the ground state\nvia the Kondo effect. The Kondo phase is characterized by the Lorentzian\ndensity of states and dynamically generated Kondo temperature $T_K$. As the\nimpurity coupling strength increases, $T_K$ increases until it reaches its\nmaximum value $T_0=2\\pi J$ which is the maximum energy carried by a single\nspinon. When the impurity coupling strength is increased further, we enter\nanother phase, the bound mode phase, where the impurity is screened in the\nground state by a single particle bound mode exponentially localized at the\nedge to which the impurity is coupled. We find that the impurity can be\nunscreened by removing the bound mode. There exists a boundary eigenstate phase\ntransition between the Kondo and the bound-mode phases, a transition which is\ncharacterized by the change in the number of towers of the Hilbert space. The\ntransition also manifests itself in ground state quantities like local impurity\ndensity of states and the local impurity magnetization. When the impurity\ncoupling is ferromagnetic, the impurity is unscreened in the ground state;\nhowever, when the absolute value of the ratio of the impurity and bulk coupling\nstrengths is greater than $\\frac{4}{5}$, the impurity can be screened by adding\na bound mode that costs energy greater than $T_0$. When two impurities are\nconsidered, the phases exhibited by each impurity remain unchanged in the\nthermodynamic limit, but nevertheless the system exhibits a rich phase diagram.","publication_date":1700233763,"paper_link":"http://arxiv.org/pdf/2311.10569v1","categories":["Mathematics","Physics"],"abstract":"We investigate the boundary effects that arise when spin-__FORMULA__ impurities interact with the edges of the antiferromagnetic spin-__FORMULA__ Heisenberg chain through spin exchange interactions. We consider both cases when the couplings are ferromagnetic or anti-ferromagnetic. We find that in the case of antiferromagnetic interaction, when the impurity coupling strength is much weaker than that in the bulk, the impurity is screened in the ground state via the Kondo effect. The Kondo phase is characterized by the Lorentzian density of states and dynamically generated Kondo temperature __FORMULA__. As the impurity coupling strength increases, __FORMULA__ increases until it reaches its maximum value __FORMULA__ which is the maximum energy carried by a single spinon. When the impurity coupling strength is increased further, we enter another phase, the bound mode phase, where the impurity is screened in the ground state by a single particle bound mode exponentially localized at the edge to which the impurity is coupled. We find that the impurity can be unscreened by removing the bound mode. There exists a boundary eigenstate phase transition between the Kondo and the bound-mode phases, a transition which is characterized by the change in the number of towers of the Hilbert space. The transition also manifests itself in ground state quantities like local impurity density of states and the local impurity magnetization. When the impurity coupling is ferromagnetic, the impurity is unscreened in the ground state; however, when the absolute value of the ratio of the impurity and bulk coupling strengths is greater than __FORMULA__, the impurity can be screened by adding a bound mode that costs energy greater than __FORMULA__. When two impurities are considered, the phases exhibited by each impurity remain unchanged in the thermodynamic limit, but nevertheless the system exhibits a rich phase diagram."}
{"title":"Alignment via friction for nonisothermal multicomponent fluid systems","authors":["Stefanos Georgiadis","Athanasios E. Tzavaras"],"raw_abstract":"The derivation of an approximate Class-I model for nonisothermal\nmulticomponent systems of fluids, as the high-friction limit of a Class-II\nmodel is justified, by validating the Chapman-Enskog expansion performed from\nthe Class-II model towards the Class-I model. The analysis proceeds by\ncomparing two thermomechanical theories via relative entropy.","publication_date":1700230441,"paper_link":"http://arxiv.org/pdf/2311.10546v1","categories":["Mathematics","Physics"],"abstract":"The derivation of an approximate Class-I model for nonisothermal multicomponent systems of fluids, as the high-friction limit of a Class-II model is justified, by validating the Chapman-Enskog expansion performed from the Class-II model towards the Class-I model. The analysis proceeds by comparing two thermomechanical theories via relative entropy."}
{"title":"A large and natural Class of $\u03a3^p_2$- and $\u03a3^p_3$-complete Problems in Bilevel and Robust Optimization","authors":["Christoph Gr\u00fcne","Lasse Wulf"],"raw_abstract":"Because $\\Sigma^p_2$- and $\\Sigma^p_3$-hardness proofs are usually tedious\nand difficult, not so many complete problems for these classes are known. This\nis especially true in the areas of min-max regret robust optimization, network\ninterdiction, most vital vertex problems, blocker problems, and two-stage\nadjustable robust optimization problems. Even though these areas are\nwell-researched for over two decades and one would naturally expect many (if\nnot most) of the problems occurring in these areas to be complete for the above\nclasses, almost no completeness results exist in the literature. We address\nthis lack of knowledge by introducing over 70 new $\\Sigma^p_2$-complete and\n$\\Sigma^p_3$-complete problems. We achieve this result by proving a new\nmeta-theorem, which shows $\\Sigma^p_2$- and $\\Sigma^p_3$-completeness\nsimultaneously for a huge class of problems. The majority of all earlier\npublications on $\\Sigma^p_2$- and $\\Sigma^p_3$-completeness in said areas are\nspecial cases of our meta-theorem. Our precise result is the following: We\nintroduce a large list of problems for which the meta-theorem is applicable\n(including clique, vertex cover, knapsack, TSP, facility location and many\nmore). For every problem on this list, we show: The interdiction/minimum cost\nblocker/most vital nodes problem (with element costs) is $\\Sigma^p_2$-complete.\nThe min-max-regret problem with interval uncertainty is $\\Sigma^p_2$-complete.\nThe two-stage adjustable robust optimization problem with discrete budgeted\nuncertainty is $\\Sigma^p_3$-complete. In summary, our work reveals the\ninteresting insight that a large amount of NP-complete problems have the\nproperty that their min-max versions are 'automatically' $\\Sigma^p_2$-complete.","publication_date":1700230069,"paper_link":"http://arxiv.org/pdf/2311.10540v1","categories":["Mathematics"],"abstract":"Because __FORMULA__- and __FORMULA__-hardness proofs are usually tedious and difficult, not so many complete problems for these classes are known. This is especially true in the areas of min-max regret robust optimization, network interdiction, most vital vertex problems, blocker problems, and two-stage adjustable robust optimization problems. Even though these areas are well-researched for over two decades and one would naturally expect many (if not most) of the problems occurring in these areas to be complete for the above classes, almost no completeness results exist in the literature. We address this lack of knowledge by introducing over 70 new __FORMULA__-complete and __FORMULA__-complete problems. We achieve this result by proving a new meta-theorem, which shows __FORMULA__- and __FORMULA__-completeness simultaneously for a huge class of problems. The majority of all earlier publications on __FORMULA__- and __FORMULA__-completeness in said areas are special cases of our meta-theorem. Our precise result is the following: We introduce a large list of problems for which the meta-theorem is applicable (including clique, vertex cover, knapsack, TSP, facility location and many more). For every problem on this list, we show: The interdiction/minimum cost blocker/most vital nodes problem (with element costs) is __FORMULA__-complete. The min-max-regret problem with interval uncertainty is __FORMULA__-complete. The two-stage adjustable robust optimization problem with discrete budgeted uncertainty is __FORMULA__-complete. In summary, our work reveals the interesting insight that a large amount of NP-complete problems have the property that their min-max versions are 'automatically' __FORMULA__-complete."}
{"title":"The Non-Hermitian Skin Effect With Three-Dimensional Long-Range Coupling","authors":["Habib Ammari","Silvio Barandun","Jinghao Cao","Bryn Davies","Erik Orvehed Hiltunen","Ping Liu"],"raw_abstract":"We study the non-Hermitian skin effect in a three-dimensional system of\nfinitely many subwavelength resonators with an imaginary gauge potential. We\nintroduce a discrete approximation of the eigenmodes and eigenfrequencies of\nthe system in terms of the eigenvectors and eigenvalues of the so-called gauge\ncapacitance matrix $\\mathcal{C}_N^\\gamma$, which is a dense matrix due to\nlong-range interactions in the system. Based on translational invariance of\nthis matrix and the decay of its off-diagonal entries, we prove the\ncondensation of the eigenmodes at one edge of the structure by showing the\nexponential decay of its pseudo-eigenvectors. In particular, we consider a\nrange-k approximation to keep the long-range interaction to a certain extent,\nthus obtaining a k-banded gauge capacitance matrix $\\mathcal{C}_{N,k}^\\gamma$ .\nUsing techniques for Toeplitz matrices and operators, we establish the\nexponential decay of the pseudo-eigenvectors of $\\mathcal{C}_{N,k}^\\gamma$ and\ndemonstrate that they approximate those of the gauge capacitance matrix\n$\\mathcal{C}_N^\\gamma$ well. Our results are numerically verified. In\nparticular, we show that long-range interactions affect only the first\neigenmodes in the system. As a result, a tridiagonal approximation of the gauge\ncapacitance matrix, similar to the nearest-neighbour approximation in quantum\nmechanics, provides a good approximation for the higher modes. Moreover, we\nalso illustrate numerically the behaviour of the eigenmodes and the stability\nof the non-Hermitian skin effect with respect to disorder in a variety of\nthree-dimensional structures.","publication_date":1700228621,"paper_link":"http://arxiv.org/pdf/2311.10521v1","categories":["Mathematics","Physics"],"abstract":"We study the non-Hermitian skin effect in a three-dimensional system of finitely many subwavelength resonators with an imaginary gauge potential. We introduce a discrete approximation of the eigenmodes and eigenfrequencies of the system in terms of the eigenvectors and eigenvalues of the so-called gauge capacitance matrix __FORMULA__, which is a dense matrix due to long-range interactions in the system. Based on translational invariance of this matrix and the decay of its off-diagonal entries, we prove the condensation of the eigenmodes at one edge of the structure by showing the exponential decay of its pseudo-eigenvectors. In particular, we consider a range-k approximation to keep the long-range interaction to a certain extent, thus obtaining a k-banded gauge capacitance matrix __FORMULA__ . Using techniques for Toeplitz matrices and operators, we establish the exponential decay of the pseudo-eigenvectors of __FORMULA__ and demonstrate that they approximate those of the gauge capacitance matrix __FORMULA__ well. Our results are numerically verified. In particular, we show that long-range interactions affect only the first eigenmodes in the system. As a result, a tridiagonal approximation of the gauge capacitance matrix, similar to the nearest-neighbour approximation in quantum mechanics, provides a good approximation for the higher modes. Moreover, we also illustrate numerically the behaviour of the eigenmodes and the stability of the non-Hermitian skin effect with respect to disorder in a variety of three-dimensional structures."}
{"title":"Groupoid intertwiner and twist for dynamical Yang--Baxter equation: part I","authors":["Muze Ren"],"raw_abstract":"Intertwiner is a homomorphism between two existing dynamical R matrices,\nfirst introduced by Baxter in eight vertex-SOS correspondence, we develop\ncertain equivalence relations among R matrices using intertwiners.\n  Twist is a homomorphism that twist a dynamical R matrix to get a new\ndynamical R matrix, we introduce a kind of notion of twist that generalize\nclassical Drinfeld twist in quasi-triangular Hopf algebra and some dynamical\ntwist. As applications, we obtain some examples of twists from Ocneanu cell\ncalculus and Fendley--Ginsparg orbifold constructions. The relations between\nintertwiner and twist are also discussed, the groupoid structures are\nemphasized.","publication_date":1700226543,"paper_link":"http://arxiv.org/pdf/2311.10504v1","categories":["Mathematics","Physics"],"abstract":"Intertwiner is a homomorphism between two existing dynamical R matrices, first introduced by Baxter in eight vertex-SOS correspondence, we develop certain equivalence relations among R matrices using intertwiners.   Twist is a homomorphism that twist a dynamical R matrix to get a new dynamical R matrix, we introduce a kind of notion of twist that generalize classical Drinfeld twist in quasi-triangular Hopf algebra and some dynamical twist. As applications, we obtain some examples of twists from Ocneanu cell calculus and Fendley--Ginsparg orbifold constructions. The relations between intertwiner and twist are also discussed, the groupoid structures are emphasized."}
{"title":"A Formalisation of Core Erlang, a Concurrent Actor Language","authors":["P\u00e9ter Bereczky","D\u00e1niel Horp\u00e1csi","Simon Thompson"],"raw_abstract":"In order to reason about the behaviour of programs described in a programming\nlanguage, a mathematically rigorous definition of that language is needed. In\nthis paper, we present a machine-checked formalisation of concurrent Core\nErlang (a subset of Erlang) based on our previous formalisations of its\nsequential sublanguage. We define a modular, frame stack semantics, show how\nprogram evaluation is carried out with it, and prove a number of properties\n(e.g. determinism, confluence). Finally, we define program equivalence based on\nbisimulations and prove that side-effect-free evaluation is a bisimulation.\nThis research is part of a wider project that aims to verify refactorings to\nprove that particular program code transformations preserve program behaviour.","publication_date":1700223843,"paper_link":"http://arxiv.org/pdf/2311.10482v1","categories":["Mathematics"],"abstract":"In order to reason about the behaviour of programs described in a programming language, a mathematically rigorous definition of that language is needed. In this paper, we present a machine-checked formalisation of concurrent Core Erlang (a subset of Erlang) based on our previous formalisations of its sequential sublanguage. We define a modular, frame stack semantics, show how program evaluation is carried out with it, and prove a number of properties (e.g. determinism, confluence). Finally, we define program equivalence based on bisimulations and prove that side-effect-free evaluation is a bisimulation. This research is part of a wider project that aims to verify refactorings to prove that particular program code transformations preserve program behaviour."}
{"title":"Conway's law, revised from a mathematical viewpoint","authors":["Shigeki Matsutani","Shousuke Ohmori","Kenji Hiranabe","Eiichi Hanyuda"],"raw_abstract":"In this article, we revise Conway's Law from a mathematical point of view. By\nintroducing a task graph, we first rigorously state Conway's Law based on the\nhomomorphisms in graph theory for the software system and the organizations\nthat created it. Though Conway did not mention it, the task graph shows the\ngeometric structure of tasks, which plays a crucial role. Furthermore, due to\nrecent requirements for high-level treatment of communication (due to security,\nknowledge hiding, etc.) in organizations and hierarchical treatment of\norganizations, we have reformulated these statements in terms of weakened\nhomomorphisms, and the continuous maps in graph topology. In order to use graph\ntopology and the continuous map in Conway's law, we have prepared them as\nmathematical tools, and then we show the natural expression of Conway's\ncorrespondences with hierarchical structures.","publication_date":1700222917,"paper_link":"http://arxiv.org/pdf/2311.10475v1","categories":["Mathematics"],"abstract":"In this article, we revise Conway's Law from a mathematical point of view. By introducing a task graph, we first rigorously state Conway's Law based on the homomorphisms in graph theory for the software system and the organizations that created it. Though Conway did not mention it, the task graph shows the geometric structure of tasks, which plays a crucial role. Furthermore, due to recent requirements for high-level treatment of communication (due to security, knowledge hiding, etc.) in organizations and hierarchical treatment of organizations, we have reformulated these statements in terms of weakened homomorphisms, and the continuous maps in graph topology. In order to use graph topology and the continuous map in Conway's law, we have prepared them as mathematical tools, and then we show the natural expression of Conway's correspondences with hierarchical structures."}
{"title":"Large values of quadratic Dirichlet $L$-functions over monic irreducible polynomial in $\\mathbb{F}_q[t]$","authors":["Pranendu Darbar","Gopal Maiti"],"raw_abstract":"We prove an $\\Omega$-result for the quadratic Dirichlet $L$-function $|L(1/2,\n\\chi_P)|$ over irreducible polynomials $P$ associated with the hyperelliptic\ncurve of genus $g$ over a fixed finite field $\\mathbb{F}_q$ in the large genus\nlimit. In particular, we showed that for any $\\epsilon\\in (0, 1/2)$,\n  \\[\n  \\max_{\\substack{P\\in \\mathcal{P}_{2g+1}}}|L(1/2, \\chi_P)|\\gg\n\\exp\\left(\\left(\\sqrt{\\left(1/2-\\epsilon\\right)\\ln q}+o(1)\\right)\\sqrt{\\frac{g\n\\ln_2 g}{\\ln g}}\\right),\n  \\]\n  where $\\mathcal{P}_{2g+1}$ is the set of all monic irreducible polynomial of\ndegree $2g+1$. This matches with the order of magnitude of the Bondarenko--Seip\nbound.","publication_date":1700214658,"paper_link":"http://arxiv.org/pdf/2311.10419v1","categories":["Mathematics"],"abstract":"We prove an __FORMULA__-result for the quadratic Dirichlet __FORMULA__-function __FORMULA__ over irreducible polynomials __FORMULA__ associated with the hyperelliptic curve of genus __FORMULA__ over a fixed finite field __FORMULA__ in the large genus limit. In particular, we showed that for any __FORMULA__,   \\[   \\max_{P\\in \\mathcal{P_{2g+1}}}|L(1/2, \\chi_P)|\\gg \\exp\\left(\\left(\\left(1/2-\\epsilon\\right)\\ln q+o(1)\\right)\\frac{g \\ln_2 g{\\ln g}}\\right),   \\]   where __FORMULA__ is the set of all monic irreducible polynomial of degree __FORMULA__. This matches with the order of magnitude of the Bondarenko--Seip bound."}
{"title":"Meta-DSP: A Meta-Learning Approach for Data-Driven Nonlinear Compensation in High-Speed Optical Fiber Systems","authors":["Xinyu Xiao","Zhennan Zhou","Bin Dong","Dingjiong Ma","Li Zhou","Jie Sun"],"raw_abstract":"Non-linear effects in long-haul, high-speed optical fiber systems\nsignificantly hinder channel capacity. While the Digital Backward Propagation\nalgorithm (DBP) with adaptive filter (ADF) can mitigate these effects, it\nsuffers from an overwhelming computational complexity. Recent solutions have\nincorporated deep neural networks in a data-driven strategy to alleviate this\ncomplexity in the DBP model. However, these models are often limited to a\nspecific symbol rate and channel number, necessitating retraining for different\nsettings, their performance declines significantly under high-speed and\nhigh-power conditions. We introduce Meta-DSP, a novel data-driven nonlinear\ncompensation model based on meta-learning that processes multi-modal data\nacross diverse transmission rates, power levels, and channel numbers. This not\nonly enhances signal quality but also substantially reduces the complexity of\nthe nonlinear processing algorithm. Our model delivers a 0.7 dB increase in the\nQ-factor over Electronic Dispersion Compensation (EDC), and compared to DBP, it\ncurtails computational complexity by a factor of ten while retaining comparable\nperformance. From the perspective of the entire signal processing system, the\ncore idea of Meta-DSP can be employed in any segment of the overall\ncommunication system to enhance the model's scalability and generalization\nperformance. Our research substantiates Meta-DSP's proficiency in addressing\nthe critical parameters defining optical communication networks.","publication_date":1700214162,"paper_link":"http://arxiv.org/pdf/2311.10416v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Non-linear effects in long-haul, high-speed optical fiber systems significantly hinder channel capacity. While the Digital Backward Propagation algorithm (DBP) with adaptive filter (ADF) can mitigate these effects, it suffers from an overwhelming computational complexity. Recent solutions have incorporated deep neural networks in a data-driven strategy to alleviate this complexity in the DBP model. However, these models are often limited to a specific symbol rate and channel number, necessitating retraining for different settings, their performance declines significantly under high-speed and high-power conditions. We introduce Meta-DSP, a novel data-driven nonlinear compensation model based on meta-learning that processes multi-modal data across diverse transmission rates, power levels, and channel numbers. This not only enhances signal quality but also substantially reduces the complexity of the nonlinear processing algorithm. Our model delivers a 0.7 dB increase in the Q-factor over Electronic Dispersion Compensation (EDC), and compared to DBP, it curtails computational complexity by a factor of ten while retaining comparable performance. From the perspective of the entire signal processing system, the core idea of Meta-DSP can be employed in any segment of the overall communication system to enhance the model's scalability and generalization performance. Our research substantiates Meta-DSP's proficiency in addressing the critical parameters defining optical communication networks."}
{"title":"About the work of Albert Lautman","authors":["Bernard Teissier"],"raw_abstract":"Some reflections on the role in the development of Mathematics of our\nunconscious perception of the world and just as unconscious organizing pulsions\nfor those perceptions.","publication_date":1700213641,"paper_link":"http://arxiv.org/pdf/2311.10412v1","categories":["Mathematics"],"abstract":"Some reflections on the role in the development of Mathematics of our unconscious perception of the world and just as unconscious organizing pulsions for those perceptions."}
{"title":"Resurgence and self-completion in renormalized gauge theories","authors":["Alessio Maiezza","Juan Carlos Vasquez"],"raw_abstract":"Under certain assumptions and independent of the instantons, we show that the\nlogarithm expansion of dimensional regularization in quantum field theory needs\na nonperturbative completion to have a renormalization-group flow valid at all\nenergies. Then, we show that such nonperturbative completion has the analytic\nproperties of the renormalons, which we find with no reference to diagrammatic\ncalculations. We demonstrate that renormalon corrections necessarily lead to\nanalyzable functions, namely, resurgent transseries. A detailed analysis of the\nresurgent properties of the renormalons is provided. The self-consistency of\nthe theory requires these nonperturbative contributions to render the running\ncoupling well-defined at any energy, thus with no Landau pole. We illustrate\nthe point within the case of QED. This way, we explicitly realize the\ncorrespondence between the nonperturbative Landau pole scale and the\nrenormalons. What is seen as a Landau pole in perturbation theory is cured by\nthe nonperturbative, resurgent contributions.","publication_date":1700210813,"paper_link":"http://arxiv.org/pdf/2311.10393v1","categories":["Mathematics","Physics"],"abstract":"Under certain assumptions and independent of the instantons, we show that the logarithm expansion of dimensional regularization in quantum field theory needs a nonperturbative completion to have a renormalization-group flow valid at all energies. Then, we show that such nonperturbative completion has the analytic properties of the renormalons, which we find with no reference to diagrammatic calculations. We demonstrate that renormalon corrections necessarily lead to analyzable functions, namely, resurgent transseries. A detailed analysis of the resurgent properties of the renormalons is provided. The self-consistency of the theory requires these nonperturbative contributions to render the running coupling well-defined at any energy, thus with no Landau pole. We illustrate the point within the case of QED. This way, we explicitly realize the correspondence between the nonperturbative Landau pole scale and the renormalons. What is seen as a Landau pole in perturbation theory is cured by the nonperturbative, resurgent contributions."}
{"title":"Modular Arithmetic Study on Ascending and Descending Operations Related with Collatz Conjecture","authors":["Kyo Jin Ihn"],"raw_abstract":"Sequence of numbers generated by the recurrence relation based on the Collatz\nconjecture is investigated. An arithmetic operation on the Collatz conjecture\nis called descending operation, and ascending operation is carried out\nreversely to the descending operation. Study on the 3x+1 problem against every\ninteger is relevant to that of descending operation against every odd. Any odds\ncan be generated by ascending operations against odds not multiple of 3, and\nvise versa. Ascending operations against an odd not a multiple of 3 can\ngenerate infinite number of odds in the next generation. The odds multiple of 3\nare terminal numbers of sequences that no further ascending operations are\npossible. Descending operations against the odds 1,5,21,.. reach 1, and\notherwise reach the odds greater than or equal to 5. No two odds except 1 in\neach sequence are the same, and the each descending sequence is unique. Every\ndescending sequence can start from terminal numbers, and reaches ultimately the\norigin of 1 as indicated by the Collatz conjecture. Sequences of odds in the\nsame pattern ignoring the sizes of the odds are generated by ascending\noperations with the same number of doubling operations against the odds in\nmodular arithmetic.","publication_date":1700207079,"paper_link":"http://arxiv.org/pdf/2311.12038v1","categories":["Mathematics"],"abstract":"Sequence of numbers generated by the recurrence relation based on the Collatz conjecture is investigated. An arithmetic operation on the Collatz conjecture is called descending operation, and ascending operation is carried out reversely to the descending operation. Study on the 3x+1 problem against every integer is relevant to that of descending operation against every odd. Any odds can be generated by ascending operations against odds not multiple of 3, and vise versa. Ascending operations against an odd not a multiple of 3 can generate infinite number of odds in the next generation. The odds multiple of 3 are terminal numbers of sequences that no further ascending operations are possible. Descending operations against the odds 1,5,21,.. reach 1, and otherwise reach the odds greater than or equal to 5. No two odds except 1 in each sequence are the same, and the each descending sequence is unique. Every descending sequence can start from terminal numbers, and reaches ultimately the origin of 1 as indicated by the Collatz conjecture. Sequences of odds in the same pattern ignoring the sizes of the odds are generated by ascending operations with the same number of doubling operations against the odds in modular arithmetic."}
{"title":"Fast algorithms for classical specifications of stabiliser states and Clifford gates","authors":["Nadish de Silva","Wilfred Salmon","Ming Yin"],"raw_abstract":"The stabiliser formalism plays a central role in quantum computing, error\ncorrection, and fault-tolerance. Stabiliser states are used to encode quantum\ndata. Clifford gates are those which can be easily performed fault-tolerantly\nin the most common error correction schemes. Their mathematical properties are\nthe subject of significant research interest.\n  Numerical experiments are critical to formulating and testing conjectures\ninvolving the stabiliser formalism. In this note, we provide fast methods for\nverifying that a vector is a stabiliser state, and interconverting between its\nspecification as amplitudes, a quadratic form, and a check matrix. We use these\nto rapidly check if a given unitary matrix is a Clifford gate and to convert\nbetween the matrix of a Clifford gate and its compact specification as a\nstabiliser tableau.\n  We provide implementations of our algorithms in Python that outperform the\nbest-known brute force methods by some orders of magnitude with asymptotic\nimprovements that are exponential in the number of qubits.","publication_date":1700204950,"paper_link":"http://arxiv.org/pdf/2311.10357v1","categories":["Physics"],"abstract":"The stabiliser formalism plays a central role in quantum computing, error correction, and fault-tolerance. Stabiliser states are used to encode quantum data. Clifford gates are those which can be easily performed fault-tolerantly in the most common error correction schemes. Their mathematical properties are the subject of significant research interest.   Numerical experiments are critical to formulating and testing conjectures involving the stabiliser formalism. In this note, we provide fast methods for verifying that a vector is a stabiliser state, and interconverting between its specification as amplitudes, a quadratic form, and a check matrix. We use these to rapidly check if a given unitary matrix is a Clifford gate and to convert between the matrix of a Clifford gate and its compact specification as a stabiliser tableau.   We provide implementations of our algorithms in Python that outperform the best-known brute force methods by some orders of magnitude with asymptotic improvements that are exponential in the number of qubits."}
{"title":"Catales: From topologies without points to categories without objects","authors":["Dusko Pavlovic"],"raw_abstract":"Locales have been studied as \"topologies without points\", mainly by tools of\ncategory theory. While traditional topology presents a space as a set of points\nwith specified neighborhoods, localic topology presents a space as a lattice of\nneighborhoods of unspecified points. Points are derivable as perfect ideals of\nneighborhoods.\n  Another convenient fiction are the objects in category theory. The\ncategorical view of mathematical structures focuses on morphisms and their\ncompositions. Objects tell which morphisms are composable. They are the black\nboxes enclosing structures accessible only along morphisms. Since we cannot\ntell when such structures identical identical, the statements that involve\nobjects' identities have been called evil. Object prohibitions have a moral\naspect.\n  A holiday quest for a categorical approach to categories as black boxes led\nto catales as categories reduced to partial semigroups of morphisms, analogous\nto locales as spaces reduced to lattices of neighborhoods. Just like points can\nbe reconstructed as perfect filters of neighborhoods in locales, objects can be\nreconstructed as the top elements of the lattices of idempotents in catales.","publication_date":1700201211,"paper_link":"http://arxiv.org/pdf/2311.10342v1","categories":["Mathematics"],"abstract":"Locales have been studied as \"topologies without points\", mainly by tools of category theory. While traditional topology presents a space as a set of points with specified neighborhoods, localic topology presents a space as a lattice of neighborhoods of unspecified points. Points are derivable as perfect ideals of neighborhoods.   Another convenient fiction are the objects in category theory. The categorical view of mathematical structures focuses on morphisms and their compositions. Objects tell which morphisms are composable. They are the black boxes enclosing structures accessible only along morphisms. Since we cannot tell when such structures identical identical, the statements that involve objects' identities have been called evil. Object prohibitions have a moral aspect.   A holiday quest for a categorical approach to categories as black boxes led to catales as categories reduced to partial semigroups of morphisms, analogous to locales as spaces reduced to lattices of neighborhoods. Just like points can be reconstructed as perfect filters of neighborhoods in locales, objects can be reconstructed as the top elements of the lattices of idempotents in catales."}
{"title":"Tensor categories of weight modules of $\\widehat{\\mathfrak{sl}}_2$ at admissible level","authors":["Thomas Creutzig"],"raw_abstract":"The category of weight modules $L_k(\\mathfrak{sl}_2)\\text{-wtmod}$ of the\nsimple affine vertex algebra of $\\mathfrak{sl}_2$ at an admissible level $k$ is\nneither finite nor semisimple and modules are usually not lower-bounded and\nhave infinite dimensional conformal weight subspaces. However this vertex\nalgebra enjoys a duality with $W_\\ell(\\mathfrak{sl}_{2|1})$, the simple\nprinicipal $W$-algebra of $\\mathfrak{sl}_{2|1}$ at level $\\ell$ (the $N=2$\nsuper conformal algebra) where the levels are related via $(k+2)(\\ell+1)=1$.\nEvery weight module of $W_\\ell(\\mathfrak{sl}_{2|1})$ is lower-bounded and has\nfinite-dimensional conformal weight spaces. The main technical result is that\nevery weight module of $W_\\ell(\\mathfrak{sl}_{2|1})$ is $C_1$-cofinite. The\nexistence of a vertex tensor category follows and the theory of vertex\nsuperalgebra extensions implies the existence of vertex tensor category\nstructure on $L_k(\\mathfrak{sl}_2)\\text{-wtmod}$ for any admissible level $k$.\nAs applications, the fusion rules of ordinary modules with any weight module\nare computed and it is shown that $L_k(\\mathfrak{sl}_2)\\text{-wtmod}$ is a\nribbon category if and only if $L_{k+1}(\\mathfrak{sl}_2)\\text{-wtmod}$ is, in\nparticular it follows that for admissible levels $k = - 2 + \\frac{u}{v}$ and $v\n\\in \\{2, 3\\}$ and $u = -1 \\mod v$ the category\n$L_k(\\mathfrak{sl}_2)\\text{-wtmod}$ is a ribbon category.","publication_date":1700179453,"paper_link":"http://arxiv.org/pdf/2311.10240v1","categories":["Mathematics","Physics"],"abstract":"The category of weight modules __FORMULA__ of the simple affine vertex algebra of __FORMULA__ at an admissible level __FORMULA__ is neither finite nor semisimple and modules are usually not lower-bounded and have infinite dimensional conformal weight subspaces. However this vertex algebra enjoys a duality with __FORMULA__, the simple prinicipal __FORMULA__-algebra of __FORMULA__ at level __FORMULA__ (the __FORMULA__ super conformal algebra) where the levels are related via __FORMULA__. Every weight module of __FORMULA__ is lower-bounded and has finite-dimensional conformal weight spaces. The main technical result is that every weight module of __FORMULA__ is __FORMULA__-cofinite. The existence of a vertex tensor category follows and the theory of vertex superalgebra extensions implies the existence of vertex tensor category structure on __FORMULA__ for any admissible level __FORMULA__. As applications, the fusion rules of ordinary modules with any weight module are computed and it is shown that __FORMULA__ is a ribbon category if and only if __FORMULA__ is, in particular it follows that for admissible levels __FORMULA__ and __FORMULA__ and __FORMULA__ the category __FORMULA__ is a ribbon category."}
{"title":"Weight representations of affine Kac-Moody algebras and small quantum groups","authors":["Tomoyuki Arakawa","Thomas Creutzig","Kazuya Kawasetsu"],"raw_abstract":"We study the weight modules over affine Kac-Moody algebras from the view\npoint of vertex algebras, and determine the abelian category of weight modules\nfor the simple affine vertex algebra $L_k(\\mathfrak{sl}_2)$ at any non-integral\nadmissible level $k$. In particular, we show that the principal block of the\ncategory of weight modules over admissible $L_k(\\mathfrak{sl}_2)$ is equivalent\nto that of the corresponding (unrolled) small quantum group.","publication_date":1700176217,"paper_link":"http://arxiv.org/pdf/2311.10233v1","categories":["Mathematics","Physics"],"abstract":"We study the weight modules over affine Kac-Moody algebras from the view point of vertex algebras, and determine the abelian category of weight modules for the simple affine vertex algebra __FORMULA__ at any non-integral admissible level __FORMULA__. In particular, we show that the principal block of the category of weight modules over admissible __FORMULA__ is equivalent to that of the corresponding (unrolled) small quantum group."}
{"title":"A Chern-Simons approach to self-dual gravity in (2+1)-dimensions and quantisation of Poisson structure","authors":["Prince K. Osei"],"raw_abstract":"The (2+1)-dimensional analog self-dual gravity which is obtained via\ndimension reduction of the (3+1)-dimensional Holst action without reducing the\ninternal gauge group is studied. A Chern-Simons formulation for this theory is\nconstructed based on the gauge group $SL(2,\\CC)_\\RR\\rcross \\Rsix$ and maps the\n3d complex self-dual dynamical variable and connection to $6d$ real variables\nwhich combines into a $12d$ Cartan connection. Quantization is given by the\napplication of the combinatorial quantisation program of Chern-Simons theory.\nThe Poisson structure for the moduli space of flat connections on\n$(SL(2,\\CC)_\\RR\\rcross \\Rsix)^{n+2g}$ which emerges in the combinatorial\ndescription of the phase space on $\\RR \\times \\Sigma_{g,n},$ where\n$\\Sigma_{g,n}$ is a genus $g$ surface with $n$ punctures is given in terms of\nthe classical $r$-matrix for the quantum double $D(SL(2,\\CC)_\\RR)$ viewed as\nthe double of a double $ D(SU(2)\\dcross AN(2))$. This quantum double provides a\nfeature for quantum symmetries of the quantum theory for the model.","publication_date":1700173120,"paper_link":"http://arxiv.org/pdf/2311.10220v1","categories":["Mathematics","Physics"],"abstract":"The (2+1)-dimensional analog self-dual gravity which is obtained via dimension reduction of the (3+1)-dimensional Holst action without reducing the internal gauge group is studied. A Chern-Simons formulation for this theory is constructed based on the gauge group __FORMULA__ and maps the 3d complex self-dual dynamical variable and connection to __FORMULA__ real variables which combines into a __FORMULA__ Cartan connection. Quantization is given by the application of the combinatorial quantisation program of Chern-Simons theory. The Poisson structure for the moduli space of flat connections on __FORMULA__ which emerges in the combinatorial description of the phase space on __FORMULA__ where __FORMULA__ is a genus __FORMULA__ surface with __FORMULA__ punctures is given in terms of the classical __FORMULA__-matrix for the quantum double __FORMULA__ viewed as the double of a double __FORMULA__. This quantum double provides a feature for quantum symmetries of the quantum theory for the model."}
{"title":"Minutes-duration Optical Flares with Supernova Luminosities","authors":["Anna Y. Q. Ho","Daniel A. Perley","Ping Chen","Steve Schulze","Vik Dhillon","Harsh Kumar","Aswin Suresh","Vishwajeet Swain","Michael Bremer","Stephen J. Smartt","Joseph P. Anderson","G. C. Anupama","Supachai Awiphan","Sudhanshu Barway","Eric C. Bellm","Sagi Ben-Ami","Varun Bhalerao","Thomas de Boer","Thomas G. Brink","Rick Burruss","Poonam Chandra","Ting-Wan Chen","Wen-Ping Chen","Jeff Cooke","Michael W. Coughlin","Kaustav K. Das","Andrew J. Drake","Alexei V. Filippenko","James Freeburn","Christoffer Fremling","Michael D. Fulton","Avishay Gal-Yam","Llu\u00eds Galbany","Hua Gao","Matthew J. Graham","Mariusz Gromadzki","Claudia P. Guti\u00e9rrez","K-Ryan Hinds","Cosimo Inserra","Nayana A. J.","Viraj Karambelkar","Mansi M. Kasliwal","Shri Kulkarni","Tom\u00e1s E. M\u00fcller-Bravo","Eugene A. Magnier","Ashish A. Mahabal","Thomas Moore","Chow-Choong Ngeow","Matt Nicholl","Eran O. Ofek","Conor M. B. Omand","Francesca Onori","Yen-Chen Pan","Priscila J. Pessi","Glen Petitpas","David Polishook","Saran Poshyachinda","Miika Pursiainen","Reed Riddle","Antonio C. Rodriguez","Ben Rusholme","Enrico Segre","Yashvi Sharma","Ken W. Smith","Jesper Sollerman","Shubham Srivastav","Nora Linn Strotjohann","Mark Suhr","Dmitry Svinkin","Yanan Wang","Philip Wiseman","Avery Wold","Sheng Yang","Yi Yang","Yuhan Yao","David R. Young","WeiKang Zheng"],"raw_abstract":"In recent years, certain luminous extragalactic optical transients have been\nobserved to last only a few days. Their short observed duration implies a\ndifferent powering mechanism from the most common luminous extragalactic\ntransients (supernovae) whose timescale is weeks. Some short-duration\ntransients, most notably AT2018cow, display blue optical colours and bright\nradio and X-ray emission. Several AT2018cow-like transients have shown hints of\na long-lived embedded energy source, such as X-ray variability, prolonged\nultraviolet emission, a tentative X-ray quasiperiodic oscillation, and large\nenergies coupled to fast (but subrelativistic) radio-emitting ejecta. Here we\nreport observations of minutes-duration optical flares in the aftermath of an\nAT2018cow-like transient, AT2022tsd (the \"Tasmanian Devil\"). The flares occur\nover a period of months, are highly energetic, and are likely nonthermal,\nimplying that they arise from a near-relativistic outflow or jet. Our\nobservations confirm that in some AT2018cow-like transients the embedded energy\nsource is a compact object, either a magnetar or an accreting black hole.","publication_date":1700168278,"paper_link":"http://arxiv.org/pdf/2311.10195v1","categories":["Physics"],"abstract":"In recent years, certain luminous extragalactic optical transients have been observed to last only a few days. Their short observed duration implies a different powering mechanism from the most common luminous extragalactic transients (supernovae) whose timescale is weeks. Some short-duration transients, most notably AT2018cow, display blue optical colours and bright radio and X-ray emission. Several AT2018cow-like transients have shown hints of a long-lived embedded energy source, such as X-ray variability, prolonged ultraviolet emission, a tentative X-ray quasiperiodic oscillation, and large energies coupled to fast (but subrelativistic) radio-emitting ejecta. Here we report observations of minutes-duration optical flares in the aftermath of an AT2018cow-like transient, AT2022tsd (the \"Tasmanian Devil\"). The flares occur over a period of months, are highly energetic, and are likely nonthermal, implying that they arise from a near-relativistic outflow or jet. Our observations confirm that in some AT2018cow-like transients the embedded energy source is a compact object, either a magnetar or an accreting black hole."}
{"title":"An optimization dichotomy for capital injections and absolutely continuous dividend strategies","authors":["Jean-Fran\u00e7ois Renaud","Alexandre Roch","Clarence Simard"],"raw_abstract":"We consider an optimal stochastic control problem in which a firm's\ncash/surplus process is controlled by dividend payments and capital injections.\nStockholders aim to maximize their dividend stream minus the cost of injecting\ncapital, if needed. We consider absolutely continuous dividend policies subject\nto a level-dependent upper bound on the dividend rate while we allow for\ngeneral capital injections behavior. We prove that the optimal strategy can\nonly be of two types: dividends are paid according to a \\textit{mean-reverting}\nstrategy with capital injections performed each time the cash process reaches\nzero; or, dividends are paid according to another \\textit{mean-reverting}\nstrategy and no injection of capital is ever made, until ruin is reached. We\ngive a complete solution to this problem and characterize this dichotomy by\ncomparing (the derivatives of) the value functions at zero of two sub-problems.\nThe first sub-problem is concerned solely with the maximization of dividends,\nwhile the second sub-problem is the corresponding bail-out optimal dividend\nproblem for which we provide also a complete solution.","publication_date":1700167338,"paper_link":"http://arxiv.org/pdf/2311.10191v1","categories":["Mathematics","Quantitative Finance"],"abstract":"We consider an optimal stochastic control problem in which a firm's cash/surplus process is controlled by dividend payments and capital injections. Stockholders aim to maximize their dividend stream minus the cost of injecting capital, if needed. We consider absolutely continuous dividend policies subject to a level-dependent upper bound on the dividend rate while we allow for general capital injections behavior. We prove that the optimal strategy can only be of two types: dividends are paid according to a mean-reverting strategy with capital injections performed each time the cash process reaches zero; or, dividends are paid according to another mean-reverting strategy and no injection of capital is ever made, until ruin is reached. We give a complete solution to this problem and characterize this dichotomy by comparing (the derivatives of) the value functions at zero of two sub-problems. The first sub-problem is concerned solely with the maximization of dividends, while the second sub-problem is the corresponding bail-out optimal dividend problem for which we provide also a complete solution."}
{"title":"An analytical investigation into solute transport and sorption via intra-particle diffusion in the dual-porosity limit","authors":["Lucy C. Auton","Maria Aguareles","Abel Valverde","Timothy G. Myers","Marc Calvo-Schwarzwalder"],"raw_abstract":"We develop a mathematical model for adsorption based on averaging the flow\naround, and diffusion inside, adsorbent particles in a column. The model\ninvolves three coupled partial differential equations for the contaminant\nconcentration both in the carrier fluid and within the particle as well as the\nadsorption rate. The adsorption rate is modelled using the Sips equation, which\nis suitable for describing both physical and chemical adsorption mechanisms.\nNon-dimensionalisation is used to determine the controlling parameter groups as\nwell as to determine negligible terms and so reduce the system complexity. The\ninclusion of intra-particle diffusion introduces new dimensionless parameters\nto those found in standard works, including a form of internal Damk\\\"ohler\nnumber and a new characteristic time scale. We provide a numerical method for\nthe full model and show how in certain situations a travelling wave approach\ncan be utilized to find analytical solutions. The model is validated against\navailable experimental data for the removal of Mercury(II) and CO$_\\text{2}$.\nThe results show excellent agreement with measurements of column outlet\ncontaminant concentration and provide insights into the underlying chemical\nreactions.","publication_date":1700163251,"paper_link":"http://arxiv.org/pdf/2311.10161v1","categories":["Physics"],"abstract":"We develop a mathematical model for adsorption based on averaging the flow around, and diffusion inside, adsorbent particles in a column. The model involves three coupled partial differential equations for the contaminant concentration both in the carrier fluid and within the particle as well as the adsorption rate. The adsorption rate is modelled using the Sips equation, which is suitable for describing both physical and chemical adsorption mechanisms. Non-dimensionalisation is used to determine the controlling parameter groups as well as to determine negligible terms and so reduce the system complexity. The inclusion of intra-particle diffusion introduces new dimensionless parameters to those found in standard works, including a form of internal Damk\\\"ohler number and a new characteristic time scale. We provide a numerical method for the full model and show how in certain situations a travelling wave approach can be utilized to find analytical solutions. The model is validated against available experimental data for the removal of Mercury(II) and CO__FORMULA__. The results show excellent agreement with measurements of column outlet contaminant concentration and provide insights into the underlying chemical reactions."}
{"title":"Notes on the Sum-Rank Weight of a Matrix with Restricted Rank","authors":["Hugo Sauerbier Couv\u00e9e","Hedongliang Liu"],"raw_abstract":"These notes cover a few calculations regarding the sum-rank weight of a\nmatrix in relation to its rank. In particular, a formula and lower bounds are\ngiven on the probability that a matrix of rank $t$ consisting of $\\ell$ blocks\nhas sum-rank weight $\\ell t$.","publication_date":1700163157,"paper_link":"http://arxiv.org/pdf/2311.10159v1","categories":["Mathematics"],"abstract":"These notes cover a few calculations regarding the sum-rank weight of a matrix in relation to its rank. In particular, a formula and lower bounds are given on the probability that a matrix of rank __FORMULA__ consisting of __FORMULA__ blocks has sum-rank weight __FORMULA__."}
{"title":"Critical well-posedness for the 2D Peskin problem with general tension","authors":["Eduardo Garc\u00eda-Ju\u00e1rez","Susanna V. Haziot"],"raw_abstract":"In this paper, we study the two dimensional Peskin problem with general\nelasticity law. Specifically, we prove global regularity for small\nperturbations, in suitable critical spaces, of the circle solution, possibly\ncontaining corners. For such initial data we prove asymptotic stability in the\nsense that as $t\\to\\infty$, the solution converges to a translated and rotated\ndisk.","publication_date":1700162970,"paper_link":"http://arxiv.org/pdf/2311.10157v1","categories":["Mathematics","Physics"],"abstract":"In this paper, we study the two dimensional Peskin problem with general elasticity law. Specifically, we prove global regularity for small perturbations, in suitable critical spaces, of the circle solution, possibly containing corners. For such initial data we prove asymptotic stability in the sense that as __FORMULA__, the solution converges to a translated and rotated disk."}
{"title":"A Systems-Theoretical Formalization of Closed Systems","authors":["Niloofar Shadab","Tyler Cody","Alejandro Salado","Peter Beling"],"raw_abstract":"There is a lack of formalism for some key foundational concepts in systems\nengineering. One of the most recently acknowledged deficits is the inadequacy\nof systems engineering practices for engineering intelligent systems. In our\nprevious works, we proposed that closed systems precepts could be used to\naccomplish a required paradigm shift for the systems engineering of intelligent\nsystems. However, to enable such a shift, formal foundations for closed systems\nprecepts that expand the theory of systems engineering are needed. The concept\nof closure is a critical concept in the formalism underlying closed systems\nprecepts. In this paper, we provide formal, systems- and information-theoretic\ndefinitions of closure to identify and distinguish different types of closed\nsystems. Then, we assert a mathematical framework to evaluate the subjective\nformation of the boundaries and constraints of such systems. Finally, we argue\nthat engineering an intelligent system can benefit from appropriate closed and\nopen systems paradigms on multiple levels of abstraction of the system. In the\nmain, this framework will provide the necessary fundamentals to aid in systems\nengineering of intelligent systems.","publication_date":1700161261,"paper_link":"http://arxiv.org/pdf/2311.10786v1","categories":["Mathematics"],"abstract":"There is a lack of formalism for some key foundational concepts in systems engineering. One of the most recently acknowledged deficits is the inadequacy of systems engineering practices for engineering intelligent systems. In our previous works, we proposed that closed systems precepts could be used to accomplish a required paradigm shift for the systems engineering of intelligent systems. However, to enable such a shift, formal foundations for closed systems precepts that expand the theory of systems engineering are needed. The concept of closure is a critical concept in the formalism underlying closed systems precepts. In this paper, we provide formal, systems- and information-theoretic definitions of closure to identify and distinguish different types of closed systems. Then, we assert a mathematical framework to evaluate the subjective formation of the boundaries and constraints of such systems. Finally, we argue that engineering an intelligent system can benefit from appropriate closed and open systems paradigms on multiple levels of abstraction of the system. In the main, this framework will provide the necessary fundamentals to aid in systems engineering of intelligent systems."}
{"title":"Worldsheet from worldline","authors":["Umut Gursoy","Guim Planella Planas"],"raw_abstract":"We take a step toward a \"microscopic\" derivation of gauge-string duality. In\nparticular, using mathematical techniques of Strebel differentials and discrete\nexterior calculus, we obtain a bosonic string worldsheet action for a string\nembedded in d+1 dimensional asymptotically AdS space from multi-loop Feynman\ngraphs of a quantum field theory of scalar matrices in d-dimensions in the\nlimit of diverging loop number. Our work is building on the program started by\n't Hooft in 1974, this time including the holographic dimension which we show\nto emerge from the continuum of Schwinger parameters of Feynman diagrams.","publication_date":1700161204,"paper_link":"http://arxiv.org/pdf/2311.10142v1","categories":["Physics"],"abstract":"We take a step toward a \"microscopic\" derivation of gauge-string duality. In particular, using mathematical techniques of Strebel differentials and discrete exterior calculus, we obtain a bosonic string worldsheet action for a string embedded in d+1 dimensional asymptotically AdS space from multi-loop Feynman graphs of a quantum field theory of scalar matrices in d-dimensions in the limit of diverging loop number. Our work is building on the program started by 't Hooft in 1974, this time including the holographic dimension which we show to emerge from the continuum of Schwinger parameters of Feynman diagrams."}
{"title":"Long time justification of wave turbulence theory","authors":["Yu Deng","Zaher Hani"],"raw_abstract":"In a series of previous works (arXiv:2104.11204, arXiv:2110.04565,\narXiv:2301.07063), we gave a rigorous derivation of the homogeneous wave\nkinetic equation (WKE) up to small multiples of the kinetic timescale, which\ncorresponds to short time solutions to the wave kinetic equation. In this work,\nwe extend this justification to arbitrarily long times that cover the full\nlifespan of the WKE. This is the first long-time derivation ever obtained in\nany nonlinear (particle or wave) collisional kinetic limit.","publication_date":1700159902,"paper_link":"http://arxiv.org/pdf/2311.10082v1","categories":["Mathematics","Physics"],"abstract":"In a series of previous works (arXiv:2104.11204, arXiv:2110.04565, arXiv:2301.07063), we gave a rigorous derivation of the homogeneous wave kinetic equation (WKE) up to small multiples of the kinetic timescale, which corresponds to short time solutions to the wave kinetic equation. In this work, we extend this justification to arbitrarily long times that cover the full lifespan of the WKE. This is the first long-time derivation ever obtained in any nonlinear (particle or wave) collisional kinetic limit."}
{"title":"Unconventional conformal invariance of maximal depth partially massless fields on $dS_{4}$ and its relation to complex partially massless SUSY","authors":["Vasileios A. Letsios"],"raw_abstract":"Although the conformal invariance of maximal depth partially massless\ntheories of higher (integer) spin on four-dimensional de Sitter spacetime\n($dS_{4}$) has been demonstrated by Deser and Waldron, the corresponding\ncovariant expressions for the infinitesimal conformal symmetry transformations\nof the fields are absent from the literature. In this article, we present these\nexpressions for the first time. In the spin-2 case, the invariance of both the\nfield equations and the covariant action is demonstrated. For spin-$s >2$, the\ninvariance is demonstrated only at the level of the field equations. For all\nspins $s \\geq 2$, we show that the algebra does not close on the conformal\nalgebra, $so(4,2)$, (in agreement with previous statements in the literature)\ndue to the appearance of new symmetry transformations that contain higher\nderivatives. We thus call our conformal transformations `unconventional'. Our\nresults concerning the closure of the full symmetry algebra are inconclusive.\nThen we shift focus to the question of supersymmetry (SUSY) on $dS_{4}$ and our\nobjective is twofold. First, we uncover a non-interacting supermultiplet that\nconsists of a complex partially massless spin-2 field and a complex spin-3/2\nfield with zero mass parameter on $dS_{4}$. Second, we showcase the appearance\nof the unconventional conformal symmetries in the bosonic subalgebra of our\nsupermultiplet (the bosonic subalgebra is neither $so(4,1)$ nor $so(4,2)$,\nwhile its full structure is currently unknown). Open questions arising from our\nfindings are also discussed.","publication_date":1700157297,"paper_link":"http://arxiv.org/pdf/2311.10060v3","categories":["Mathematics","Physics"],"abstract":"Although the conformal invariance of maximal depth partially massless theories of higher (integer) spin on four-dimensional de Sitter spacetime (__FORMULA__) has been demonstrated by Deser and Waldron, the corresponding covariant expressions for the infinitesimal conformal symmetry transformations of the fields are absent from the literature. In this article, we present these expressions for the first time. In the spin-2 case, the invariance of both the field equations and the covariant action is demonstrated. For spin-__FORMULA__, the invariance is demonstrated only at the level of the field equations. For all spins __FORMULA__, we show that the algebra does not close on the conformal algebra, __FORMULA__, (in agreement with previous statements in the literature) due to the appearance of new symmetry transformations that contain higher derivatives. We thus call our conformal transformations `unconventional'. Our results concerning the closure of the full symmetry algebra are inconclusive. Then we shift focus to the question of supersymmetry (SUSY) on __FORMULA__ and our objective is twofold. First, we uncover a non-interacting supermultiplet that consists of a complex partially massless spin-2 field and a complex spin-3/2 field with zero mass parameter on __FORMULA__. Second, we showcase the appearance of the unconventional conformal symmetries in the bosonic subalgebra of our supermultiplet (the bosonic subalgebra is neither __FORMULA__ nor __FORMULA__, while its full structure is currently unknown). Open questions arising from our findings are also discussed."}
{"title":"Spontaneous Opinion Swings in the Voter Model with Latency","authors":["Giovanni Palermo","Anna Mancini","Antonio Desiderio","Riccardo Di Clemente","Giulio Cimini"],"raw_abstract":"The cognitive process of opinion formation is often characterized by\nstubbornness or resistance of agents to changes of opinion. To capture such a\nfeature we introduce a constant latency time in the standard voter model of\nopinion dynamics: after switching opinion, an agent must keep it for a while.\nThis seemingly simple modification drastically changes the stochastic diffusive\nbehavior of the original model, leading to deterministic dynamical oscillations\nin the average opinion of the agents. We explain the origin of the oscillations\nand develop a mathematical formulation of the dynamics that is confirmed by\nextensive numerical simulations. We further characterize the rich phase space\nof the model and its asymptotic behavior. Our work offers insights into\nunderstanding and modeling opinion swings in diverse social contexts.","publication_date":1700156484,"paper_link":"http://arxiv.org/pdf/2311.10045v1","categories":["Physics"],"abstract":"The cognitive process of opinion formation is often characterized by stubbornness or resistance of agents to changes of opinion. To capture such a feature we introduce a constant latency time in the standard voter model of opinion dynamics: after switching opinion, an agent must keep it for a while. This seemingly simple modification drastically changes the stochastic diffusive behavior of the original model, leading to deterministic dynamical oscillations in the average opinion of the agents. We explain the origin of the oscillations and develop a mathematical formulation of the dynamics that is confirmed by extensive numerical simulations. We further characterize the rich phase space of the model and its asymptotic behavior. Our work offers insights into understanding and modeling opinion swings in diverse social contexts."}
{"title":"Convergence of bipartite open quantum systems stabilized by reservoir engineering","authors":["R\u00e9mi Robin","Pierre Rouchon","Lev-Arcady Sellem"],"raw_abstract":"We study a generic family of Lindblad master equations modeling bipartite\nopen quantum systems, where one tries to stabilize a quantum system by\ncarefully designing its interaction with another, dissipative, quantum system -\na strategy known as quantum reservoir engineering. We provide sufficient\nconditions for convergence of the considered Lindblad equations; our setting\naccommodates the case where steady states are not unique but rather supported\non a given subspace of the underlying Hilbert space. We apply our result to a\nLindblad master equation proposed for the stabilization of so-called cat\nqubits, a system that received considerable attention in recent years due to\nits potential applications in quantum information processing.","publication_date":1700156076,"paper_link":"http://arxiv.org/pdf/2311.10037v1","categories":["Mathematics","Physics"],"abstract":"We study a generic family of Lindblad master equations modeling bipartite open quantum systems, where one tries to stabilize a quantum system by carefully designing its interaction with another, dissipative, quantum system - a strategy known as quantum reservoir engineering. We provide sufficient conditions for convergence of the considered Lindblad equations; our setting accommodates the case where steady states are not unique but rather supported on a given subspace of the underlying Hilbert space. We apply our result to a Lindblad master equation proposed for the stabilization of so-called cat qubits, a system that received considerable attention in recent years due to its potential applications in quantum information processing."}
{"title":"A Tale of Three Coauthors","authors":["Barry Simon"],"raw_abstract":"We tell the story of the discovery of an interesting bound on finite sums and\nits application to comparison of Ising models.","publication_date":1700155807,"paper_link":"http://arxiv.org/pdf/2311.10031v1","categories":["Mathematics","Physics"],"abstract":"We tell the story of the discovery of an interesting bound on finite sums and its application to comparison of Ising models."}
{"title":"Worst-Case Optimal Investment in Incomplete Markets","authors":["Sascha Desmettre","Sebastian Merkel","Annalena Mickel","Alexander Steinicke"],"raw_abstract":"We study and solve the worst-case optimal portfolio problem as pioneered by\nKorn and Wilmott (2002) of an investor with logarithmic preferences facing the\npossibility of a market crash with stochastic market coefficients by enhancing\nthe martingale approach developed by Seifried in 2010. With the help of\nbackward stochastic differential equations (BSDEs), we are able to characterize\nthe resulting indifference optimal strategies in a fairly general setting. We\nalso deal with the question of existence of those indifference strategies for\nmarket models with an unbounded market price of risk. We therefore solve the\ncorresponding BSDEs via solving their associated PDEs using a utility\ncrash-exposure transformation. Our approach is subsequently demonstrated for\nHeston's stochastic volatility model, Bates' stochastic volatility model\nincluding jumps, and Kim-Omberg's model for a stochastic excess return.","publication_date":1700154357,"paper_link":"http://arxiv.org/pdf/2311.10021v1","categories":["Mathematics","Quantitative Finance"],"abstract":"We study and solve the worst-case optimal portfolio problem as pioneered by Korn and Wilmott (2002) of an investor with logarithmic preferences facing the possibility of a market crash with stochastic market coefficients by enhancing the martingale approach developed by Seifried in 2010. With the help of backward stochastic differential equations (BSDEs), we are able to characterize the resulting indifference optimal strategies in a fairly general setting. We also deal with the question of existence of those indifference strategies for market models with an unbounded market price of risk. We therefore solve the corresponding BSDEs via solving their associated PDEs using a utility crash-exposure transformation. Our approach is subsequently demonstrated for Heston's stochastic volatility model, Bates' stochastic volatility model including jumps, and Kim-Omberg's model for a stochastic excess return."}
{"title":"Realistic Runtime Analysis for Quantum Simplex Computation","authors":["Sabrina Ammann","Maximilian Hess","Debora Ramacciotti","S\u00e1ndor P. Fekete","Paulina L. A. Goedicke","David Gross","Andreea Lefterovici","Tobias J. Osborne","Michael Perk","Antonio Rotundo","S. E. Skelton","Sebastian Stiller","Timo de Wolff"],"raw_abstract":"In recent years, strong expectations have been raised for the possible power\nof quantum computing for solving difficult optimization problems, based on\ntheoretical, asymptotic worst-case bounds. Can we expect this to have\nconsequences for Linear and Integer Programming when solving instances of\npractically relevant size, a fundamental goal of Mathematical Programming,\nOperations Research and Algorithm Engineering? Answering this question faces a\ncrucial impediment: The lack of sufficiently large quantum platforms prevents\nperforming real-world tests for comparison with classical methods.\n  In this paper, we present a quantum analog for classical runtime analysis\nwhen solving real-world instances of important optimization problems. To this\nend, we measure the expected practical performance of quantum computers by\nanalyzing the expected gate complexity of a quantum algorithm. The lack of\npractical quantum platforms for experimental comparison is addressed by hybrid\nbenchmarking, in which the algorithm is performed on a classical system,\nlogging the expected cost of the various subroutines that are employed by the\nquantum versions. In particular, we provide an analysis of quantum methods for\nLinear Programming, for which recent work has provided asymptotic speedup\nthrough quantum subroutines for the Simplex method. We show that a practical\nquantum advantage for realistic problem sizes would require quantum gate\noperation times that are considerably below current physical limitations.","publication_date":1700151104,"paper_link":"http://arxiv.org/pdf/2311.09995v1","categories":["Mathematics","Physics"],"abstract":"In recent years, strong expectations have been raised for the possible power of quantum computing for solving difficult optimization problems, based on theoretical, asymptotic worst-case bounds. Can we expect this to have consequences for Linear and Integer Programming when solving instances of practically relevant size, a fundamental goal of Mathematical Programming, Operations Research and Algorithm Engineering? Answering this question faces a crucial impediment: The lack of sufficiently large quantum platforms prevents performing real-world tests for comparison with classical methods.   In this paper, we present a quantum analog for classical runtime analysis when solving real-world instances of important optimization problems. To this end, we measure the expected practical performance of quantum computers by analyzing the expected gate complexity of a quantum algorithm. The lack of practical quantum platforms for experimental comparison is addressed by hybrid benchmarking, in which the algorithm is performed on a classical system, logging the expected cost of the various subroutines that are employed by the quantum versions. In particular, we provide an analysis of quantum methods for Linear Programming, for which recent work has provided asymptotic speedup through quantum subroutines for the Simplex method. We show that a practical quantum advantage for realistic problem sizes would require quantum gate operation times that are considerably below current physical limitations."}
{"title":"$q$-Racah probability distribution","authors":["Masahito Hayashi","Akihito Hora","Shintarou Yanagida"],"raw_abstract":"We introduce a certain discrete probability distribution $P_{n,m,k,l;q}$\nhaving non-negative integer parameters $n,m,k,l$ and quantum parameter $q$\nwhich arises from a zonal spherical function of the Grassmannian over the\nfinite field $\\mathbb{F}_q$ with a distinguished spherical vector. Using\nrepresentation theoretic arguments and hypergeometric summation technique, we\nderive the presentation of the probability mass function by a single $q$-Racah\npolynomial, and also the presentation of the cumulative distribution function\nin terms of a terminating ${}_4 \\phi_3$-hypergeometric series.","publication_date":1700150973,"paper_link":"http://arxiv.org/pdf/2311.09992v1","categories":["Mathematics","Physics"],"abstract":"We introduce a certain discrete probability distribution __FORMULA__ having non-negative integer parameters __FORMULA__ and quantum parameter __FORMULA__ which arises from a zonal spherical function of the Grassmannian over the finite field __FORMULA__ with a distinguished spherical vector. Using representation theoretic arguments and hypergeometric summation technique, we derive the presentation of the probability mass function by a single __FORMULA__-Racah polynomial, and also the presentation of the cumulative distribution function in terms of a terminating __FORMULA__-hypergeometric series."}
{"title":"Deficiency indices for singular magnetic Schr\u00f6dinger operators","authors":["Michele Correggi","Davide Fermi"],"raw_abstract":"We show that the deficiency indices of magnetic Schr\\\"odinger operators with\nseveral local singularities can be computed in terms of the deficiency indices\nof operators carrying just one singularity each. We discuss some applications\nto physically relevant operators.","publication_date":1700150774,"paper_link":"http://arxiv.org/pdf/2311.09987v1","categories":["Mathematics","Physics"],"abstract":"We show that the deficiency indices of magnetic Schr\\\"odinger operators with several local singularities can be computed in terms of the deficiency indices of operators carrying just one singularity each. We discuss some applications to physically relevant operators."}
{"title":"Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation","authors":["Mark Stocks"],"raw_abstract":"We demonstrate a multiplication method based on numbers represented as set of\npolynomial radix 2 indices stored as an integer list. The 'polynomial integer\nindex multiplication' method is a set of algorithms implemented in python code.\nWe demonstrate the method to be faster than both the Number Theoretic Transform\n(NTT) and Karatsuba for multiplication within a certain bit range. Also\nimplemented in python code for comparison purposes with the polynomial radix 2\ninteger method. We demonstrate that it is possible to express any integer or\nreal number as a list of integer indices, representing a finite series in base\ntwo. The finite series of integer index representation of a number can then be\nstored and distributed across multiple CPUs / GPUs. We show that operations of\naddition and multiplication can be applied as two's complement additions\noperating on the index integer representations and can be fully distributed\nacross a given CPU / GPU architecture. We demonstrate fully distributed\narithmetic operations such that the 'polynomial integer index multiplication'\nmethod overcomes the current limitation of parallel multiplication methods. Ie,\nthe need to share common core memory and common disk for the calculation of\nresults and intermediate results.","publication_date":1700144473,"paper_link":"http://arxiv.org/pdf/2311.09922v1","categories":["Mathematics"],"abstract":"We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distributed arithmetic operations such that the 'polynomial integer index multiplication' method overcomes the current limitation of parallel multiplication methods. Ie, the need to share common core memory and common disk for the calculation of results and intermediate results."}
{"title":"Self-similar shock dynamics satisfying the inviscid Burgers equation in planar, cylindrical and spherical problems","authors":["Matei Ioan R\u0103dulescu"],"raw_abstract":"The solution of self-similar shock dynamics satisfying the inviscid Burgers\nequation are provided in closed form for planar, cylindrical and spherical\nproblems. The approach follows Lee's method for obtaining self-similar\nsolutions for the Euler equations describing compressible fluid dynamics.\nClosed form solutions are provided for the two types of self-similar solutions:\nof the first kind, where the shock dynamics are constrained by integral\nrelations, and the second kind, constrained by the internal requirement of the\nregularity of solutions along limiting characteristics. The solutions obtained\nillustrate simply the theoretical underpinning of Taylor-Sedov blast waves\n(self-similarity of the first kind) and Guderley implosion problems\n(self-similarity of the second kind).","publication_date":1700143176,"paper_link":"http://arxiv.org/pdf/2311.09909v1","categories":["Mathematics","Physics"],"abstract":"The solution of self-similar shock dynamics satisfying the inviscid Burgers equation are provided in closed form for planar, cylindrical and spherical problems. The approach follows Lee's method for obtaining self-similar solutions for the Euler equations describing compressible fluid dynamics. Closed form solutions are provided for the two types of self-similar solutions: of the first kind, where the shock dynamics are constrained by integral relations, and the second kind, constrained by the internal requirement of the regularity of solutions along limiting characteristics. The solutions obtained illustrate simply the theoretical underpinning of Taylor-Sedov blast waves (self-similarity of the first kind) and Guderley implosion problems (self-similarity of the second kind)."}
{"title":"Capacitated Network Bargaining Games: Stability and Structure","authors":["Laura Sanit\u00e0","Lucy Verberk"],"raw_abstract":"Capacitated network bargaining games are popular combinatorial games that\ninvolve the structure of matchings in graphs. We show that it is always\npossible to stabilize unweighted instances of this problem (that is, ensure\nthat they admit a stable outcome) via capacity-reduction and edge-removal\noperations, without decreasing the total value that the players can get.\n  Furthermore, for general weighted instances, we show that computing a minimum\namount of vertex-capacity to reduce to make an instance stable is a\npolynomial-time solvable problem. We then exploit this to give approximation\nresults for the NP-hard problem of stabilizing a graph via edge-removal\noperations.\n  Our work extends and generalizes previous results in the literature that\ndealt with an uncapacitated version of the problem, using several new\narguments. In particular, while previous results mainly used combinatorial\ntechniques, we here rely on polyhedral arguments and, more specifically, on the\nnotion of circuits of a polytope.","publication_date":1700142974,"paper_link":"http://arxiv.org/pdf/2311.09904v1","categories":["Mathematics"],"abstract":"Capacitated network bargaining games are popular combinatorial games that involve the structure of matchings in graphs. We show that it is always possible to stabilize unweighted instances of this problem (that is, ensure that they admit a stable outcome) via capacity-reduction and edge-removal operations, without decreasing the total value that the players can get.   Furthermore, for general weighted instances, we show that computing a minimum amount of vertex-capacity to reduce to make an instance stable is a polynomial-time solvable problem. We then exploit this to give approximation results for the NP-hard problem of stabilizing a graph via edge-removal operations.   Our work extends and generalizes previous results in the literature that dealt with an uncapacitated version of the problem, using several new arguments. In particular, while previous results mainly used combinatorial techniques, we here rely on polyhedral arguments and, more specifically, on the notion of circuits of a polytope."}
{"title":"Spectrum of Hatano-Nelson model with strictly ergodic potentials","authors":["Xueyin Wang","Zhenfu Wang","Jiangong You","Qi Zhou"],"raw_abstract":"We provide a precise formula for the spectrum of the Hatano-Nelson model with\nstrictly ergodic potentials in terms of its Lyapunov exponent. As applications,\none clearly observes the real-complex spectrum transition. Moreover, if the\nLyapunov exponent is continuous, the spectrum of the Hatano-Nelson model in\n$\\ell^{2}(\\mathbb{Z})$ can be approximated by the spectrum of its\nfinite-interval truncation with periodic boundary conditions. Both of these\nresults are strikingly different from the Hatano-Nelson model with random\npotentials \\cite{Dav01A, Dav01, Dav02}.","publication_date":1700142732,"paper_link":"http://arxiv.org/pdf/2311.09899v1","categories":["Mathematics","Physics"],"abstract":"We provide a precise formula for the spectrum of the Hatano-Nelson model with strictly ergodic potentials in terms of its Lyapunov exponent. As applications, one clearly observes the real-complex spectrum transition. Moreover, if the Lyapunov exponent is continuous, the spectrum of the Hatano-Nelson model in __FORMULA__ can be approximated by the spectrum of its finite-interval truncation with periodic boundary conditions. Both of these results are strikingly different from the Hatano-Nelson model with random potentials Dav01A, Dav01, Dav02."}
{"title":"Dynamic modeling of an alkaline electrolyzer plant for process simulation and optimization","authors":["Nicola Cantisani","Josefine Dovits","John Bagterp J\u00f8rgensen"],"raw_abstract":"We develop a mathematical model for dynamical simulation of an alkaline\nelectrolyzer plant. We model each component of the system with mass and energy\nbalances. Our modeling strategy consists of a rigorous and systematic\nformulation using differential algebraic equations (DAE), along with a\nthermodynamic library that evaluates thermophysical properties. We show steady\nstate diagrams for the electrolyzer stack, and perform dynamic simulations.\nDynamic modelling of an electrolyzer enables simulation and model-based\noptimization and control for optimal hydrogen production under varying\noperating conditions.","publication_date":1700140779,"paper_link":"http://arxiv.org/pdf/2311.09882v1","categories":["Electrical Engineering and Systems Science"],"abstract":"We develop a mathematical model for dynamical simulation of an alkaline electrolyzer plant. We model each component of the system with mass and energy balances. Our modeling strategy consists of a rigorous and systematic formulation using differential algebraic equations (DAE), along with a thermodynamic library that evaluates thermophysical properties. We show steady state diagrams for the electrolyzer stack, and perform dynamic simulations. Dynamic modelling of an electrolyzer enables simulation and model-based optimization and control for optimal hydrogen production under varying operating conditions."}
{"title":"Polynomially Over-Parameterized Convolutional Neural Networks Contain Structured Strong Winning Lottery Tickets","authors":["Arthur da Cunha","Francesco d'Amore","Emanuele Natale"],"raw_abstract":"The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised\nneural networks likely contain subnetworks that perform well without any\ntraining. Although unstructured pruning has been extensively studied in this\ncontext, its structured counterpart, which can deliver significant\ncomputational and memory efficiency gains, has been largely unexplored. One of\nthe main reasons for this gap is the limitations of the underlying mathematical\ntools used in formal analyses of the SLTH. In this paper, we overcome these\nlimitations: we leverage recent advances in the multidimensional generalisation\nof the Random Subset-Sum Problem and obtain a variant that admits the\nstochastic dependencies that arise when addressing structured pruning in the\nSLTH. We apply this result to prove, for a wide class of random Convolutional\nNeural Networks, the existence of structured subnetworks that can approximate\nany sufficiently smaller network.\n  This result provides the first sub-exponential bound around the SLTH for\nstructured pruning, opening up new avenues for further research on the\nhypothesis and contributing to the understanding of the role of\nover-parameterization in deep learning.","publication_date":1700138325,"paper_link":"http://arxiv.org/pdf/2311.09858v1","categories":["Mathematics"],"abstract":"The Strong Lottery Ticket Hypothesis (SLTH) states that randomly-initialised neural networks likely contain subnetworks that perform well without any training. Although unstructured pruning has been extensively studied in this context, its structured counterpart, which can deliver significant computational and memory efficiency gains, has been largely unexplored. One of the main reasons for this gap is the limitations of the underlying mathematical tools used in formal analyses of the SLTH. In this paper, we overcome these limitations: we leverage recent advances in the multidimensional generalisation of the Random Subset-Sum Problem and obtain a variant that admits the stochastic dependencies that arise when addressing structured pruning in the SLTH. We apply this result to prove, for a wide class of random Convolutional Neural Networks, the existence of structured subnetworks that can approximate any sufficiently smaller network.   This result provides the first sub-exponential bound around the SLTH for structured pruning, opening up new avenues for further research on the hypothesis and contributing to the understanding of the role of over-parameterization in deep learning."}
{"title":"Contribution Evaluation in Federated Learning: Examining Current Approaches","authors":["Vasilis Siomos","Jonathan Passerat-Palmbach"],"raw_abstract":"Federated Learning (FL) has seen increasing interest in cases where entities\nwant to collaboratively train models while maintaining privacy and governance\nover their data. In FL, clients with private and potentially heterogeneous data\nand compute resources come together to train a common model without raw data\never leaving their locale. Instead, the participants contribute by sharing\nlocal model updates, which, naturally, differ in quality. Quantitatively\nevaluating the worth of these contributions is termed the Contribution\nEvaluation (CE) problem. We review current CE approaches from the underlying\nmathematical framework to efficiently calculate a fair value for each client.\nFurthermore, we benchmark some of the most promising state-of-the-art\napproaches, along with a new one we introduce, on MNIST and CIFAR-10, to\nshowcase their differences. Designing a fair and efficient CE method, while a\nsmall part of the overall FL system design, is tantamount to the mainstream\nadoption of FL.","publication_date":1700137964,"paper_link":"http://arxiv.org/pdf/2311.09856v1","categories":["Mathematics"],"abstract":"Federated Learning (FL) has seen increasing interest in cases where entities want to collaboratively train models while maintaining privacy and governance over their data. In FL, clients with private and potentially heterogeneous data and compute resources come together to train a common model without raw data ever leaving their locale. Instead, the participants contribute by sharing local model updates, which, naturally, differ in quality. Quantitatively evaluating the worth of these contributions is termed the Contribution Evaluation (CE) problem. We review current CE approaches from the underlying mathematical framework to efficiently calculate a fair value for each client. Furthermore, we benchmark some of the most promising state-of-the-art approaches, along with a new one we introduce, on MNIST and CIFAR-10, to showcase their differences. Designing a fair and efficient CE method, while a small part of the overall FL system design, is tantamount to the mainstream adoption of FL."}
{"title":"Typical dropping asymptotics of quasiclassical approximations to solutions of the nonlinear Schr\u00f6dinger equation","authors":["Sergej Melikhov","Bulat Suleimanov","Azamat Shavlukov"],"raw_abstract":"Formal asymptotics are substantiated that describe typical dropping cusp\nsingularity of quasiclassical approximations to solutions of two cases of the\nintegrable nonlinear Schr\\\"odinger equation\n$-i\\varepsilon\\Psi'_{t}=\\varepsilon^2\\Psi''_{xx}\\pm2|\\Psi| ^2\\Psi$, where\n$\\varepsilon$ is a small parameter. The substantiation uses the ideology and\nfacts of the mathematical catastrophe theory and the part of the theorem of Yu.\nF. Korobeinik, concerning analytical as $h\\to 0$ solutions $G(h,u)$ of the\nmixed type linear equation $hG''_{hh}=G''_{uu}$ to which the hodograph images\nof both cases of the systems of equations of these quasiclassical\napproximations are equivalent.","publication_date":1700137023,"paper_link":"http://arxiv.org/pdf/2311.09845v1","categories":["Mathematics","Physics"],"abstract":"Formal asymptotics are substantiated that describe typical dropping cusp singularity of quasiclassical approximations to solutions of two cases of the integrable nonlinear Schr\\\"odinger equation __FORMULA__, where __FORMULA__ is a small parameter. The substantiation uses the ideology and facts of the mathematical catastrophe theory and the part of the theorem of Yu. F. Korobeinik, concerning analytical as __FORMULA__ solutions __FORMULA__ of the mixed type linear equation __FORMULA__ to which the hodograph images of both cases of the systems of equations of these quasiclassical approximations are equivalent."}
{"title":"Towards Formal Fault Injection for Safety Assessment of Automated Systems","authors":["Ashfaq Farooqui","Behrooz Sangchoolie"],"raw_abstract":"Reasoning about safety, security, and other dependability attributes of\nautonomous systems is a challenge that needs to be addressed before the\nadoption of such systems in day-to-day life. Formal methods is a class of\nmethods that mathematically reason about a system's behavior. Thus, a\ncorrectness proof is sufficient to conclude the system's dependability.\nHowever, these methods are usually applied to abstract models of the system,\nwhich might not fully represent the actual system. Fault injection, on the\nother hand, is a testing method to evaluate the dependability of systems.\nHowever, the amount of testing required to evaluate the system is rather large\nand often a problem. This vision paper introduces formal fault injection, a\nfusion of these two techniques throughout the development lifecycle to enhance\nthe dependability of autonomous systems. We advocate for a more cohesive\napproach by identifying five areas of mutual support between formal methods and\nfault injection. By forging stronger ties between the two fields, we pave the\nway for developing safe and dependable autonomous systems. This paper delves\ninto the integration's potential and outlines future research avenues,\naddressing open challenges along the way.","publication_date":1700134458,"paper_link":"http://arxiv.org/pdf/2311.09810v1","categories":["Mathematics"],"abstract":"Reasoning about safety, security, and other dependability attributes of autonomous systems is a challenge that needs to be addressed before the adoption of such systems in day-to-day life. Formal methods is a class of methods that mathematically reason about a system's behavior. Thus, a correctness proof is sufficient to conclude the system's dependability. However, these methods are usually applied to abstract models of the system, which might not fully represent the actual system. Fault injection, on the other hand, is a testing method to evaluate the dependability of systems. However, the amount of testing required to evaluate the system is rather large and often a problem. This vision paper introduces formal fault injection, a fusion of these two techniques throughout the development lifecycle to enhance the dependability of autonomous systems. We advocate for a more cohesive approach by identifying five areas of mutual support between formal methods and fault injection. By forging stronger ties between the two fields, we pave the way for developing safe and dependable autonomous systems. This paper delves into the integration's potential and outlines future research avenues, addressing open challenges along the way."}
{"title":"Arrow's Impossibility Theorem: Computability in Social Choice Theory","authors":["Alex Hall"],"raw_abstract":"Arrow's Impossibility Theorem establishes bounds on what we can require from\nvoting systems. Given satisfaction of a small collection of \"fairness\" axioms,\nit shows votes can only exist as dictatorships in which one voter determines\nall outcomes. Votes are modelled as maps from a collection of partial orders,\nthe preferences of voters, to a single verdict which is another aggregated\npartial ordering. This result is classic and has an extension called the\nPossibility Theorem that shows these dictatorships needn't exist with infinite\nvoter sets. Mihara extends this work by examining the computability of each of\nthese results. He found that the only voting systems that are in any sense\ncomputable are necessarily dictatorial, which takes away from the usefulness of\nthe Possibility Theorem.\n  In this paper we primarily survey the results of Mihara, focusing not on\napplied consequences, as much of the surrounding literature does, but going\ninto greater details on the underlying Mathematics and Computability of the\nproofs. We give detailed exposition on the methods used and introduce all\nnotation. We first see complete proofs of the classical results and a\nsufficient introduction to computability that an unfamiliar reader should be\nable to follow without prior knowledge of the field. We then expand into\nMihara's results, and using our established knowledge of computability show the\nproblems with trying to compute non-dictatorial social welfare functions. This\ninvolves introducing an extended definition of computability called pairwise\ncomputability.","publication_date":1700132773,"paper_link":"http://arxiv.org/pdf/2311.09789v1","categories":["Mathematics"],"abstract":"Arrow's Impossibility Theorem establishes bounds on what we can require from voting systems. Given satisfaction of a small collection of \"fairness\" axioms, it shows votes can only exist as dictatorships in which one voter determines all outcomes. Votes are modelled as maps from a collection of partial orders, the preferences of voters, to a single verdict which is another aggregated partial ordering. This result is classic and has an extension called the Possibility Theorem that shows these dictatorships needn't exist with infinite voter sets. Mihara extends this work by examining the computability of each of these results. He found that the only voting systems that are in any sense computable are necessarily dictatorial, which takes away from the usefulness of the Possibility Theorem.   In this paper we primarily survey the results of Mihara, focusing not on applied consequences, as much of the surrounding literature does, but going into greater details on the underlying Mathematics and Computability of the proofs. We give detailed exposition on the methods used and introduce all notation. We first see complete proofs of the classical results and a sufficient introduction to computability that an unfamiliar reader should be able to follow without prior knowledge of the field. We then expand into Mihara's results, and using our established knowledge of computability show the problems with trying to compute non-dictatorial social welfare functions. This involves introducing an extended definition of computability called pairwise computability."}
{"title":"Massless field equations for spin 3/2 in dimension 6","authors":["Roman Lavicka","Vladimir Soucek","Wei Wang"],"raw_abstract":"Main topic of the paper is a study of properties of massless fields of spin\n3/2 in its Euclidean version. A lot of information is available already for\nmassless fields in dimension 4. Here, we concentrate on dimension 6 and we are\nusing the fact that the group SL(4,C) is isomorphic with the group Spin(6,C).\nIt makes it possible to use tensor formalism for massless fields. Main problems\ntreated in the paper are a description of fields which need to be considered in\nthe spin 3/2 case, a suitable choice of equations they should satisfy,\nirreducibility of homogeneous solutions of massless field equations, the\nFischer decomposition and the Howe duality for such fields.","publication_date":1700128731,"paper_link":"http://arxiv.org/pdf/2311.09728v1","categories":["Mathematics","Physics"],"abstract":"Main topic of the paper is a study of properties of massless fields of spin 3/2 in its Euclidean version. A lot of information is available already for massless fields in dimension 4. Here, we concentrate on dimension 6 and we are using the fact that the group SL(4,C) is isomorphic with the group Spin(6,C). It makes it possible to use tensor formalism for massless fields. Main problems treated in the paper are a description of fields which need to be considered in the spin 3/2 case, a suitable choice of equations they should satisfy, irreducibility of homogeneous solutions of massless field equations, the Fischer decomposition and the Howe duality for such fields."}
{"title":"Outcome-supervised Verifiers for Planning in Mathematical Reasoning","authors":["Fei Yu","Anningzhe Gao","Benyou Wang"],"raw_abstract":"Large language models (LLMs) often struggle with maintaining accuracy across\na sequence of intermediate reasoning steps in mathematical reasoning, leading\nto error propagation that undermines the final result. The current methodology\nto mitigate this issue primarily involves using a verifier model to assess the\ncorrectness of generated solution candidates, focusing either on the overall\nreasoning path or on an incomplete reasoning path. By rethinking this approach,\nwe argue that assessing potentials of incomplete reasoning paths could be more\nadvantageous as it guides towards correct final answers, transforming the task\ninto a \\textit{planning} problem. Our proposed verifier, the\nOutcome-supervision Value Model (OVM), employs outcome supervision for\ntraining, offering an efficient and intuitive method for \\textit{planning} by\nprioritizing steps that lead to accurate conclusions over mere per-step\ncorrectness. Furthermore, the OVM eschews the need for labor-intensive\nannotations on step-level correctness, enhancing its scalability. Our\nexperiments on two multi-step mathematical reasoning datasets, GSM8K and Game\nof 24, demonstrate the superior performance of the OVM model. Notably, in\nGSM8K, our \\textbf{OVM-7B model achieves state-of-the-art results among LLMs up\nto 13B parameters}; especially it does not utilize GPT-4 or code execution.\nThese findings offer a novel perspective on the role of outcome supervision in\ntraining verifiers for multi-step reasoning tasks and provide theoretical\njustification for its advantage in value estimation for planning.","publication_date":1700128588,"paper_link":"http://arxiv.org/pdf/2311.09724v1","categories":["Mathematics"],"abstract":"Large language models (LLMs) often struggle with maintaining accuracy across a sequence of intermediate reasoning steps in mathematical reasoning, leading to error propagation that undermines the final result. The current methodology to mitigate this issue primarily involves using a verifier model to assess the correctness of generated solution candidates, focusing either on the overall reasoning path or on an incomplete reasoning path. By rethinking this approach, we argue that assessing potentials of incomplete reasoning paths could be more advantageous as it guides towards correct final answers, transforming the task into a planning problem. Our proposed verifier, the Outcome-supervision Value Model (OVM), employs outcome supervision for training, offering an efficient and intuitive method for planning by prioritizing steps that lead to accurate conclusions over mere per-step correctness. Furthermore, the OVM eschews the need for labor-intensive annotations on step-level correctness, enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training verifiers for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for planning."}
{"title":"International System of Quantities library in VDM","authors":["Leo Freitas"],"raw_abstract":"The International Systems of Quantities (ISQ) standard was published in 1960\nto tame the wide diversity of measurement systems being developed across the\nworld, such as the centimetre-gram-second versus the meter-kilogram-second for\nexample. Such a standard is highly motivated by the potential of ``trivial''\n(rather error-prone) mistakes in converting between incompatible units. There\nhave been such accidents in space missions, medical devices, etc. Thus,\nrendering modelling or simulation experiments unusable or unsafe. We address\nthis problem by providing a \\textbf{SAFE}-ISQ VDM-library that is: Simple,\nAccurate, Fast, and Effective. It extends an ecosystem of other VDM\nmathematical toolkit extensions, which include a translation and proof\nenvironment for VDM in Isabelle at https://github.com/leouk/VDM_Toolkit.","publication_date":1700126942,"paper_link":"http://arxiv.org/pdf/2311.09704v1","categories":["Mathematics"],"abstract":"The International Systems of Quantities (ISQ) standard was published in 1960 to tame the wide diversity of measurement systems being developed across the world, such as the centimetre-gram-second versus the meter-kilogram-second for example. Such a standard is highly motivated by the potential of ``trivial'' (rather error-prone) mistakes in converting between incompatible units. There have been such accidents in space missions, medical devices, etc. Thus, rendering modelling or simulation experiments unusable or unsafe. We address this problem by providing a SAFE-ISQ VDM-library that is: Simple, Accurate, Fast, and Effective. It extends an ecosystem of other VDM mathematical toolkit extensions, which include a translation and proof environment for VDM in Isabelle at https://github.com/leouk/VDM_Toolkit."}
{"title":"Partial Stabilization of an Orbiting Satellite Model with a Flexible Attachment","authors":["Julia Kalosha","Yevgeniia Yevgenieva","Alexander Zuyev"],"raw_abstract":"We consider a mathematical model of an orbiting satellite, comprising a rigid\ncarrier body and a flexible boom, operating under the influence of gravity\ngradient torque. This model is represented by a nonlinear control system, which\nincludes ordinary differential equations governing the carrier body's angular\nvelocity and attitude quaternion, coupled with the Euler-Bernoulli equations\nthat describe the vibration of the flexible component. We propose an explicit\nfeedback design aimed at guaranteeing the partial stability of the closed-loop\nsystem in an appropriate Hilbert space.","publication_date":1700125589,"paper_link":"http://arxiv.org/pdf/2311.09691v1","categories":["Mathematics"],"abstract":"We consider a mathematical model of an orbiting satellite, comprising a rigid carrier body and a flexible boom, operating under the influence of gravity gradient torque. This model is represented by a nonlinear control system, which includes ordinary differential equations governing the carrier body's angular velocity and attitude quaternion, coupled with the Euler-Bernoulli equations that describe the vibration of the flexible component. We propose an explicit feedback design aimed at guaranteeing the partial stability of the closed-loop system in an appropriate Hilbert space."}
{"title":"Towards Accurate Quantum Chemical Calculations on Noisy Quantum Computers","authors":["Naoki Iijima","Satoshi Imamura","Mikio Morita","Sho Takemori","Akihiko Kasagi","Yuhei Umeda","Eiji Yoshida"],"raw_abstract":"Variational quantum eigensolver (VQE) is a hybrid quantum-classical algorithm\ndesigned for noisy intermediate-scale quantum (NISQ) computers. It is promising\nfor quantum chemical calculations (QCC) because it can calculate the\nground-state energy of a target molecule. Although VQE has a potential to\nachieve a higher accuracy than classical approximation methods in QCC, it is\nchallenging to achieve it on current NISQ computers due to the significant\nimpact of noises. Density matrix embedding theory (DMET) is a well-known\ntechnique to divide a molecule into multiple fragments, which is available to\nmitigate the noise impact on VQE. However, our preliminary evaluation shows\nthat the naive combination of DMET and VQE does not outperform a gold standard\nclassical method.\n  In this work, we present three approaches to mitigate the noise impact for\nthe DMET+VQE combination. (1) The size of quantum circuits used by VQE is\ndecreased by reducing the number of bath orbitals which represent interactions\nbetween multiple fragments in DMET. (2) Reduced density matrices (RDMs), which\nare used to calculate a molecular energy in DMET, are calculated accurately\nbased on expectation values obtained by executing quantum circuits using a\nnoise-less quantum computer simulator. (3) The parameters of a quantum circuit\noptimized by VQE are refined with mathematical post-processing. The evaluation\nusing a noisy quantum computer simulator shows that our approaches\nsignificantly improve the accuracy of the DMET+VQE combination. Moreover, we\ndemonstrate that on a real NISQ device, the DMET+VQE combination applying our\nthree approaches achieves a higher accuracy than the gold standard classical\nmethod.","publication_date":1700120175,"paper_link":"http://arxiv.org/pdf/2311.09634v1","categories":["Physics"],"abstract":"Variational quantum eigensolver (VQE) is a hybrid quantum-classical algorithm designed for noisy intermediate-scale quantum (NISQ) computers. It is promising for quantum chemical calculations (QCC) because it can calculate the ground-state energy of a target molecule. Although VQE has a potential to achieve a higher accuracy than classical approximation methods in QCC, it is challenging to achieve it on current NISQ computers due to the significant impact of noises. Density matrix embedding theory (DMET) is a well-known technique to divide a molecule into multiple fragments, which is available to mitigate the noise impact on VQE. However, our preliminary evaluation shows that the naive combination of DMET and VQE does not outperform a gold standard classical method.   In this work, we present three approaches to mitigate the noise impact for the DMET+VQE combination. (1) The size of quantum circuits used by VQE is decreased by reducing the number of bath orbitals which represent interactions between multiple fragments in DMET. (2) Reduced density matrices (RDMs), which are used to calculate a molecular energy in DMET, are calculated accurately based on expectation values obtained by executing quantum circuits using a noise-less quantum computer simulator. (3) The parameters of a quantum circuit optimized by VQE are refined with mathematical post-processing. The evaluation using a noisy quantum computer simulator shows that our approaches significantly improve the accuracy of the DMET+VQE combination. Moreover, we demonstrate that on a real NISQ device, the DMET+VQE combination applying our three approaches achieves a higher accuracy than the gold standard classical method."}
{"title":"Geometry and quasi-classical quantization of magnetic monopoles","authors":["I. A. Taimanov"],"raw_abstract":"We present the basic physical and mathematical ideas (P. Curie, Darboux,\nPoincare, Dirac) that led to the concept of magnetic charge, the general\nconstruction of magnetic Laplacians for magnetic monopoles on Riemannian\nmanifolds, and the results of Yu.A. Kordyukov and the author on the\nquasi-classical approximation for the eigensections of these operators.","publication_date":1700113864,"paper_link":"http://arxiv.org/pdf/2311.09586v1","categories":["Mathematics","Physics"],"abstract":"We present the basic physical and mathematical ideas (P. Curie, Darboux, Poincare, Dirac) that led to the concept of magnetic charge, the general construction of magnetic Laplacians for magnetic monopoles on Riemannian manifolds, and the results of Yu.A. Kordyukov and the author on the quasi-classical approximation for the eigensections of these operators."}
{"title":"A Dichotomy Hierarchy Characterizing Linear Time Subgraph Counting in Bounded Degeneracy Graphs","authors":["Daniel Paul-Pena","C. Seshadhri"],"raw_abstract":"Subgraph and homomorphism counting are fundamental algorithmic problems.\nGiven a constant-sized pattern graph $H$ and a large input graph $G$, we wish\nto count the number of $H$-homomorphisms/subgraphs in $G$. Given the massive\nsizes of real-world graphs and the practical importance of counting problems,\nwe focus on when (near) linear time algorithms are possible. The seminal work\nof Chiba-Nishizeki (SICOMP 1985) shows that for bounded degeneracy graphs $G$,\nclique and $4$-cycle counting can be done linear time. Recent works (Bera et\nal, SODA 2021, JACM 2022) show a dichotomy theorem characterizing the patterns\n$H$ for which $H$-homomorphism counting is possible in linear time, for bounded\ndegeneracy inputs $G$. At the other end, Ne\\v{s}et\\v{r}il and Ossona de Mendez\nused their deep theory of \"sparsity\" to define bounded expansion graphs. They\nprove that, for all $H$, $H$-homomorphism counting can be done in linear time\nfor bounded expansion inputs. What lies between? For a specific $H$, can we\ncharacterize input classes where $H$-homomorphism counting is possible in\nlinear time?\n  We discover a hierarchy of dichotomy theorems that precisely answer the above\nquestions. We show the existence of an infinite sequence of graph classes\n$\\mathcal{G}_0$ $\\supseteq$ $\\mathcal{G}_1$ $\\supseteq$ ... $\\supseteq$\n$\\mathcal{G}_\\infty$ where $\\mathcal{G}_0$ is the class of bounded degeneracy\ngraphs, and $\\mathcal{G}_\\infty$ is the class of bounded expansion graphs. Fix\nany constant sized pattern graph $H$. Let $LICL(H)$ denote the length of the\nlongest induced cycle in $H$. We prove the following. If $LICL(H) < 3(r+2)$,\nthen $H$-homomorphisms can be counted in linear time for inputs in\n$\\mathcal{G}_r$. If $LICL(H) \\geq 3(r+2)$, then $H$-homomorphism counting on\ninputs from $\\mathcal{G}_r$ takes $\\Omega(m^{1+\\gamma})$ time. We prove similar\ndichotomy theorems for subgraph counting.","publication_date":1700113223,"paper_link":"http://arxiv.org/pdf/2311.09584v1","categories":["Mathematics"],"abstract":"Subgraph and homomorphism counting are fundamental algorithmic problems. Given a constant-sized pattern graph __FORMULA__ and a large input graph __FORMULA__, we wish to count the number of __FORMULA__-homomorphisms/subgraphs in __FORMULA__. Given the massive sizes of real-world graphs and the practical importance of counting problems, we focus on when (near) linear time algorithms are possible. The seminal work of Chiba-Nishizeki (SICOMP 1985) shows that for bounded degeneracy graphs __FORMULA__, clique and __FORMULA__-cycle counting can be done linear time. Recent works (Bera et al, SODA 2021, JACM 2022) show a dichotomy theorem characterizing the patterns __FORMULA__ for which __FORMULA__-homomorphism counting is possible in linear time, for bounded degeneracy inputs __FORMULA__. At the other end, Nesetril and Ossona de Mendez used their deep theory of \"sparsity\" to define bounded expansion graphs. They prove that, for all __FORMULA__, __FORMULA__-homomorphism counting can be done in linear time for bounded expansion inputs. What lies between? For a specific __FORMULA__, can we characterize input classes where __FORMULA__-homomorphism counting is possible in linear time?   We discover a hierarchy of dichotomy theorems that precisely answer the above questions. We show the existence of an infinite sequence of graph classes __FORMULA__ __FORMULA__ __FORMULA__ __FORMULA__ ... __FORMULA__ __FORMULA__ where __FORMULA__ is the class of bounded degeneracy graphs, and __FORMULA__ is the class of bounded expansion graphs. Fix any constant sized pattern graph __FORMULA__. Let __FORMULA__ denote the length of the longest induced cycle in __FORMULA__. We prove the following. If __FORMULA__, then __FORMULA__-homomorphisms can be counted in linear time for inputs in __FORMULA__. If __FORMULA__, then __FORMULA__-homomorphism counting on inputs from __FORMULA__ takes __FORMULA__ time. We prove similar dichotomy theorems for subgraph counting."}
{"title":"A Meta Logarithmic-Sobolev Inequality for Phase-Covariant Gaussian Channels","authors":["Salman Beigi","Saleh Rahimi-Keshari"],"raw_abstract":"We introduce a meta logarithmic-Sobolev (log-Sobolev) inequality for the\nLindbladian of all single-mode phase-covariant Gaussian channels of bosonic\nquantum systems, and prove that this inequality is saturated by thermal states.\nWe show that our inequality provides a general framework to derive information\ntheoretic results regarding phase-covariant Gaussian channels. Specifically, by\nusing the optimality of thermal states, we explicitly compute the optimal\nconstant $\\alpha_p$, for $1\\leq p\\leq 2$, of the $p$-log-Sobolev inequality\nassociated to the quantum Ornstein-Uhlenbeck semigroup. These constants were\npreviously known for $p=1$ only. Our meta log-Sobolev inequality also enables\nus to provide an alternative proof for the constrained minimum output entropy\nconjecture in the single-mode case. Specifically, we show that for any\nsingle-mode phase-covariant Gaussian channel $\\Phi$, the minimum of the von\nNeumann entropy $S\\big(\\Phi(\\rho)\\big)$ over all single-mode states $\\rho$ with\na given lower bound on $S(\\rho)$, is achieved at a thermal state.","publication_date":1700111657,"paper_link":"http://arxiv.org/pdf/2311.09572v1","categories":["Mathematics","Physics"],"abstract":"We introduce a meta logarithmic-Sobolev (log-Sobolev) inequality for the Lindbladian of all single-mode phase-covariant Gaussian channels of bosonic quantum systems, and prove that this inequality is saturated by thermal states. We show that our inequality provides a general framework to derive information theoretic results regarding phase-covariant Gaussian channels. Specifically, by using the optimality of thermal states, we explicitly compute the optimal constant __FORMULA__, for __FORMULA__, of the __FORMULA__-log-Sobolev inequality associated to the quantum Ornstein-Uhlenbeck semigroup. These constants were previously known for __FORMULA__ only. Our meta log-Sobolev inequality also enables us to provide an alternative proof for the constrained minimum output entropy conjecture in the single-mode case. Specifically, we show that for any single-mode phase-covariant Gaussian channel __FORMULA__, the minimum of the von Neumann entropy __FORMULA__ over all single-mode states __FORMULA__ with a given lower bound on __FORMULA__, is achieved at a thermal state."}
{"title":"Employing Gaussian process priors for studying spatial variation in the parameters of a cardiac action potential model","authors":["Alejandro Nieto Ramos","Christopher L. Krapu","Elizabeth M. Cherry","Flavio H. Fenton"],"raw_abstract":"Cardiac cells exhibit variability in the shape and duration of their action\npotentials in space within a single individual. To create a mathematical model\nof cardiac action potentials (AP) which captures this spatial variability and\nalso allows for rigorous uncertainty quantification regarding within-tissue\nspatial correlation structure, we developed a novel hierarchical Bayesian model\nmaking use of a latent Gaussian process prior on the parameters of a simplified\ncardiac AP model which is used to map forcing behavior to observed voltage\nsignals. This model allows for prediction of cardiac electrophysiological\ndynamics at new points in space and also allows for reconstruction of surface\nelectrical dynamics with a relatively small number of spatial observation\npoints. Furthermore, we make use of Markov chain Monte Carlo methods via the\nStan modeling framework for parameter estimation. We employ a synthetic data\ncase study oriented around the reconstruction of a sparsely-observed spatial\nparameter surface to highlight how this approach can be used for spatial or\nspatiotemporal analyses of cardiac electrophysiology.","publication_date":1700105281,"paper_link":"http://arxiv.org/pdf/2311.10114v1","categories":["Statistics"],"abstract":"Cardiac cells exhibit variability in the shape and duration of their action potentials in space within a single individual. To create a mathematical model of cardiac action potentials (AP) which captures this spatial variability and also allows for rigorous uncertainty quantification regarding within-tissue spatial correlation structure, we developed a novel hierarchical Bayesian model making use of a latent Gaussian process prior on the parameters of a simplified cardiac AP model which is used to map forcing behavior to observed voltage signals. This model allows for prediction of cardiac electrophysiological dynamics at new points in space and also allows for reconstruction of surface electrical dynamics with a relatively small number of spatial observation points. Furthermore, we make use of Markov chain Monte Carlo methods via the Stan modeling framework for parameter estimation. We employ a synthetic data case study oriented around the reconstruction of a sparsely-observed spatial parameter surface to highlight how this approach can be used for spatial or spatiotemporal analyses of cardiac electrophysiology."}
{"title":"FunctionMarker: Watermarking Language Datasets via Knowledge Injection","authors":["Shuai Li","Kejiang Chen","Kunsheng Tang","Wen Huang","Jie Zhang","Weiming Zhang","Nenghai Yu"],"raw_abstract":"Large Language Models (LLMs) have demonstrated superior performance in\nvarious natural language processing tasks. Meanwhile, they require extensive\ntraining data, raising concerns related to dataset copyright protection.\nBackdoor-based watermarking is a viable approach to protect the copyright of\nclassification datasets. However, these methods may introduce malicious\nmisclassification behaviors into watermarked LLMs by attackers and also affect\nthe semantic information of the watermarked text. To address these issues, we\npropose FunctionMarker, a novel copyright protection method for language\ndatasets via knowledge injection. FunctionMarker enables LLMs to learn specific\nknowledge through fine-tuning on watermarked datasets, and we can extract the\nembedded watermark by obtaining the responses of LLMs to specific\nknowledge-related queries. Considering watermark capacity and stealthness, we\nselect customizable functions as specific knowledge for LLMs to learn and embed\nthe watermark into them. Moreover, FunctionMarker can embed multi-bit\nwatermarks while preserving the original semantic information, thereby\nincreasing the difficulty of adaptive attacks. We take mathematical functions\nas an instance to evaluate the effectiveness of FunctionMarker, and experiments\nshow that only 0.3% of watermarked text achieves a 90% watermark extraction\naccuracy in most cases, validating our method's effectiveness.","publication_date":1700104973,"paper_link":"http://arxiv.org/pdf/2311.09535v2","categories":["Mathematics"],"abstract":"Large Language Models (LLMs) have demonstrated superior performance in various natural language processing tasks. Meanwhile, they require extensive training data, raising concerns related to dataset copyright protection. Backdoor-based watermarking is a viable approach to protect the copyright of classification datasets. However, these methods may introduce malicious misclassification behaviors into watermarked LLMs by attackers and also affect the semantic information of the watermarked text. To address these issues, we propose FunctionMarker, a novel copyright protection method for language datasets via knowledge injection. FunctionMarker enables LLMs to learn specific knowledge through fine-tuning on watermarked datasets, and we can extract the embedded watermark by obtaining the responses of LLMs to specific knowledge-related queries. Considering watermark capacity and stealthness, we select customizable functions as specific knowledge for LLMs to learn and embed the watermark into them. Moreover, FunctionMarker can embed multi-bit watermarks while preserving the original semantic information, thereby increasing the difficulty of adaptive attacks. We take mathematical functions as an instance to evaluate the effectiveness of FunctionMarker, and experiments show that only 0.3% of watermarked text achieves a 90% watermark extraction accuracy in most cases, validating our method's effectiveness."}
{"title":"Mori-Zwanzig Modal Decomposition","authors":["Michael Woodward","Yifeng Tian","Yen Ting Lin","Christoph Hader","Hermann Fasel","Daniel Livescu"],"raw_abstract":"We introduce the Mori-Zwanzig (MZ) Modal Decomposition (MZMD), a novel\ntechnique for performing modal analysis of large scale spatio-temporal\nstructures in complex dynamical systems, and show that it represents an\nefficient generalization of Dynamic Mode Decomposition (DMD). The MZ formalism\nprovides a mathematical framework for constructing non-Markovian reduced-order\nmodels of resolved variables from high-dimensional dynamical systems,\nincorporating the effects of unresolved dynamics through the memory kernel and\northogonal dynamics. We present a formulation and analysis of the modes and\nspectrum from MZMD and compare it to DMD when applied to a complex flow: a\nDirect Numerical Simulation (DNS) data-set of laminar-turbulent boundary-layer\ntransition flow over a flared cone at Mach 6. We show that the addition of\nmemory terms by MZMD improves the resolution of spatio-temporal structures\nwithin the transitional/turbulent regime, which contains features that arise\ndue to nonlinear mechanisms, such as the generation of the so-called \"hot\"\nstreaks on the surface of the flared cone. As a result, compared to DMD, MZMD\nimproves future state prediction accuracy, while requiring nearly the same\ncomputational cost.","publication_date":1700103788,"paper_link":"http://arxiv.org/pdf/2311.09524v2","categories":["Physics"],"abstract":"We introduce the Mori-Zwanzig (MZ) Modal Decomposition (MZMD), a novel technique for performing modal analysis of large scale spatio-temporal structures in complex dynamical systems, and show that it represents an efficient generalization of Dynamic Mode Decomposition (DMD). The MZ formalism provides a mathematical framework for constructing non-Markovian reduced-order models of resolved variables from high-dimensional dynamical systems, incorporating the effects of unresolved dynamics through the memory kernel and orthogonal dynamics. We present a formulation and analysis of the modes and spectrum from MZMD and compare it to DMD when applied to a complex flow: a Direct Numerical Simulation (DNS) data-set of laminar-turbulent boundary-layer transition flow over a flared cone at Mach 6. We show that the addition of memory terms by MZMD improves the resolution of spatio-temporal structures within the transitional/turbulent regime, which contains features that arise due to nonlinear mechanisms, such as the generation of the so-called \"hot\" streaks on the surface of the flared cone. As a result, compared to DMD, MZMD improves future state prediction accuracy, while requiring nearly the same computational cost."}
{"title":"Hydrodynamics of polydisperse gas-solid flows: Kinetic theory and multifluid simulation","authors":["Bidan Zhao","Kun Shi","Mingming He","Junwu Wang"],"raw_abstract":"Polydisperse gas-solid flows, which is notoriously difficult to model due to\nthe complex gas-particle and particle-particle interactions, are widely\nencountered in industry. In this article, a refined kinetic theory for\npolydisperse flow is developed, which features single-parameter Chapman-Enskog\nexpansion (the Knudsen number) and exact calculation of the integrations\nrelated to pair distribution function of particle velocity without any\nmathematical approximations. The Navier-Stokes order constitutive relations for\nmultifluid modeling of polydisperse gas-solid flow are then obtained\nanalytically, including the solid stress tensor, the solid-solid drag force,\nthe granular heat flux and the energy dissipation rate. Finally, the model is\npreliminarily validated by showing that the hydrodynamic characteristics of\ngas-solid flows in a bubbling fluidized bed containing bidisperse particles can\nbe successfully predicted.","publication_date":1700097905,"paper_link":"http://arxiv.org/pdf/2311.09495v1","categories":["Physics"],"abstract":"Polydisperse gas-solid flows, which is notoriously difficult to model due to the complex gas-particle and particle-particle interactions, are widely encountered in industry. In this article, a refined kinetic theory for polydisperse flow is developed, which features single-parameter Chapman-Enskog expansion (the Knudsen number) and exact calculation of the integrations related to pair distribution function of particle velocity without any mathematical approximations. The Navier-Stokes order constitutive relations for multifluid modeling of polydisperse gas-solid flow are then obtained analytically, including the solid stress tensor, the solid-solid drag force, the granular heat flux and the energy dissipation rate. Finally, the model is preliminarily validated by showing that the hydrodynamic characteristics of gas-solid flows in a bubbling fluidized bed containing bidisperse particles can be successfully predicted."}
{"title":"An elementary model for an advancing autoignition front in laminar reactive co-flow jets injected into supercritical water","authors":["Amanda Matson","Michael C. Hicks","Uday G. Hegde","Peter V. Gordon"],"raw_abstract":"In this paper we formulate and analyze an elementary model for the\npropagation of advancing autoignition fronts in reactive co-flow fuel/oxidizer\njets injected into an aqueous environment at high pressure. This work is\nmotivated by the experimental studies of autoignition of hydrothermal flames\nperformed at the high pressure laboratory of NASA Glenn Research Center. Guided\nby experimental observations, we use several simplifying assumptions that allow\nthe derivation of a simple, still experimentally feasible, mathematical model\nfor the propagation of advancing ignition fronts. The model consists of a\nsingle diffusion-absorption-advection equation posed in an infinite cylindrical\ndomain with a non-linear condition on the boundary of the cylinder and\ndescribes the temperature distribution within the jet. This model manifests an\ninterplay of thermal diffusion, advection and volumetric heat loss within a\nfuel jet which are balanced by the weak chemical reaction on the jet's\nboundary. We analyze the model by means of asymptotic and numerical techniques\nand discuss feasible regimes of propagation of advancing ignition fronts. In\nparticular, we show that in the most interesting parametric regime when the\nadvancing ignition front is on the verge of extinction this model reduces to a\none dimensional reaction-diffusion equation with bistable non-linearity. We\nhope that the present study will be helpful for the interpretation of existing\nexperimental data and guiding of future experiments.","publication_date":1700096944,"paper_link":"http://arxiv.org/pdf/2311.09485v1","categories":["Physics"],"abstract":"In this paper we formulate and analyze an elementary model for the propagation of advancing autoignition fronts in reactive co-flow fuel/oxidizer jets injected into an aqueous environment at high pressure. This work is motivated by the experimental studies of autoignition of hydrothermal flames performed at the high pressure laboratory of NASA Glenn Research Center. Guided by experimental observations, we use several simplifying assumptions that allow the derivation of a simple, still experimentally feasible, mathematical model for the propagation of advancing ignition fronts. The model consists of a single diffusion-absorption-advection equation posed in an infinite cylindrical domain with a non-linear condition on the boundary of the cylinder and describes the temperature distribution within the jet. This model manifests an interplay of thermal diffusion, advection and volumetric heat loss within a fuel jet which are balanced by the weak chemical reaction on the jet's boundary. We analyze the model by means of asymptotic and numerical techniques and discuss feasible regimes of propagation of advancing ignition fronts. In particular, we show that in the most interesting parametric regime when the advancing ignition front is on the verge of extinction this model reduces to a one dimensional reaction-diffusion equation with bistable non-linearity. We hope that the present study will be helpful for the interpretation of existing experimental data and guiding of future experiments."}
{"title":"On Convex Optimal Value Functions For POSGs","authors":["Rafael F. Cunha","Jacopo Castellini","Johan Peralez","Jilles S. Dibangoye"],"raw_abstract":"Multi-agent planning and reinforcement learning can be challenging when\nagents cannot see the state of the world or communicate with each other due to\ncommunication costs, latency, or noise. Partially Observable Stochastic Games\n(POSGs) provide a mathematical framework for modelling such scenarios. This\npaper aims to improve the efficiency of planning and reinforcement learning\nalgorithms for POSGs by identifying the underlying structure of optimal\nstate-value functions. The approach involves reformulating the original game\nfrom the perspective of a trusted third party who plans on behalf of the agents\nsimultaneously. From this viewpoint, the original POSGs can be viewed as Markov\ngames where states are occupancy states, \\ie posterior probability\ndistributions over the hidden states of the world and the stream of actions and\nobservations that agents have experienced so far. This study mainly proves that\nthe optimal state-value function is a convex function of occupancy states\nexpressed on an appropriate basis in all zero-sum, common-payoff, and\nStackelberg POSGs.","publication_date":1700092101,"paper_link":"http://arxiv.org/pdf/2311.09459v1","categories":["Mathematics"],"abstract":"Multi-agent planning and reinforcement learning can be challenging when agents cannot see the state of the world or communicate with each other due to communication costs, latency, or noise. Partially Observable Stochastic Games (POSGs) provide a mathematical framework for modelling such scenarios. This paper aims to improve the efficiency of planning and reinforcement learning algorithms for POSGs by identifying the underlying structure of optimal state-value functions. The approach involves reformulating the original game from the perspective of a trusted third party who plans on behalf of the agents simultaneously. From this viewpoint, the original POSGs can be viewed as Markov games where states are occupancy states, \\ie posterior probability distributions over the hidden states of the world and the stream of actions and observations that agents have experienced so far. This study mainly proves that the optimal state-value function is a convex function of occupancy states expressed on an appropriate basis in all zero-sum, common-payoff, and Stackelberg POSGs."}
{"title":"Singular Lagrangians in the Faddeev-Jackiw Formalism in Classical Mechanics","authors":["Carlos Manuel L\u00f3pez Arellano","Jaime Manuel Cabrera","Luis Gerardo Romero Hern\u00e1ndez","Jorge Paulin Fuente"],"raw_abstract":"Classical mechanical systems with internal constraints will be studied using\nthe extended symplectic formalism of Faddeev-Jackiw. Generalized brackets of\nthe theory and equations of motion will be derived. A gauge system will be\nexamined, leading to the associated gauge transformations. The results will be\ncompared with the Dirac-Bergmann algorithm, which has already been reported in\nthe literature. It will be shown that the symplectic approach is simpler and\nmore economical than the Dirac method.","publication_date":1700086421,"paper_link":"http://arxiv.org/pdf/2311.09407v1","categories":["Mathematics","Physics"],"abstract":"Classical mechanical systems with internal constraints will be studied using the extended symplectic formalism of Faddeev-Jackiw. Generalized brackets of the theory and equations of motion will be derived. A gauge system will be examined, leading to the associated gauge transformations. The results will be compared with the Dirac-Bergmann algorithm, which has already been reported in the literature. It will be shown that the symplectic approach is simpler and more economical than the Dirac method."}
{"title":"Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation","authors":["Daichi Horita","Naoto Inoue","Kotaro Kikuchi","Kota Yamaguchi","Kiyoharu Aizawa"],"raw_abstract":"Content-aware graphic layout generation aims to automatically arrange visual\nelements along with a given content, such as an e-commerce product image. In\nthis paper, we argue that the current layout generation approaches suffer from\nthe limited training data for the high-dimensional layout structure. We show\nthat a simple retrieval augmentation can significantly improve the generation\nquality. Our model, which is named Retrieval-Augmented Layout Transformer\n(RALF), retrieves nearest neighbor layout examples based on an input image and\nfeeds these results into an autoregressive generator. Our model can apply\nretrieval augmentation to various controllable generation tasks and yield\nhigh-quality layouts within a unified architecture. Our extensive experiments\nshow that RALF successfully generates content-aware layouts in both constrained\nand unconstrained settings and significantly outperforms the baselines.","publication_date":1700679593,"paper_link":"http://arxiv.org/pdf/2311.13602v1","categories":["Computer Science"],"abstract":"Content-aware graphic layout generation aims to automatically arrange visual elements along with a given content, such as an e-commerce product image. In this paper, we argue that the current layout generation approaches suffer from the limited training data for the high-dimensional layout structure. We show that a simple retrieval augmentation can significantly improve the generation quality. Our model, which is named Retrieval-Augmented Layout Transformer (RALF), retrieves nearest neighbor layout examples based on an input image and feeds these results into an autoregressive generator. Our model can apply retrieval augmentation to various controllable generation tasks and yield high-quality layouts within a unified architecture. Our extensive experiments show that RALF successfully generates content-aware layouts in both constrained and unconstrained settings and significantly outperforms the baselines."}
{"title":"Visual In-Context Prompting","authors":["Feng Li","Qing Jiang","Hao Zhang","Tianhe Ren","Shilong Liu","Xueyan Zou","Huaizhe Xu","Hongyang Li","Chunyuan Li","Jianwei Yang","Lei Zhang","Jianfeng Gao"],"raw_abstract":"In-context prompting in large language models (LLMs) has become a prevalent\napproach to improve zero-shot capabilities, but this idea is less explored in\nthe vision domain. Existing visual prompting methods focus on referring\nsegmentation to segment the most relevant object, falling short of addressing\nmany generic vision tasks like open-set segmentation and detection. In this\npaper, we introduce a universal visual in-context prompting framework for both\ntasks. In particular, we build on top of an encoder-decoder architecture, and\ndevelop a versatile prompt encoder to support a variety of prompts like\nstrokes, boxes, and points. We further enhance it to take an arbitrary number\nof reference image segments as the context. Our extensive explorations show\nthat the proposed visual in-context prompting elicits extraordinary referring\nand generic segmentation capabilities to refer and detect, yielding competitive\nperformance to close-set in-domain datasets and showing promising results on\nmany open-set segmentation datasets. By joint training on COCO and SA-1B, our\nmodel achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be\navailable at https://github.com/UX-Decoder/DINOv.","publication_date":1700679588,"paper_link":"http://arxiv.org/pdf/2311.13601v1","categories":["Computer Science"],"abstract":"In-context prompting in large language models (LLMs) has become a prevalent approach to improve zero-shot capabilities, but this idea is less explored in the vision domain. Existing visual prompting methods focus on referring segmentation to segment the most relevant object, falling short of addressing many generic vision tasks like open-set segmentation and detection. In this paper, we introduce a universal visual in-context prompting framework for both tasks. In particular, we build on top of an encoder-decoder architecture, and develop a versatile prompt encoder to support a variety of prompts like strokes, boxes, and points. We further enhance it to take an arbitrary number of reference image segments as the context. Our extensive explorations show that the proposed visual in-context prompting elicits extraordinary referring and generic segmentation capabilities to refer and detect, yielding competitive performance to close-set in-domain datasets and showing promising results on many open-set segmentation datasets. By joint training on COCO and SA-1B, our model achieves __FORMULA__ PQ on COCO and __FORMULA__ PQ on ADE20K. Code will be available at https://github.com/UX-Decoder/DINOv."}
{"title":"ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs","authors":["Viraj Shah","Nataniel Ruiz","Forrester Cole","Erika Lu","Svetlana Lazebnik","Yuanzhen Li","Varun Jampani"],"raw_abstract":"Methods for finetuning generative models for concept-driven personalization\ngenerally achieve strong results for subject-driven or style-driven generation.\nRecently, low-rank adaptations (LoRA) have been proposed as a\nparameter-efficient way of achieving concept-driven personalization. While\nrecent work explores the combination of separate LoRAs to achieve joint\ngeneration of learned styles and subjects, existing techniques do not reliably\naddress the problem; they often compromise either subject fidelity or style\nfidelity. We propose ZipLoRA, a method to cheaply and effectively merge\nindependently trained style and subject LoRAs in order to achieve generation of\nany user-provided subject in any user-provided style. Experiments on a wide\nrange of subject and style combinations show that ZipLoRA can generate\ncompelling results with meaningful improvements over baselines in subject and\nstyle fidelity while preserving the ability to recontextualize. Project page:\nhttps://ziplora.github.io","publication_date":1700679576,"paper_link":"http://arxiv.org/pdf/2311.13600v1","categories":["Computer Science"],"abstract":"Methods for finetuning generative models for concept-driven personalization generally achieve strong results for subject-driven or style-driven generation. Recently, low-rank adaptations (LoRA) have been proposed as a parameter-efficient way of achieving concept-driven personalization. While recent work explores the combination of separate LoRAs to achieve joint generation of learned styles and subjects, existing techniques do not reliably address the problem; they often compromise either subject fidelity or style fidelity. We propose ZipLoRA, a method to cheaply and effectively merge independently trained style and subject LoRAs in order to achieve generation of any user-provided subject in any user-provided style. Experiments on a wide range of subject and style combinations show that ZipLoRA can generate compelling results with meaningful improvements over baselines in subject and style fidelity while preserving the ability to recontextualize. Project page: https://ziplora.github.io"}
{"title":"Optical Absorption Effects in Thermal Radiation Barrier Coating Materials","authors":["Georgios Koutsakis","David R. Clarke"],"raw_abstract":"Future gas turbine engines will operate at higher gas temperatures and\nconsequentially hot-section components such as blades, vanes and combustors,\nwill be subject to higher thermal radiation fluxes than today. Current thermal\nbarrier coating materials are translucent over the spectral region of the heat\nflux so future coatings will also have to provide a barrier to thermal\nradiation. The effects of optical absorption and scattering properties of\ncoating materials on the temperatures and heat fluxes through coatings are\nexplored using a two-flux heat transfer model, and promising combinations are\nidentified that reduce the coating-alloy interface temperatures. Lower\ninterface temperatures occur for thickness normalized absorptions of\n$\\overline{\\kappa} L$ $>$1. The effect of both a narrow and a broad band\nspectrally selective absorbing Gd${_2}$Zr${_2}$O$_{7}$ based coating materials\nare then studied. These show that large values of the product of the normalized\nabsorption length and the spectral width of the absorption are required to\nsignificantly decrease the radiative heat transport through a coating. The\nresults emphasize the importance of enhancing the optical absorption of the\nnext generation barrier materials as a strategy to increase gas turbine engine\nefficiency by decreasing compressor bleed air cooling requirements.","publication_date":1700679538,"paper_link":"http://arxiv.org/pdf/2311.13599v1","categories":["Physics"],"abstract":"Future gas turbine engines will operate at higher gas temperatures and consequentially hot-section components such as blades, vanes and combustors, will be subject to higher thermal radiation fluxes than today. Current thermal barrier coating materials are translucent over the spectral region of the heat flux so future coatings will also have to provide a barrier to thermal radiation. The effects of optical absorption and scattering properties of coating materials on the temperatures and heat fluxes through coatings are explored using a two-flux heat transfer model, and promising combinations are identified that reduce the coating-alloy interface temperatures. Lower interface temperatures occur for thickness normalized absorptions of __FORMULA__ __FORMULA__1. The effect of both a narrow and a broad band spectrally selective absorbing Gd__FORMULA__Zr__FORMULA__O__FORMULA__ based coating materials are then studied. These show that large values of the product of the normalized absorption length and the spectral width of the absorption are required to significantly decrease the radiative heat transport through a coating. The results emphasize the importance of enhancing the optical absorption of the next generation barrier materials as a strategy to increase gas turbine engine efficiency by decreasing compressor bleed air cooling requirements."}
{"title":"T-Rex: Counting by Visual Prompting","authors":["Qing Jiang","Feng Li","Tianhe Ren","Shilong Liu","Zhaoyang Zeng","Kent Yu","Lei Zhang"],"raw_abstract":"We introduce T-Rex, an interactive object counting model designed to first\ndetect and then count any objects. We formulate object counting as an open-set\nobject detection task with the integration of visual prompts. Users can specify\nthe objects of interest by marking points or boxes on a reference image, and\nT-Rex then detects all objects with a similar pattern. Guided by the visual\nfeedback from T-Rex, users can also interactively refine the counting results\nby prompting on missing or falsely-detected objects. T-Rex has achieved\nstate-of-the-art performance on several class-agnostic counting benchmarks. To\nfurther exploit its potential, we established a new counting benchmark\nencompassing diverse scenarios and challenges. Both quantitative and\nqualitative results show that T-Rex possesses exceptional zero-shot counting\ncapabilities. We also present various practical application scenarios for\nT-Rex, illustrating its potential in the realm of visual prompting.","publication_date":1700679444,"paper_link":"http://arxiv.org/pdf/2311.13596v1","categories":["Computer Science"],"abstract":"We introduce T-Rex, an interactive object counting model designed to first detect and then count any objects. We formulate object counting as an open-set object detection task with the integration of visual prompts. Users can specify the objects of interest by marking points or boxes on a reference image, and T-Rex then detects all objects with a similar pattern. Guided by the visual feedback from T-Rex, users can also interactively refine the counting results by prompting on missing or falsely-detected objects. T-Rex has achieved state-of-the-art performance on several class-agnostic counting benchmarks. To further exploit its potential, we established a new counting benchmark encompassing diverse scenarios and challenges. Both quantitative and qualitative results show that T-Rex possesses exceptional zero-shot counting capabilities. We also present various practical application scenarios for T-Rex, illustrating its potential in the realm of visual prompting."}
{"title":"Covariance alignment: from maximum likelihood estimation to Gromov-Wasserstein","authors":["Yanjun Han","Philippe Rigollet","George Stepaniants"],"raw_abstract":"Feature alignment methods are used in many scientific disciplines for data\npooling, annotation, and comparison. As an instance of a permutation learning\nproblem, feature alignment presents significant statistical and computational\nchallenges. In this work, we propose the covariance alignment model to study\nand compare various alignment methods and establish a minimax lower bound for\ncovariance alignment that has a non-standard dimension scaling because of the\npresence of a nuisance parameter. This lower bound is in fact minimax optimal\nand is achieved by a natural quasi MLE. However, this estimator involves a\nsearch over all permutations which is computationally infeasible even when the\nproblem has moderate size. To overcome this limitation, we show that the\ncelebrated Gromov-Wasserstein algorithm from optimal transport which is more\namenable to fast implementation even on large-scale problems is also minimax\noptimal. These results give the first statistical justification for the\ndeployment of the Gromov-Wasserstein algorithm in practice.","publication_date":1700679327,"paper_link":"http://arxiv.org/pdf/2311.13595v1","categories":["Mathematics","Statistics"],"abstract":"Feature alignment methods are used in many scientific disciplines for data pooling, annotation, and comparison. As an instance of a permutation learning problem, feature alignment presents significant statistical and computational challenges. In this work, we propose the covariance alignment model to study and compare various alignment methods and establish a minimax lower bound for covariance alignment that has a non-standard dimension scaling because of the presence of a nuisance parameter. This lower bound is in fact minimax optimal and is achieved by a natural quasi MLE. However, this estimator involves a search over all permutations which is computationally infeasible even when the problem has moderate size. To overcome this limitation, we show that the celebrated Gromov-Wasserstein algorithm from optimal transport which is more amenable to fast implementation even on large-scale problems is also minimax optimal. These results give the first statistical justification for the deployment of the Gromov-Wasserstein algorithm in practice."}
{"title":"Labeling Neural Representations with Inverse Recognition","authors":["Kirill Bykov","Laura Kopf","Shinichi Nakajima","Marius Kloft","Marina M. -C. H\u00f6hne"],"raw_abstract":"Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning\ncomplex hierarchical data representations, but the nature of these\nrepresentations remains largely unknown. Existing global explainability\nmethods, such as Network Dissection, face limitations such as reliance on\nsegmentation masks, lack of statistical significance testing, and high\ncomputational demands. We propose Inverse Recognition (INVERT), a scalable\napproach for connecting learned representations with human-understandable\nconcepts by leveraging their capacity to discriminate between these concepts.\nIn contrast to prior work, INVERT is capable of handling diverse types of\nneurons, exhibits less computational complexity, and does not rely on the\navailability of segmentation masks. Moreover, INVERT provides an interpretable\nmetric assessing the alignment between the representation and its corresponding\nexplanation and delivering a measure of statistical significance, emphasizing\nits utility and credibility. We demonstrate the applicability of INVERT in\nvarious scenarios, including the identification of representations affected by\nspurious correlations, and the interpretation of the hierarchical structure of\ndecision-making within the models.","publication_date":1700679325,"paper_link":"http://arxiv.org/pdf/2311.13594v1","categories":["Statistics"],"abstract":"Deep Neural Networks (DNNs) demonstrated remarkable capabilities in learning complex hierarchical data representations, but the nature of these representations remains largely unknown. Existing global explainability methods, such as Network Dissection, face limitations such as reliance on segmentation masks, lack of statistical significance testing, and high computational demands. We propose Inverse Recognition (INVERT), a scalable approach for connecting learned representations with human-understandable concepts by leveraging their capacity to discriminate between these concepts. In contrast to prior work, INVERT is capable of handling diverse types of neurons, exhibits less computational complexity, and does not rely on the availability of segmentation masks. Moreover, INVERT provides an interpretable metric assessing the alignment between the representation and its corresponding explanation and delivering a measure of statistical significance, emphasizing its utility and credibility. We demonstrate the applicability of INVERT in various scenarios, including the identification of representations affected by spurious correlations, and the interpretation of the hierarchical structure of decision-making within the models."}
{"title":"User-guided Page Merging for Memory Deduplication in Serverless Systems","authors":["Wei Qiu","Marcin Copik","Yun Wang","Alexandru Calotoiu","Torsten Hoefler"],"raw_abstract":"Serverless computing is an emerging cloud paradigm that offers an elastic and\nscalable allocation of computing resources with pay-as-you-go billing. In the\nFunction-as-a-Service (FaaS) programming model, applications comprise\nshort-lived and stateless serverless functions executed in isolated containers\nor microVMs, which can quickly scale to thousands of instances and process\nterabytes of data. This flexibility comes at the cost of duplicated runtimes,\nlibraries, and user data spread across many function instances, and cloud\nproviders do not utilize this redundancy. The memory footprint of serverless\nforces removing idle containers to make space for new ones, which decreases\nperformance through more cold starts and fewer data caching opportunities. We\naddress this issue by proposing deduplicating memory pages of serverless\nworkers with identical content, based on the content-based page-sharing concept\nof Linux Kernel Same-page Merging (KSM). We replace the background memory\nscanning process of KSM, as it is too slow to locate sharing candidates in\nshort-lived functions. Instead, we design User-Guided Page Merging (UPM), a\nbuilt-in Linux kernel module that leverages the madvise system call: we enable\nusers to advise the kernel of memory areas that can be shared with others. We\nshow that UPM reduces memory consumption by up to 55% on 16 concurrent\ncontainers executing a typical image recognition function, more than doubling\nthe density for containers of the same function that can run on a system.","publication_date":1700678940,"paper_link":"http://arxiv.org/pdf/2311.13588v1","categories":["Computer Science"],"abstract":"Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system."}
{"title":"A Survey of Serverless Machine Learning Model Inference","authors":["Kamil Kojs"],"raw_abstract":"Recent developments in Generative AI, Computer Vision, and Natural Language\nProcessing have led to an increased integration of AI models into various\nproducts. This widespread adoption of AI requires significant efforts in\ndeploying these models in production environments. When hosting machine\nlearning models for real-time predictions, it is important to meet defined\nService Level Objectives (SLOs), ensuring reliability, minimal downtime, and\noptimizing operational costs of the underlying infrastructure. Large machine\nlearning models often demand GPU resources for efficient inference to meet\nSLOs. In the context of these trends, there is growing interest in hosting AI\nmodels in a serverless architecture while still providing GPU access for\ninference tasks. This survey aims to summarize and categorize the emerging\nchallenges and optimization opportunities for large-scale deep learning serving\nsystems. By providing a novel taxonomy and summarizing recent trends, we hope\nthat this survey could shed light on new optimization perspectives and motivate\nnovel works in large-scale deep learning serving systems.","publication_date":1700678765,"paper_link":"http://arxiv.org/pdf/2311.13587v1","categories":["Computer Science"],"abstract":"Recent developments in Generative AI, Computer Vision, and Natural Language Processing have led to an increased integration of AI models into various products. This widespread adoption of AI requires significant efforts in deploying these models in production environments. When hosting machine learning models for real-time predictions, it is important to meet defined Service Level Objectives (SLOs), ensuring reliability, minimal downtime, and optimizing operational costs of the underlying infrastructure. Large machine learning models often demand GPU resources for efficient inference to meet SLOs. In the context of these trends, there is growing interest in hosting AI models in a serverless architecture while still providing GPU access for inference tasks. This survey aims to summarize and categorize the emerging challenges and optimization opportunities for large-scale deep learning serving systems. By providing a novel taxonomy and summarizing recent trends, we hope that this survey could shed light on new optimization perspectives and motivate novel works in large-scale deep learning serving systems."}
{"title":"Adaptive Sampling for Deep Learning via Efficient Nonparametric Proxies","authors":["Shabnam Daghaghi","Benjamin Coleman","Benito Geordie","Anshumali Shrivastava"],"raw_abstract":"Data sampling is an effective method to improve the training speed of neural\nnetworks, with recent results demonstrating that it can even break the neural\nscaling laws. These results critically rely on high-quality scores to estimate\nthe importance of an input to the network. We observe that there are two\ndominant strategies: static sampling, where the scores are determined before\ntraining, and dynamic sampling, where the scores can depend on the model\nweights. Static algorithms are computationally inexpensive but less effective\nthan their dynamic counterparts, which can cause end-to-end slowdown due to\ntheir need to explicitly compute losses. To address this problem, we propose a\nnovel sampling distribution based on nonparametric kernel regression that\nlearns an effective importance score as the neural network trains. However,\nnonparametric regression models are too computationally expensive to accelerate\nend-to-end training. Therefore, we develop an efficient sketch-based\napproximation to the Nadaraya-Watson estimator. Using recent techniques from\nhigh-dimensional statistics and randomized algorithms, we prove that our\nNadaraya-Watson sketch approximates the estimator with exponential convergence\nguarantees. Our sampling algorithm outperforms the baseline in terms of\nwall-clock time and accuracy on four datasets.","publication_date":1700678418,"paper_link":"http://arxiv.org/pdf/2311.13583v1","categories":["Computer Science"],"abstract":"Data sampling is an effective method to improve the training speed of neural networks, with recent results demonstrating that it can even break the neural scaling laws. These results critically rely on high-quality scores to estimate the importance of an input to the network. We observe that there are two dominant strategies: static sampling, where the scores are determined before training, and dynamic sampling, where the scores can depend on the model weights. Static algorithms are computationally inexpensive but less effective than their dynamic counterparts, which can cause end-to-end slowdown due to their need to explicitly compute losses. To address this problem, we propose a novel sampling distribution based on nonparametric kernel regression that learns an effective importance score as the neural network trains. However, nonparametric regression models are too computationally expensive to accelerate end-to-end training. Therefore, we develop an efficient sketch-based approximation to the Nadaraya-Watson estimator. Using recent techniques from high-dimensional statistics and randomized algorithms, we prove that our Nadaraya-Watson sketch approximates the estimator with exponential convergence guarantees. Our sampling algorithm outperforms the baseline in terms of wall-clock time and accuracy on four datasets."}
{"title":"PaSS: Parallel Speculative Sampling","authors":["Giovanni Monea","Armand Joulin","Edouard Grave"],"raw_abstract":"Scaling the size of language models to tens of billions of parameters has led\nto impressive performance on a wide range of tasks. At generation, these models\nare used auto-regressively, requiring a forward pass for each generated token,\nand thus reading the full set of parameters from memory. This memory access\nforms the primary bottleneck for generation and it worsens as the model size\nincreases. Moreover, executing a forward pass for multiple tokens in parallel\noften takes nearly the same time as it does for just one token. These two\nobservations lead to the development of speculative sampling, where a second\nsmaller model is used to draft a few tokens, that are then validated or\nrejected using a single forward pass of the large model. Unfortunately, this\nmethod requires two models that share the same tokenizer and thus limits its\nadoption. As an alternative, we propose to use parallel decoding as a way to\ndraft multiple tokens from a single model with no computational cost, nor the\nneed for a second model. Our approach only requires an additional input token\nthat marks the words that will be generated simultaneously. We show promising\nperformance (up to $30\\%$ speed-up) while requiring only as few as $O(d_{emb})$\nadditional parameters.","publication_date":1700678247,"paper_link":"http://arxiv.org/pdf/2311.13581v1","categories":["Computer Science"],"abstract":"Scaling the size of language models to tens of billions of parameters has led to impressive performance on a wide range of tasks. At generation, these models are used auto-regressively, requiring a forward pass for each generated token, and thus reading the full set of parameters from memory. This memory access forms the primary bottleneck for generation and it worsens as the model size increases. Moreover, executing a forward pass for multiple tokens in parallel often takes nearly the same time as it does for just one token. These two observations lead to the development of speculative sampling, where a second smaller model is used to draft a few tokens, that are then validated or rejected using a single forward pass of the large model. Unfortunately, this method requires two models that share the same tokenizer and thus limits its adoption. As an alternative, we propose to use parallel decoding as a way to draft multiple tokens from a single model with no computational cost, nor the need for a second model. Our approach only requires an additional input token that marks the words that will be generated simultaneously. We show promising performance (up to __FORMULA__ speed-up) while requiring only as few as __FORMULA__ additional parameters."}
{"title":"XAGen: 3D Expressive Human Avatars Generation","authors":["Zhongcong Xu","Jianfeng Zhang","Jun Hao Liew","Jiashi Feng","Mike Zheng Shou"],"raw_abstract":"Recent advances in 3D-aware GAN models have enabled the generation of\nrealistic and controllable human body images. However, existing methods focus\non the control of major body joints, neglecting the manipulation of expressive\nattributes, such as facial expressions, jaw poses, hand poses, and so on. In\nthis work, we present XAGen, the first 3D generative model for human avatars\ncapable of expressive control over body, face, and hands. To enhance the\nfidelity of small-scale regions like face and hands, we devise a multi-scale\nand multi-part 3D representation that models fine details. Based on this\nrepresentation, we propose a multi-part rendering technique that disentangles\nthe synthesis of body, face, and hands to ease model training and enhance\ngeometric quality. Furthermore, we design multi-part discriminators that\nevaluate the quality of the generated avatars with respect to their appearance\nand fine-grained control capabilities. Experiments show that XAGen surpasses\nstate-of-the-art methods in terms of realism, diversity, and expressive control\nabilities. Code and data will be made available at\nhttps://showlab.github.io/xagen.","publication_date":1700677842,"paper_link":"http://arxiv.org/pdf/2311.13574v1","categories":["Computer Science"],"abstract":"Recent advances in 3D-aware GAN models have enabled the generation of realistic and controllable human body images. However, existing methods focus on the control of major body joints, neglecting the manipulation of expressive attributes, such as facial expressions, jaw poses, hand poses, and so on. In this work, we present XAGen, the first 3D generative model for human avatars capable of expressive control over body, face, and hands. To enhance the fidelity of small-scale regions like face and hands, we devise a multi-scale and multi-part 3D representation that models fine details. Based on this representation, we propose a multi-part rendering technique that disentangles the synthesis of body, face, and hands to ease model training and enhance geometric quality. Furthermore, we design multi-part discriminators that evaluate the quality of the generated avatars with respect to their appearance and fine-grained control capabilities. Experiments show that XAGen surpasses state-of-the-art methods in terms of realism, diversity, and expressive control abilities. Code and data will be made available at https://showlab.github.io/xagen."}
{"title":"$h$-vectors of edge rings of odd-cycle compositions","authors":["Kieran Bhaskara","Akihiro Higashitani","Nayana Shibu Deepthi"],"raw_abstract":"Let $\\mathbb{K}[G]$ be the edge ring of a finite simple graph $G$.\nInvestigating properties of the $h$-vector of $\\mathbb{K}[G]$ is of great\ninterest in combinatorial commutative algebra. However, there are few families\nof graphs for which the $h$-vector has been explicitly determined. In this\npaper, we compute the $h$-vectors of a certain family of graphs that satisfy\nthe odd-cycle condition, generalizing a result of the second and third named\nauthors. As a corollary, we obtain a characterization of the graphs in this\nfamily whose edge rings are almost Gorenstein.","publication_date":1700677668,"paper_link":"http://arxiv.org/pdf/2311.13573v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be the edge ring of a finite simple graph __FORMULA__. Investigating properties of the __FORMULA__-vector of __FORMULA__ is of great interest in combinatorial commutative algebra. However, there are few families of graphs for which the __FORMULA__-vector has been explicitly determined. In this paper, we compute the __FORMULA__-vectors of a certain family of graphs that satisfy the odd-cycle condition, generalizing a result of the second and third named authors. As a corollary, we obtain a characterization of the graphs in this family whose edge rings are almost Gorenstein."}
{"title":"Likelihood Geometry of Reflexive Polytopes","authors":["Carlos Am\u00e9ndola","Janike Oldekop"],"raw_abstract":"We study the problem of maximum likelihood (ML) estimation for statistical\nmodels defined by reflexive polytopes. Our focus is on the maximum likelihood\ndegree of these models as an algebraic measure of complexity of the\ncorresponding optimization problem. We compute the ML degrees of all 4319\nclasses of three-dimensional reflexive polytopes, and observe some surprising\nbehavior in terms of the presence of gaps between ML degrees and degrees of the\nassociated toric varieties. We interpret these drops in the context of\ndiscriminants and prove formulas for the ML degree for families of reflexive\npolytopes, including the hypercube and its dual, the cross polytope, in\narbitrary dimension. In particular, we determine a family of embeddings for the\n$d$-cube that implies ML degree one. Finally, we discuss generalized\nconstructions of families of reflexive polytopes in terms of their ML degrees.","publication_date":1700677666,"paper_link":"http://arxiv.org/pdf/2311.13572v1","categories":["Mathematics","Statistics"],"abstract":"We study the problem of maximum likelihood (ML) estimation for statistical models defined by reflexive polytopes. Our focus is on the maximum likelihood degree of these models as an algebraic measure of complexity of the corresponding optimization problem. We compute the ML degrees of all 4319 classes of three-dimensional reflexive polytopes, and observe some surprising behavior in terms of the presence of gaps between ML degrees and degrees of the associated toric varieties. We interpret these drops in the context of discriminants and prove formulas for the ML degree for families of reflexive polytopes, including the hypercube and its dual, the cross polytope, in arbitrary dimension. In particular, we determine a family of embeddings for the __FORMULA__-cube that implies ML degree one. Finally, we discuss generalized constructions of families of reflexive polytopes in terms of their ML degrees."}
{"title":"WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space","authors":["Katja Schwarz","Seung Wook Kim","Jun Gao","Sanja Fidler","Andreas Geiger","Karsten Kreis"],"raw_abstract":"Modern learning-based approaches to 3D-aware image synthesis achieve high\nphotorealism and 3D-consistent viewpoint changes for the generated images.\nExisting approaches represent instances in a shared canonical space. However,\nfor in-the-wild datasets a shared canonical system can be difficult to define\nor might not even exist. In this work, we instead model instances in view\nspace, alleviating the need for posed images and learned camera distributions.\nWe find that in this setting, existing GAN-based methods are prone to\ngenerating flat geometry and struggle with distribution coverage. We hence\npropose WildFusion, a new approach to 3D-aware image synthesis based on latent\ndiffusion models (LDMs). We first train an autoencoder that infers a compressed\nlatent representation, which additionally captures the images' underlying 3D\nstructure and enables not only reconstruction but also novel view synthesis. To\nlearn a faithful 3D representation, we leverage cues from monocular depth\nprediction. Then, we train a diffusion model in the 3D-aware latent space,\nthereby enabling synthesis of high-quality 3D-consistent image samples,\noutperforming recent state-of-the-art GAN-based methods. Importantly, our\n3D-aware LDM is trained without any direct supervision from multiview images or\n3D geometry and does not require posed images or learned pose or camera\ndistributions. It directly learns a 3D representation without relying on\ncanonical camera coordinates. This opens up promising research avenues for\nscalable 3D-aware image synthesis and 3D content creation from in-the-wild\nimage data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D\nresults.","publication_date":1700677551,"paper_link":"http://arxiv.org/pdf/2311.13570v1","categories":["Computer Science"],"abstract":"Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to define or might not even exist. In this work, we instead model instances in view space, alleviating the need for posed images and learned camera distributions. We find that in this setting, existing GAN-based methods are prone to generating flat geometry and struggle with distribution coverage. We hence propose WildFusion, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs). We first train an autoencoder that infers a compressed latent representation, which additionally captures the images' underlying 3D structure and enables not only reconstruction but also novel view synthesis. To learn a faithful 3D representation, we leverage cues from monocular depth prediction. Then, we train a diffusion model in the 3D-aware latent space, thereby enabling synthesis of high-quality 3D-consistent image samples, outperforming recent state-of-the-art GAN-based methods. Importantly, our 3D-aware LDM is trained without any direct supervision from multiview images or 3D geometry and does not require posed images or learned pose or camera distributions. It directly learns a 3D representation without relying on canonical camera coordinates. This opens up promising research avenues for scalable 3D-aware image synthesis and 3D content creation from in-the-wild image data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D results."}
{"title":"Learning from similar systems and online data-driven LQR using iterative randomised data compression","authors":["Vatsal Kedia","Sneha Susan George","Debraj Chakraborty"],"raw_abstract":"The problem of data-driven recursive computation of receding horizon LQR\ncontrol through a randomized combination of online/current and\nhistorical/recorded data is considered. It is assumed that large amounts of\nhistorical input-output data from a system, which is similar but not identical\nto the current system under consideration, is available. This (possibly large)\ndata set is compressed through a novel randomized subspace algorithm to\ndirectly synthesize an initial solution of the standard LQR problem, which\nhowever is sub-optimal due to the inaccuracy of the historical model. The first\ninstance of this input is used to actuate the current system and the\ncorresponding instantaneous output is used to iteratively re-solve the LQR\nproblem through a computationally inexpensive randomized rank-one update of the\nold compressed data. The first instance of the re-computed input is applied to\nthe system at the next instant, output recorded and the entire procedure is\nrepeated at each subsequent instant. As more current data becomes available,\nthe algorithm learns automatically from the new data while simultaneously\ncontrolling the system in near optimal manner. The proposed algorithm is\ncomputationally inexpensive due to the initial and repeated compression of old\nand newly available data. Moreover, the simultaneous learning and control makes\nthis algorithm particularly suited for adapting to unknown, poorly modeled and\ntime-varying systems without any explicit exploration stage. Simulations\ndemonstrate the effectiveness of the proposed algorithm vs popular\nexploration/exploitation approaches to LQR control.","publication_date":1700677451,"paper_link":"http://arxiv.org/pdf/2311.13568v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The problem of data-driven recursive computation of receding horizon LQR control through a randomized combination of online/current and historical/recorded data is considered. It is assumed that large amounts of historical input-output data from a system, which is similar but not identical to the current system under consideration, is available. This (possibly large) data set is compressed through a novel randomized subspace algorithm to directly synthesize an initial solution of the standard LQR problem, which however is sub-optimal due to the inaccuracy of the historical model. The first instance of this input is used to actuate the current system and the corresponding instantaneous output is used to iteratively re-solve the LQR problem through a computationally inexpensive randomized rank-one update of the old compressed data. The first instance of the re-computed input is applied to the system at the next instant, output recorded and the entire procedure is repeated at each subsequent instant. As more current data becomes available, the algorithm learns automatically from the new data while simultaneously controlling the system in near optimal manner. The proposed algorithm is computationally inexpensive due to the initial and repeated compression of old and newly available data. Moreover, the simultaneous learning and control makes this algorithm particularly suited for adapting to unknown, poorly modeled and time-varying systems without any explicit exploration stage. Simulations demonstrate the effectiveness of the proposed algorithm vs popular exploration/exploitation approaches to LQR control."}
{"title":"Drilling Down into the Discourse Structure with LLMs for Long Document Question Answering","authors":["Inderjeet Nair","Shwetha Somasundaram","Apoorv Saxena","Koustava Goswami"],"raw_abstract":"We address the task of evidence retrieval for long document question\nanswering, which involves locating relevant paragraphs within a document to\nanswer a question. We aim to assess the applicability of large language models\n(LLMs) in the task of zero-shot long document evidence retrieval, owing to\ntheir unprecedented performance across various NLP tasks. However, currently\nthe LLMs can consume limited context lengths as input, thus providing document\nchunks as inputs might overlook the global context while missing out on\ncapturing the inter-segment dependencies. Moreover, directly feeding the large\ninput sets can incur significant computational costs, particularly when\nprocessing the entire document (and potentially incurring monetary expenses\nwith enterprise APIs like OpenAI's GPT variants). To address these challenges,\nwe propose a suite of techniques that exploit the discourse structure commonly\nfound in documents. By utilizing this structure, we create a condensed\nrepresentation of the document, enabling a more comprehensive understanding and\nanalysis of relationships between different parts. We retain $99.6\\%$ of the\nbest zero-shot approach's performance, while processing only $26\\%$ of the\ntotal tokens used by the best approach in the information seeking evidence\nretrieval setup. We also show how our approach can be combined with\n\\textit{self-ask} reasoning agent to achieve best zero-shot performance in\ncomplex multi-hop question answering, just $\\approx 4\\%$ short of zero-shot\nperformance using gold evidence.","publication_date":1700677376,"paper_link":"http://arxiv.org/pdf/2311.13565v1","categories":["Computer Science"],"abstract":"We address the task of evidence retrieval for long document question answering, which involves locating relevant paragraphs within a document to answer a question. We aim to assess the applicability of large language models (LLMs) in the task of zero-shot long document evidence retrieval, owing to their unprecedented performance across various NLP tasks. However, currently the LLMs can consume limited context lengths as input, thus providing document chunks as inputs might overlook the global context while missing out on capturing the inter-segment dependencies. Moreover, directly feeding the large input sets can incur significant computational costs, particularly when processing the entire document (and potentially incurring monetary expenses with enterprise APIs like OpenAI's GPT variants). To address these challenges, we propose a suite of techniques that exploit the discourse structure commonly found in documents. By utilizing this structure, we create a condensed representation of the document, enabling a more comprehensive understanding and analysis of relationships between different parts. We retain __FORMULA__ of the best zero-shot approach's performance, while processing only __FORMULA__ of the total tokens used by the best approach in the information seeking evidence retrieval setup. We also show how our approach can be combined with self-ask reasoning agent to achieve best zero-shot performance in complex multi-hop question answering, just __FORMULA__ short of zero-shot performance using gold evidence."}
{"title":"Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object","authors":["Junhao Chen","Peng Rong","Jingbo Sun","Chao Li","Xiang Li","Hongwu Lv"],"raw_abstract":"Image style transfer occupies an important place in both computer graphics\nand computer vision. However, most current methods require reference to\nstylized images and cannot individually stylize specific objects. To overcome\nthis limitation, we propose the \"Soulstyler\" framework, which allows users to\nguide the stylization of specific objects in an image through simple textual\ndescriptions. We introduce a large language model to parse the text and\nidentify stylization goals and specific styles. Combined with a CLIP-based\nsemantic visual embedding encoder, the model understands and matches text and\nimage content. We also introduce a novel localized text-image block matching\nloss that ensures that style transfer is performed only on specified target\nobjects, while non-target regions remain in their original style. Experimental\nresults demonstrate that our model is able to accurately perform style transfer\non target objects according to textual descriptions without affecting the style\nof background regions. Our code will be available at\nhttps://github.com/yisuanwang/Soulstyler.","publication_date":1700676943,"paper_link":"http://arxiv.org/pdf/2311.13562v1","categories":["Computer Science"],"abstract":"Image style transfer occupies an important place in both computer graphics and computer vision. However, most current methods require reference to stylized images and cannot individually stylize specific objects. To overcome this limitation, we propose the \"Soulstyler\" framework, which allows users to guide the stylization of specific objects in an image through simple textual descriptions. We introduce a large language model to parse the text and identify stylization goals and specific styles. Combined with a CLIP-based semantic visual embedding encoder, the model understands and matches text and image content. We also introduce a novel localized text-image block matching loss that ensures that style transfer is performed only on specified target objects, while non-target regions remain in their original style. Experimental results demonstrate that our model is able to accurately perform style transfer on target objects according to textual descriptions without affecting the style of background regions. Our code will be available at https://github.com/yisuanwang/Soulstyler."}
{"title":"Transfer Learning-based Real-time Handgun Detection","authors":["Youssef Elmir","Sid Ahmed Laouar","Larbi Hamdaoui"],"raw_abstract":"Traditional surveillance systems rely on human attention, limiting their\neffectiveness. This study employs convolutional neural networks and transfer\nlearning to develop a real-time computer vision system for automatic handgun\ndetection. Comprehensive analysis of online handgun detection methods is\nconducted, emphasizing reducing false positives and learning time. Transfer\nlearning is demonstrated as an effective approach. Despite technical\nchallenges, the proposed system achieves a precision rate of 84.74%,\ndemonstrating promising performance comparable to related works, enabling\nfaster learning and accurate automatic handgun detection for enhanced security.\nThis research advances security measures by reducing human monitoring\ndependence, showcasing the potential of transfer learning-based approaches for\nefficient and reliable handgun detection.","publication_date":1700676582,"paper_link":"http://arxiv.org/pdf/2311.13559v1","categories":["Computer Science"],"abstract":"Traditional surveillance systems rely on human attention, limiting their effectiveness. This study employs convolutional neural networks and transfer learning to develop a real-time computer vision system for automatic handgun detection. Comprehensive analysis of online handgun detection methods is conducted, emphasizing reducing false positives and learning time. Transfer learning is demonstrated as an effective approach. Despite technical challenges, the proposed system achieves a precision rate of 84.74%, demonstrating promising performance comparable to related works, enabling faster learning and accurate automatic handgun detection for enhanced security. This research advances security measures by reducing human monitoring dependence, showcasing the potential of transfer learning-based approaches for efficient and reliable handgun detection."}
{"title":"ADriver-I: A General World Model for Autonomous Driving","authors":["Fan Jia","Weixin Mao","Yingfei Liu","Yucheng Zhao","Yuqing Wen","Chi Zhang","Xiangyu Zhang","Tiancai Wang"],"raw_abstract":"Typically, autonomous driving adopts a modular design, which divides the full\nstack into perception, prediction, planning and control parts. Though\ninterpretable, such modular design tends to introduce a substantial amount of\nredundancy. Recently, multimodal large language models (MLLM) and diffusion\ntechniques have demonstrated their superior performance on comprehension and\ngeneration ability. In this paper, we first introduce the concept of\ninterleaved vision-action pair, which unifies the format of visual features and\ncontrol signals. Based on the vision-action pairs, we construct a general world\nmodel based on MLLM and diffusion model for autonomous driving, termed\nADriver-I. It takes the vision-action pairs as inputs and autoregressively\npredicts the control signal of the current frame. The generated control signals\ntogether with the historical vision-action pairs are further conditioned to\npredict the future frames. With the predicted next frame, ADriver-I performs\nfurther control signal prediction. Such a process can be repeated infinite\ntimes, ADriver-I achieves autonomous driving in the world created by itself.\nExtensive experiments are conducted on nuScenes and our large-scale private\ndatasets. ADriver-I shows impressive performance compared to several\nconstructed baselines. We hope our ADriver-I can provide some new insights for\nfuture autonomous driving and embodied intelligence.","publication_date":1700675069,"paper_link":"http://arxiv.org/pdf/2311.13549v1","categories":["Computer Science"],"abstract":"Typically, autonomous driving adopts a modular design, which divides the full stack into perception, prediction, planning and control parts. Though interpretable, such modular design tends to introduce a substantial amount of redundancy. Recently, multimodal large language models (MLLM) and diffusion techniques have demonstrated their superior performance on comprehension and generation ability. In this paper, we first introduce the concept of interleaved vision-action pair, which unifies the format of visual features and control signals. Based on the vision-action pairs, we construct a general world model based on MLLM and diffusion model for autonomous driving, termed ADriver-I. It takes the vision-action pairs as inputs and autoregressively predicts the control signal of the current frame. The generated control signals together with the historical vision-action pairs are further conditioned to predict the future frames. With the predicted next frame, ADriver-I performs further control signal prediction. Such a process can be repeated infinite times, ADriver-I achieves autonomous driving in the world created by itself. Extensive experiments are conducted on nuScenes and our large-scale private datasets. ADriver-I shows impressive performance compared to several constructed baselines. We hope our ADriver-I can provide some new insights for future autonomous driving and embodied intelligence."}
{"title":"Efficient Numerical Integration in Reproducing Kernel Hilbert Spaces via Leverage Scores Sampling","authors":["Antoine Chatalic","Nicolas Schreuder","Ernesto De Vito","Lorenzo Rosasco"],"raw_abstract":"In this work we consider the problem of numerical integration, i.e.,\napproximating integrals with respect to a target probability measure using only\npointwise evaluations of the integrand. We focus on the setting in which the\ntarget distribution is only accessible through a set of $n$ i.i.d.\nobservations, and the integrand belongs to a reproducing kernel Hilbert space.\nWe propose an efficient procedure which exploits a small i.i.d. random subset\nof $m<n$ samples drawn either uniformly or using approximate leverage scores\nfrom the initial observations. Our main result is an upper bound on the\napproximation error of this procedure for both sampling strategies. It yields\nsufficient conditions on the subsample size to recover the standard (optimal)\n$n^{-1/2}$ rate while reducing drastically the number of functions evaluations,\nand thus the overall computational cost. Moreover, we obtain rates with respect\nto the number $m$ of evaluations of the integrand which adapt to its\nsmoothness, and match known optimal rates for instance for Sobolev spaces. We\nillustrate our theoretical findings with numerical experiments on real\ndatasets, which highlight the attractive efficiency-accuracy tradeoff of our\nmethod compared to existing randomized and greedy quadrature methods. We note\nthat, the problem of numerical integration in RKHS amounts to designing a\ndiscrete approximation of the kernel mean embedding of the target distribution.\nAs a consequence, direct applications of our results also include the efficient\ncomputation of maximum mean discrepancies between distributions and the design\nof efficient kernel-based tests.","publication_date":1700675058,"paper_link":"http://arxiv.org/pdf/2311.13548v1","categories":["Mathematics","Statistics"],"abstract":"In this work we consider the problem of numerical integration, i.e., approximating integrals with respect to a target probability measure using only pointwise evaluations of the integrand. We focus on the setting in which the target distribution is only accessible through a set of __FORMULA__ i.i.d. observations, and the integrand belongs to a reproducing kernel Hilbert space. We propose an efficient procedure which exploits a small i.i.d. random subset of __FORMULA__ samples drawn either uniformly or using approximate leverage scores from the initial observations. Our main result is an upper bound on the approximation error of this procedure for both sampling strategies. It yields sufficient conditions on the subsample size to recover the standard (optimal) __FORMULA__ rate while reducing drastically the number of functions evaluations, and thus the overall computational cost. Moreover, we obtain rates with respect to the number __FORMULA__ of evaluations of the integrand which adapt to its smoothness, and match known optimal rates for instance for Sobolev spaces. We illustrate our theoretical findings with numerical experiments on real datasets, which highlight the attractive efficiency-accuracy tradeoff of our method compared to existing randomized and greedy quadrature methods. We note that, the problem of numerical integration in RKHS amounts to designing a discrete approximation of the kernel mean embedding of the target distribution. As a consequence, direct applications of our results also include the efficient computation of maximum mean discrepancies between distributions and the design of efficient kernel-based tests."}
{"title":"Medical Image Retrieval Using Pretrained Embeddings","authors":["Farnaz Khun Jush","Tuan Truong","Steffen Vogler","Matthias Lenga"],"raw_abstract":"A wide range of imaging techniques and data formats available for medical\nimages make accurate retrieval from image databases challenging.\n  Efficient retrieval systems are crucial in advancing medical research,\nenabling large-scale studies and innovative diagnostic tools. Thus, addressing\nthe challenges of medical image retrieval is essential for the continued\nenhancement of healthcare and research.\n  In this study, we evaluated the feasibility of employing four\nstate-of-the-art pretrained models for medical image retrieval at modality,\nbody region, and organ levels and compared the results of two similarity\nindexing approaches. Since the employed networks take 2D images, we analyzed\nthe impacts of weighting and sampling strategies to incorporate 3D information\nduring retrieval of 3D volumes. We showed that medical image retrieval is\nfeasible using pretrained networks without any additional training or\nfine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for\nvarious tasks at modality, body region, and organ level.","publication_date":1700674953,"paper_link":"http://arxiv.org/pdf/2311.13547v1","categories":["Computer Science"],"abstract":"A wide range of imaging techniques and data formats available for medical images make accurate retrieval from image databases challenging.   Efficient retrieval systems are crucial in advancing medical research, enabling large-scale studies and innovative diagnostic tools. Thus, addressing the challenges of medical image retrieval is essential for the continued enhancement of healthcare and research.   In this study, we evaluated the feasibility of employing four state-of-the-art pretrained models for medical image retrieval at modality, body region, and organ levels and compared the results of two similarity indexing approaches. Since the employed networks take 2D images, we analyzed the impacts of weighting and sampling strategies to incorporate 3D information during retrieval of 3D volumes. We showed that medical image retrieval is feasible using pretrained networks without any additional training or fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for various tasks at modality, body region, and organ level."}
{"title":"Piecewise polynomial regression of tame functions via integer programming","authors":["Jiri Nemecek","Gilles Bareilles","Johannes Aspman","Jakub Marecek"],"raw_abstract":"We consider the task of estimating functions belonging to a specific class of\nnonsmooth functions, namely so-called tame functions. These functions appear in\na wide range of applications: training deep learning, value functions of\nmixed-integer programs, or wave functions of small molecules. We show that tame\nfunctions are approximable by piecewise polynomials on any full-dimensional\ncube. We then present the first ever mixed-integer programming formulation of\npiecewise polynomial regression. Together, these can be used to estimate tame\nfunctions. We demonstrate promising computational results.","publication_date":1700674662,"paper_link":"http://arxiv.org/pdf/2311.13544v1","categories":["Mathematics","Statistics"],"abstract":"We consider the task of estimating functions belonging to a specific class of nonsmooth functions, namely so-called tame functions. These functions appear in a wide range of applications: training deep learning, value functions of mixed-integer programs, or wave functions of small molecules. We show that tame functions are approximable by piecewise polynomials on any full-dimensional cube. We then present the first ever mixed-integer programming formulation of piecewise polynomial regression. Together, these can be used to estimate tame functions. We demonstrate promising computational results."}
{"title":"Finding eigenvectors with a quantum variational algorithm","authors":["Juan Carlos Garcia-Escartin"],"raw_abstract":"This paper presents a hybrid variational quantum algorithm that finds a\nrandom eigenvector of a unitary matrix with a known quantum circuit. The\nalgorithm is based on the SWAP test on trial states generated by a parametrized\nquantum circuit. The eigenvector is described by a compact set of classical\nparameters that can be used to reproduce the found approximation to the\neigenstate on demand. This variational eigenvector finder can be adapted to\nsolve the generalized eigenvalue problem, to find the eigenvectors of normal\nmatrices and to perform quantum principal component analysis (QPCA) on unknown\ninput mixed states. These algorithms can all be run with low depth quantum\ncircuits, suitable for an efficient implementation on noisy intermediate state\nquantum computers (NISQC) and, with some restrictions, on linear optical\nsystems. Limitations and potential applications are discussed.","publication_date":1700674602,"paper_link":"http://arxiv.org/pdf/2311.13543v1","categories":["Physics"],"abstract":"This paper presents a hybrid variational quantum algorithm that finds a random eigenvector of a unitary matrix with a known quantum circuit. The algorithm is based on the SWAP test on trial states generated by a parametrized quantum circuit. The eigenvector is described by a compact set of classical parameters that can be used to reproduce the found approximation to the eigenstate on demand. This variational eigenvector finder can be adapted to solve the generalized eigenvalue problem, to find the eigenvectors of normal matrices and to perform quantum principal component analysis (QPCA) on unknown input mixed states. These algorithms can all be run with low depth quantum circuits, suitable for an efficient implementation on noisy intermediate state quantum computers (NISQC) and, with some restrictions, on linear optical systems. Limitations and potential applications are discussed."}
{"title":"Learned Nonlinear Predictor for Critically Sampled 3D Point Cloud Attribute Compression","authors":["Tam Thuc Do","Philip A. Chou","Gene Cheung"],"raw_abstract":"We study 3D point cloud attribute compression via a volumetric approach:\nassuming point cloud geometry is known at both encoder and decoder, parameters\n$\\theta$ of a continuous attribute function $f: \\mathbb{R}^3 \\mapsto\n\\mathbb{R}$ are quantized to $\\hat{\\theta}$ and encoded, so that discrete\nsamples $f_{\\hat{\\theta}}(\\mathbf{x}_i)$ can be recovered at known 3D points\n$\\mathbf{x}_i \\in \\mathbb{R}^3$ at the decoder. Specifically, we consider a\nnested sequences of function subspaces $\\mathcal{F}^{(p)}_{l_0} \\subseteq\n\\cdots \\subseteq \\mathcal{F}^{(p)}_L$, where $\\mathcal{F}_l^{(p)}$ is a family\nof functions spanned by B-spline basis functions of order $p$, $f_l^*$ is the\nprojection of $f$ on $\\mathcal{F}_l^{(p)}$ and encoded as low-pass coefficients\n$F_l^*$, and $g_l^*$ is the residual function in orthogonal subspace\n$\\mathcal{G}_l^{(p)}$ (where $\\mathcal{G}_l^{(p)} \\oplus \\mathcal{F}_l^{(p)} =\n\\mathcal{F}_{l+1}^{(p)}$) and encoded as high-pass coefficients $G_l^*$. In\nthis paper, to improve coding performance over [1], we study predicting\n$f_{l+1}^*$ at level $l+1$ given $f_l^*$ at level $l$ and encoding of $G_l^*$\nfor the $p=1$ case (RAHT($1$)). For the prediction, we formalize RAHT(1) linear\nprediction in MPEG-PCC in a theoretical framework, and propose a new nonlinear\npredictor using a polynomial of bilateral filter. We derive equations to\nefficiently compute the critically sampled high-pass coefficients $G_l^*$\namenable to encoding. We optimize parameters in our resulting feed-forward\nnetwork on a large training set of point clouds by minimizing a rate-distortion\nLagrangian. Experimental results show that our improved framework outperformed\nthe MPEG G-PCC predictor by $11$ to $12\\%$ in bit rate reduction.","publication_date":1700674014,"paper_link":"http://arxiv.org/pdf/2311.13539v1","categories":["Electrical Engineering and Systems Science"],"abstract":"We study 3D point cloud attribute compression via a volumetric approach: assuming point cloud geometry is known at both encoder and decoder, parameters __FORMULA__ of a continuous attribute function __FORMULA__ are quantized to __FORMULA__ and encoded, so that discrete samples __FORMULA__ can be recovered at known 3D points __FORMULA__ at the decoder. Specifically, we consider a nested sequences of function subspaces __FORMULA__, where __FORMULA__ is a family of functions spanned by B-spline basis functions of order __FORMULA__, __FORMULA__ is the projection of __FORMULA__ on __FORMULA__ and encoded as low-pass coefficients __FORMULA__, and __FORMULA__ is the residual function in orthogonal subspace __FORMULA__ (where __FORMULA__) and encoded as high-pass coefficients __FORMULA__. In this paper, to improve coding performance over [1], we study predicting __FORMULA__ at level __FORMULA__ given __FORMULA__ at level __FORMULA__ and encoding of __FORMULA__ for the __FORMULA__ case (RAHT(__FORMULA__)). For the prediction, we formalize RAHT(1) linear prediction in MPEG-PCC in a theoretical framework, and propose a new nonlinear predictor using a polynomial of bilateral filter. We derive equations to efficiently compute the critically sampled high-pass coefficients __FORMULA__ amenable to encoding. We optimize parameters in our resulting feed-forward network on a large training set of point clouds by minimizing a rate-distortion Lagrangian. Experimental results show that our improved framework outperformed the MPEG G-PCC predictor by __FORMULA__ to __FORMULA__ in bit rate reduction."}
{"title":"ab initio informed inelastic neutron scattering for time-resolved local dynamics in molten MgCl2","authors":["Shubhojit Banerjee","Rajni Chahal","Alexander S. Ivanov","Santanu Roy","Vyacheslav S. Bryantsev","Yuya Shinohara","Stephen T Lam"],"raw_abstract":"Ion dynamics that drive the transport and thermophysical properties of molten\nsalts are poorly understood due to challenges in precisely quantifying the\nspatial and temporal fluctuations of specific ions in highly disordered\nsystems. While the Van Hove correlation function (VHF) obtained from inelastic\nneutron scattering (INS) probes these dynamics directly, its interpretation is\nlimited by the inherent species-averaging of experiments, which obscures\nanalysis of key ion transport and solvation mechanisms. Here, ab initio\nmolecular dynamics (AIMD) is used to model the VHF, unravel its partial\ncontributions, and elucidate its underlying ionic transport mechanisms. Slow\ndecorrelation is revealed for oppositely charged ions (Mg2+ and Cl-) caused by\nion exchange across the solvation shell between adjoining ionocovalent\ncomplexes. Furthermore, transport coefficients are accurately recovered and\nconnections between macroscopic properties and ion dynamics are revealed. This\nstudy demonstrates the potential of ab initio-informed VHF to resolve\nlong-standing challenges in uncovering relationships between picosecond-scale\nion dynamics, mechanisms, and emergent physical properties of molten salts.","publication_date":1700673589,"paper_link":"http://arxiv.org/pdf/2311.13537v1","categories":["Physics"],"abstract":"Ion dynamics that drive the transport and thermophysical properties of molten salts are poorly understood due to challenges in precisely quantifying the spatial and temporal fluctuations of specific ions in highly disordered systems. While the Van Hove correlation function (VHF) obtained from inelastic neutron scattering (INS) probes these dynamics directly, its interpretation is limited by the inherent species-averaging of experiments, which obscures analysis of key ion transport and solvation mechanisms. Here, ab initio molecular dynamics (AIMD) is used to model the VHF, unravel its partial contributions, and elucidate its underlying ionic transport mechanisms. Slow decorrelation is revealed for oppositely charged ions (Mg2+ and Cl-) caused by ion exchange across the solvation shell between adjoining ionocovalent complexes. Furthermore, transport coefficients are accurately recovered and connections between macroscopic properties and ion dynamics are revealed. This study demonstrates the potential of ab initio-informed VHF to resolve long-standing challenges in uncovering relationships between picosecond-scale ion dynamics, mechanisms, and emergent physical properties of molten salts."}
{"title":"DiffusionMat: Alpha Matting as Sequential Refinement Learning","authors":["Yangyang Xu","Shengfeng He","Wenqi Shao","Kwan-Yee K. Wong","Yu Qiao","Ping Luo"],"raw_abstract":"In this paper, we introduce DiffusionMat, a novel image matting framework\nthat employs a diffusion model for the transition from coarse to refined alpha\nmattes. Diverging from conventional methods that utilize trimaps merely as\nloose guidance for alpha matte prediction, our approach treats image matting as\na sequential refinement learning process. This process begins with the addition\nof noise to trimaps and iteratively denoises them using a pre-trained diffusion\nmodel, which incrementally guides the prediction towards a clean alpha matte.\nThe key innovation of our framework is a correction module that adjusts the\noutput at each denoising step, ensuring that the final result is consistent\nwith the input image's structures. We also introduce the Alpha Reliability\nPropagation, a novel technique designed to maximize the utility of available\nguidance by selectively enhancing the trimap regions with confident alpha\ninformation, thus simplifying the correction task. To train the correction\nmodule, we devise specialized loss functions that target the accuracy of the\nalpha matte's edges and the consistency of its opaque and transparent regions.\nWe evaluate our model across several image matting benchmarks, and the results\nindicate that DiffusionMat consistently outperforms existing methods. Project\npage at~\\url{https://cnnlstm.github.io/DiffusionMat","publication_date":1700673404,"paper_link":"http://arxiv.org/pdf/2311.13535v1","categories":["Computer Science"],"abstract":"In this paper, we introduce DiffusionMat, a novel image matting framework that employs a diffusion model for the transition from coarse to refined alpha mattes. Diverging from conventional methods that utilize trimaps merely as loose guidance for alpha matte prediction, our approach treats image matting as a sequential refinement learning process. This process begins with the addition of noise to trimaps and iteratively denoises them using a pre-trained diffusion model, which incrementally guides the prediction towards a clean alpha matte. The key innovation of our framework is a correction module that adjusts the output at each denoising step, ensuring that the final result is consistent with the input image's structures. We also introduce the Alpha Reliability Propagation, a novel technique designed to maximize the utility of available guidance by selectively enhancing the trimap regions with confident alpha information, thus simplifying the correction task. To train the correction module, we devise specialized loss functions that target the accuracy of the alpha matte's edges and the consistency of its opaque and transparent regions. We evaluate our model across several image matting benchmarks, and the results indicate that DiffusionMat consistently outperforms existing methods. Project page at~\\url{https://cnnlstm.github.io/DiffusionMat"}
{"title":"LM-Cocktail: Resilient Tuning of Language Models via Model Merging","authors":["Shitao Xiao","Zheng Liu","Peitian Zhang","Xingrun Xing"],"raw_abstract":"The pre-trained language models are continually fine-tuned to better support\ndownstream applications. However, this operation may result in significant\nperformance degeneration on general tasks beyond the targeted domain. To\novercome this problem, we propose a novel method which enables the fine-tuned\nmodel to stay resilient in general perspectives. Our method is conducted in the\nform of model merging (namely LM-Cocktail), where the fine-tuned language model\nis merged with the pre-trained base model or the peer models from other domains\nthrough weighted average. Despite simplicity, LM-Cocktail is surprisingly\neffective: the resulted model is able to achieve a strong empirical performance\nin the whole scope of general tasks while preserving a superior capacity in its\ntargeted domain. We conduct comprehensive experiments with LLama and BGE model\non popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the\nefficacy of our proposed method. The code and checkpoints are available at\nhttps://github.com/FlagOpen/FlagEmbedding.","publication_date":1700673294,"paper_link":"http://arxiv.org/pdf/2311.13534v1","categories":["Computer Science"],"abstract":"The pre-trained language models are continually fine-tuned to better support downstream applications. However, this operation may result in significant performance degeneration on general tasks beyond the targeted domain. To overcome this problem, we propose a novel method which enables the fine-tuned model to stay resilient in general perspectives. Our method is conducted in the form of model merging (namely LM-Cocktail), where the fine-tuned language model is merged with the pre-trained base model or the peer models from other domains through weighted average. Despite simplicity, LM-Cocktail is surprisingly effective: the resulted model is able to achieve a strong empirical performance in the whole scope of general tasks while preserving a superior capacity in its targeted domain. We conduct comprehensive experiments with LLama and BGE model on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the efficacy of our proposed method. The code and checkpoints are available at https://github.com/FlagOpen/FlagEmbedding."}
{"title":"Boosting ensemble refinement with transferable force field corrections: synergistic optimization for molecular simulations","authors":["Ivan Gilardoni","Thorben Fr\u00f6hlking","Giovanni Bussi"],"raw_abstract":"A novel method combining the ensemble refinement by maximum entropy principle\nand the force field fitting approach is presented. Its formulation allows to\ncontinuously interpolate in between these two methods, which can thus be\ninterpreted as two limiting cases. A cross-validation procedure enables to\ncorrectly assess the relative weight of both of them, distinguishing scenarios\nwhere the combined approach is meaningful from those in which either ensemble\nrefinement or force field fitting separately prevails. The efficacy of their\ncombination is examined for a realistic case study of RNA oligomers. Within the\nnew scheme, molecular dynamics simulations are integrated with experimental\ndata provided by nuclear-magnetic-resonance measures. We show that force field\ncorrections are in general superior when applied to the appropriate force field\nterms, but are automatically discarded by the method when applied to\ninappropriate force field terms.","publication_date":1700673101,"paper_link":"http://arxiv.org/pdf/2311.13532v1","categories":["Quantitative Biology","Physics"],"abstract":"A novel method combining the ensemble refinement by maximum entropy principle and the force field fitting approach is presented. Its formulation allows to continuously interpolate in between these two methods, which can thus be interpreted as two limiting cases. A cross-validation procedure enables to correctly assess the relative weight of both of them, distinguishing scenarios where the combined approach is meaningful from those in which either ensemble refinement or force field fitting separately prevails. The efficacy of their combination is examined for a realistic case study of RNA oligomers. Within the new scheme, molecular dynamics simulations are integrated with experimental data provided by nuclear-magnetic-resonance measures. We show that force field corrections are in general superior when applied to the appropriate force field terms, but are automatically discarded by the method when applied to inappropriate force field terms."}
{"title":"Leveraging CNNs and Ensemble Learning for Automated Disaster Image Classification","authors":["Archit Rathod","Veer Pariawala","Mokshit Surana","Kumkum Saxena"],"raw_abstract":"Natural disasters act as a serious threat globally, requiring effective and\nefficient disaster management and recovery. This paper focuses on classifying\nnatural disaster images using Convolutional Neural Networks (CNNs). Multiple\nCNN architectures were built and trained on a dataset containing images of\nearthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach\nproved to be the most effective, achieving 95% accuracy and an F1 score going\nup to 0.96 for individual classes. Tuning hyperparameters of individual models\nfor optimization was critical to maximize the models' performance. The stacking\nof CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN\nand ResNet models to improve the overall accuracy of the classification.\nResults obtained from the models illustrated the potency of CNN-based models\nfor automated disaster image classification. This lays the foundation for\nexpanding these techniques to build robust systems for disaster response,\ndamage assessment, and recovery management.","publication_date":1700672817,"paper_link":"http://arxiv.org/pdf/2311.13531v1","categories":["Computer Science"],"abstract":"Natural disasters act as a serious threat globally, requiring effective and efficient disaster management and recovery. This paper focuses on classifying natural disaster images using Convolutional Neural Networks (CNNs). Multiple CNN architectures were built and trained on a dataset containing images of earthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach proved to be the most effective, achieving 95% accuracy and an F1 score going up to 0.96 for individual classes. Tuning hyperparameters of individual models for optimization was critical to maximize the models' performance. The stacking of CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN and ResNet models to improve the overall accuracy of the classification. Results obtained from the models illustrated the potency of CNN-based models for automated disaster image classification. This lays the foundation for expanding these techniques to build robust systems for disaster response, damage assessment, and recovery management."}
{"title":"The Gauss map on bielliptic Prym varieties","authors":["Constantin Podelski"],"raw_abstract":"We compute the degree of the Gauss map on Prym varieties corresponding to\ndouble coverings of bielliptic curves, and their degenerations. This leads in\nparticular to a complete description of the degree of the Gauss map on the\nAndreotti-Mayer locus in dimension $5$.","publication_date":1700671936,"paper_link":"http://arxiv.org/pdf/2311.13521v1","categories":["Mathematics"],"abstract":"We compute the degree of the Gauss map on Prym varieties corresponding to double coverings of bielliptic curves, and their degenerations. This leads in particular to a complete description of the degree of the Gauss map on the Andreotti-Mayer locus in dimension __FORMULA__."}
{"title":"Photon Number Resolving Detection with a Single-Photon Detector and Adaptive Storage Loop","authors":["Nicholas M. Sullivan","Boris Braverman","Jeremy Upham","Robert W. Boyd"],"raw_abstract":"Photon number resolving (PNR) measurements are beneficial or even necessary\nfor many applications in quantum optics. Unfortunately, PNR detectors are\nusually large, slow, expensive, and difficult to operate. However, if the input\nsignal is multiplexed, photon \"click\" detectors, that lack an intrinsic photon\nnumber resolving capability, can still be used to realize photon number\nresolution. Here, we investigate the operation of a single click detector,\ntogether with a storage line with tunable outcoupling. Using adaptive feedback\nto adjust the storage outcoupling rate, the dynamic range of the detector can\nin certain situations be extended by up to an order of magnitude relative to a\npurely passive setup. An adaptive approach can thus allow for photon number\nvariance below the quantum shot noise limit under a wider range of conditions\nthan using a passive multiplexing approach. This can enable applications in\nquantum enhanced metrology and quantum computing.","publication_date":1700671175,"paper_link":"http://arxiv.org/pdf/2311.13515v1","categories":["Physics"],"abstract":"Photon number resolving (PNR) measurements are beneficial or even necessary for many applications in quantum optics. Unfortunately, PNR detectors are usually large, slow, expensive, and difficult to operate. However, if the input signal is multiplexed, photon \"click\" detectors, that lack an intrinsic photon number resolving capability, can still be used to realize photon number resolution. Here, we investigate the operation of a single click detector, together with a storage line with tunable outcoupling. Using adaptive feedback to adjust the storage outcoupling rate, the dynamic range of the detector can in certain situations be extended by up to an order of magnitude relative to a purely passive setup. An adaptive approach can thus allow for photon number variance below the quantum shot noise limit under a wider range of conditions than using a passive multiplexing approach. This can enable applications in quantum enhanced metrology and quantum computing."}
{"title":"Hybrid Whale-Mud-Ring Optimization for Precise Color Skin Cancer Image Segmentation","authors":["Amir Hamza","Badis Lekouaghet","Yassine Himeur"],"raw_abstract":"Timely identification and treatment of rapidly progressing skin cancers can\nsignificantly contribute to the preservation of patients' health and\nwell-being. Dermoscopy, a dependable and accessible tool, plays a pivotal role\nin the initial stages of skin cancer detection. Consequently, the effective\nprocessing of digital dermoscopy images holds significant importance in\nelevating the accuracy of skin cancer diagnoses. Multilevel thresholding is a\nkey tool in medical imaging that extracts objects within the image to\nfacilitate its analysis. In this paper, an enhanced version of the Mud Ring\nAlgorithm hybridized with the Whale Optimization Algorithm, named WMRA, is\nproposed. The proposed approach utilizes bubble-net attack and mud ring\nstrategy to overcome stagnation in local optima and obtain optimal thresholds.\nThe experimental results show that WMRA is powerful against a cluster of recent\nmethods in terms of fitness, Peak Signal to Noise Ratio (PSNR), and Mean Square\nError (MSE).","publication_date":1700670943,"paper_link":"http://arxiv.org/pdf/2311.13512v1","categories":["Computer Science"],"abstract":"Timely identification and treatment of rapidly progressing skin cancers can significantly contribute to the preservation of patients' health and well-being. Dermoscopy, a dependable and accessible tool, plays a pivotal role in the initial stages of skin cancer detection. Consequently, the effective processing of digital dermoscopy images holds significant importance in elevating the accuracy of skin cancer diagnoses. Multilevel thresholding is a key tool in medical imaging that extracts objects within the image to facilitate its analysis. In this paper, an enhanced version of the Mud Ring Algorithm hybridized with the Whale Optimization Algorithm, named WMRA, is proposed. The proposed approach utilizes bubble-net attack and mud ring strategy to overcome stagnation in local optima and obtain optimal thresholds. The experimental results show that WMRA is powerful against a cluster of recent methods in terms of fitness, Peak Signal to Noise Ratio (PSNR), and Mean Square Error (MSE)."}
{"title":"On remoteness functions of k-NIM with k+1 piles in normal and in mis\u00e8re versions","authors":["Vladimir Gurvich","Vladislav Maximchuk","Georgy Miheenkov","Mariya Naumova"],"raw_abstract":"Given integer $n$ and $k$ such that $0 < k \\leq n$ and $n$ piles of stones,\ntwo players alternate turns. By one move it is allowed to choose any $k$ piles\nand remove exactly one stone from each. The player who has to move but cannot\nis the loser. in the normal version of the game and (s)he is the winner in the\nmis\\`ere version. Cases $k=1$ and $k = n$ are trivial. For $k=2$ the game was\nsolved for $n \\leq 6$. For $n \\leq 4$ the Sprague-Grundy function was\nefficiently computed (for both versions). For $n = 5,6$ a polynomial algorithm\ncomputing P-positions was obtained for the normal version. \\newline Then, for\nthe case $k = n-1$, a very simple explicit rule that determines the Smith\nremoteness function was found for the normal version of the game: the player\nwho has to move keeps a pile with the minimum even number of stones; if all\npiles have odd number of stones then (s)he keeps a maximum one, while the $n-1$\nremaining piles are reduced by one stone each in accordance with the rules of\nthe game. \\newline Computations show that the same rule works efficiently for\nthe mis\\`ere version too. The exceptions are sparse and are listed in Section\n2. Denote a position by $x = (x_1, \\dots, x_n)$. Due to symmetry, we can assume\nwlog that $x_1 \\leq \\ldots \\leq x_n$. Our computations partition all exceptions\ninto the following three families: $x_1$ is even, $x_1 = 1$, and $x_1 \\geq 3$\nis odd. In all three cases we suggest explicit formulas that cover all found\nexceptions, but this is not proven.","publication_date":1700670877,"paper_link":"http://arxiv.org/pdf/2311.13511v1","categories":["Mathematics"],"abstract":"Given integer __FORMULA__ and __FORMULA__ such that __FORMULA__ and __FORMULA__ piles of stones, two players alternate turns. By one move it is allowed to choose any __FORMULA__ piles and remove exactly one stone from each. The player who has to move but cannot is the loser. in the normal version of the game and (s)he is the winner in the mis\\`ere version. Cases __FORMULA__ and __FORMULA__ are trivial. For __FORMULA__ the game was solved for __FORMULA__. For __FORMULA__ the Sprague-Grundy function was efficiently computed (for both versions). For __FORMULA__ a polynomial algorithm computing P-positions was obtained for the normal version. \\newline Then, for the case __FORMULA__, a very simple explicit rule that determines the Smith remoteness function was found for the normal version of the game: the player who has to move keeps a pile with the minimum even number of stones; if all piles have odd number of stones then (s)he keeps a maximum one, while the __FORMULA__ remaining piles are reduced by one stone each in accordance with the rules of the game. \\newline Computations show that the same rule works efficiently for the mis\\`ere version too. The exceptions are sparse and are listed in Section 2. Denote a position by __FORMULA__. Due to symmetry, we can assume wlog that __FORMULA__. Our computations partition all exceptions into the following three families: __FORMULA__ is even, __FORMULA__, and __FORMULA__ is odd. In all three cases we suggest explicit formulas that cover all found exceptions, but this is not proven."}
{"title":"Energy and Time-Aware Inference Offloading for DNN-based Applications in LEO Satellites","authors":["Yijie Chen","Qiyang Zhang","Yiran Zhang","Xiao Ma","Ao Zhou"],"raw_abstract":"In recent years, Low Earth Orbit (LEO) satellites have witnessed rapid\ndevelopment, with inference based on Deep Neural Network (DNN) models emerging\nas the prevailing technology for remote sensing satellite image recognition.\nHowever, the substantial computation capability and energy demands of DNN\nmodels, coupled with the instability of the satellite-ground link, pose\nsignificant challenges, burdening satellites with limited power intake and\nhindering the timely completion of tasks. Existing approaches, such as\ntransmitting all images to the ground for processing or executing DNN models on\nthe satellite, is unable to effectively address this issue. By exploiting the\ninternal hierarchical structure of DNNs and treating each layer as an\nindependent subtask, we propose a satellite-ground collaborative computation\npartial offloading approach to address this challenge. We formulate the problem\nof minimizing the inference task execution time and onboard energy consumption\nthrough offloading as an integer linear programming (ILP) model. The complexity\nin solving the problem arises from the combinatorial explosion in the discrete\nsolution space. To address this, we have designed an improved optimization\nalgorithm based on branch and bound. Simulation results illustrate that,\ncompared to the existing approaches, our algorithm improve the performance by\n10%-18%","publication_date":1700670857,"paper_link":"http://arxiv.org/pdf/2311.13509v1","categories":["Computer Science"],"abstract":"In recent years, Low Earth Orbit (LEO) satellites have witnessed rapid development, with inference based on Deep Neural Network (DNN) models emerging as the prevailing technology for remote sensing satellite image recognition. However, the substantial computation capability and energy demands of DNN models, coupled with the instability of the satellite-ground link, pose significant challenges, burdening satellites with limited power intake and hindering the timely completion of tasks. Existing approaches, such as transmitting all images to the ground for processing or executing DNN models on the satellite, is unable to effectively address this issue. By exploiting the internal hierarchical structure of DNNs and treating each layer as an independent subtask, we propose a satellite-ground collaborative computation partial offloading approach to address this challenge. We formulate the problem of minimizing the inference task execution time and onboard energy consumption through offloading as an integer linear programming (ILP) model. The complexity in solving the problem arises from the combinatorial explosion in the discrete solution space. To address this, we have designed an improved optimization algorithm based on branch and bound. Simulation results illustrate that, compared to the existing approaches, our algorithm improve the performance by 10%-18%"}
{"title":"Applying Dimensionality Reduction as Precursor to LSTM-CNN Models for Classifying Imagery and Motor Signals in ECoG-Based BCIs","authors":["Soham Bafana"],"raw_abstract":"Motor impairments, frequently caused by neurological incidents like strokes\nor traumatic brain injuries, present substantial obstacles in rehabilitation\ntherapy. This research aims to elevate the field by optimizing motor imagery\nclassification algorithms within Brain-Computer Interfaces (BCIs). By improving\nthe efficiency of BCIs, we offer a novel approach that holds significant\npromise for enhancing motor rehabilitation outcomes. Utilizing unsupervised\ntechniques for dimensionality reduction, namely Uniform Manifold Approximation\nand Projection (UMAP) coupled with K-Nearest Neighbors (KNN), we evaluate the\nnecessity of employing supervised methods such as Long Short-Term Memory (LSTM)\nand Convolutional Neural Networks (CNNs) for classification tasks. Importantly,\nparticipants who exhibited high KNN scores following UMAP dimensionality\nreduction also achieved high accuracy in supervised deep learning (DL) models.\nDue to individualized model requirements and massive neural training data,\ndimensionality reduction becomes an effective preprocessing step that minimizes\nthe need for extensive data labeling and supervised deep learning techniques.\nThis approach has significant implications not only for targeted therapies in\nmotor dysfunction but also for addressing regulatory, safety, and reliability\nconcerns in the rapidly evolving BCI field.","publication_date":1700670846,"paper_link":"http://arxiv.org/pdf/2311.13507v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Motor impairments, frequently caused by neurological incidents like strokes or traumatic brain injuries, present substantial obstacles in rehabilitation therapy. This research aims to elevate the field by optimizing motor imagery classification algorithms within Brain-Computer Interfaces (BCIs). By improving the efficiency of BCIs, we offer a novel approach that holds significant promise for enhancing motor rehabilitation outcomes. Utilizing unsupervised techniques for dimensionality reduction, namely Uniform Manifold Approximation and Projection (UMAP) coupled with K-Nearest Neighbors (KNN), we evaluate the necessity of employing supervised methods such as Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNNs) for classification tasks. Importantly, participants who exhibited high KNN scores following UMAP dimensionality reduction also achieved high accuracy in supervised deep learning (DL) models. Due to individualized model requirements and massive neural training data, dimensionality reduction becomes an effective preprocessing step that minimizes the need for extensive data labeling and supervised deep learning techniques. This approach has significant implications not only for targeted therapies in motor dysfunction but also for addressing regulatory, safety, and reliability concerns in the rapidly evolving BCI field."}
{"title":"Bitformer: An efficient Transformer with bitwise operation-based attention for Big Data Analytics at low-cost low-precision devices","authors":["Gaoxiang Duan","Junkai Zhang","Xiaoying Zheng","Yongxin Zhu"],"raw_abstract":"In the current landscape of large models, the Transformer stands as a\ncornerstone, playing a pivotal role in shaping the trajectory of modern models.\nHowever, its application encounters challenges attributed to the substantial\ncomputational intricacies intrinsic to its attention mechanism. Moreover, its\nreliance on high-precision floating-point operations presents specific hurdles,\nparticularly evident in computation-intensive scenarios such as edge computing\nenvironments. These environments, characterized by resource-constrained devices\nand a preference for lower precision, necessitate innovative solutions.\n  To tackle the exacting data processing demands posed by edge devices, we\nintroduce the Bitformer model, an inventive extension of the Transformer\nparadigm. Central to this innovation is a novel attention mechanism that\nadeptly replaces conventional floating-point matrix multiplication with bitwise\noperations. This strategic substitution yields dual advantages. Not only does\nit maintain the attention mechanism's prowess in capturing intricate long-range\ninformation dependencies, but it also orchestrates a profound reduction in the\ncomputational complexity inherent in the attention operation. The transition\nfrom an $O(n^2d)$ complexity, typical of floating-point operations, to an\n$O(n^2T)$ complexity characterizing bitwise operations, substantiates this\nadvantage. Notably, in this context, the parameter $T$ remains markedly smaller\nthan the conventional dimensionality parameter $d$.\n  The Bitformer model in essence endeavors to reconcile the indomitable\nrequirements of modern computing landscapes with the constraints posed by edge\ncomputing scenarios. By forging this innovative path, we bridge the gap between\nhigh-performing models and resource-scarce environments, thus unveiling a\npromising trajectory for further advancements in the field.","publication_date":1700670024,"paper_link":"http://arxiv.org/pdf/2311.13502v1","categories":["Computer Science"],"abstract":"In the current landscape of large models, the Transformer stands as a cornerstone, playing a pivotal role in shaping the trajectory of modern models. However, its application encounters challenges attributed to the substantial computational intricacies intrinsic to its attention mechanism. Moreover, its reliance on high-precision floating-point operations presents specific hurdles, particularly evident in computation-intensive scenarios such as edge computing environments. These environments, characterized by resource-constrained devices and a preference for lower precision, necessitate innovative solutions.   To tackle the exacting data processing demands posed by edge devices, we introduce the Bitformer model, an inventive extension of the Transformer paradigm. Central to this innovation is a novel attention mechanism that adeptly replaces conventional floating-point matrix multiplication with bitwise operations. This strategic substitution yields dual advantages. Not only does it maintain the attention mechanism's prowess in capturing intricate long-range information dependencies, but it also orchestrates a profound reduction in the computational complexity inherent in the attention operation. The transition from an __FORMULA__ complexity, typical of floating-point operations, to an __FORMULA__ complexity characterizing bitwise operations, substantiates this advantage. Notably, in this context, the parameter __FORMULA__ remains markedly smaller than the conventional dimensionality parameter __FORMULA__.   The Bitformer model in essence endeavors to reconcile the indomitable requirements of modern computing landscapes with the constraints posed by edge computing scenarios. By forging this innovative path, we bridge the gap between high-performing models and resource-scarce environments, thus unveiling a promising trajectory for further advancements in the field."}
{"title":"Multiscale cortical morphometry reveals pronounced regional and scale-dependent variations across the lifespan","authors":["Karoline Leiberg","Timo Blattner","Bethany Little","Victor B. B. Mello","Fernanda H. P. de Moraes","Christian Rummel","Peter N. Taylor","Bruno Mota","Yujiang Wang"],"raw_abstract":"Motivation: Characterising the changes in cortical morphology across the\nlifespan is fundamental for a range of research and clinical applications. Most\nstudies to date have found a monotonic decrease in commonly used morphometrics,\nsuch as cortical thickness and volume, across the entire brain with increasing\nage. Any regional variations reported are subtle changes in the rate of\ndecrease. However, these descriptions of morphological changes have been\nlimited to a single length scale. Here, we delineate the morphological changes\nassociated with the healthy lifespan in multi-scale morphometrics.\n  Methods: Using MRI from subjects aged 6-88 years from NKI (n=833) and CamCAN\n(n=641), we computed several morphometrics at spatial scales ranging from 0.32\nmm to 3 mm.\n  Results: On the level of whole cortical hemispheres, lifespan trajectories\nshow diverging and even opposing trends at different spatial scales, in\ncontrast to the monotonic decreases of volume and thickness described so far.\nPronounced regional differences between lobes also became apparent in scales\nover 0.7 mm.\n  Conclusion: Our study provides a comprehensive multi-scale description of\nlifespan effects on cortical morphology in an age range from 6-88 years. In\nfuture, this can be used as a normative model to compare individuals or\ncohorts, hence identifying morphological abnormalities. Our results reveal the\ncomplementary information contained in different spatial scales, suggesting\nthat morphometrics should not be considered as mere scalars, but as functions\nof length scale.","publication_date":1700669992,"paper_link":"http://arxiv.org/pdf/2311.13501v1","categories":["Quantitative Biology"],"abstract":"Motivation: Characterising the changes in cortical morphology across the lifespan is fundamental for a range of research and clinical applications. Most studies to date have found a monotonic decrease in commonly used morphometrics, such as cortical thickness and volume, across the entire brain with increasing age. Any regional variations reported are subtle changes in the rate of decrease. However, these descriptions of morphological changes have been limited to a single length scale. Here, we delineate the morphological changes associated with the healthy lifespan in multi-scale morphometrics.   Methods: Using MRI from subjects aged 6-88 years from NKI (n=833) and CamCAN (n=641), we computed several morphometrics at spatial scales ranging from 0.32 mm to 3 mm.   Results: On the level of whole cortical hemispheres, lifespan trajectories show diverging and even opposing trends at different spatial scales, in contrast to the monotonic decreases of volume and thickness described so far. Pronounced regional differences between lobes also became apparent in scales over 0.7 mm.   Conclusion: Our study provides a comprehensive multi-scale description of lifespan effects on cortical morphology in an age range from 6-88 years. In future, this can be used as a normative model to compare individuals or cohorts, hence identifying morphological abnormalities. Our results reveal the complementary information contained in different spatial scales, suggesting that morphometrics should not be considered as mere scalars, but as functions of length scale."}
{"title":"Current Topological and Machine Learning Applications for Bias Detection in Text","authors":["Colleen Farrelly","Yashbir Singh","Quincy A. Hathaway","Gunnar Carlsson","Ashok Choudhary","Rahul Paul","Gianfranco Doretto","Yassine Himeur","Shadi Atalls","Wathiq Mansoor"],"raw_abstract":"Institutional bias can impact patient outcomes, educational attainment, and\nlegal system navigation. Written records often reflect bias, and once bias is\nidentified; it is possible to refer individuals for training to reduce bias.\nMany machine learning tools exist to explore text data and create predictive\nmodels that can search written records to identify real-time bias. However, few\nprevious studies investigate large language model embeddings and geometric\nmodels of biased text data to understand geometry's impact on bias modeling\naccuracy. To overcome this issue, this study utilizes the RedditBias database\nto analyze textual biases. Four transformer models, including BERT and RoBERTa\nvariants, were explored. Post-embedding, t-SNE allowed two-dimensional\nvisualization of data. KNN classifiers differentiated bias types, with lower\nk-values proving more effective. Findings suggest BERT, particularly mini BERT,\nexcels in bias classification, while multilingual models lag. The\nrecommendation emphasizes refining monolingual models and exploring\ndomain-specific biases.","publication_date":1700669562,"paper_link":"http://arxiv.org/pdf/2311.13495v1","categories":["Computer Science"],"abstract":"Institutional bias can impact patient outcomes, educational attainment, and legal system navigation. Written records often reflect bias, and once bias is identified; it is possible to refer individuals for training to reduce bias. Many machine learning tools exist to explore text data and create predictive models that can search written records to identify real-time bias. However, few previous studies investigate large language model embeddings and geometric models of biased text data to understand geometry's impact on bias modeling accuracy. To overcome this issue, this study utilizes the RedditBias database to analyze textual biases. Four transformer models, including BERT and RoBERTa variants, were explored. Post-embedding, t-SNE allowed two-dimensional visualization of data. KNN classifiers differentiated bias types, with lower k-values proving more effective. Findings suggest BERT, particularly mini BERT, excels in bias classification, while multilingual models lag. The recommendation emphasizes refining monolingual models and exploring domain-specific biases."}
{"title":"Grad-Shafranov equilibria via data-free physics informed neural networks","authors":["Byoungchan Jang","Alan A. Kaptanoglu","Rahul Gaur","Shaowu Pan","Matt Landreman","William Dorland"],"raw_abstract":"A large number of magnetohydrodynamic (MHD) equilibrium calculations are\noften required for uncertainty quantification, optimization, and real-time\ndiagnostic information, making MHD equilibrium codes vital to the field of\nplasma physics. In this paper, we explore a method for solving the\nGrad-Shafranov equation by using Physics-Informed Neural Networks (PINNs). For\nPINNs, we optimize neural networks by directly minimizing the residual of the\nPDE as a loss function. We show that PINNs can accurately and effectively solve\nthe Grad-Shafranov equation with several different boundary conditions. We also\nexplore the parameter space by varying the size of the model, the learning\nrate, and boundary conditions to map various trade-offs such as between\nreconstruction error and computational speed. Additionally, we introduce a\nparameterized PINN framework, expanding the input space to include variables\nsuch as pressure, aspect ratio, elongation, and triangularity in order to\nhandle a broader range of plasma scenarios within a single network.\nParametrized PINNs could be used in future work to solve inverse problems such\nas shape optimization.","publication_date":1700669318,"paper_link":"http://arxiv.org/pdf/2311.13491v1","categories":["Physics"],"abstract":"A large number of magnetohydrodynamic (MHD) equilibrium calculations are often required for uncertainty quantification, optimization, and real-time diagnostic information, making MHD equilibrium codes vital to the field of plasma physics. In this paper, we explore a method for solving the Grad-Shafranov equation by using Physics-Informed Neural Networks (PINNs). For PINNs, we optimize neural networks by directly minimizing the residual of the PDE as a loss function. We show that PINNs can accurately and effectively solve the Grad-Shafranov equation with several different boundary conditions. We also explore the parameter space by varying the size of the model, the learning rate, and boundary conditions to map various trade-offs such as between reconstruction error and computational speed. Additionally, we introduce a parameterized PINN framework, expanding the input space to include variables such as pressure, aspect ratio, elongation, and triangularity in order to handle a broader range of plasma scenarios within a single network. Parametrized PINNs could be used in future work to solve inverse problems such as shape optimization."}
{"title":"Aerothermodynamic Analysis of Faceted Aeroshell at Hypersonic Speed","authors":["Pietro Innocenzi","Michela Gramola","Tom B. Fisher","Mark K. Quinn","Paul J. K. Bruce","Salvador Navarro-Martinez"],"raw_abstract":"This study explores the aerothermal behaviour of a rigid mechanically\ndeployable aeroshell developed at Imperial College London for high payload\natmospheric entry missions. The multiphysics CFD software STAR-CCM+ is used to\nperform a Conjugate Heat Transfer analysis on the aeroshell's faceted geometry.\nResults are presented for four different geometry models tested in air at Mach\n5 with angles of attack 0{\\deg}, 5{\\deg} and 10{\\deg}. The predicted surface\nheat transfer reveals areas of elevated heat loads at the ribs between facets\nand at the aeroshell shoulder, due to local boundary layer thinning. The\nincrease in heat transfer at the ribs depends on the sharpness of the rib: more\nrounded shapes result in lower heat fluxes. Comparison with high-speed wind\ntunnel tests shows good agreement with experimental data. Stanton number and\ntemperature profiles agree within 8% and 2%, respectively. The discrepancies\nbetween experiments and simulations are largest at the sharp ribs of the\naeroshell. The sources of error can be associated with three-dimensional\neffects neglected in the heat flux derivations from temperature measurements as\nwell as experimental uncertainties.","publication_date":1700668952,"paper_link":"http://arxiv.org/pdf/2311.13487v1","categories":["Physics"],"abstract":"This study explores the aerothermal behaviour of a rigid mechanically deployable aeroshell developed at Imperial College London for high payload atmospheric entry missions. The multiphysics CFD software STAR-CCM+ is used to perform a Conjugate Heat Transfer analysis on the aeroshell's faceted geometry. Results are presented for four different geometry models tested in air at Mach 5 with angles of attack 0{\\deg}, 5{\\deg} and 10{\\deg}. The predicted surface heat transfer reveals areas of elevated heat loads at the ribs between facets and at the aeroshell shoulder, due to local boundary layer thinning. The increase in heat transfer at the ribs depends on the sharpness of the rib: more rounded shapes result in lower heat fluxes. Comparison with high-speed wind tunnel tests shows good agreement with experimental data. Stanton number and temperature profiles agree within 8% and 2%, respectively. The discrepancies between experiments and simulations are largest at the sharp ribs of the aeroshell. The sources of error can be associated with three-dimensional effects neglected in the heat flux derivations from temperature measurements as well as experimental uncertainties."}
{"title":"Deep-learning-based acceleration of MRI for radiotherapy planning of pediatric patients with brain tumors","authors":["Shahinur Alam","Jinsoo Uh","Alexander Dresner","Chia-ho Hua","Khaled Khairy"],"raw_abstract":"Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and\nradiotherapy (RT) planning tool, offering detailed insights into the anatomy of\nthe human body. The extensive scan time is stressful for patients, who must\nremain motionless in a prolonged imaging procedure that prioritizes reduction\nof imaging artifacts. This is challenging for pediatric patients who may\nrequire measures for managing voluntary motions such as anesthesia. Several\ncomputational approaches reduce scan time (fast MRI), by recording fewer\nmeasurements and digitally recovering full information via post-acquisition\nreconstruction. However, most fast MRI approaches were developed for diagnostic\nimaging, without addressing reconstruction challenges specific to RT planning.\nIn this work, we developed a deep learning-based method (DeepMRIRec) for MRI\nreconstruction from undersampled data acquired with RT-specific receiver coil\narrangements. We evaluated our method against fully sampled data of T1-weighted\nMR images acquired from 73 children with brain tumors/surgical beds using loop\nand posterior coils (12 channels), with and without applying virtual\ncompression of coil elements. DeepMRIRec reduced scanning time by a factor of\nfour producing a structural similarity score surpassing the evaluated\nstate-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential\nfor accelerating MRI scanning for RT planning.","publication_date":1700668904,"paper_link":"http://arxiv.org/pdf/2311.13485v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and radiotherapy (RT) planning tool, offering detailed insights into the anatomy of the human body. The extensive scan time is stressful for patients, who must remain motionless in a prolonged imaging procedure that prioritizes reduction of imaging artifacts. This is challenging for pediatric patients who may require measures for managing voluntary motions such as anesthesia. Several computational approaches reduce scan time (fast MRI), by recording fewer measurements and digitally recovering full information via post-acquisition reconstruction. However, most fast MRI approaches were developed for diagnostic imaging, without addressing reconstruction challenges specific to RT planning. In this work, we developed a deep learning-based method (DeepMRIRec) for MRI reconstruction from undersampled data acquired with RT-specific receiver coil arrangements. We evaluated our method against fully sampled data of T1-weighted MR images acquired from 73 children with brain tumors/surgical beds using loop and posterior coils (12 channels), with and without applying virtual compression of coil elements. DeepMRIRec reduced scanning time by a factor of four producing a structural similarity score surpassing the evaluated state-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential for accelerating MRI scanning for RT planning."}
{"title":"Machine Translation to Control Formality Features in the Target Language","authors":["Harshita Tyagi","Prashasta Jung","Hyowon Lee"],"raw_abstract":"Formality plays a significant role in language communication, especially in\nlow-resource languages such as Hindi, Japanese and Korean. These languages\nutilise formal and informal expressions to convey messages based on social\ncontexts and relationships. When a language translation technique is used to\ntranslate from a source language that does not pertain the formality (e.g.\nEnglish) to a target language that does, there is a missing information on\nformality that could be a challenge in producing an accurate outcome. This\nresearch explores how this issue should be resolved when machine learning\nmethods are used to translate from English to languages with formality, using\nHindi as the example data. This was done by training a bilingual model in a\nformality-controlled setting and comparing its performance with a pre-trained\nmultilingual model in a similar setting. Since there are not a lot of training\ndata with ground truth, automated annotation techniques were employed to\nincrease the data size. The primary modeling approach involved leveraging\ntransformer models, which have demonstrated effectiveness in various natural\nlanguage processing tasks. We evaluate the official formality accuracy(ACC) by\ncomparing the predicted masked tokens with the ground truth. This metric\nprovides a quantitative measure of how well the translations align with the\ndesired outputs. Our study showcases a versatile translation strategy that\nconsiders the nuances of formality in the target language, catering to diverse\nlanguage communication needs and scenarios.","publication_date":1700667771,"paper_link":"http://arxiv.org/pdf/2311.13475v1","categories":["Computer Science"],"abstract":"Formality plays a significant role in language communication, especially in low-resource languages such as Hindi, Japanese and Korean. These languages utilise formal and informal expressions to convey messages based on social contexts and relationships. When a language translation technique is used to translate from a source language that does not pertain the formality (e.g. English) to a target language that does, there is a missing information on formality that could be a challenge in producing an accurate outcome. This research explores how this issue should be resolved when machine learning methods are used to translate from English to languages with formality, using Hindi as the example data. This was done by training a bilingual model in a formality-controlled setting and comparing its performance with a pre-trained multilingual model in a similar setting. Since there are not a lot of training data with ground truth, automated annotation techniques were employed to increase the data size. The primary modeling approach involved leveraging transformer models, which have demonstrated effectiveness in various natural language processing tasks. We evaluate the official formality accuracy(ACC) by comparing the predicted masked tokens with the ground truth. This metric provides a quantitative measure of how well the translations align with the desired outputs. Our study showcases a versatile translation strategy that considers the nuances of formality in the target language, catering to diverse language communication needs and scenarios."}
{"title":"Complexity-Guided Curriculum Learning for Text Graphs","authors":["Nidhi Vakil","Hadi Amiri"],"raw_abstract":"Curriculum learning provides a systematic approach to training. It refines\ntraining progressively, tailors training to task requirements, and improves\ngeneralization through exposure to diverse examples. We present a curriculum\nlearning approach that builds on existing knowledge about text and graph\ncomplexity formalisms for training with text graph data. The core part of our\napproach is a novel data scheduler, which employs \"spaced repetition\" and\ncomplexity formalisms to guide the training process. We demonstrate the\neffectiveness of the proposed approach on several text graph tasks and graph\nneural network architectures. The proposed model gains more and uses less data;\nconsistently prefers text over graph complexity indices throughout training,\nwhile the best curricula derived from text and graph complexity indices are\nequally effective; and it learns transferable curricula across GNN models and\ndatasets. In addition, we find that both node-level (local) and graph-level\n(global) graph complexity indices, as well as shallow and traditional text\ncomplexity indices play a crucial role in effective curriculum learning.","publication_date":1700667657,"paper_link":"http://arxiv.org/pdf/2311.13472v1","categories":["Computer Science"],"abstract":"Curriculum learning provides a systematic approach to training. It refines training progressively, tailors training to task requirements, and improves generalization through exposure to diverse examples. We present a curriculum learning approach that builds on existing knowledge about text and graph complexity formalisms for training with text graph data. The core part of our approach is a novel data scheduler, which employs \"spaced repetition\" and complexity formalisms to guide the training process. We demonstrate the effectiveness of the proposed approach on several text graph tasks and graph neural network architectures. The proposed model gains more and uses less data; consistently prefers text over graph complexity indices throughout training, while the best curricula derived from text and graph complexity indices are equally effective; and it learns transferable curricula across GNN models and datasets. In addition, we find that both node-level (local) and graph-level (global) graph complexity indices, as well as shallow and traditional text complexity indices play a crucial role in effective curriculum learning."}
{"title":"Comparative Analysis of Linear Regression, Gaussian Elimination, and LU Decomposition for CT Real Estate Purchase Decisions","authors":["Xilin Cheng"],"raw_abstract":"This paper presents a comprehensive evaluation of three distinct\ncomputational algorithms applied to the decision-making process of real estate\npurchases. Specifically, we analyze the efficacy of Linear Regression from\nScikit-learn library, Gaussian Elimination with partial pivoting, and LU\nDecomposition in predicting the advisability of buying a house in the State of\nConnecticut based on a set of financial and market-related parameters. The\nalgorithms' performances were compared using a dataset encompassing\ntown-specific details, yearly data, interest rates, and median sale ratios. Our\nresults demonstrate significant differences in predictive accuracy, with Linear\nRegression and LU Decomposition providing the most reliable recommendations and\nGaussian Elimination showing limitations in stability and performance. The\nstudy's findings emphasize the importance of algorithm selection in predictive\nanalytic and offer insights into the practical applications of computational\nmethods in real estate investment strategies. By evaluating model efficacy\nthrough metrics such as R-squared scores and Mean Squared Error, we provide a\nnuanced understanding of each method's strengths and weaknesses, contributing\nvaluable knowledge to the fields of real estate analysis and predictive\nmodeling.","publication_date":1700667356,"paper_link":"http://arxiv.org/pdf/2311.13471v1","categories":["Mathematics"],"abstract":"This paper presents a comprehensive evaluation of three distinct computational algorithms applied to the decision-making process of real estate purchases. Specifically, we analyze the efficacy of Linear Regression from Scikit-learn library, Gaussian Elimination with partial pivoting, and LU Decomposition in predicting the advisability of buying a house in the State of Connecticut based on a set of financial and market-related parameters. The algorithms' performances were compared using a dataset encompassing town-specific details, yearly data, interest rates, and median sale ratios. Our results demonstrate significant differences in predictive accuracy, with Linear Regression and LU Decomposition providing the most reliable recommendations and Gaussian Elimination showing limitations in stability and performance. The study's findings emphasize the importance of algorithm selection in predictive analytic and offer insights into the practical applications of computational methods in real estate investment strategies. By evaluating model efficacy through metrics such as R-squared scores and Mean Squared Error, we provide a nuanced understanding of each method's strengths and weaknesses, contributing valuable knowledge to the fields of real estate analysis and predictive modeling."}
{"title":"Exactly Solvable Floquet Dynamics for Conformal Field Theories in Dimensions Greater than Two","authors":["Diptarka Das","Sumit R. Das","Arnab Kundu","Krishnendu Sengupta"],"raw_abstract":"We find classes of driven conformal field theories (CFT) in d+1 dimensions\nwith d > 1, whose quench and floquet dynamics can be computed exactly. The\nsetup is suitable for studying periodic drives, consisting of square pulse\nprotocols for which Hamiltonian evolution takes place with different\ndeformations of the original CFT Hamiltonian in successive time intervals.\nThese deformations are realized by specific combinations of conformal\ngenerators with a deformation parameter $\\beta$; the $\\beta < 1$ ($\\beta > 1$)\nHamiltonians can be unitarily related to the standard (L\\\"uscher-Mack) CFT\nHamiltonians. The resulting time evolution can be then calculated by performing\nappropriate conformal transformations. For d <= 3 we show that the\ntransformations can be easily obtained in a quaternion formalism; we use this\nformalism to obtain exact expressions for the fidelity, unequal-time\ncorrelator, and the energy density for the driven system for d = 3. Our results\nfor a single square pulse drive cycle reveal qualitatively different behaviors\ndepending on the value of $\\beta$, with exponential decays characteristic of\nheating for $\\beta > 1$, oscillations for $\\beta < 1$ and power law decays for\n$\\beta = 1$. When the Hamiltonians in one cycle involve generators of a single\nSL(2, R) subalgebra we find fixed points or fixed surfaces of the corresponding\ntransformations. Successive cycles lead to either convergence to one of the\nfixed points, or oscillations, depending on the conjugacy class. This indicates\nthat the system can be in different dynamical phases as we vary the parameters\nof the drive protocol. We also point out that our results are expected to hold\nfor a broader class of QFTs that possesses an SL(2,C) symmetry with fields that\ntransform as quasi-primaries under this. As an example, we briefly comment on\ncelestial CFTs in this context.","publication_date":1700667249,"paper_link":"http://arxiv.org/pdf/2311.13468v1","categories":["Physics"],"abstract":"We find classes of driven conformal field theories (CFT) in d+1 dimensions with d > 1, whose quench and floquet dynamics can be computed exactly. The setup is suitable for studying periodic drives, consisting of square pulse protocols for which Hamiltonian evolution takes place with different deformations of the original CFT Hamiltonian in successive time intervals. These deformations are realized by specific combinations of conformal generators with a deformation parameter __FORMULA__; the __FORMULA__ (__FORMULA__) Hamiltonians can be unitarily related to the standard (L\\\"uscher-Mack) CFT Hamiltonians. The resulting time evolution can be then calculated by performing appropriate conformal transformations. For d <= 3 we show that the transformations can be easily obtained in a quaternion formalism; we use this formalism to obtain exact expressions for the fidelity, unequal-time correlator, and the energy density for the driven system for d = 3. Our results for a single square pulse drive cycle reveal qualitatively different behaviors depending on the value of __FORMULA__, with exponential decays characteristic of heating for __FORMULA__, oscillations for __FORMULA__ and power law decays for __FORMULA__. When the Hamiltonians in one cycle involve generators of a single SL(2, R) subalgebra we find fixed points or fixed surfaces of the corresponding transformations. Successive cycles lead to either convergence to one of the fixed points, or oscillations, depending on the conjugacy class. This indicates that the system can be in different dynamical phases as we vary the parameters of the drive protocol. We also point out that our results are expected to hold for a broader class of QFTs that possesses an SL(2,C) symmetry with fields that transform as quasi-primaries under this. As an example, we briefly comment on celestial CFTs in this context."}
{"title":"Controlling crystal cleavage in Focused Ion Beam shaped specimens for surface spectroscopy","authors":["A. Hunter","C. Putzke","I. Gaponenko","A. Tamai","F. Baumberger","P. J. W. Moll"],"raw_abstract":"Our understanding of quantum materials is commonly based on precise\ndeterminations of their electronic spectrum by spectroscopic means, most\nnotably angle-resolved photoemission spectroscopy (ARPES) and scanning\ntunneling microscopy (STM). Both require atomically clean and flat crystal\nsurfaces which traditionally are prepared by in-situ mechanical cleaving in\nultrahigh vacuum chambers. We present a new approach that addresses three main\nissues of the current state-of-the-art methods: 1) Cleaving is a highly\nstochastic and thus inefficient process; 2) Fracture processes are governed by\nthe bonds in a bulk crystal, and many materials and surfaces simply do not\ncleave; 3) The location of the cleave is random, preventing data collection at\nspecified regions of interest. Our new workflow is based on Focused Ion Beam\n(FIB) machining of micro-stress lenses in which shape (rather than crystalline)\nanisotropy dictates the plane of cleavage, which can be placed at a specific\ntarget layer. As proof-of-principle we show ARPES results from micro-cleaves of\nSr$_2$RuO$_4$ along the ac plane and from two surface orientations of\nSrTiO$_3$, a notoriously difficult to cleave cubic perovskite.","publication_date":1700666659,"paper_link":"http://arxiv.org/pdf/2311.13458v1","categories":["Physics"],"abstract":"Our understanding of quantum materials is commonly based on precise determinations of their electronic spectrum by spectroscopic means, most notably angle-resolved photoemission spectroscopy (ARPES) and scanning tunneling microscopy (STM). Both require atomically clean and flat crystal surfaces which traditionally are prepared by in-situ mechanical cleaving in ultrahigh vacuum chambers. We present a new approach that addresses three main issues of the current state-of-the-art methods: 1) Cleaving is a highly stochastic and thus inefficient process; 2) Fracture processes are governed by the bonds in a bulk crystal, and many materials and surfaces simply do not cleave; 3) The location of the cleave is random, preventing data collection at specified regions of interest. Our new workflow is based on Focused Ion Beam (FIB) machining of micro-stress lenses in which shape (rather than crystalline) anisotropy dictates the plane of cleavage, which can be placed at a specific target layer. As proof-of-principle we show ARPES results from micro-cleaves of Sr__FORMULA__RuO__FORMULA__ along the ac plane and from two surface orientations of SrTiO__FORMULA__, a notoriously difficult to cleave cubic perovskite."}
{"title":"Generation of Explanations for Logic Reasoning","authors":["Yanyi Pu"],"raw_abstract":"This thesis delves into a fortiori arguments in deductive reasoning,\nunderscoring their relevance in various domains such as law, philosophy, and\nartificial intelligence. The research is centred on employing GPT-3.5-turbo to\nautomate the analysis of these arguments, with a focus on understanding\nintricate reasoning processes, generating clear and coherent explanations, and\ncreating novel arguments. The methodology encompasses a series of tasks\nincluding detailed reasoning, interpretation, and the augmentation of a\nfortiori arguments. It involves meticulously identifying these arguments in\ndiverse contexts, differentiating comparative elements, and categorizing them\nbased on their logical structure.\n  Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in\naccurately detecting and classifying a fortiori arguments. Nevertheless, the\nmodel demonstrates a performance that rivals specialized models, particularly\nin extracting key components and interpreting underlying properties. The\nintegration of external information into the model's processing significantly\nelevates the quality of the generated explanations. Additionally, the model\nexhibits a noteworthy capability in augmenting arguments, thus contributing to\nthe enrichment of the data set.\n  Despite facing certain limitations, this thesis makes significant\ncontributions to the fields of artificial intelligence and logical reasoning.\nIt introduces novel methodologies, establishes a rigorous evaluation framework,\nand provides deep insights that set the stage for future advancements in\nautomated logical reasoning. The findings and methodologies presented herein\nnot only underscore the potential of AI in complex reasoning tasks but also\nhighlight areas for future research and development.","publication_date":1700666524,"paper_link":"http://arxiv.org/pdf/2311.13455v1","categories":["Computer Science"],"abstract":"This thesis delves into a fortiori arguments in deductive reasoning, underscoring their relevance in various domains such as law, philosophy, and artificial intelligence. The research is centred on employing GPT-3.5-turbo to automate the analysis of these arguments, with a focus on understanding intricate reasoning processes, generating clear and coherent explanations, and creating novel arguments. The methodology encompasses a series of tasks including detailed reasoning, interpretation, and the augmentation of a fortiori arguments. It involves meticulously identifying these arguments in diverse contexts, differentiating comparative elements, and categorizing them based on their logical structure.   Extensive experiments reveals the challenges encountered by GPT-3.5-turbo in accurately detecting and classifying a fortiori arguments. Nevertheless, the model demonstrates a performance that rivals specialized models, particularly in extracting key components and interpreting underlying properties. The integration of external information into the model's processing significantly elevates the quality of the generated explanations. Additionally, the model exhibits a noteworthy capability in augmenting arguments, thus contributing to the enrichment of the data set.   Despite facing certain limitations, this thesis makes significant contributions to the fields of artificial intelligence and logical reasoning. It introduces novel methodologies, establishes a rigorous evaluation framework, and provides deep insights that set the stage for future advancements in automated logical reasoning. The findings and methodologies presented herein not only underscore the potential of AI in complex reasoning tasks but also highlight areas for future research and development."}
{"title":"Explaining high-dimensional text classifiers","authors":["Odelia Melamed","Rich Caruana"],"raw_abstract":"Explainability has become a valuable tool in the last few years, helping\nhumans better understand AI-guided decisions. However, the classic\nexplainability tools are sometimes quite limited when considering\nhigh-dimensional inputs and neural network classifiers. We present a new\nexplainability method using theoretically proven high-dimensional properties in\nneural network classifiers. We present two usages of it: 1) On the classical\nsentiment analysis task for the IMDB reviews dataset, and 2) our\nMalware-Detection task for our PowerShell scripts dataset.","publication_date":1700666412,"paper_link":"http://arxiv.org/pdf/2311.13454v1","categories":["Statistics"],"abstract":"Explainability has become a valuable tool in the last few years, helping humans better understand AI-guided decisions. However, the classic explainability tools are sometimes quite limited when considering high-dimensional inputs and neural network classifiers. We present a new explainability method using theoretically proven high-dimensional properties in neural network classifiers. We present two usages of it: 1) On the classical sentiment analysis task for the IMDB reviews dataset, and 2) our Malware-Detection task for our PowerShell scripts dataset."}
{"title":"Non-Hermitian molecular dynamics simulations of exciton-polaritons in lossy cavities","authors":["Ilia Sokolovskii","Gerrit Groenhof"],"raw_abstract":"The observation that materials can change their properties when placed inside\nor near an optical resonator, has sparked a fervid interest in understanding\nthe effects of strong light-matter coupling on molecular dynamics, and several\napproaches have been proposed to extend the methods of computational chemistry\ninto this regime. Whereas the majority of these approaches have focused on\nmodelling a single molecule coupled to a single cavity mode, changes to\nchemistry have so far only been observed experimentally when very many\nmolecules are coupled collectively to multiple modes with short lifetimes.\nWhile atomistic simulations of many molecules coupled to multiple cavity modes\nhave been performed with semi-classical molecular dynamics, an explicit\ndescription of cavity losses has so far been restricted to simulations in which\nonly a very few molecular degrees of freedom were considered. Here, we have\nimplemented an effective non-Hermitian Hamiltonian to explicitly treat cavity\nlosses in large-scale semi-classical molecular dynamics simulations of organic\npolaritons and used it to perform both mean-field and surface hopping\nsimulations of polariton relaxation, propagation and energy transfer.","publication_date":1700666336,"paper_link":"http://arxiv.org/pdf/2311.13453v1","categories":["Physics"],"abstract":"The observation that materials can change their properties when placed inside or near an optical resonator, has sparked a fervid interest in understanding the effects of strong light-matter coupling on molecular dynamics, and several approaches have been proposed to extend the methods of computational chemistry into this regime. Whereas the majority of these approaches have focused on modelling a single molecule coupled to a single cavity mode, changes to chemistry have so far only been observed experimentally when very many molecules are coupled collectively to multiple modes with short lifetimes. While atomistic simulations of many molecules coupled to multiple cavity modes have been performed with semi-classical molecular dynamics, an explicit description of cavity losses has so far been restricted to simulations in which only a very few molecular degrees of freedom were considered. Here, we have implemented an effective non-Hermitian Hamiltonian to explicitly treat cavity losses in large-scale semi-classical molecular dynamics simulations of organic polaritons and used it to perform both mean-field and surface hopping simulations of polariton relaxation, propagation and energy transfer."}
{"title":"On the Low-Frequency Dynamics of Turbulent Separation Bubbles","authors":["C. Cura","A. Hanifi","A. V. G. Cavalieri","J. Weiss"],"raw_abstract":"The low-frequency modal and non-modal stability characteristics of an\nincompressible, pressure-gradient-induced turbulent separation bubble (TSB) are\ninvestigated with the objective of studying the mechanism responsible for the\nlow-frequency contraction and expansion (breathing) commonly observed in\nexperimental studies. The configuration of interest is a TSB generated on a\nflat test surface by a succession of adverse and favourable pressure gradients.\nThe base flow selected for the analysis is the average TSB from the direct\nnumerical simulation of Coleman et al. (J. Fluid Mech., vol. 847, 2018). Global\nlinear stability analysis reveals that the flow is globally stable for\nwavenumbers. The mode closest to the stability threshold appears to occur at\nzero frequency and low, non-zero spanwise wavenumber. Resolvent analysis is\nthen employed to examine the forced dynamics of the flow. At low frequency, a\nregion of low, non-zero spanwise wavenumber is also discernible, where the\nreceptivity appears to be driven by the identified weakly damped global mode.\nThe results from resolvent analysis are compared to the unsteady experimental\ndatabase of Le Floc'h et al. (J. Fluid Mech., vol. 902, 2020) in a similar TSB\nflow. The alignment between the optimal response and the first spectral proper\northogonal decomposition mode computed from the experiments is shown to exceed\n95 %, while the spanwise wavenumber of the optimal response is consistent with\nthat of the low-frequency breathing motion captured experimentally. This\nindicates that the fluctuations observed experimentally at low frequency\nclosely match the response computed from resolvent analysis. Based on these\nresults, we propose that the forced dynamics of the flow, driven by the weakly\ndamped global mode, serve as a plausible mechanism for the origin of the\nlow-frequency breathing motion commonly observed in experimental studies of\nTSBs.","publication_date":1700665956,"paper_link":"http://arxiv.org/pdf/2311.13446v1","categories":["Physics"],"abstract":"The low-frequency modal and non-modal stability characteristics of an incompressible, pressure-gradient-induced turbulent separation bubble (TSB) are investigated with the objective of studying the mechanism responsible for the low-frequency contraction and expansion (breathing) commonly observed in experimental studies. The configuration of interest is a TSB generated on a flat test surface by a succession of adverse and favourable pressure gradients. The base flow selected for the analysis is the average TSB from the direct numerical simulation of Coleman et al. (J. Fluid Mech., vol. 847, 2018). Global linear stability analysis reveals that the flow is globally stable for wavenumbers. The mode closest to the stability threshold appears to occur at zero frequency and low, non-zero spanwise wavenumber. Resolvent analysis is then employed to examine the forced dynamics of the flow. At low frequency, a region of low, non-zero spanwise wavenumber is also discernible, where the receptivity appears to be driven by the identified weakly damped global mode. The results from resolvent analysis are compared to the unsteady experimental database of Le Floc'h et al. (J. Fluid Mech., vol. 902, 2020) in a similar TSB flow. The alignment between the optimal response and the first spectral proper orthogonal decomposition mode computed from the experiments is shown to exceed 95 %, while the spanwise wavenumber of the optimal response is consistent with that of the low-frequency breathing motion captured experimentally. This indicates that the fluctuations observed experimentally at low frequency closely match the response computed from resolvent analysis. Based on these results, we propose that the forced dynamics of the flow, driven by the weakly damped global mode, serve as a plausible mechanism for the origin of the low-frequency breathing motion commonly observed in experimental studies of TSBs."}
{"title":"SkeletonGait: Gait Recognition Using Skeleton Maps","authors":["Chao Fan","Jingzhe Ma","Dongyang Jin","Chuanfu Shen","Shiqi Yu"],"raw_abstract":"The choice of the representations is essential for deep gait recognition\nmethods. The binary silhouettes and skeletal coordinates are two dominant\nrepresentations in recent literature, achieving remarkable advances in many\nscenarios. However, inherent challenges remain, in which silhouettes are not\nalways guaranteed in unconstrained scenes, and structural cues have not been\nfully utilized from skeletons. In this paper, we introduce a novel skeletal\ngait representation named Skeleton Map, together with SkeletonGait, a\nskeleton-based method to exploit structural information from human skeleton\nmaps. Specifically, the skeleton map represents the coordinates of human joints\nas a heatmap with Gaussian approximation, exhibiting a silhouette-like image\ndevoid of exact body structure. Beyond achieving state-of-the-art performances\nover five popular gait datasets, more importantly, SkeletonGait uncovers novel\ninsights about how important structural features are in describing gait and\nwhen do they play a role. Furthermore, we propose a multi-branch architecture,\nnamed SkeletonGait++, to make use of complementary features from both skeletons\nand silhouettes. Experiments indicate that SkeletonGait++ outperforms existing\nstate-of-the-art methods by a significant margin in various scenarios. For\ninstance, it achieves an impressive rank-1 accuracy of over $85\\%$ on the\nchallenging GREW dataset. All the source code will be available at\nhttps://github.com/ShiqiYu/OpenGait.","publication_date":1700665799,"paper_link":"http://arxiv.org/pdf/2311.13444v1","categories":["Computer Science"],"abstract":"The choice of the representations is essential for deep gait recognition methods. The binary silhouettes and skeletal coordinates are two dominant representations in recent literature, achieving remarkable advances in many scenarios. However, inherent challenges remain, in which silhouettes are not always guaranteed in unconstrained scenes, and structural cues have not been fully utilized from skeletons. In this paper, we introduce a novel skeletal gait representation named Skeleton Map, together with SkeletonGait, a skeleton-based method to exploit structural information from human skeleton maps. Specifically, the skeleton map represents the coordinates of human joints as a heatmap with Gaussian approximation, exhibiting a silhouette-like image devoid of exact body structure. Beyond achieving state-of-the-art performances over five popular gait datasets, more importantly, SkeletonGait uncovers novel insights about how important structural features are in describing gait and when do they play a role. Furthermore, we propose a multi-branch architecture, named SkeletonGait++, to make use of complementary features from both skeletons and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing state-of-the-art methods by a significant margin in various scenarios. For instance, it achieves an impressive rank-1 accuracy of over __FORMULA__ on the challenging GREW dataset. All the source code will be available at https://github.com/ShiqiYu/OpenGait."}
{"title":"Guided Flows for Generative Modeling and Decision Making","authors":["Qinqing Zheng","Matt Le","Neta Shaul","Yaron Lipman","Aditya Grover","Ricky T. Q. Chen"],"raw_abstract":"Classifier-free guidance is a key component for improving the performance of\nconditional generative models for many downstream tasks. It drastically\nimproves the quality of samples produced, but has so far only been used for\ndiffusion models. Flow Matching (FM), an alternative simulation-free approach,\ntrains Continuous Normalizing Flows (CNFs) based on regressing vector fields.\nIt remains an open question whether classifier-free guidance can be performed\nfor Flow Matching models, and to what extent does it improve performance. In\nthis paper, we explore the usage of Guided Flows for a variety of downstream\napplications involving conditional image generation, speech synthesis, and\nreinforcement learning. In particular, we are the first to apply flow models to\nthe offline reinforcement learning setting. We also show that Guided Flows\nsignificantly improves the sample quality in image generation and zero-shot\ntext-to-speech synthesis, and can make use of drastically low amounts of\ncomputation without affecting the agent's overall performance.","publication_date":1700665679,"paper_link":"http://arxiv.org/pdf/2311.13443v1","categories":["Statistics"],"abstract":"Classifier-free guidance is a key component for improving the performance of conditional generative models for many downstream tasks. It drastically improves the quality of samples produced, but has so far only been used for diffusion models. Flow Matching (FM), an alternative simulation-free approach, trains Continuous Normalizing Flows (CNFs) based on regressing vector fields. It remains an open question whether classifier-free guidance can be performed for Flow Matching models, and to what extent does it improve performance. In this paper, we explore the usage of Guided Flows for a variety of downstream applications involving conditional image generation, speech synthesis, and reinforcement learning. In particular, we are the first to apply flow models to the offline reinforcement learning setting. We also show that Guided Flows significantly improves the sample quality in image generation and zero-shot text-to-speech synthesis, and can make use of drastically low amounts of computation without affecting the agent's overall performance."}
{"title":"Temporal Network Analysis of Email Communication Patterns in a Long Standing Hierarchy","authors":["Matthew Russell Barnes","Mladen Karan","Stephen McQuistin","Colin Perkins","Gareth Tyson","Matthew Purver","Ignacio Castro","Richard G. Clegg"],"raw_abstract":"An important concept in organisational behaviour is how hierarchy affects the\nvoice of individuals, whereby members of a given organisation exhibit differing\npower relations based on their hierarchical position. Although there have been\nprior studies of the relationship between hierarchy and voice, they tend to\nfocus on more qualitative small-scale methods and do not account for structural\naspects of the organisation. This paper develops large-scale computational\ntechniques utilising temporal network analysis to measure the effect that\norganisational hierarchy has on communication patterns within an organisation,\nfocusing on the structure of pairwise interactions between individuals. We\nfocus on one major organisation as a case study - the Internet Engineering Task\nForce (IETF) - a major technical standards development organisation for the\nInternet. A particularly useful feature of the IETF is a transparent hierarchy,\nwhere participants take on explicit roles (e.g. Area Directors, Working Group\nChairs). Its processes are also open, so we have visibility into the\ncommunication of people at different hierarchy levels over a long time period.\nWe utilise a temporal network dataset of 989,911 email interactions among\n23,741 participants to study how hierarchy impacts communication patterns. We\nshow that the middle levels of the IETF are growing in terms of their dominance\nin communications. Higher levels consistently experience a higher proportion of\nincoming communication than lower levels, with higher levels initiating more\ncommunications too. We find that communication tends to flow \"up\" the hierarchy\nmore than \"down\". Finally, we find that communication with higher-levels is\nassociated with future communication more than for lower-levels, which we\ninterpret as \"facilitation\". We conclude by discussing the implications this\nhas on patterns within the wider IETF and for other organisations.","publication_date":1700665663,"paper_link":"http://arxiv.org/pdf/2311.13442v1","categories":["Computer Science"],"abstract":"An important concept in organisational behaviour is how hierarchy affects the voice of individuals, whereby members of a given organisation exhibit differing power relations based on their hierarchical position. Although there have been prior studies of the relationship between hierarchy and voice, they tend to focus on more qualitative small-scale methods and do not account for structural aspects of the organisation. This paper develops large-scale computational techniques utilising temporal network analysis to measure the effect that organisational hierarchy has on communication patterns within an organisation, focusing on the structure of pairwise interactions between individuals. We focus on one major organisation as a case study - the Internet Engineering Task Force (IETF) - a major technical standards development organisation for the Internet. A particularly useful feature of the IETF is a transparent hierarchy, where participants take on explicit roles (e.g. Area Directors, Working Group Chairs). Its processes are also open, so we have visibility into the communication of people at different hierarchy levels over a long time period. We utilise a temporal network dataset of 989,911 email interactions among 23,741 participants to study how hierarchy impacts communication patterns. We show that the middle levels of the IETF are growing in terms of their dominance in communications. Higher levels consistently experience a higher proportion of incoming communication than lower levels, with higher levels initiating more communications too. We find that communication tends to flow \"up\" the hierarchy more than \"down\". Finally, we find that communication with higher-levels is associated with future communication more than for lower-levels, which we interpret as \"facilitation\". We conclude by discussing the implications this has on patterns within the wider IETF and for other organisations."}
{"title":"New efficient ADMM algorithm for the Unit Commitment Problem","authors":["Rogier Hans Wuijts","Marjan van den Akker","Machteld van den Broek"],"raw_abstract":"The unit commitment problem (UC) is an optimization problem concerning the\noperation of electrical generators. Many algorithms have been proposed for the\nUC and in recent years a more decentralized approach, by solving the UC with\nalternating direction method of multipliers (ADMM), has been investigated. For\nconvex problems ADMM is guaranteed to find an optimal solution. However,\nbecause UC is non-convex additional steps need to be taken in order to ensure\nconvergence to a feasible solution of high quality. Therefore, solving UC by a\nMIL(Q)P formulation and running an off-the-shelf solver like Gurobi until now\nseems to be the most efficient approach to obtain high quality solutions. In\nthis paper, we introduce a new and efficient way to solve the UC with ADMM to\nnear optimality. We relax the supply-demand balance constraint and deal with\nthe non-convexity by iteratively increasing a penalty coefficient until we\neventually force convergence and feasibility. At each iteration the subproblems\nare solved by our efficient algorithm for the single UC subproblem developed in\nearlier work and our new ADMM algorithm for the transmission subproblems.\nComputational experiments on benchmark instances demonstrated that our\nalgorithm produces high-quality solutions. The computation time seems to grow\npractically linear with the length of the time horizon. For the case with\nquadratic generation cost our algorithm is significantly faster than solving\nthe problem by a state-of-the-art MIL(Q)P formulation. For the case of linear\ngeneration cost, it outperforms the MILP approach for longer time horizons.","publication_date":1700664979,"paper_link":"http://arxiv.org/pdf/2311.13438v1","categories":["Mathematics"],"abstract":"The unit commitment problem (UC) is an optimization problem concerning the operation of electrical generators. Many algorithms have been proposed for the UC and in recent years a more decentralized approach, by solving the UC with alternating direction method of multipliers (ADMM), has been investigated. For convex problems ADMM is guaranteed to find an optimal solution. However, because UC is non-convex additional steps need to be taken in order to ensure convergence to a feasible solution of high quality. Therefore, solving UC by a MIL(Q)P formulation and running an off-the-shelf solver like Gurobi until now seems to be the most efficient approach to obtain high quality solutions. In this paper, we introduce a new and efficient way to solve the UC with ADMM to near optimality. We relax the supply-demand balance constraint and deal with the non-convexity by iteratively increasing a penalty coefficient until we eventually force convergence and feasibility. At each iteration the subproblems are solved by our efficient algorithm for the single UC subproblem developed in earlier work and our new ADMM algorithm for the transmission subproblems. Computational experiments on benchmark instances demonstrated that our algorithm produces high-quality solutions. The computation time seems to grow practically linear with the length of the time horizon. For the case with quadratic generation cost our algorithm is significantly faster than solving the problem by a state-of-the-art MIL(Q)P formulation. For the case of linear generation cost, it outperforms the MILP approach for longer time horizons."}
{"title":"Simulating METIS SCAO System","authors":["Markus Feldt","Horst Steuer","Carlos Correia","Andreas Obereder","Stefan Raffetseder","Thomas Bertram","Julia Shatokina","Faustine Cantalloube"],"raw_abstract":"METIS, the Mid-Infrared ELT Imager and Spectrograph, is one of the four\nfirst-generation ELT instruments scheduled to see first light in 2028. Its two\nmain science modules are supported by an adaptive optics system featuring a\npyramid sensor with 90x90 subapertures working in the H and K bands. During the\nPDR and FDR phases, extensive simulations were carried out to support the\nsensing, reconstruction, and control concept of METIS single-conjugate adaptive\noptics (SCAO) system. We present details on the implementation of the\nCOMPASS-based environment used for the simulations, the metrics used for\nanalyzing our performance expectations, an overview of the main results, and\nsome details on special cases like non-common path aberrations (NCPA) and water\nvapor seeing, as well as the low-wind effect.","publication_date":1700664887,"paper_link":"http://arxiv.org/pdf/2311.13437v1","categories":["Physics"],"abstract":"METIS, the Mid-Infrared ELT Imager and Spectrograph, is one of the four first-generation ELT instruments scheduled to see first light in 2028. Its two main science modules are supported by an adaptive optics system featuring a pyramid sensor with 90x90 subapertures working in the H and K bands. During the PDR and FDR phases, extensive simulations were carried out to support the sensing, reconstruction, and control concept of METIS single-conjugate adaptive optics (SCAO) system. We present details on the implementation of the COMPASS-based environment used for the simulations, the metrics used for analyzing our performance expectations, an overview of the main results, and some details on special cases like non-common path aberrations (NCPA) and water vapor seeing, as well as the low-wind effect."}
{"title":"PG-Video-LLaVA: Pixel Grounding Large Video-Language Models","authors":["Shehan Munasinghe","Rusiru Thushara","Muhammad Maaz","Hanoona Abdul Rasheed","Salman Khan","Mubarak Shah","Fahad Khan"],"raw_abstract":"Extending image-based Large Multimodal Models (LMM) to videos is challenging\ndue to the inherent complexity of video data. The recent approaches extending\nimage-based LMM to videos either lack the grounding capabilities (e.g.,\nVideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for\nbetter video understanding (e.g., Video-ChatGPT). Addressing these gaps, we\npropose Video-LLaVA, the first LMM with pixel-level grounding capability,\nintegrating audio cues by transcribing them into text to enrich video-context\nunderstanding. Our framework uses an off-the-shelf tracker and a novel\ngrounding module, enabling it to spatially and temporally localize objects in\nvideos following user instructions. We evaluate Video-LLaVA using video-based\ngenerative and question-answering benchmarks and introduce new benchmarks\nspecifically designed to measure prompt-based object grounding performance in\nvideos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in\nVideo-ChatGPT, for video-based conversation benchmarking, ensuring\nreproducibility of results which is a concern with the proprietary nature of\nGPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its\nadvantages to the video domain, delivering promising gains on video-based\nconversation and grounding tasks. Project Page:\nhttps://github.com/mbzuai-oryx/Video-LLaVA","publication_date":1700664510,"paper_link":"http://arxiv.org/pdf/2311.13435v1","categories":["Computer Science"],"abstract":"Extending image-based Large Multimodal Models (LMM) to videos is challenging due to the inherent complexity of video data. The recent approaches extending image-based LMM to videos either lack the grounding capabilities (e.g., VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we propose Video-LLaVA, the first LMM with pixel-level grounding capability, integrating audio cues by transcribing them into text to enrich video-context understanding. Our framework uses an off-the-shelf tracker and a novel grounding module, enabling it to spatially and temporally localize objects in videos following user instructions. We evaluate Video-LLaVA using video-based generative and question-answering benchmarks and introduce new benchmarks specifically designed to measure prompt-based object grounding performance in videos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in Video-ChatGPT, for video-based conversation benchmarking, ensuring reproducibility of results which is a concern with the proprietary nature of GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its advantages to the video domain, delivering promising gains on video-based conversation and grounding tasks. Project Page: https://github.com/mbzuai-oryx/Video-LLaVA"}
{"title":"Recurrent neural networks and transfer learning for elasto-plasticity in woven composites","authors":["Ehsan Ghane","Martin Fagerstr\u00f6m","Mohsen Mirkhalaf"],"raw_abstract":"As a surrogate for computationally intensive meso-scale simulation of woven\ncomposites, this article presents Recurrent Neural Network (RNN) models.\nLeveraging the power of transfer learning, the initialization challenges and\nsparse data issues inherent in cyclic shear strain loads are addressed in the\nRNN models. A mean-field model generates a comprehensive data set representing\nelasto-plastic behavior. In simulations, arbitrary six-dimensional strain\nhistories are used to predict stresses under random walking as the source task\nand cyclic loading conditions as the target task. Incorporating sub-scale\nproperties enhances RNN versatility. In order to achieve accurate predictions,\nthe model uses a grid search method to tune network architecture and\nhyper-parameter configurations. The results of this study demonstrate that\ntransfer learning can be used to effectively adapt the RNN to varying strain\nconditions, which establishes its potential as a useful tool for modeling\npath-dependent responses in woven composites.","publication_date":1700664474,"paper_link":"http://arxiv.org/pdf/2311.13434v1","categories":["Physics"],"abstract":"As a surrogate for computationally intensive meso-scale simulation of woven composites, this article presents Recurrent Neural Network (RNN) models. Leveraging the power of transfer learning, the initialization challenges and sparse data issues inherent in cyclic shear strain loads are addressed in the RNN models. A mean-field model generates a comprehensive data set representing elasto-plastic behavior. In simulations, arbitrary six-dimensional strain histories are used to predict stresses under random walking as the source task and cyclic loading conditions as the target task. Incorporating sub-scale properties enhances RNN versatility. In order to achieve accurate predictions, the model uses a grid search method to tune network architecture and hyper-parameter configurations. The results of this study demonstrate that transfer learning can be used to effectively adapt the RNN to varying strain conditions, which establishes its potential as a useful tool for modeling path-dependent responses in woven composites."}
{"title":"A catalogue of observed geo-effective CME/ICME characteristics","authors":["R. Mugatwala","S. Chierichini","G. Francisco","G. Napoletano","R. Foldes","L. Giovannelli","G. De Gasperis","E. Camporeale","R. Erd\u00e9lyi","D. Del Moro"],"raw_abstract":"One of the goals of Space Weather studies is to achieve a better\nunderstanding of impulsive phenomena, such as Coronal Mass Ejections (CMEs), in\norder to improve our ability to forecast them and mitigate the risk to our\ntechnologically driven society. The essential part of achieving this goal is to\nassess the performance of forecasting models. To this end, the quality and\navailability of suitable data are of paramount importance. In this work, we\nhave merged already publicly available data of CMEs from both in-situ and\nremote instrumentation in order to build a database of CME properties. To\nevaluate the accuracy of such a database and confirm the relationship between\nin-situ and remote observations, we have employed the drag-based model (DBM)\ndue to its simplicity and inexpensive cost of computational resources. In this\nstudy, we have also explored the parameter space for the drag parameter and\nsolar wind speed using a Monte Carlo approach to evaluate how well the DBM\ndetermines the propagation of CMEs for the events in the dataset. The dataset\nof geoeffective CMEs constructed as a result of this work provides validation\nof the initial hypothesis about DBM, and solar wind speed and also yields\nfurther insight into CME features like arrival time, arrival speed, lift-off\ntime, etc. Using a data-driven approach, this procedure allows us to present a\nhomogeneous, reliable, and robust dataset for the investigation of CME\npropagation. On the other hand, possible CME events are identified where DBM\napproximation is not valid due to model limitations and higher uncertainties in\nthe input parameters, those events require more thorough investigation.","publication_date":1700664329,"paper_link":"http://arxiv.org/pdf/2311.13429v1","categories":["Physics"],"abstract":"One of the goals of Space Weather studies is to achieve a better understanding of impulsive phenomena, such as Coronal Mass Ejections (CMEs), in order to improve our ability to forecast them and mitigate the risk to our technologically driven society. The essential part of achieving this goal is to assess the performance of forecasting models. To this end, the quality and availability of suitable data are of paramount importance. In this work, we have merged already publicly available data of CMEs from both in-situ and remote instrumentation in order to build a database of CME properties. To evaluate the accuracy of such a database and confirm the relationship between in-situ and remote observations, we have employed the drag-based model (DBM) due to its simplicity and inexpensive cost of computational resources. In this study, we have also explored the parameter space for the drag parameter and solar wind speed using a Monte Carlo approach to evaluate how well the DBM determines the propagation of CMEs for the events in the dataset. The dataset of geoeffective CMEs constructed as a result of this work provides validation of the initial hypothesis about DBM, and solar wind speed and also yields further insight into CME features like arrival time, arrival speed, lift-off time, etc. Using a data-driven approach, this procedure allows us to present a homogeneous, reliable, and robust dataset for the investigation of CME propagation. On the other hand, possible CME events are identified where DBM approximation is not valid due to model limitations and higher uncertainties in the input parameters, those events require more thorough investigation."}
{"title":"Coherent sheaves on surfaces, COHAs and deformed $W_{1+\\infty}$-algebras","authors":["Anton Mellit","Alexandre Minets","Olivier Schiffmann","Eric Vasserot"],"raw_abstract":"We compute the cohomological Hall algebra of zero-dimensional sheaves on an\narbitrary smooth quasi-projective surface $S$ with pure cohomology, deriving an\nexplicit presentation by generators and relations. When $S$ has trivial\ncanonical bundle, this COHA is isomorphic to the enveloping algebra of deformed\ntrigonometric $W_{1+\\infty}$-algebra associated to the ring\n$H^*(S,\\mathbb{Q})$. We also define a double of this COHA, show that it acts on\nthe homology of various moduli stacks of sheaves on $S$ and explicitly describe\nthis action on the products of tautological classes. Examples include Hilbert\nschemes of points on surfaces, the moduli stack of Higgs bundles on a smooth\nprojective curve and the moduli stack of $1$-dimensional sheaves on a $K3$\nsurface in an ample class. The double COHA is shown to contain Nakajima's\nHeisenberg algebra, as well as a copy of the Virasoro algebra.","publication_date":1700663126,"paper_link":"http://arxiv.org/pdf/2311.13415v1","categories":["Mathematics"],"abstract":"We compute the cohomological Hall algebra of zero-dimensional sheaves on an arbitrary smooth quasi-projective surface __FORMULA__ with pure cohomology, deriving an explicit presentation by generators and relations. When __FORMULA__ has trivial canonical bundle, this COHA is isomorphic to the enveloping algebra of deformed trigonometric __FORMULA__-algebra associated to the ring __FORMULA__. We also define a double of this COHA, show that it acts on the homology of various moduli stacks of sheaves on __FORMULA__ and explicitly describe this action on the products of tautological classes. Examples include Hilbert schemes of points on surfaces, the moduli stack of Higgs bundles on a smooth projective curve and the moduli stack of __FORMULA__-dimensional sheaves on a __FORMULA__ surface in an ample class. The double COHA is shown to contain Nakajima's Heisenberg algebra, as well as a copy of the Virasoro algebra."}
{"title":"From Images to Connections: Can DQN with GNNs learn the Strategic Game of Hex?","authors":["Yannik Keller","Jannis Bl\u00fcml","Gopika Sudhakaran","Kristian Kersting"],"raw_abstract":"The gameplay of strategic board games such as chess, Go and Hex is often\ncharacterized by combinatorial, relational structures -- capturing distinct\ninteractions and non-local patterns -- and not just images. Nonetheless, most\ncommon self-play reinforcement learning (RL) approaches simply approximate\npolicy and value functions using convolutional neural networks (CNN). A key\nfeature of CNNs is their relational inductive bias towards locality and\ntranslational invariance. In contrast, graph neural networks (GNN) can encode\nmore complicated and distinct relational structures. Hence, we investigate the\ncrucial question: Can GNNs, with their ability to encode complex connections,\nreplace CNNs in self-play reinforcement learning? To this end, we do a\ncomparison with Hex -- an abstract yet strategically rich board game -- serving\nas our experimental platform. Our findings reveal that GNNs excel at dealing\nwith long range dependency situations in game states and are less prone to\noverfitting, but also showing a reduced proficiency in discerning local\npatterns. This suggests a potential paradigm shift, signaling the use of\ngame-specific structures to reshape self-play reinforcement learning.","publication_date":1700662815,"paper_link":"http://arxiv.org/pdf/2311.13414v1","categories":["Computer Science"],"abstract":"The gameplay of strategic board games such as chess, Go and Hex is often characterized by combinatorial, relational structures -- capturing distinct interactions and non-local patterns -- and not just images. Nonetheless, most common self-play reinforcement learning (RL) approaches simply approximate policy and value functions using convolutional neural networks (CNN). A key feature of CNNs is their relational inductive bias towards locality and translational invariance. In contrast, graph neural networks (GNN) can encode more complicated and distinct relational structures. Hence, we investigate the crucial question: Can GNNs, with their ability to encode complex connections, replace CNNs in self-play reinforcement learning? To this end, we do a comparison with Hex -- an abstract yet strategically rich board game -- serving as our experimental platform. Our findings reveal that GNNs excel at dealing with long range dependency situations in game states and are less prone to overfitting, but also showing a reduced proficiency in discerning local patterns. This suggests a potential paradigm shift, signaling the use of game-specific structures to reshape self-play reinforcement learning."}
{"title":"CompenHR: Efficient Full Compensation for High-resolution Projector","authors":["Yuxi Wang","Haibin Ling","Bingyao Huang"],"raw_abstract":"Full projector compensation is a practical task of projector-camera systems.\nIt aims to find a projector input image, named compensation image, such that\nwhen projected it cancels the geometric and photometric distortions due to the\nphysical environment and hardware. State-of-the-art methods use deep learning\nto address this problem and show promising performance for low-resolution\nsetups. However, directly applying deep learning to high-resolution setups is\nimpractical due to the long training time and high memory cost. To address this\nissue, this paper proposes a practical full compensation solution. Firstly, we\ndesign an attention-based grid refinement network to improve geometric\ncorrection quality. Secondly, we integrate a novel sampling scheme into an\nend-to-end compensation network to alleviate computation and introduce\nattention blocks to preserve key features. Finally, we construct a benchmark\ndataset for high-resolution projector full compensation. In experiments, our\nmethod demonstrates clear advantages in both efficiency and quality.","publication_date":1700662407,"paper_link":"http://arxiv.org/pdf/2311.13409v1","categories":["Computer Science"],"abstract":"Full projector compensation is a practical task of projector-camera systems. It aims to find a projector input image, named compensation image, such that when projected it cancels the geometric and photometric distortions due to the physical environment and hardware. State-of-the-art methods use deep learning to address this problem and show promising performance for low-resolution setups. However, directly applying deep learning to high-resolution setups is impractical due to the long training time and high memory cost. To address this issue, this paper proposes a practical full compensation solution. Firstly, we design an attention-based grid refinement network to improve geometric correction quality. Secondly, we integrate a novel sampling scheme into an end-to-end compensation network to alleviate computation and introduce attention blocks to preserve key features. Finally, we construct a benchmark dataset for high-resolution projector full compensation. In experiments, our method demonstrates clear advantages in both efficiency and quality."}
{"title":"Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions","authors":["Keyang Ye","Tianjia Shao","Kun Zhou"],"raw_abstract":"We present a novel animatable 3D Gaussian model for rendering high-fidelity\nfree-view human motions in real time. Compared to existing NeRF-based methods,\nthe model owns better capability in synthesizing high-frequency details without\nthe jittering problem across video frames. The core of our model is a novel\naugmented 3D Gaussian representation, which attaches each Gaussian with a\nlearnable code. The learnable code serves as a pose-dependent appearance\nembedding for refining the erroneous appearance caused by geometric\ntransformation of Gaussians, based on which an appearance refinement model is\nlearned to produce residual Gaussian properties to match the appearance in\ntarget pose. To force the Gaussians to learn the foreground human only without\nbackground interference, we further design a novel alpha loss to explicitly\nconstrain the Gaussians within the human body. We also propose to jointly\noptimize the human joint parameters to improve the appearance accuracy. The\nanimatable 3D Gaussian model can be learned with shallow MLPs, so new human\nmotions can be synthesized in real time (66 fps on avarage). Experiments show\nthat our model has superior performance over NeRF-based methods.","publication_date":1700661623,"paper_link":"http://arxiv.org/pdf/2311.13404v1","categories":["Computer Science"],"abstract":"We present a novel animatable 3D Gaussian model for rendering high-fidelity free-view human motions in real time. Compared to existing NeRF-based methods, the model owns better capability in synthesizing high-frequency details without the jittering problem across video frames. The core of our model is a novel augmented 3D Gaussian representation, which attaches each Gaussian with a learnable code. The learnable code serves as a pose-dependent appearance embedding for refining the erroneous appearance caused by geometric transformation of Gaussians, based on which an appearance refinement model is learned to produce residual Gaussian properties to match the appearance in target pose. To force the Gaussians to learn the foreground human only without background interference, we further design a novel alpha loss to explicitly constrain the Gaussians within the human body. We also propose to jointly optimize the human joint parameters to improve the appearance accuracy. The animatable 3D Gaussian model can be learned with shallow MLPs, so new human motions can be synthesized in real time (66 fps on avarage). Experiments show that our model has superior performance over NeRF-based methods."}
{"title":"Condensed Matter Systems Exposed to Radiation: Multiscale Theory, Simulations, and Experiment","authors":["Andrey V. Solov'yov","Alexey V. Verkhovtsev","Nigel J. Mason","Richard A. Amos","Ilko Bald","G\u00e9rard Baldacchino","Brendan Dromey","Martin Falk","Luca Gerhards","Michael Hausmann","Georg Hildenbrand","Milo\u0161 Hrabovsk\u00fd","Stanislav Kadlec","Jaroslav Ko\u010di\u0161ek","Franck L\u00e9pine","Siyi Ming","Andrew Nisbet","Kate Ricketts","Leo Sala","Thomas Schlath\u00f6lter","Andrew Wheatley","Ilia A. Solov'yov"],"raw_abstract":"This paper reviews the new highly interdisciplinary research field studying\nthe behavior of condensed matter systems exposed to radiation. The paper\nhighlights several relevant examples of recent advances in the field and\nprovides a roadmap for the development of the field in the next decade.\nCondensed matter systems exposed to radiation may have very different natures,\nbeing inorganic, organic or biological, finite or infinite, be composed of many\ndifferent molecular species or materials, existing in different phases (solid,\nliquid, gaseous or plasma) and operating under different thermodynamic\nconditions. The essential and novel element of this research is that, despite\nthe vast diversity of such systems, many of the key phenomena related to the\nbehavior of irradiated systems (such as radiation-induced damage, mechanisms of\ndamage repair and control, radiation protection, etc.) are very similar and can\nbe understood based on the same fundamental theoretical principles and\ncomputational approaches. One of the essential features of the aforementioned\nphenomena concerns their multiscale nature as the manifestation of the\nradiation-induced effects occurring at different spatial and temporal scales\nranging from the atomic to the macroscopic. The multiscale nature of the\neffects and similarity of their manifestation in systems of different origins\nnecessarily brings together different disciplines, such as physics, chemistry,\nbiology, materials and nano-science, and biomedical research, demonstrating\nnumerous interlinks and commonalities between them. This research field is\nhighly relevant to many novel and emerging technologies and medical\napplications.","publication_date":1700661378,"paper_link":"http://arxiv.org/pdf/2311.13402v1","categories":["Physics"],"abstract":"This paper reviews the new highly interdisciplinary research field studying the behavior of condensed matter systems exposed to radiation. The paper highlights several relevant examples of recent advances in the field and provides a roadmap for the development of the field in the next decade. Condensed matter systems exposed to radiation may have very different natures, being inorganic, organic or biological, finite or infinite, be composed of many different molecular species or materials, existing in different phases (solid, liquid, gaseous or plasma) and operating under different thermodynamic conditions. The essential and novel element of this research is that, despite the vast diversity of such systems, many of the key phenomena related to the behavior of irradiated systems (such as radiation-induced damage, mechanisms of damage repair and control, radiation protection, etc.) are very similar and can be understood based on the same fundamental theoretical principles and computational approaches. One of the essential features of the aforementioned phenomena concerns their multiscale nature as the manifestation of the radiation-induced effects occurring at different spatial and temporal scales ranging from the atomic to the macroscopic. The multiscale nature of the effects and similarity of their manifestation in systems of different origins necessarily brings together different disciplines, such as physics, chemistry, biology, materials and nano-science, and biomedical research, demonstrating numerous interlinks and commonalities between them. This research field is highly relevant to many novel and emerging technologies and medical applications."}
{"title":"Isotropic spin and inverse spin Hall effect in epitaxial (111)-oriented Pt/Co bilayers","authors":["Adri\u00e1n Gud\u00edn","Alberto Anad\u00f3n","Iciar Arnay","Rub\u00e9n Guerrero","Julio Camarero","Sebastien Petit-Watelot","Paolo Perna","Juan-Carlos Rojas-S\u00e1nchez"],"raw_abstract":"The spin-to-charge current interconversion in bilayers composed of\nferromagnetic and nonmagnetic layers with strong spin-orbit coupling has\ngarnered considerable attention due to its exceptional potential in advancing\nspintronics devices for data storage and logic applications. Platinum (Pt)\nstands out as one of the most effective materials for generating spin current.\nWhile the spin conversion efficiency is isotropic in polycrystalline Pt\nsamples, an ongoing debate persists regarding its dependence on the crystalline\ndirection in single crystalline samples. In this study, we aim to\ncomprehensively evaluate the in-plane anisotropy of spin-charge interconversion\nusing an array of complementary Spin Hall and inverse Spin Hall techniques with\nboth incoherent and coherent excitation. Specifically, we investigate the\nspin-to-charge interconversion in epitaxial, (111)-oriented, Co/Pt bilayers\nwith low surface roughness, as resulted from x-ray experiments. By varying the\nthickness of the Pt layer, we gain insights into the spin-charge\ninterconversion in epitaxial Pt and highlight the effects of the interfaces.\nOur results demonstrate an isotropic behavior within the limits of our\ndetection uncertainty. This finding significantly enhances our understanding of\nspin conversion in one of the most relevant systems in spintronics and paves\nthe way for future research in this field.","publication_date":1700661297,"paper_link":"http://arxiv.org/pdf/2311.13400v1","categories":["Physics"],"abstract":"The spin-to-charge current interconversion in bilayers composed of ferromagnetic and nonmagnetic layers with strong spin-orbit coupling has garnered considerable attention due to its exceptional potential in advancing spintronics devices for data storage and logic applications. Platinum (Pt) stands out as one of the most effective materials for generating spin current. While the spin conversion efficiency is isotropic in polycrystalline Pt samples, an ongoing debate persists regarding its dependence on the crystalline direction in single crystalline samples. In this study, we aim to comprehensively evaluate the in-plane anisotropy of spin-charge interconversion using an array of complementary Spin Hall and inverse Spin Hall techniques with both incoherent and coherent excitation. Specifically, we investigate the spin-to-charge interconversion in epitaxial, (111)-oriented, Co/Pt bilayers with low surface roughness, as resulted from x-ray experiments. By varying the thickness of the Pt layer, we gain insights into the spin-charge interconversion in epitaxial Pt and highlight the effects of the interfaces. Our results demonstrate an isotropic behavior within the limits of our detection uncertainty. This finding significantly enhances our understanding of spin conversion in one of the most relevant systems in spintronics and paves the way for future research in this field."}
{"title":"Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images","authors":["Jaeyoung Chung","Jeongtaek Oh","Kyoung Mu Lee"],"raw_abstract":"In this paper, we present a method to optimize Gaussian splatting with a\nlimited number of images while avoiding overfitting. Representing a 3D scene by\ncombining numerous Gaussian splats has yielded outstanding visual quality.\nHowever, it tends to overfit the training views when only a small number of\nimages are available. To address this issue, we introduce a dense depth map as\na geometry guide to mitigate overfitting. We obtained the depth map using a\npre-trained monocular depth estimation model and aligning the scale and offset\nusing sparse COLMAP feature points. The adjusted depth aids in the color-based\noptimization of 3D Gaussian splatting, mitigating floating artifacts, and\nensuring adherence to geometric constraints. We verify the proposed method on\nthe NeRF-LLFF dataset with varying numbers of few images. Our approach\ndemonstrates robust geometry compared to the original method that relies solely\non images.","publication_date":1700661184,"paper_link":"http://arxiv.org/pdf/2311.13398v1","categories":["Computer Science"],"abstract":"In this paper, we present a method to optimize Gaussian splatting with a limited number of images while avoiding overfitting. Representing a 3D scene by combining numerous Gaussian splats has yielded outstanding visual quality. However, it tends to overfit the training views when only a small number of images are available. To address this issue, we introduce a dense depth map as a geometry guide to mitigate overfitting. We obtained the depth map using a pre-trained monocular depth estimation model and aligning the scale and offset using sparse COLMAP feature points. The adjusted depth aids in the color-based optimization of 3D Gaussian splatting, mitigating floating artifacts, and ensuring adherence to geometric constraints. We verify the proposed method on the NeRF-LLFF dataset with varying numbers of few images. Our approach demonstrates robust geometry compared to the original method that relies solely on images."}
{"title":"Spatial Audio and Individualized HRTFs using a Convolutional Neural Network (CNN)","authors":["Ludovic Pirard"],"raw_abstract":"Spatial audio and 3-Dimensional sound rendering techniques play a pivotal and\nessential role in immersive audio experiences. Head-Related Transfer Functions\n(HRTFs) are acoustic filters which represent how sound interacts with an\nindividual's unique head and ears anatomy. The use of HRTFs compliant to the\nsubjects anatomical traits is crucial to ensure a personalized and unique\nspatial experience. This work proposes the implementation of an HRTF\nindividualization method based on anthropometric features automatically\nextracted from ear images using a Convolutional Neural Network (CNN). Firstly,\na CNN is implemented and tested to assess the performance of machine learning\non positioning landmarks on ear images. The I-BUG dataset, containing ear\nimages with corresponding 55 landmarks, was used to train and test the neural\nnetwork. Subsequently, 12 relevant landmarks were selected to correspond to 7\nspecific anthropometric measurements established by the HUTUBS database. These\nlandmarks serve as a reference for distance computation in pixels in order to\nretrieve the anthropometric measurements from the ear images. Once the 7\ndistances in pixels are extracted from the ear image, they are converted in\ncentimetres using conversion factors, a best match method vector is implemented\ncomputing the Euclidean distance for each set in a database of 116 ears with\ntheir corresponding 7 anthropometric measurements provided by the HUTUBS\ndatabase. The closest match of anthropometry can be identified and the\ncorresponding set of HRTFs can be obtained for personnalized use. The method is\nevaluated in its validity instead of the accuracy of the results. The\nconceptual scope of each stage has been verified and substantiated to function\ncorrectly. The various steps and the available elements in the process are\nreviewed and challenged to define a greater algorithm entity designed for the\ndesired task.","publication_date":1700661171,"paper_link":"http://arxiv.org/pdf/2311.13397v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Spatial audio and 3-Dimensional sound rendering techniques play a pivotal and essential role in immersive audio experiences. Head-Related Transfer Functions (HRTFs) are acoustic filters which represent how sound interacts with an individual's unique head and ears anatomy. The use of HRTFs compliant to the subjects anatomical traits is crucial to ensure a personalized and unique spatial experience. This work proposes the implementation of an HRTF individualization method based on anthropometric features automatically extracted from ear images using a Convolutional Neural Network (CNN). Firstly, a CNN is implemented and tested to assess the performance of machine learning on positioning landmarks on ear images. The I-BUG dataset, containing ear images with corresponding 55 landmarks, was used to train and test the neural network. Subsequently, 12 relevant landmarks were selected to correspond to 7 specific anthropometric measurements established by the HUTUBS database. These landmarks serve as a reference for distance computation in pixels in order to retrieve the anthropometric measurements from the ear images. Once the 7 distances in pixels are extracted from the ear image, they are converted in centimetres using conversion factors, a best match method vector is implemented computing the Euclidean distance for each set in a database of 116 ears with their corresponding 7 anthropometric measurements provided by the HUTUBS database. The closest match of anthropometry can be identified and the corresponding set of HRTFs can be obtained for personnalized use. The method is evaluated in its validity instead of the accuracy of the results. The conceptual scope of each stage has been verified and substantiated to function correctly. The various steps and the available elements in the process are reviewed and challenged to define a greater algorithm entity designed for the desired task."}
{"title":"Performance Analysis Of Binaural Signal Matching (BSM) in the Time-Frequency Domain","authors":["Ami Berger","Vladimir Tourbabin","Jacob Donley","Zamir Ben-Hur","Boaz Rafaely"],"raw_abstract":"The capture and reproduction of spatial audio is becoming increasingly\npopular, with the mushrooming of applications in teleconferencing,\nentertainment and virtual reality. Many binaural reproduction methods have been\ndeveloped and studied extensively for spherical and other specially designed\narrays. However, the recent increased popularity of wearable and mobile arrays\nrequires the development of binaural reproduction methods for these arrays. One\nsuch method is binaural signal matching (BSM). However, to date this method has\nonly been investigated with fixed matched filters designed for long audio\nrecordings. With the aim of making the BSM method more adaptive to dynamic\nenvironments, this paper analyzes BSM with a parameterized sound-field in the\ntime-frequency domain. The paper presents results of implementing the BSM\nmethod on a sound-field that was decomposed into its direct and reverberant\ncomponents, and compares this implementation with the BSM computed for the\nentire sound-field, to compare performance for binaural reproduction of\nreverberant speech in a simulated environment.","publication_date":1700660317,"paper_link":"http://arxiv.org/pdf/2311.13390v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The capture and reproduction of spatial audio is becoming increasingly popular, with the mushrooming of applications in teleconferencing, entertainment and virtual reality. Many binaural reproduction methods have been developed and studied extensively for spherical and other specially designed arrays. However, the recent increased popularity of wearable and mobile arrays requires the development of binaural reproduction methods for these arrays. One such method is binaural signal matching (BSM). However, to date this method has only been investigated with fixed matched filters designed for long audio recordings. With the aim of making the BSM method more adaptive to dynamic environments, this paper analyzes BSM with a parameterized sound-field in the time-frequency domain. The paper presents results of implementing the BSM method on a sound-field that was decomposed into its direct and reverberant components, and compares this implementation with the BSM computed for the entire sound-field, to compare performance for binaural reproduction of reverberant speech in a simulated environment."}
{"title":"Conflict Management in the Near-RT-RIC of Open RAN: A Game Theoretic Approach","authors":["Abdul Wadud","Fatemeh Golpayegani","Nima Afraz"],"raw_abstract":"Open Radio Access Network (RAN) was introduced recently to incorporate\nintelligence and openness into the upcoming generation of RAN. Open RAN offers\nstandardized interfaces and the capacity to accommodate network applications\nfrom external vendors through extensible applications (xApps), which enhance\nnetwork management flexibility. The Near-Real-Time Radio Intelligent Controller\n(Near-RT-RIC) employs specialized and intelligent xApps for achieving\ntime-critical optimization objectives, but conflicts may arise due to different\nvendors' xApps modifying the same parameters or indirectly affecting each\nothers' performance. A standardized Conflict Management System (CMS) is absent\nin most of the popular Open RAN architectures including the most prominent\nO-RAN Alliance architecture. To address this, we propose a CMS with independent\ncontrollers for conflict detection and mitigation between xApps in the\nNear-RT-RIC. We utilize cooperative bargain game theory, including Nash Social\nWelfare Function (NSWF) and the Equal Gains (EG) solution, to find optimal\nconfigurations for conflicting parameters. Experimental results demonstrate the\neffectiveness of the proposed Conflict Management Controller (CMC) in balancing\nconflicting parameters and mitigating adverse impacts in the Near-RT-RIC on a\ntheoretical example scenario.","publication_date":1700660311,"paper_link":"http://arxiv.org/pdf/2311.13389v1","categories":["Computer Science"],"abstract":"Open Radio Access Network (RAN) was introduced recently to incorporate intelligence and openness into the upcoming generation of RAN. Open RAN offers standardized interfaces and the capacity to accommodate network applications from external vendors through extensible applications (xApps), which enhance network management flexibility. The Near-Real-Time Radio Intelligent Controller (Near-RT-RIC) employs specialized and intelligent xApps for achieving time-critical optimization objectives, but conflicts may arise due to different vendors' xApps modifying the same parameters or indirectly affecting each others' performance. A standardized Conflict Management System (CMS) is absent in most of the popular Open RAN architectures including the most prominent O-RAN Alliance architecture. To address this, we propose a CMS with independent controllers for conflict detection and mitigation between xApps in the Near-RT-RIC. We utilize cooperative bargain game theory, including Nash Social Welfare Function (NSWF) and the Equal Gains (EG) solution, to find optimal configurations for conflicting parameters. Experimental results demonstrate the effectiveness of the proposed Conflict Management Controller (CMC) in balancing conflicting parameters and mitigating adverse impacts in the Near-RT-RIC on a theoretical example scenario."}
{"title":"SegVol: Universal and Interactive Volumetric Medical Image Segmentation","authors":["Yuxin Du","Fan Bai","Tiejun Huang","Bo Zhao"],"raw_abstract":"Precise image segmentation provides clinical study with meaningful and\nwell-structured information. Despite the remarkable progress achieved in\nmedical image segmentation, there is still an absence of foundation\nsegmentation model that can segment a wide range of anatomical categories with\neasy user interaction. In this paper, we propose a universal and interactive\nvolumetric medical image segmentation model, named SegVol. By training on 90k\nunlabeled Computed Tomography (CT) volumes and 6k labeled CTs, this foundation\nmodel supports the segmentation of over 200 anatomical categories using\nsemantic and spatial prompts. Extensive experiments verify that SegVol\noutperforms the state of the art by a large margin on multiple segmentation\nbenchmarks. Notably, on three challenging lesion datasets, our method achieves\naround 20% higher Dice score than nnU-Net. The model and data are publicly\navailable at: https://github.com/BAAI-DCAI/SegVol.","publication_date":1700659656,"paper_link":"http://arxiv.org/pdf/2311.13385v1","categories":["Computer Science"],"abstract":"Precise image segmentation provides clinical study with meaningful and well-structured information. Despite the remarkable progress achieved in medical image segmentation, there is still an absence of foundation segmentation model that can segment a wide range of anatomical categories with easy user interaction. In this paper, we propose a universal and interactive volumetric medical image segmentation model, named SegVol. By training on 90k unlabeled Computed Tomography (CT) volumes and 6k labeled CTs, this foundation model supports the segmentation of over 200 anatomical categories using semantic and spatial prompts. Extensive experiments verify that SegVol outperforms the state of the art by a large margin on multiple segmentation benchmarks. Notably, on three challenging lesion datasets, our method achieves around 20% higher Dice score than nnU-Net. The model and data are publicly available at: https://github.com/BAAI-DCAI/SegVol."}
{"title":"LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes","authors":["Jaeyoung Chung","Suyoung Lee","Hyeongjin Nam","Jaerin Lee","Kyoung Mu Lee"],"raw_abstract":"With the widespread usage of VR devices and contents, demands for 3D scene\ngeneration techniques become more popular. Existing 3D scene generation models,\nhowever, limit the target scene to specific domain, primarily due to their\ntraining strategies using 3D scan dataset that is far from the real-world. To\naddress such limitation, we propose LucidDreamer, a domain-free scene\ngeneration pipeline by fully leveraging the power of existing large-scale\ndiffusion-based generative model. Our LucidDreamer has two alternate steps:\nDreaming and Alignment. First, to generate multi-view consistent images from\ninputs, we set the point cloud as a geometrical guideline for each image\ngeneration. Specifically, we project a portion of point cloud to the desired\nview and provide the projection as a guidance for inpainting using the\ngenerative model. The inpainted images are lifted to 3D space with estimated\ndepth maps, composing a new points. Second, to aggregate the new points into\nthe 3D scene, we propose an aligning algorithm which harmoniously integrates\nthe portions of newly generated 3D scenes. The finally obtained 3D scene serves\nas initial points for optimizing Gaussian splats. LucidDreamer produces\nGaussian splats that are highly-detailed compared to the previous 3D scene\ngeneration methods, with no constraint on domain of the target scene.","publication_date":1700659654,"paper_link":"http://arxiv.org/pdf/2311.13384v1","categories":["Computer Science"],"abstract":"With the widespread usage of VR devices and contents, demands for 3D scene generation techniques become more popular. Existing 3D scene generation models, however, limit the target scene to specific domain, primarily due to their training strategies using 3D scan dataset that is far from the real-world. To address such limitation, we propose LucidDreamer, a domain-free scene generation pipeline by fully leveraging the power of existing large-scale diffusion-based generative model. Our LucidDreamer has two alternate steps: Dreaming and Alignment. First, to generate multi-view consistent images from inputs, we set the point cloud as a geometrical guideline for each image generation. Specifically, we project a portion of point cloud to the desired view and provide the projection as a guidance for inpainting using the generative model. The inpainted images are lifted to 3D space with estimated depth maps, composing a new points. Second, to aggregate the new points into the 3D scene, we propose an aligning algorithm which harmoniously integrates the portions of newly generated 3D scenes. The finally obtained 3D scene serves as initial points for optimizing Gaussian splats. LucidDreamer produces Gaussian splats that are highly-detailed compared to the previous 3D scene generation methods, with no constraint on domain of the target scene."}
{"title":"Confidant: Customizing Transformer-based LLMs via Collaborative Edge Training","authors":["Yuhao Chen","Yuxuan Yan","Qianqian Yang","Yuanchao Shu","Shibo He","Jiming Chen"],"raw_abstract":"Transformer-based large language models (LLMs) have demonstrated impressive\ncapabilities in a variety of natural language processing (NLP) tasks.\nNonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge\ndevices with limited computing, memory, and energy budgets. In this paper, we\npropose Confidant, a multi-backend collaborative training framework for\ncustomizing state-of-the-art LLMs on commodity mobile devices like smartphones.\nConfidant partitions an LLM into several sub-models so that each fits into a\nmobile device's memory. A pipeline parallel training mechanism is further\ndeveloped to ensure fast and efficient distributed training. In addition, we\npropose a novel backend scheduler to allocate different attention heads to\nheterogeneous compute hardware, including mobile CPU and GPUs, to maximize the\ncompute resource utilization on each edge device. Our preliminary experimental\nresults show that Confidant achieves at most 45.3% memory reduction and 8.03x\ninference speedup in practical settings.","publication_date":1700659259,"paper_link":"http://arxiv.org/pdf/2311.13381v1","categories":["Computer Science"],"abstract":"Transformer-based large language models (LLMs) have demonstrated impressive capabilities in a variety of natural language processing (NLP) tasks. Nonetheless, it is challenging to deploy and fine-tune LLMs on mobile edge devices with limited computing, memory, and energy budgets. In this paper, we propose Confidant, a multi-backend collaborative training framework for customizing state-of-the-art LLMs on commodity mobile devices like smartphones. Confidant partitions an LLM into several sub-models so that each fits into a mobile device's memory. A pipeline parallel training mechanism is further developed to ensure fast and efficient distributed training. In addition, we propose a novel backend scheduler to allocate different attention heads to heterogeneous compute hardware, including mobile CPU and GPUs, to maximize the compute resource utilization on each edge device. Our preliminary experimental results show that Confidant achieves at most 45.3% memory reduction and 8.03x inference speedup in practical settings."}
{"title":"Deriving Comprehensible Theories from Probabilistic Circuits","authors":["Sieben Bocklandt","Wannes Meert","Koen Vanderstraeten","Wouter Pijpops","Kurt Jaspers"],"raw_abstract":"The field of Explainable AI (XAI) is seeking to shed light on the inner\nworkings of complex AI models and uncover the rationale behind their decisions.\nOne of the models gaining attention are probabilistic circuits (PCs), which are\na general and unified framework for tractable probabilistic models that support\nefficient computation of various probabilistic queries. Probabilistic circuits\nguarantee inference that is polynomial in the size of the circuit. In this\npaper, we improve the explainability of probabilistic circuits by computing a\ncomprehensible, readable logical theory that covers the high-density regions\ngenerated by a PC. To achieve this, pruning approaches based on generative\nsignificance are used in a new method called PUTPUT (Probabilistic circuit\nUnderstanding Through Pruning Underlying logical Theories). The method is\napplied to a real world use case where music playlists are automatically\ngenerated and expressed as readable (database) queries. Evaluation shows that\nthis approach can effectively produce a comprehensible logical theory that\ndescribes the high-density regions of a PC and outperforms state of the art\nmethods when exploring the performance-comprehensibility trade-off.","publication_date":1700659185,"paper_link":"http://arxiv.org/pdf/2311.13379v1","categories":["Computer Science"],"abstract":"The field of Explainable AI (XAI) is seeking to shed light on the inner workings of complex AI models and uncover the rationale behind their decisions. One of the models gaining attention are probabilistic circuits (PCs), which are a general and unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries. Probabilistic circuits guarantee inference that is polynomial in the size of the circuit. In this paper, we improve the explainability of probabilistic circuits by computing a comprehensible, readable logical theory that covers the high-density regions generated by a PC. To achieve this, pruning approaches based on generative significance are used in a new method called PUTPUT (Probabilistic circuit Understanding Through Pruning Underlying logical Theories). The method is applied to a real world use case where music playlists are automatically generated and expressed as readable (database) queries. Evaluation shows that this approach can effectively produce a comprehensible logical theory that describes the high-density regions of a PC and outperforms state of the art methods when exploring the performance-comprehensibility trade-off."}
{"title":"Point Projection Mapping System for Tracking, Registering, Labeling and Validating Optical Tissue Measurements","authors":["Lianne Feenstra","Stefan D. van der Stel","Marcos Da Silva Guimaraes","Theo J. M Ruers","Behdad Dashtbozorg"],"raw_abstract":"Validation of newly developed optical tissue sensing techniques for tumor\ndetection during cancer surgery requires an accurate correlation with\nhistological results. Additionally, such accurate correlation facilitates\nprecise data labeling for developing high-performance machine-learning tissue\nclassification models. In this paper, a newly developed Point Projection\nMapping system will be introduced, which allows non-destructive tracking of the\nmeasurement locations on tissue specimens. Additionally, a framework for\naccurate registration, validation, and labeling with histopathology results is\nproposed and validated on a case study. The proposed framework provides a more\nrobust and accurate method for tracking and validation of optical tissue\nsensing techniques, which saves time and resources compared to conventional\ntechniques available.","publication_date":1700659181,"paper_link":"http://arxiv.org/pdf/2311.13378v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Validation of newly developed optical tissue sensing techniques for tumor detection during cancer surgery requires an accurate correlation with histological results. Additionally, such accurate correlation facilitates precise data labeling for developing high-performance machine-learning tissue classification models. In this paper, a newly developed Point Projection Mapping system will be introduced, which allows non-destructive tracking of the measurement locations on tissue specimens. Additionally, a framework for accurate registration, validation, and labeling with histopathology results is proposed and validated on a case study. The proposed framework provides a more robust and accurate method for tracking and validation of optical tissue sensing techniques, which saves time and resources compared to conventional techniques available."}
{"title":"Thermal Pair Production from Photon-Photon Collision: Breit-Wheeler Process at Finite Temperature","authors":["D. S. Cabral","A. F. Santos","R. Bufalo"],"raw_abstract":"In this paper we examine the pair production through the Breit-Wheeler\nprocess $\\gamma~\\gamma \\to e^+ e^-$ in a thermal background. We compute the\nthermal contribution to the Breit-Wheeler differential cross section within the\nthermofield dynamics formalism. We evaluate in details the cross section for\nthis process, which possess a surprisingly simple expression valid for any\ntemperature $\\beta$, from which we discuss some physically relevant aspects. We\nalso consider the high temperature regime of the cross section in order to have\na better understanding about its thermal behavior.","publication_date":1700659132,"paper_link":"http://arxiv.org/pdf/2311.13376v1","categories":["Physics"],"abstract":"In this paper we examine the pair production through the Breit-Wheeler process __FORMULA__ in a thermal background. We compute the thermal contribution to the Breit-Wheeler differential cross section within the thermofield dynamics formalism. We evaluate in details the cross section for this process, which possess a surprisingly simple expression valid for any temperature __FORMULA__, from which we discuss some physically relevant aspects. We also consider the high temperature regime of the cross section in order to have a better understanding about its thermal behavior."}
{"title":"MRGazer: Decoding Eye Gaze Points from Functional Magnetic Resonance Imaging in Individual Space","authors":["Xiuwen Wu","Rongjie Hu","Jie Liang","Yanming Wang","Bensheng Qiu","Xiaoxiao Wang"],"raw_abstract":"Eye-tracking research has proven valuable in understanding numerous cognitive\nfunctions. Recently, Frey et al. provided an exciting deep learning method for\nlearning eye movements from fMRI data. However, it needed to co-register fMRI\ninto standard space to obtain eyeballs masks, and thus required additional\ntemplates and was time consuming. To resolve this issue, in this paper, we\npropose a framework named MRGazer for predicting eye gaze points from fMRI in\nindividual space. The MRGazer consisted of eyeballs extraction module and a\nresidual network-based eye gaze prediction. Compared to the previous method,\nthe proposed framework skips the fMRI co-registration step, simplifies the\nprocessing protocol and achieves end-to-end eye gaze regression. The proposed\nmethod achieved superior performance in a variety of eye movement tasks than\nthe co-registration-based method, and delivered objective results within a\nshorter time (~ 0.02 Seconds for each volume) than prior method (~0.3 Seconds\nfor each volume).","publication_date":1700658799,"paper_link":"http://arxiv.org/pdf/2311.13372v1","categories":["Quantitative Biology"],"abstract":"Eye-tracking research has proven valuable in understanding numerous cognitive functions. Recently, Frey et al. provided an exciting deep learning method for learning eye movements from fMRI data. However, it needed to co-register fMRI into standard space to obtain eyeballs masks, and thus required additional templates and was time consuming. To resolve this issue, in this paper, we propose a framework named MRGazer for predicting eye gaze points from fMRI in individual space. The MRGazer consisted of eyeballs extraction module and a residual network-based eye gaze prediction. Compared to the previous method, the proposed framework skips the fMRI co-registration step, simplifies the processing protocol and achieves end-to-end eye gaze regression. The proposed method achieved superior performance in a variety of eye movement tasks than the co-registration-based method, and delivered objective results within a shorter time (~ 0.02 Seconds for each volume) than prior method (~0.3 Seconds for each volume)."}
{"title":"Model-based aberration corrected microscopy inside a glass tube","authors":["D. W. S. Cox","T. Knop","I. M. Vellekoop"],"raw_abstract":"Microscope objectives achieve near diffraction-limited performance only when\nused under the conditions they are designed for. In non-standard geometries,\nsuch as thick cover slips or curved surfaces, severe aberrations arise,\ninevitably impairing high-resolution imaging. Correcting such large aberrations\nusing standard adaptive optics can be challenging: existing solutions are\neither not suited for strong aberrations, or require extensive feedback\nmeasurements, consequently taking a significant portion of the photon budget.\nWe demonstrate that it is possible to pre-compute the corrections needed for\nhigh-resolution imaging inside a glass tube based on a priori information only.\nOur ray-tracing based method achieved over an order of magnitude increase in\nimage contrast without the need for a feedback signal.","publication_date":1700658078,"paper_link":"http://arxiv.org/pdf/2311.13363v1","categories":["Physics"],"abstract":"Microscope objectives achieve near diffraction-limited performance only when used under the conditions they are designed for. In non-standard geometries, such as thick cover slips or curved surfaces, severe aberrations arise, inevitably impairing high-resolution imaging. Correcting such large aberrations using standard adaptive optics can be challenging: existing solutions are either not suited for strong aberrations, or require extensive feedback measurements, consequently taking a significant portion of the photon budget. We demonstrate that it is possible to pre-compute the corrections needed for high-resolution imaging inside a glass tube based on a priori information only. Our ray-tracing based method achieved over an order of magnitude increase in image contrast without the need for a feedback signal."}
{"title":"Improvements in charged lepton and photon propagation for the software PROPOSAL","authors":["Jean-Marco Alameddine","Johannes Albrecht","Hans Dembinski","Pascal Gutjahr","Karl-Heinz Kampert","Wolfgang Rhode","Maximilian Sackel","Alexander Sandrock","Jan Soedingrekso"],"raw_abstract":"Accurate particle simulations are essential for the next generation of\nexperiments in astroparticle physics. The Monte Carlo simulation library\nPROPOSAL is a flexible tool to efficiently propagate high-energy leptons and\nphotons through large volumes of media, for example in the context of\nunderground observatories. It is written as a C++ library, including a Python\ninterface. In this paper, the most recent updates of PROPOSAL are described,\nincluding the addition of electron, positron, and photon propagation, for which\nnew interaction types have been implemented. This allows the usage of PROPOSAL\nto simulate electromagnetic particle cascades, for example in the context of\nair shower simulations. The precision of the propagation has been improved by\nincluding rare interaction processes, new photonuclear parametrizations,\ndeflections in stochastic interactions, and the possibility of propagating in\ninhomogeneous density distributions. Additional technical improvements\nregarding the interpolation routine and the propagation algorithm are\ndescribed.","publication_date":1700657326,"paper_link":"http://arxiv.org/pdf/2311.13357v1","categories":["Physics"],"abstract":"Accurate particle simulations are essential for the next generation of experiments in astroparticle physics. The Monte Carlo simulation library PROPOSAL is a flexible tool to efficiently propagate high-energy leptons and photons through large volumes of media, for example in the context of underground observatories. It is written as a C++ library, including a Python interface. In this paper, the most recent updates of PROPOSAL are described, including the addition of electron, positron, and photon propagation, for which new interaction types have been implemented. This allows the usage of PROPOSAL to simulate electromagnetic particle cascades, for example in the context of air shower simulations. The precision of the propagation has been improved by including rare interaction processes, new photonuclear parametrizations, deflections in stochastic interactions, and the possibility of propagating in inhomogeneous density distributions. Additional technical improvements regarding the interpolation routine and the propagation algorithm are described."}
{"title":"Uncertainty Estimation in Multi-Agent Distributed Learning","authors":["Gleb Radchenko","Victoria Andrea Fill"],"raw_abstract":"Traditionally, IoT edge devices have been perceived primarily as low-power\ncomponents with limited capabilities for autonomous operations. Yet, with\nemerging advancements in embedded AI hardware design, a foundational shift\npaves the way for future possibilities. Thus, the aim of the KDT NEUROKIT2E\nproject is to establish a new open-source framework to further facilitate AI\napplications on edge devices by developing new methods in quantization,\npruning-aware training, and sparsification. These innovations hold the\npotential to expand the functional range of such devices considerably, enabling\nthem to manage complex Machine Learning (ML) tasks utilizing local resources\nand laying the groundwork for innovative learning approaches.\n  In the context of 6G's transformative potential, distributed learning among\nindependent agents emerges as a pivotal application, attributed to 6G networks'\nsupport for ultra-reliable low-latency communication, enhanced data rates, and\nadvanced edge computing capabilities.\n  Our research focuses on the mechanisms and methodologies that allow edge\nnetwork-enabled agents to engage in collaborative learning in distributed\nenvironments. Particularly, one of the key issues within distributed\ncollaborative learning is determining the degree of confidence in the learning\nresults, considering the spatio-temporal locality of data sets perceived by\nindependent agents.","publication_date":1700657300,"paper_link":"http://arxiv.org/pdf/2311.13356v1","categories":["Computer Science"],"abstract":"Traditionally, IoT edge devices have been perceived primarily as low-power components with limited capabilities for autonomous operations. Yet, with emerging advancements in embedded AI hardware design, a foundational shift paves the way for future possibilities. Thus, the aim of the KDT NEUROKIT2E project is to establish a new open-source framework to further facilitate AI applications on edge devices by developing new methods in quantization, pruning-aware training, and sparsification. These innovations hold the potential to expand the functional range of such devices considerably, enabling them to manage complex Machine Learning (ML) tasks utilizing local resources and laying the groundwork for innovative learning approaches.   In the context of 6G's transformative potential, distributed learning among independent agents emerges as a pivotal application, attributed to 6G networks' support for ultra-reliable low-latency communication, enhanced data rates, and advanced edge computing capabilities.   Our research focuses on the mechanisms and methodologies that allow edge network-enabled agents to engage in collaborative learning in distributed environments. Particularly, one of the key issues within distributed collaborative learning is determining the degree of confidence in the learning results, considering the spatio-temporal locality of data sets perceived by independent agents."}
{"title":"Unified Classification and Rejection: A One-versus-All Framework","authors":["Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"raw_abstract":"Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performing poorly in rejecting OOD. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K + 1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K + 1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network.\nExperiments on popular OSR and OOD detection datasets demonstrate that the\nproposed framework, using a single multi-class classifier, yields competitive\nperformance in closed-set classification, OOD detection, and misclassification\ndetection.","publication_date":1700657232,"paper_link":"http://arxiv.org/pdf/2311.13355v1","categories":["Computer Science"],"abstract":"Classifying patterns of known classes and rejecting ambiguous and novel (also called as out-of-distribution (OOD)) inputs are involved in open world pattern recognition. Deep neural network models usually excel in closed-set classification while performing poorly in rejecting OOD. To tackle this problem, numerous methods have been designed to perform open set recognition (OSR) or OOD rejection/detection tasks. Previous methods mostly take post-training score transformation or hybrid models to ensure low scores on OOD inputs while separating known classes. In this paper, we attempt to build a unified framework for building open set classifiers for both classification and OOD rejection. We formulate the open set recognition of __FORMULA__-known-class as a __FORMULA__-class classification problem with model trained on known-class samples only. By decomposing the __FORMULA__-class problem into __FORMULA__ one-versus-all (OVA) binary classification tasks and binding some parameters, we show that combining the scores of OVA classifiers can give __FORMULA__-class posterior probabilities, which enables classification and OOD rejection in a unified framework. To maintain the closed-set classification accuracy of the OVA trained classifier, we propose a hybrid training strategy combining OVA loss and multi-class cross-entropy loss. We implement the OVA framework and hybrid training strategy on the recently proposed convolutional prototype network. Experiments on popular OSR and OOD detection datasets demonstrate that the proposed framework, using a single multi-class classifier, yields competitive performance in closed-set classification, OOD detection, and misclassification detection."}
{"title":"Durable, ultrathin, and antifouling polymer brush coating for efficient condensation heat transfer","authors":["Shuai Li","Cheuk Wing Edmond Lam","Matteo Donati","Kartik Regulagadda","Emre Yavuz","Till Pfeiffer","Panagiotis Sarkiris","Evangelos Gogolides","Athanasios Milionis","Dimos Poulikakos","Hans-J\u00fcrgen Butt","Michael Kappl"],"raw_abstract":"Heat exchangers are made of metals because of their high heat conductivity\nand mechanical stability. Metal surfaces are inherently hydrophilic, leading to\ninefficient filmwise condensation. It is still a challenge to coat these metal\nsurfaces with a durable, robust and thin hydrophobic layer, which is required\nfor efficient dropwise condensation. Here, we report the non-structured and\nultrathin (~6 nm) polydimethylsiloxane (PDMS) brushes on copper that sustain\nhigh-performing dropwise condensation in high supersaturation. Due to the\nflexible hydrophobic siloxane polymer chains, the coating has low resistance to\ndrop sliding and excellent chemical stability. The PDMS brushes can sustain\ndropwise condensation for up to ~8 h during exposure to 111 {\\deg}C saturated\nsteam flowing at 3 m/s, with a 5-7 times higher heat transfer coefficient\ncompared to filmwise condensation. The surface is self-cleaning and can reduce\nbacterial attachment by 99%. This low-cost, facile, fluorine-free, and scalable\nmethod is suitable for a great variety of condensation heat transfer\napplications.","publication_date":1700657118,"paper_link":"http://arxiv.org/pdf/2311.13353v1","categories":["Physics"],"abstract":"Heat exchangers are made of metals because of their high heat conductivity and mechanical stability. Metal surfaces are inherently hydrophilic, leading to inefficient filmwise condensation. It is still a challenge to coat these metal surfaces with a durable, robust and thin hydrophobic layer, which is required for efficient dropwise condensation. Here, we report the non-structured and ultrathin (~6 nm) polydimethylsiloxane (PDMS) brushes on copper that sustain high-performing dropwise condensation in high supersaturation. Due to the flexible hydrophobic siloxane polymer chains, the coating has low resistance to drop sliding and excellent chemical stability. The PDMS brushes can sustain dropwise condensation for up to ~8 h during exposure to 111 {\\deg}C saturated steam flowing at 3 m/s, with a 5-7 times higher heat transfer coefficient compared to filmwise condensation. The surface is self-cleaning and can reduce bacterial attachment by 99%. This low-cost, facile, fluorine-free, and scalable method is suitable for a great variety of condensation heat transfer applications."}
{"title":"Gradual Verification for Smart Contracts","authors":["Haojia Sun","Kunal Singh","Jan-Paul Ramos-D\u00e1vila","Jonathan Aldrich","Jenna DiVincenzo"],"raw_abstract":"Blockchains facilitate secure resource transactions through smart contracts,\nyet these digital agreements are prone to vulnerabilities, particularly when\ninteracting with external contracts, leading to substantial monetary losses.\nTraditional verification techniques fall short in providing comprehensive\nsecurity assurances, especially against re-entrancy attacks, due to the\nunavailable implementations of external contracts. This paper introduces an\nincremental approach: gradual verification. We combine static and dynamic\nverification techniques to enhance security, guarantee soundness and\nflexibility, and optimize resource usage in smart contract interactions. By\nimplementing a prototype for gradually verifying Algorand smart contracts via\nthe pyTEAL language, we demonstrate the effectiveness of our approach,\ncontributing to the safe and efficient execution of smart contracts.","publication_date":1700656946,"paper_link":"http://arxiv.org/pdf/2311.13351v1","categories":["Computer Science"],"abstract":"Blockchains facilitate secure resource transactions through smart contracts, yet these digital agreements are prone to vulnerabilities, particularly when interacting with external contracts, leading to substantial monetary losses. Traditional verification techniques fall short in providing comprehensive security assurances, especially against re-entrancy attacks, due to the unavailable implementations of external contracts. This paper introduces an incremental approach: gradual verification. We combine static and dynamic verification techniques to enhance security, guarantee soundness and flexibility, and optimize resource usage in smart contract interactions. By implementing a prototype for gradually verifying Algorand smart contracts via the pyTEAL language, we demonstrate the effectiveness of our approach, contributing to the safe and efficient execution of smart contracts."}
{"title":"Fact-based Court Judgment Prediction","authors":["Shubham Kumar Nigam","Aniket Deroy"],"raw_abstract":"This extended abstract extends the research presented in \"ILDC for CJPE:\nIndian Legal Documents Corpus for Court Judgment Prediction and Explanation\"\n\\cite{malik-etal-2021-ildc}, focusing on fact-based judgment prediction within\nthe context of Indian legal documents. We introduce two distinct problem\nvariations: one based solely on facts, and another combining facts with rulings\nfrom lower courts (RLC). Our research aims to enhance early-phase case outcome\nprediction, offering significant benefits to legal professionals and the\ngeneral public. The results, however, indicated a performance decline compared\nto the original ILDC for CJPE study, even after implementing various weightage\nschemes in our DELSumm algorithm. Additionally, using only facts for legal\njudgment prediction with different transformer models yielded results inferior\nto the state-of-the-art outcomes reported in the \"ILDC for CJPE\" study.","publication_date":1700656768,"paper_link":"http://arxiv.org/pdf/2311.13350v1","categories":["Computer Science"],"abstract":"This extended abstract extends the research presented in \"ILDC for CJPE: Indian Legal Documents Corpus for Court Judgment Prediction and Explanation\" malik-etal-2021-ildc, focusing on fact-based judgment prediction within the context of Indian legal documents. We introduce two distinct problem variations: one based solely on facts, and another combining facts with rulings from lower courts (RLC). Our research aims to enhance early-phase case outcome prediction, offering significant benefits to legal professionals and the general public. The results, however, indicated a performance decline compared to the original ILDC for CJPE study, even after implementing various weightage schemes in our DELSumm algorithm. Additionally, using only facts for legal judgment prediction with different transformer models yielded results inferior to the state-of-the-art outcomes reported in the \"ILDC for CJPE\" study."}
{"title":"REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints","authors":["Francesco Corti","Balz Maag","Joachim Schauer","Ulrich Pferschy","Olga Saukh"],"raw_abstract":"Deep models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models, not capable to\nadapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks\n(REDS) to tackle model adaptation to variable resources. In contrast to the\nstate-of-the-art, REDS use structured sparsity constructively by exploiting\npermutation invariance of neurons, which allows for hardware-specific\noptimizations. Specifically, REDS achieve computational efficiency by (1)\nskipping sequential computational blocks identified by a novel iterative\nknapsack optimizer, and (2) leveraging simple math to re-arrange the order of\noperations in REDS computational graph to take advantage of the data cache.\nREDS support conventional deep networks frequently deployed on the edge and\nprovide computational benefits even for small and simple networks. We evaluate\nREDS on six benchmark architectures trained on the Google Speech Commands,\nFMNIST and CIFAR10 datasets, and test on four off-the-shelf mobile and embedded\nhardware platforms. We provide a theoretical result and empirical evidence for\nREDS outstanding performance in terms of submodels' test set accuracy, and\ndemonstrate an adaptation time in response to dynamic resource constraints of\nunder 40$\\mu$s, utilizing a 2-layer fully-connected network on Arduino Nano 33\nBLE Sense.","publication_date":1700656491,"paper_link":"http://arxiv.org/pdf/2311.13349v1","categories":["Computer Science"],"abstract":"Deep models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models, not capable to adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS use structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieve computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) leveraging simple math to re-arrange the order of operations in REDS computational graph to take advantage of the data cache. REDS support conventional deep networks frequently deployed on the edge and provide computational benefits even for small and simple networks. We evaluate REDS on six benchmark architectures trained on the Google Speech Commands, FMNIST and CIFAR10 datasets, and test on four off-the-shelf mobile and embedded hardware platforms. We provide a theoretical result and empirical evidence for REDS outstanding performance in terms of submodels' test set accuracy, and demonstrate an adaptation time in response to dynamic resource constraints of under 40__FORMULA__s, utilizing a 2-layer fully-connected network on Arduino Nano 33 BLE Sense."}
{"title":"MergeSFL: Split Federated Learning with Feature Merging and Batch Size Regulation","authors":["Yunming Liao","Yang Xu","Hongli Xu","Lun Wang","Zhiwei Yao","Chunming Qiao"],"raw_abstract":"Recently, federated learning (FL) has emerged as a popular technique for edge\nAI to mine valuable knowledge in edge computing (EC) systems. To mitigate the\ncomputing/communication burden on resource-constrained workers and protect\nmodel privacy, split federated learning (SFL) has been released by integrating\nboth data and model parallelism. Despite resource limitations, SFL still faces\ntwo other critical challenges in EC, i.e., statistical heterogeneity and system\nheterogeneity. To address these challenges, we propose a novel SFL framework,\ntermed MergeSFL, by incorporating feature merging and batch size regulation in\nSFL. Concretely, feature merging aims to merge the features from workers into a\nmixed feature sequence, which is approximately equivalent to the features\nderived from IID data and is employed to promote model accuracy. While batch\nsize regulation aims to assign diverse and suitable batch sizes for\nheterogeneous workers to improve training efficiency. Moreover, MergeSFL\nexplores to jointly optimize these two strategies upon their coupled\nrelationship to better enhance the performance of SFL. Extensive experiments\nare conducted on a physical platform with 80 NVIDIA Jetson edge devices, and\nthe experimental results show that MergeSFL can improve the final model\naccuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared\nto the baselines.","publication_date":1700655902,"paper_link":"http://arxiv.org/pdf/2311.13348v1","categories":["Computer Science"],"abstract":"Recently, federated learning (FL) has emerged as a popular technique for edge AI to mine valuable knowledge in edge computing (EC) systems. To mitigate the computing/communication burden on resource-constrained workers and protect model privacy, split federated learning (SFL) has been released by integrating both data and model parallelism. Despite resource limitations, SFL still faces two other critical challenges in EC, i.e., statistical heterogeneity and system heterogeneity. To address these challenges, we propose a novel SFL framework, termed MergeSFL, by incorporating feature merging and batch size regulation in SFL. Concretely, feature merging aims to merge the features from workers into a mixed feature sequence, which is approximately equivalent to the features derived from IID data and is employed to promote model accuracy. While batch size regulation aims to assign diverse and suitable batch sizes for heterogeneous workers to improve training efficiency. Moreover, MergeSFL explores to jointly optimize these two strategies upon their coupled relationship to better enhance the performance of SFL. Extensive experiments are conducted on a physical platform with 80 NVIDIA Jetson edge devices, and the experimental results show that MergeSFL can improve the final model accuracy by 5.82% to 26.22%, with a speedup by about 1.74x to 4.14x, compared to the baselines."}
{"title":"Rashba-splitting-induced topological flat band detected by anomalous resistance oscillations beyond the quantum limit in ZrTe$_5$","authors":["Dong Xing","Bingbing Tong","Senyang Pan","Zezhi Wang","Jianlin Luo","Jinglei Zhang","Cheng-Long Zhang"],"raw_abstract":"Topological flat band, on which the kinetic energy of topological electrons\nis quenched, represents a platform for investigating the topological properties\nof correlated systems. Recent experimental studies on flattened electronic\nbands have mainly concentrated on 2-dimensional materials created by van der\nWaals heterostructure-based engineering. Here, we report the observation of a\ntopological flat band formed by polar-distortion-assisted Rashba splitting in a\n3-dimensional Dirac material ZrTe$_5$. The polar distortion and resulting\nRashba splitting on the band are directly detected by torque magnetometry and\nthe anomalous Hall effect, respectively. The local symmetry breaking further\nflattens the band, on which we observe resistance oscillations beyond the\nquantum limit. These oscillations follow the temperature dependence of the\nLifshitz-Kosevich formula but are evenly distributed in B instead of 1/B in\nhigh magnetic fields. Furthermore, the cyclotron mass anomalously gets enhanced\nabout 10$^2$ times at field ~20 T. These anomalous properties of oscillations\noriginate from a topological flat band with quenched kinetic energy. The\ntopological flat band, realized by polar-distortion-assisted Rashba splitting\nin the 3-dimensional Dirac system ZrTe$_5$, signifies an intrinsic platform\nwithout invoking moir\\'e or order-stacking engineering, and also opens the door\nfor studying topologically correlated phenomena beyond the dimensionality of\ntwo.","publication_date":1700655676,"paper_link":"http://arxiv.org/pdf/2311.13346v1","categories":["Physics"],"abstract":"Topological flat band, on which the kinetic energy of topological electrons is quenched, represents a platform for investigating the topological properties of correlated systems. Recent experimental studies on flattened electronic bands have mainly concentrated on 2-dimensional materials created by van der Waals heterostructure-based engineering. Here, we report the observation of a topological flat band formed by polar-distortion-assisted Rashba splitting in a 3-dimensional Dirac material ZrTe__FORMULA__. The polar distortion and resulting Rashba splitting on the band are directly detected by torque magnetometry and the anomalous Hall effect, respectively. The local symmetry breaking further flattens the band, on which we observe resistance oscillations beyond the quantum limit. These oscillations follow the temperature dependence of the Lifshitz-Kosevich formula but are evenly distributed in B instead of 1/B in high magnetic fields. Furthermore, the cyclotron mass anomalously gets enhanced about 10__FORMULA__ times at field ~20 T. These anomalous properties of oscillations originate from a topological flat band with quenched kinetic energy. The topological flat band, realized by polar-distortion-assisted Rashba splitting in the 3-dimensional Dirac system ZrTe__FORMULA__, signifies an intrinsic platform without invoking moir\\'e or order-stacking engineering, and also opens the door for studying topologically correlated phenomena beyond the dimensionality of two."}
{"title":"Directed Evolution of Microorganisms for Engineered Living Materials","authors":["Julie M. Laurent","Ankit Jain","Anton Kan","Mathias Steinacher","Nadia Enrriquez Casimiro","Stavros Stavrakis","Andrew J. deMello","Andr\u00e9 R. Studart"],"raw_abstract":"Microorganisms can create engineered materials with exquisite structures and\nliving functionalities. Although synthetic biology tools to genetically\nmanipulate microorganisms continue to expand, the bottom-up rational design of\nengineered living materials still relies on prior knowledge of\ngenotype-phenotype links for the function of interest. Here, we utilize a\nhigh-throughput directed evolution platform to enhance the fitness of whole\nmicroorganisms under selection pressure and identify novel genetic pathways to\nprogram the functionalities of engineered living materials. Using\nKomagataeibacter sucrofermentans as a model cellulose-producing microorganism,\nwe show that our droplet-based microfluidic platform enables the directed\nevolution of these bacteria towards a small number of cellulose overproducers\nfrom an initial pool of 40'000 random mutants. Sequencing of the evolved\nstrains reveals an unexpected link between the cellulose-forming ability of the\nbacteria and a gene encoding a protease complex responsible for protein\nturnover in the cell. The ability to enhance the fitness of microorganisms\ntowards specific phenotypes and to discover new genotype-phenotype links makes\nthis high-throughput directed evolution platform a promising tool for the\ndevelopment of the next generation of engineered living materials.","publication_date":1700655124,"paper_link":"http://arxiv.org/pdf/2311.13342v1","categories":["Quantitative Biology","Physics"],"abstract":"Microorganisms can create engineered materials with exquisite structures and living functionalities. Although synthetic biology tools to genetically manipulate microorganisms continue to expand, the bottom-up rational design of engineered living materials still relies on prior knowledge of genotype-phenotype links for the function of interest. Here, we utilize a high-throughput directed evolution platform to enhance the fitness of whole microorganisms under selection pressure and identify novel genetic pathways to program the functionalities of engineered living materials. Using Komagataeibacter sucrofermentans as a model cellulose-producing microorganism, we show that our droplet-based microfluidic platform enables the directed evolution of these bacteria towards a small number of cellulose overproducers from an initial pool of 40'000 random mutants. Sequencing of the evolved strains reveals an unexpected link between the cellulose-forming ability of the bacteria and a gene encoding a protease complex responsible for protein turnover in the cell. The ability to enhance the fitness of microorganisms towards specific phenotypes and to discover new genotype-phenotype links makes this high-throughput directed evolution platform a promising tool for the development of the next generation of engineered living materials."}
{"title":"High-Quality Face Caricature via Style Translation","authors":["Lamyanba Laishram","Muhammad Shaheryar","Jong Taek Lee","Soon Ki Jung"],"raw_abstract":"Caricature is an exaggerated form of artistic portraiture that accentuates\nunique yet subtle characteristics of human faces. Recently, advancements in\ndeep end-to-end techniques have yielded encouraging outcomes in capturing both\nstyle and elevated exaggerations in creating face caricatures. Most of these\napproaches tend to produce cartoon-like results that could be more practical\nfor real-world applications. In this study, we proposed a high-quality,\nunpaired face caricature method that is appropriate for use in the real world\nand uses computer vision techniques and GAN models. We attain the exaggeration\nof facial features and the stylization of appearance through a two-step\nprocess: Face caricature generation and face caricature projection. The face\ncaricature generation step creates new caricature face datasets from real\nimages and trains a generative model using the real and newly created\ncaricature datasets. The Face caricature projection employs an encoder trained\nwith real and caricature faces with the pretrained generator to project real\nand caricature faces. We perform an incremental facial exaggeration from the\nreal image to the caricature faces using the encoder and generator's latent\nspace. Our projection preserves the facial identity, attributes, and\nexpressions from the input image. Also, it accounts for facial occlusions, such\nas reading glasses or sunglasses, to enhance the robustness of our model.\nFurthermore, we conducted a comprehensive comparison of our approach with\nvarious state-of-the-art face caricature methods, highlighting our process's\ndistinctiveness and exceptional realism.","publication_date":1700654613,"paper_link":"http://arxiv.org/pdf/2311.13338v1","categories":["Computer Science"],"abstract":"Caricature is an exaggerated form of artistic portraiture that accentuates unique yet subtle characteristics of human faces. Recently, advancements in deep end-to-end techniques have yielded encouraging outcomes in capturing both style and elevated exaggerations in creating face caricatures. Most of these approaches tend to produce cartoon-like results that could be more practical for real-world applications. In this study, we proposed a high-quality, unpaired face caricature method that is appropriate for use in the real world and uses computer vision techniques and GAN models. We attain the exaggeration of facial features and the stylization of appearance through a two-step process: Face caricature generation and face caricature projection. The face caricature generation step creates new caricature face datasets from real images and trains a generative model using the real and newly created caricature datasets. The Face caricature projection employs an encoder trained with real and caricature faces with the pretrained generator to project real and caricature faces. We perform an incremental facial exaggeration from the real image to the caricature faces using the encoder and generator's latent space. Our projection preserves the facial identity, attributes, and expressions from the input image. Also, it accounts for facial occlusions, such as reading glasses or sunglasses, to enhance the robustness of our model. Furthermore, we conducted a comprehensive comparison of our approach with various state-of-the-art face caricature methods, highlighting our process's distinctiveness and exceptional realism."}
{"title":"Vast TVB parameter space exploration: A Modular Framework for Accelerating the Multi-Scale Simulation of Human Brain Dynamics","authors":["Michiel van der Vlag","Lionel Kusch","Alain Destexhe","Viktor Jirsa","Sandra Diaz-Pier","Jennifer S. Goldman"],"raw_abstract":"Global neural dynamics emerge from multi-scale brain structures, with neurons\ncommunicating through synapses to form transiently communicating networks.\nNetwork activity arises from intercellular communication that depends on the\nstructure of connectome tracts and local connection, intracellular signalling\ncascades, and the extracellular molecular milieu that regulate cellular\nproperties. Multi-scale models of brain function have begun to directly link\nthe emergence of global brain dynamics in conscious and unconscious brain\nstates to microscopic changes at the level of cells. In particular, AdEx\nmean-field models representing statistical properties of local populations of\nneurons have been connected following human tractography data to represent\nmulti-scale neural phenomena in simulations using The Virtual Brain (TVB).\nWhile mean-field models can be run on personal computers for short simulations,\nor in parallel on high-performance computing (HPC) architectures for longer\nsimulations and parameter scans, the computational burden remains high and vast\nareas of the parameter space remain unexplored. In this work, we report that\nour TVB-HPC framework, a modular set of methods used here to implement the\nTVB-AdEx model for GPU and analyze emergent dynamics, notably accelerates\nsimulations and substantially reduces computational resource requirements. The\nframework preserves the stability and robustness of the TVB-AdEx model, thus\nfacilitating finer resolution exploration of vast parameter spaces as well as\nlonger simulations previously near impossible to perform. Given that simulation\nand analysis toolkits are made public as open-source packages, our framework\nserves as a template onto which other models can be easily scripted and\npersonalized datasets can be used for studies of inter-individual variability\nof parameters related to functional brain dynamics.","publication_date":1700654493,"paper_link":"http://arxiv.org/pdf/2311.13337v1","categories":["Quantitative Biology"],"abstract":"Global neural dynamics emerge from multi-scale brain structures, with neurons communicating through synapses to form transiently communicating networks. Network activity arises from intercellular communication that depends on the structure of connectome tracts and local connection, intracellular signalling cascades, and the extracellular molecular milieu that regulate cellular properties. Multi-scale models of brain function have begun to directly link the emergence of global brain dynamics in conscious and unconscious brain states to microscopic changes at the level of cells. In particular, AdEx mean-field models representing statistical properties of local populations of neurons have been connected following human tractography data to represent multi-scale neural phenomena in simulations using The Virtual Brain (TVB). While mean-field models can be run on personal computers for short simulations, or in parallel on high-performance computing (HPC) architectures for longer simulations and parameter scans, the computational burden remains high and vast areas of the parameter space remain unexplored. In this work, we report that our TVB-HPC framework, a modular set of methods used here to implement the TVB-AdEx model for GPU and analyze emergent dynamics, notably accelerates simulations and substantially reduces computational resource requirements. The framework preserves the stability and robustness of the TVB-AdEx model, thus facilitating finer resolution exploration of vast parameter spaces as well as longer simulations previously near impossible to perform. Given that simulation and analysis toolkits are made public as open-source packages, our framework serves as a template onto which other models can be easily scripted and personalized datasets can be used for studies of inter-individual variability of parameters related to functional brain dynamics."}
{"title":"Quantum learning and essential cognition under the traction of meta-characteristics in an open world","authors":["Jin Wang","Changlin Song"],"raw_abstract":"Artificial intelligence has made significant progress in the Close World\nproblem, being able to accurately recognize old knowledge through training and\nclassification. However, AI faces significant challenges in the Open World\nproblem, as it involves a new and unknown exploration journey. AI is not\ninherently proactive in exploration, and its challenge lies in not knowing how\nto approach and adapt to the unknown world. How do humans acquire knowledge of\nthe unknown world. Humans identify new knowledge through intrinsic cognition.\nIn the process of recognizing new colors, the cognitive cues are different from\nknown color features and involve hue, saturation, brightness, and other\ncharacteristics. When AI encounters objects with different features in the new\nworld, it faces another challenge: where are the distinguishing features\nbetween influential features of new and old objects? AI often mistakes a new\nworld's brown bear for a known dog because it has not learned the differences\nin feature distributions between knowledge systems. This is because things in\nthe new and old worlds have different units and dimensions for their features.\nThis paper proposes an open-world model and elemental feature system that\nfocuses on fundamentally recognizing the distribution differences in objective\nfeatures between the new and old worlds. The quantum tunneling effect of\nlearning ability in the new and old worlds is realized through the tractive\nforce of meta-characteristic. The outstanding performance of the model system\nin learning new knowledge (using pedestrian re-identification datasets as an\nexample) demonstrates that AI has acquired the ability to recognize the new\nworld with an accuracy of $96.71\\%$ at most and has gained the capability to\nexplore new knowledge, similar to humans.","publication_date":1700654141,"paper_link":"http://arxiv.org/pdf/2311.13335v1","categories":["Computer Science"],"abstract":"Artificial intelligence has made significant progress in the Close World problem, being able to accurately recognize old knowledge through training and classification. However, AI faces significant challenges in the Open World problem, as it involves a new and unknown exploration journey. AI is not inherently proactive in exploration, and its challenge lies in not knowing how to approach and adapt to the unknown world. How do humans acquire knowledge of the unknown world. Humans identify new knowledge through intrinsic cognition. In the process of recognizing new colors, the cognitive cues are different from known color features and involve hue, saturation, brightness, and other characteristics. When AI encounters objects with different features in the new world, it faces another challenge: where are the distinguishing features between influential features of new and old objects? AI often mistakes a new world's brown bear for a known dog because it has not learned the differences in feature distributions between knowledge systems. This is because things in the new and old worlds have different units and dimensions for their features. This paper proposes an open-world model and elemental feature system that focuses on fundamentally recognizing the distribution differences in objective features between the new and old worlds. The quantum tunneling effect of learning ability in the new and old worlds is realized through the tractive force of meta-characteristic. The outstanding performance of the model system in learning new knowledge (using pedestrian re-identification datasets as an example) demonstrates that AI has acquired the ability to recognize the new world with an accuracy of __FORMULA__ at most and has gained the capability to explore new knowledge, similar to humans."}
{"title":"MagGen: A graph aided deep generative model for inverse design of stable, permanent magnets","authors":["Sourav Mal","Gaurav Seal","Prasenjit Sen"],"raw_abstract":"A significant development towards inverse design of materials with\nwell-defined target properties is reported. A deep generative model based on\nvariational autoencoder (VAE), conditioned simultaneously by two target\nproperties, is developed to inverse design stable magnetic materials. Structure\nof the physics informed, property embedded latent space of the model is\nanalyzed using graph theory, based on the idea of similarity index. The graph\nidea is shown to be useful for generating new materials that are likely to\nsatisfy target properties. An impressive ~96% of the generated materials is\nfound to satisfy the target properties as per predictions from the target\nlearning branches. This is a huge improvement over approaches that do not\ncondition the VAE latent space by target properties, or do not consider\nconnectivity of the parent materials perturbing which the new materials are\ngenerated. In such models, the fraction of materials satisfying targets can be\nas low as ~5%. This impressive feat is achieved using a simple real-space only\nrepresentation called Invertible Real-space Crystallographic Representation\n(IRCR), that can be directly read from material cif files. Model predictions\nare finally validated by performing DFT calculations on a randomly chosen\nsubset of materials. Performance of the present model using IRCR is comparable\nor superior to that of the models reported earlier. This model for magnetic\nmaterial generation, MagGen, is applied to the problem of designing rare earth\nfree permanent magnets with promising results.","publication_date":1700653817,"paper_link":"http://arxiv.org/pdf/2311.13328v1","categories":["Physics"],"abstract":"A significant development towards inverse design of materials with well-defined target properties is reported. A deep generative model based on variational autoencoder (VAE), conditioned simultaneously by two target properties, is developed to inverse design stable magnetic materials. Structure of the physics informed, property embedded latent space of the model is analyzed using graph theory, based on the idea of similarity index. The graph idea is shown to be useful for generating new materials that are likely to satisfy target properties. An impressive ~96% of the generated materials is found to satisfy the target properties as per predictions from the target learning branches. This is a huge improvement over approaches that do not condition the VAE latent space by target properties, or do not consider connectivity of the parent materials perturbing which the new materials are generated. In such models, the fraction of materials satisfying targets can be as low as ~5%. This impressive feat is achieved using a simple real-space only representation called Invertible Real-space Crystallographic Representation (IRCR), that can be directly read from material cif files. Model predictions are finally validated by performing DFT calculations on a randomly chosen subset of materials. Performance of the present model using IRCR is comparable or superior to that of the models reported earlier. This model for magnetic material generation, MagGen, is applied to the problem of designing rare earth free permanent magnets with promising results."}
{"title":"Revisiting Supervision for Continual Representation Learning","authors":["Daniel Marczak","Sebastian Cygert","Tomasz Trzci\u0144ski","Bart\u0142omiej Twardowski"],"raw_abstract":"In the field of continual learning, models are designed to learn tasks one\nafter the other. While most research has centered on supervised continual\nlearning, recent studies have highlighted the strengths of self-supervised\ncontinual representation learning. The improved transferability of\nrepresentations built with self-supervised methods is often associated with the\nrole played by the multi-layer perceptron projector. In this work, we depart\nfrom this observation and reexamine the role of supervision in continual\nrepresentation learning. We reckon that additional information, such as human\nannotations, should not deteriorate the quality of representations. Our\nfindings show that supervised models when enhanced with a multi-layer\nperceptron head, can outperform self-supervised models in continual\nrepresentation learning.","publication_date":1700652244,"paper_link":"http://arxiv.org/pdf/2311.13321v1","categories":["Computer Science"],"abstract":"In the field of continual learning, models are designed to learn tasks one after the other. While most research has centered on supervised continual learning, recent studies have highlighted the strengths of self-supervised continual representation learning. The improved transferability of representations built with self-supervised methods is often associated with the role played by the multi-layer perceptron projector. In this work, we depart from this observation and reexamine the role of supervision in continual representation learning. We reckon that additional information, such as human annotations, should not deteriorate the quality of representations. Our findings show that supervised models when enhanced with a multi-layer perceptron head, can outperform self-supervised models in continual representation learning."}
{"title":"Brownian motion of droplets induced by thermal noise","authors":["Haodong Zhang","Fei Wang","Lorenz Ratke","Britta Nestler"],"raw_abstract":"Brownian motion (BM) is pivotal in natural science for the stochastic motion\nof microscopic droplets. In this study, we investigate BM driven by thermal\ncomposition noise at sub-micro scales, where inter-molecular diffusion and\nsurface tension both are significant. To address BM of microscopic droplets, we\ndevelop two stochastic multi-phase-field models coupled with the full\nNavier-Stokes equation, namely Allen-Cahn-Navier-Stokes (ACNS) and\nCahn-Hilliard-Navier-Stokes (CHNS). Both models are validated against capillary\nwave theory; the Einstein's relation for the Brownian coefficient at\nthermodynamic equilibrium is recovered. Moreover, by adjusting the co-action of\nthe diffusion, Marangoni effect, and viscous friction, two non-equilibrium\nphenomena are observed. (I) The droplet motion transits from the Brownian to\nBallistic with increasing Marangoni effect which is emanated from the energy\ndissipation mechanism distinct from the conventional fluctuation-dissipation\ntheorem. (II) The deterministic droplet motion is triggered by the noise\ninduced non-uniform velocity field which leads to a novel droplet coalescence\nmechanism associated with the thermal noise.","publication_date":1700652058,"paper_link":"http://arxiv.org/pdf/2311.13320v1","categories":["Physics"],"abstract":"Brownian motion (BM) is pivotal in natural science for the stochastic motion of microscopic droplets. In this study, we investigate BM driven by thermal composition noise at sub-micro scales, where inter-molecular diffusion and surface tension both are significant. To address BM of microscopic droplets, we develop two stochastic multi-phase-field models coupled with the full Navier-Stokes equation, namely Allen-Cahn-Navier-Stokes (ACNS) and Cahn-Hilliard-Navier-Stokes (CHNS). Both models are validated against capillary wave theory; the Einstein's relation for the Brownian coefficient at thermodynamic equilibrium is recovered. Moreover, by adjusting the co-action of the diffusion, Marangoni effect, and viscous friction, two non-equilibrium phenomena are observed. (I) The droplet motion transits from the Brownian to Ballistic with increasing Marangoni effect which is emanated from the energy dissipation mechanism distinct from the conventional fluctuation-dissipation theorem. (II) The deterministic droplet motion is triggered by the noise induced non-uniform velocity field which leads to a novel droplet coalescence mechanism associated with the thermal noise."}
{"title":"Deep Learning for Vascular Segmentation and Applications in Phase Contrast Tomography Imaging","authors":["Ekin Yagis","Shahab Aslani","Yashvardhan Jain","Yang Zhou","Shahrokh Rahmani","Joseph Brunet","Alexandre Bellier","Christopher Werlein","Maximilian Ackermann","Danny Jonigk","Paul Tafforeau","Peter D Lee","Claire Walsh"],"raw_abstract":"Automated blood vessel segmentation is vital for biomedical imaging, as\nvessel changes indicate many pathologies. Still, precise segmentation is\ndifficult due to the complexity of vascular structures, anatomical variations\nacross patients, the scarcity of annotated public datasets, and the quality of\nimages. We present a thorough literature review, highlighting the state of\nmachine learning techniques across diverse organs. Our goal is to provide a\nfoundation on the topic and identify a robust baseline model for application to\nvascular segmentation in a new imaging modality, Hierarchical Phase Contrast\nTomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation\nFacility, HiP CT enables 3D imaging of complete organs at an unprecedented\nresolution of ca. 20mm per voxel, with the capability for localized zooms in\nselected regions down to 1mm per voxel without sectioning. We have created a\ntraining dataset with double annotator validated vascular data from three\nkidneys imaged with HiP CT in the context of the Human Organ Atlas Project.\nFinally, utilising the nnU Net model, we conduct experiments to assess the\nmodels performance on both familiar and unseen samples, employing vessel\nspecific metrics. Our results show that while segmentations yielded reasonably\nhigh scores such as clDice values ranging from 0.82 to 0.88, certain errors\npersisted. Large vessels that collapsed due to the lack of hydrostatic pressure\n(HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased\nconnectivity in finer vessels and higher segmentation errors at vessel\nboundaries were observed. Such errors obstruct the understanding of the\nstructures by interrupting vascular tree connectivity. Through our review and\noutputs, we aim to set a benchmark for subsequent model evaluations using\nvarious modalities, especially with the HiP CT imaging database.","publication_date":1700651738,"paper_link":"http://arxiv.org/pdf/2311.13319v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Automated blood vessel segmentation is vital for biomedical imaging, as vessel changes indicate many pathologies. Still, precise segmentation is difficult due to the complexity of vascular structures, anatomical variations across patients, the scarcity of annotated public datasets, and the quality of images. We present a thorough literature review, highlighting the state of machine learning techniques across diverse organs. Our goal is to provide a foundation on the topic and identify a robust baseline model for application to vascular segmentation in a new imaging modality, Hierarchical Phase Contrast Tomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation Facility, HiP CT enables 3D imaging of complete organs at an unprecedented resolution of ca. 20mm per voxel, with the capability for localized zooms in selected regions down to 1mm per voxel without sectioning. We have created a training dataset with double annotator validated vascular data from three kidneys imaged with HiP CT in the context of the Human Organ Atlas Project. Finally, utilising the nnU Net model, we conduct experiments to assess the models performance on both familiar and unseen samples, employing vessel specific metrics. Our results show that while segmentations yielded reasonably high scores such as clDice values ranging from 0.82 to 0.88, certain errors persisted. Large vessels that collapsed due to the lack of hydrostatic pressure (HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased connectivity in finer vessels and higher segmentation errors at vessel boundaries were observed. Such errors obstruct the understanding of the structures by interrupting vascular tree connectivity. Through our review and outputs, we aim to set a benchmark for subsequent model evaluations using various modalities, especially with the HiP CT imaging database."}
{"title":"Phase-selective growth of $\u03ba$- vs $\u03b2$-Ga$_2$O$_3$ and (In$_x$Ga$_{1-x}$)$_2$O$_3$ by In-mediated metal exchange catalysis in plasma-assisted molecular beam epitaxy","authors":["A. Ardenghi","O. Bierwagen","J. L\u00e4hnemann","J. Kler","A. Falkenstein","M. Martin","P. Mazzolini"],"raw_abstract":"Its piezo- and potentially ferroelectric properties make the metastable kappa\npolymorph of Ga$_2$O$_3$ an interesting material for multiple applications,\nwhile In-incorporation into any polymorphs of Ga$_2$O$_3$ allows to lower their\nbandgap. In this work, we provide a guideline to achieve single phase\n$\\kappa$-, $\\beta$-Ga$_2$O$_3$ as well as their (In$_x$Ga$_{1-x}$)$_2$O$_3$\nalloys up to x = 0.14 and x = 0.17 respectively, using In-mediated metal\nexchange catalysis in plasma assisted molecular beam epitaxy (MEXCAT-MBE). The\npolymorph transition from $\\kappa$ to $\\beta$ is also addressed, highlighting\nthe fundamental role played by the thermal stability of the\n$\\kappa$-Ga$_2$O$_3$. Additionally, we also demonstrate the possibility to grow\n($\\bar{2}$01) $\\beta$-Ga$_2$O$_3$ on top of $\\alpha$-Al$_2$O$_3$ (0001) at\ntemperatures at least 100 {\\deg}C above those achievable with conventional\nnon-catalyzed MBE, opening the road for increased crystal quality in\nheteroepitaxy. The role of the substrate, as well as strain and structural\ndefects in the growth of $\\kappa$-Ga$_2$O$_3$ is also investigated by growing\nsimultaneously on three different materials: (i) $\\alpha$-Al$_2$O$_3$ (0001),\n(ii) 20 nm of ($\\bar{2}$01) $\\beta$-Ga$_2$O$_3$ on $\\alpha$-Al$_2$O$_3$ (0001)\nand (iii) ($\\bar{2}$01) $\\beta$-Ga$_2$O$_3$ single crystal.","publication_date":1700651641,"paper_link":"http://arxiv.org/pdf/2311.13318v1","categories":["Physics"],"abstract":"Its piezo- and potentially ferroelectric properties make the metastable kappa polymorph of Ga__FORMULA__O__FORMULA__ an interesting material for multiple applications, while In-incorporation into any polymorphs of Ga__FORMULA__O__FORMULA__ allows to lower their bandgap. In this work, we provide a guideline to achieve single phase __FORMULA__-, __FORMULA__-Ga__FORMULA__O__FORMULA__ as well as their (In__FORMULA__Ga__FORMULA__)__FORMULA__O__FORMULA__ alloys up to x = 0.14 and x = 0.17 respectively, using In-mediated metal exchange catalysis in plasma assisted molecular beam epitaxy (MEXCAT-MBE). The polymorph transition from __FORMULA__ to __FORMULA__ is also addressed, highlighting the fundamental role played by the thermal stability of the __FORMULA__-Ga__FORMULA__O__FORMULA__. Additionally, we also demonstrate the possibility to grow (__FORMULA__01) __FORMULA__-Ga__FORMULA__O__FORMULA__ on top of __FORMULA__-Al__FORMULA__O__FORMULA__ (0001) at temperatures at least 100 {\\deg}C above those achievable with conventional non-catalyzed MBE, opening the road for increased crystal quality in heteroepitaxy. The role of the substrate, as well as strain and structural defects in the growth of __FORMULA__-Ga__FORMULA__O__FORMULA__ is also investigated by growing simultaneously on three different materials: (i) __FORMULA__-Al__FORMULA__O__FORMULA__ (0001), (ii) 20 nm of (__FORMULA__01) __FORMULA__-Ga__FORMULA__O__FORMULA__ on __FORMULA__-Al__FORMULA__O__FORMULA__ (0001) and (iii) (__FORMULA__01) __FORMULA__-Ga__FORMULA__O__FORMULA__ single crystal."}
{"title":"Recognition-Guided Diffusion Model for Scene Text Image Super-Resolution","authors":["Yuxuan Zhou","Liangcai Gao","Zhi Tang","Baole Wei"],"raw_abstract":"Scene Text Image Super-Resolution (STISR) aims to enhance the resolution and\nlegibility of text within low-resolution (LR) images, consequently elevating\nrecognition accuracy in Scene Text Recognition (STR). Previous methods\npredominantly employ discriminative Convolutional Neural Networks (CNNs)\naugmented with diverse forms of text guidance to address this issue.\nNevertheless, they remain deficient when confronted with severely blurred\nimages, due to their insufficient generation capability when little structural\nor semantic information can be extracted from original images. Therefore, we\nintroduce RGDiffSR, a Recognition-Guided Diffusion model for scene text image\nSuper-Resolution, which exhibits great generative diversity and fidelity even\nin challenging scenarios. Moreover, we propose a Recognition-Guided Denoising\nNetwork, to guide the diffusion model generating LR-consistent results through\nsuccinct semantic guidance. Experiments on the TextZoom dataset demonstrate the\nsuperiority of RGDiffSR over prior state-of-the-art methods in both text\nrecognition accuracy and image fidelity.","publication_date":1700651445,"paper_link":"http://arxiv.org/pdf/2311.13317v1","categories":["Computer Science"],"abstract":"Scene Text Image Super-Resolution (STISR) aims to enhance the resolution and legibility of text within low-resolution (LR) images, consequently elevating recognition accuracy in Scene Text Recognition (STR). Previous methods predominantly employ discriminative Convolutional Neural Networks (CNNs) augmented with diverse forms of text guidance to address this issue. Nevertheless, they remain deficient when confronted with severely blurred images, due to their insufficient generation capability when little structural or semantic information can be extracted from original images. Therefore, we introduce RGDiffSR, a Recognition-Guided Diffusion model for scene text image Super-Resolution, which exhibits great generative diversity and fidelity even in challenging scenarios. Moreover, we propose a Recognition-Guided Denoising Network, to guide the diffusion model generating LR-consistent results through succinct semantic guidance. Experiments on the TextZoom dataset demonstrate the superiority of RGDiffSR over prior state-of-the-art methods in both text recognition accuracy and image fidelity."}
{"title":"Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting","authors":["Xinyan Guan","Yanjiang Liu","Hongyu Lin","Yaojie Lu","Ben He","Xianpei Han","Le Sun"],"raw_abstract":"Incorporating factual knowledge in knowledge graph is regarded as a promising\napproach for mitigating the hallucination of large language models (LLMs).\nExisting methods usually only use the user's input to query the knowledge\ngraph, thus failing to address the factual hallucination generated by LLMs\nduring its reasoning process. To address this problem, this paper proposes\nKnowledge Graph-based Retrofitting (KGR), a new framework that incorporates\nLLMs with KGs to mitigate factual hallucination during the reasoning process by\nretrofitting the initial draft responses of LLMs based on the factual knowledge\nstored in KGs. Specifically, KGR leverages LLMs to extract, select, validate,\nand retrofit factual statements within the model-generated responses, which\nenables an autonomous knowledge verifying and refining procedure without any\nadditional manual efforts. Experiments show that KGR can significantly improve\nthe performance of LLMs on factual QA benchmarks especially when involving\ncomplex reasoning processes, which demonstrates the necessity and effectiveness\nof KGR in mitigating hallucination and enhancing the reliability of LLMs.","publication_date":1700651318,"paper_link":"http://arxiv.org/pdf/2311.13314v1","categories":["Computer Science"],"abstract":"Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs."}
{"title":"Towards the Intuitive Understanding of Quantum World: Sonification of Rabi Oscillations, Wigner functions, and Quantum Simulators","authors":["Reiko Yamada","Eloy Pi\u00f1ol","Samuele Grandi","Jakub Zakrzewski","Maciej Lewenstein"],"raw_abstract":"Recently, there has been considerable interest in \"sonifying\" scientific\ndata; however, sonifying quantum processes using the newest quantum\ntechnologies, including Noise Intermediate Scale Quantum devices and quantum\nrandom number generators, is still an emerging area of research. Music\ntechnologists and composers employ the growing accessibility to diverse data\nfrom quantum mechanics as musical tools in the hope of generating new sound\nexpressions. How different is the quantum world from the classical one, and is\nit possible to express the quantum world using sounds? Quantum phenomena are\nvery different from those that we experience in our everyday lives. Thus, it is\nchallenging to understand them intuitively. In this paper, we propose\nsonification as a method toward an intuitive understanding of various quantum\nmechanical phenomena, from Rabi oscillations and resonance fluorescence of a\nsingle atom through the generation of Schr\\\"odinger cat states in strong laser\nfield physics to insulator-superfluid transition in quantum many-body systems.\nThis paper illustrates various methods we experimented with in sonification and\nscore representations of quantum data depending on the source data and\nperformance settings.","publication_date":1700651214,"paper_link":"http://arxiv.org/pdf/2311.13313v1","categories":["Physics"],"abstract":"Recently, there has been considerable interest in \"sonifying\" scientific data; however, sonifying quantum processes using the newest quantum technologies, including Noise Intermediate Scale Quantum devices and quantum random number generators, is still an emerging area of research. Music technologists and composers employ the growing accessibility to diverse data from quantum mechanics as musical tools in the hope of generating new sound expressions. How different is the quantum world from the classical one, and is it possible to express the quantum world using sounds? Quantum phenomena are very different from those that we experience in our everyday lives. Thus, it is challenging to understand them intuitively. In this paper, we propose sonification as a method toward an intuitive understanding of various quantum mechanical phenomena, from Rabi oscillations and resonance fluorescence of a single atom through the generation of Schr\\\"odinger cat states in strong laser field physics to insulator-superfluid transition in quantum many-body systems. This paper illustrates various methods we experimented with in sonification and score representations of quantum data depending on the source data and performance settings."}
{"title":"Optimization of Synthesis Parameters and Superconducting Properties of GdFeAsO1-xFx","authors":["Mohammad Azam","Manasa Manasa","Tatiana Zajarniuk","Svitlana Stelmakh","Tomasz Cetner","Andrzej Morawski","Andrzej Wi\u015bniewski","Shiv J. Singh"],"raw_abstract":"REFeAsO (RE1111; RE: rare earth) belongs to the 1111 family of iron-based\nsuperconductors (FBS), which illustrates the enhancement of the superconducting\ntransition (Tc) with smaller radii of RE. However, the synthesis of the 1111\nphase with a heavy rare-earth is always challenging. In this paper, we report\nthe optimization of the growth and superconducting properties of F-doped\nGdFeAsO1-xFx bulks by preparing the samples in a wide temperature range\n(700-1100{\\deg}C) at ambient pressure. The optimized synthesis parameters are\nconcluded based on structural, microstructural, transport, and magnetic\nmeasurements. These findings suggest that the optimal conditions for preparing\nF-doped Gd1111 bulks involve a two-step process at 900{\\deg}C for 61 hours at\nambient pressure, which is lower than previously reported. The optimized\nsamples have revealed the superconducting transition temperature (Tconset) of\n43 K for GdFeAsO0.83F0.17. The first-time reported critical current Jc value\nfor this Gd1111 is observed of the order of 10^3 (A/cm^2) at 0 T and 5 K. Our\ninvestigation also concluded that highly pure precursors, particularly\ngadolinium metal, are required to achieve the superconducting properties of\nF-doped Gd1111. A high growth pressure of 1 GPa reduces the superconducting\nproperties of F-doped Gd1111.","publication_date":1700651104,"paper_link":"http://arxiv.org/pdf/2311.13312v1","categories":["Physics"],"abstract":"REFeAsO (RE1111; RE: rare earth) belongs to the 1111 family of iron-based superconductors (FBS), which illustrates the enhancement of the superconducting transition (Tc) with smaller radii of RE. However, the synthesis of the 1111 phase with a heavy rare-earth is always challenging. In this paper, we report the optimization of the growth and superconducting properties of F-doped GdFeAsO1-xFx bulks by preparing the samples in a wide temperature range (700-1100{\\deg}C) at ambient pressure. The optimized synthesis parameters are concluded based on structural, microstructural, transport, and magnetic measurements. These findings suggest that the optimal conditions for preparing F-doped Gd1111 bulks involve a two-step process at 900{\\deg}C for 61 hours at ambient pressure, which is lower than previously reported. The optimized samples have revealed the superconducting transition temperature (Tconset) of 43 K for GdFeAsO0.83F0.17. The first-time reported critical current Jc value for this Gd1111 is observed of the order of 10^3 (A/cm^2) at 0 T and 5 K. Our investigation also concluded that highly pure precursors, particularly gadolinium metal, are required to achieve the superconducting properties of F-doped Gd1111. A high growth pressure of 1 GPa reduces the superconducting properties of F-doped Gd1111."}
{"title":"Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation","authors":["Xiao Song","Jiafan Liu","Yun Li","Wenbin Lei","Ruxin Wang"],"raw_abstract":"Radiology Report Generation (RRG) draws attention as an interaction between\nvision and language fields. Previous works inherited the ideology of\nvision-to-language generation tasks,aiming to generate paragraphs with high\nconsistency as reports. However, one unique characteristic of RRG, the\nindependence between diseases, was neglected, leading to the injection of the\nspurious confounder, i.e., the disease co-occurrence. Unfortunately, this\nconfounder confuses the process of report generation worse because of the\nbiased RRG data distribution. In this paper, to rethink this issue thoroughly,\nwe reason about its causes and effects from a novel perspective of statistics\nand causality, where the Joint Vision Coupling and the Conditional Sentence\nCoherence Coupling are two aspects prone to implicitly decrease the accuracy of\nreports. Then, a counterfactual augmentation strategy that contains the\nCounterfactual Sample Synthesis and the Counterfactual Report Reconstruction\nsub-methods is proposed to break these two aspects of spurious effects.\nExperimental results and further analyses on two widely used datasets justify\nour reasoning and proposed methods.","publication_date":1700650536,"paper_link":"http://arxiv.org/pdf/2311.13307v1","categories":["Computer Science"],"abstract":"Radiology Report Generation (RRG) draws attention as an interaction between vision and language fields. Previous works inherited the ideology of vision-to-language generation tasks,aiming to generate paragraphs with high consistency as reports. However, one unique characteristic of RRG, the independence between diseases, was neglected, leading to the injection of the spurious confounder, i.e., the disease co-occurrence. Unfortunately, this confounder confuses the process of report generation worse because of the biased RRG data distribution. In this paper, to rethink this issue thoroughly, we reason about its causes and effects from a novel perspective of statistics and causality, where the Joint Vision Coupling and the Conditional Sentence Coherence Coupling are two aspects prone to implicitly decrease the accuracy of reports. Then, a counterfactual augmentation strategy that contains the Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction sub-methods is proposed to break these two aspects of spurious effects. Experimental results and further analyses on two widely used datasets justify our reasoning and proposed methods."}
{"title":"Retargeting Visual Data with Deformation Fields","authors":["Tim Elsner","Julia Berger","Tong Wu","Victor Czech","Lin Gao","Leif Kobbelt"],"raw_abstract":"Seam carving is an image editing method that enable content-aware resizing,\nincluding operations like removing objects. However, the seam-finding strategy\nbased on dynamic programming or graph-cut limits its applications to broader\nvisual data formats and degrees of freedom for editing. Our observation is that\ndescribing the editing and retargeting of images more generally by a\ndisplacement field yields a generalisation of content-aware deformations. We\npropose to learn a deformation with a neural network that keeps the output\nplausible while trying to deform it only in places with low information\ncontent. This technique applies to different kinds of visual data, including\nimages, 3D scenes given as neural radiance fields, or even polygon meshes.\nExperiments conducted on different visual data show that our method achieves\nbetter content-aware retargeting compared to previous methods.","publication_date":1700648839,"paper_link":"http://arxiv.org/pdf/2311.13297v1","categories":["Computer Science"],"abstract":"Seam carving is an image editing method that enable content-aware resizing, including operations like removing objects. However, the seam-finding strategy based on dynamic programming or graph-cut limits its applications to broader visual data formats and degrees of freedom for editing. Our observation is that describing the editing and retargeting of images more generally by a displacement field yields a generalisation of content-aware deformations. We propose to learn a deformation with a neural network that keeps the output plausible while trying to deform it only in places with low information content. This technique applies to different kinds of visual data, including images, 3D scenes given as neural radiance fields, or even polygon meshes. Experiments conducted on different visual data show that our method achieves better content-aware retargeting compared to previous methods."}
{"title":"Feedback control of plant-soil autotoxicity via pulse-width modulation","authors":["Tancredi Rino","Francesco Giannino","Davide Fiore"],"raw_abstract":"Plant-soil negative feedback (PSNF) is the rise in soil of negative\nconditions for plant performance induced by the plants themselves, limiting the\nfull potential yield and thus representing a loss for the agricultural\nindustry. It has been recently shown that detrimental effects the PSNF has on\nthe growth of plant's biomass can be mitigated by periodically intervening on\nthe plant/soil system, for example by washing the soil. The periodic control\ninputs were computed by using an average model of the system and then applied\nin open-loop. In this paper we present two feedback control strategies, namely\na PI and a MPC-based controllers, that, by adapting online the duty-cycle of\nthe periodic control input, guarantee precise regulation of the biomass yield\nand at the same time robustness to unavoidable modeling errors and\nperturbations acting on the system. The performance of the proposed control\nstrategies is then validated by means of extensive numerical simulations.","publication_date":1700648651,"paper_link":"http://arxiv.org/pdf/2311.13295v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Plant-soil negative feedback (PSNF) is the rise in soil of negative conditions for plant performance induced by the plants themselves, limiting the full potential yield and thus representing a loss for the agricultural industry. It has been recently shown that detrimental effects the PSNF has on the growth of plant's biomass can be mitigated by periodically intervening on the plant/soil system, for example by washing the soil. The periodic control inputs were computed by using an average model of the system and then applied in open-loop. In this paper we present two feedback control strategies, namely a PI and a MPC-based controllers, that, by adapting online the duty-cycle of the periodic control input, guarantee precise regulation of the biomass yield and at the same time robustness to unavoidable modeling errors and perturbations acting on the system. The performance of the proposed control strategies is then validated by means of extensive numerical simulations."}
{"title":"Probabilistic Inference in Reinforcement Learning Done Right","authors":["Jean Tarbouriech","Tor Lattimore","Brendan O'Donoghue"],"raw_abstract":"A popular perspective in Reinforcement learning (RL) casts the problem as\nprobabilistic inference on a graphical model of the Markov decision process\n(MDP). The core object of study is the probability of each state-action pair\nbeing visited under the optimal policy. Previous approaches to approximate this\nquantity can be arbitrarily poor, leading to algorithms that do not implement\ngenuine statistical inference and consequently do not perform well in\nchallenging problems. In this work, we undertake a rigorous Bayesian treatment\nof the posterior probability of state-action optimality and clarify how it\nflows through the MDP. We first reveal that this quantity can indeed be used to\ngenerate a policy that explores efficiently, as measured by regret.\nUnfortunately, computing it is intractable, so we derive a new variational\nBayesian approximation yielding a tractable convex optimization problem and\nestablish that the resulting policy also explores efficiently. We call our\napproach VAPOR and show that it has strong connections to Thompson sampling,\nK-learning, and maximum entropy exploration. We conclude with some experiments\ndemonstrating the performance advantage of a deep RL version of VAPOR.","publication_date":1700648594,"paper_link":"http://arxiv.org/pdf/2311.13294v1","categories":["Computer Science"],"abstract":"A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR."}
{"title":"Analytical formula for the cross section of hadron production from $e^+ e^-$ collisions around the narrow charmouinum resonances","authors":["Yanan Wang","Yadi Wang","Ping Wang"],"raw_abstract":"An analytical formula of the production cross section for $e^+ e^-$\nannihilates to hadrons considering the initial state radiation is reported in\nthe paper. Comparisons between the analytical formula and the direct\nintegration of ISR shows good accuracy which satisfy the current experimental\nrequirements. Besides, comparison in the cross section between the analytical\nformula and the calculation with ConExc Monte Carlo generator is also\npresented. The analytical formula greatly shorten the computing time which\ncould be used for fitting procedure to extract the parameters of narrow\ncharmonium resonances.","publication_date":1700648537,"paper_link":"http://arxiv.org/pdf/2311.13292v1","categories":["Physics"],"abstract":"An analytical formula of the production cross section for __FORMULA__ annihilates to hadrons considering the initial state radiation is reported in the paper. Comparisons between the analytical formula and the direct integration of ISR shows good accuracy which satisfy the current experimental requirements. Besides, comparison in the cross section between the analytical formula and the calculation with ConExc Monte Carlo generator is also presented. The analytical formula greatly shorten the computing time which could be used for fitting procedure to extract the parameters of narrow charmonium resonances."}
{"title":"Intention and Context Elicitation with Large Language Models in the Legal Aid Intake Process","authors":["Nick Goodson","Rongfei Lu"],"raw_abstract":"Large Language Models (LLMs) and chatbots show significant promise in\nstreamlining the legal intake process. This advancement can greatly reduce the\nworkload and costs for legal aid organizations, improving availability while\nmaking legal assistance more accessible to a broader audience. However, a key\nchallenge with current LLMs is their tendency to overconfidently deliver an\nimmediate 'best guess' to a client's question based on the output distribution\nlearned over the training data. This approach often overlooks the client's\nactual intentions or the specifics of their legal situation. As a result,\nclients may not realize the importance of providing essential additional\ncontext or expressing their underlying intentions, which are crucial for their\nlegal cases. Traditionally, logic based decision trees have been used to\nautomate intake for specific access to justice issues, such as immigration and\neviction. But those solutions lack scalability. We demonstrate a\nproof-of-concept using LLMs to elicit and infer clients' underlying intentions\nand specific legal circumstances through free-form, language-based\ninteractions. We also propose future research directions to use supervised\nfine-tuning or offline reinforcement learning to automatically incorporate\nintention and context elicitation in chatbots without explicit prompting.","publication_date":1700647469,"paper_link":"http://arxiv.org/pdf/2311.13281v1","categories":["Computer Science"],"abstract":"Large Language Models (LLMs) and chatbots show significant promise in streamlining the legal intake process. This advancement can greatly reduce the workload and costs for legal aid organizations, improving availability while making legal assistance more accessible to a broader audience. However, a key challenge with current LLMs is their tendency to overconfidently deliver an immediate 'best guess' to a client's question based on the output distribution learned over the training data. This approach often overlooks the client's actual intentions or the specifics of their legal situation. As a result, clients may not realize the importance of providing essential additional context or expressing their underlying intentions, which are crucial for their legal cases. Traditionally, logic based decision trees have been used to automate intake for specific access to justice issues, such as immigration and eviction. But those solutions lack scalability. We demonstrate a proof-of-concept using LLMs to elicit and infer clients' underlying intentions and specific legal circumstances through free-form, language-based interactions. We also propose future research directions to use supervised fine-tuning or offline reinforcement learning to automatically incorporate intention and context elicitation in chatbots without explicit prompting."}
{"title":"Robustness of chaotic behavior in iterated quantum protocols","authors":["Attila Portik","Orosolya K\u00e1lm\u00e1n","Igor Jex","Tam\u00e1s Kiss"],"raw_abstract":"One of the simplest possible quantum circuits, consisting of a CNOT gate, a\nHadamard gate and a measurement on one of the outputs is known to lead to\nchaotic dynamics when applied iteratively on an ensemble of equally prepared\nqubits. The evolution of pure initial quantum states is characterized by a\nfractal (in the space of states), formed by the border of different convergence\nregions. We examine how the ideal evolution is distorted in the presence of\nboth coherent error and incoherent initial noise, which are typical\nimperfections in current implementations of quantum computers. It is known that\nunder the influence of initial noise only, the fractal is preserved, moreover,\nits dimension remains constant below a critical noise level. We systematically\nanalyze the effect of coherent Hadamard gate errors by determining fixed points\nand cycles of the evolution. We combine analytic and numerical methods to\nexplore to what extent the dynamics is altered by coherent errors in the\npresence of preparation noise as well. We show that the main features of the\ndynamics, and especially the fractal borders, are robust against the discussed\nnoise, they will only be slightly distorted. We identify a range of error\nparameters, for which the characteristic properties of the dynamics are not\nsignificantly altered. Hence, our results allow to identify reliable regimes of\noperation of iterative protocols.","publication_date":1700647172,"paper_link":"http://arxiv.org/pdf/2311.13280v1","categories":["Physics"],"abstract":"One of the simplest possible quantum circuits, consisting of a CNOT gate, a Hadamard gate and a measurement on one of the outputs is known to lead to chaotic dynamics when applied iteratively on an ensemble of equally prepared qubits. The evolution of pure initial quantum states is characterized by a fractal (in the space of states), formed by the border of different convergence regions. We examine how the ideal evolution is distorted in the presence of both coherent error and incoherent initial noise, which are typical imperfections in current implementations of quantum computers. It is known that under the influence of initial noise only, the fractal is preserved, moreover, its dimension remains constant below a critical noise level. We systematically analyze the effect of coherent Hadamard gate errors by determining fixed points and cycles of the evolution. We combine analytic and numerical methods to explore to what extent the dynamics is altered by coherent errors in the presence of preparation noise as well. We show that the main features of the dynamics, and especially the fractal borders, are robust against the discussed noise, they will only be slightly distorted. We identify a range of error parameters, for which the characteristic properties of the dynamics are not significantly altered. Hence, our results allow to identify reliable regimes of operation of iterative protocols."}
{"title":"Comprehensive Evaluation of GNN Training Systems: A Data Management Perspective","authors":["Hao Yuan","Yajiong Liu","Yanfeng Zhang","Xin Ai","Qiange Wang","Chaoyi Chen","Yu Gu","Ge Yu"],"raw_abstract":"Many Graph Neural Network (GNN) training systems have emerged recently to\nsupport efficient GNN training. Since GNNs embody complex data dependencies\nbetween training samples, the training of GNNs should address distinct\nchallenges different from DNN training in data management, such as data\npartitioning, batch preparation for mini-batch training, and data transferring\nbetween CPUs and GPUs. These factors, which take up a large proportion of\ntraining time, make data management in GNN training more significant. This\npaper reviews GNN training from a data management perspective and provides a\ncomprehensive analysis and evaluation of the representative approaches. We\nconduct extensive experiments on various benchmark datasets and show many\ninteresting and valuable results. We also provide some practical tips learned\nfrom these experiments, which are helpful for designing GNN training systems in\nthe future.","publication_date":1700646920,"paper_link":"http://arxiv.org/pdf/2311.13279v1","categories":["Computer Science"],"abstract":"Many Graph Neural Network (GNN) training systems have emerged recently to support efficient GNN training. Since GNNs embody complex data dependencies between training samples, the training of GNNs should address distinct challenges different from DNN training in data management, such as data partitioning, batch preparation for mini-batch training, and data transferring between CPUs and GPUs. These factors, which take up a large proportion of training time, make data management in GNN training more significant. This paper reviews GNN training from a data management perspective and provides a comprehensive analysis and evaluation of the representative approaches. We conduct extensive experiments on various benchmark datasets and show many interesting and valuable results. We also provide some practical tips learned from these experiments, which are helpful for designing GNN training systems in the future."}
{"title":"Exoplanets detection limits using spectral cross-correlation with spectro-imaging. An analytical model applied to the case of ELT-HARMONI","authors":["Alexis Bidot","David Mouillet","Alexis Carlotti"],"raw_abstract":"The combination of high-contrast imaging and medium to high spectral\nresolution spectroscopy offers new possibilities for the detection and\ncharacterization of exoplanets. The molecular mapping technique uses the\ndifference between the planetary and stellar spectra. While traditional\npost-processing techniques are quickly limited by speckle noise at short\nangular separation, it efficiently suppresses speckles. Its performance depends\non multiple parameters such as the star magnitude, the adaptive optics residual\nhalo, the companion spectrum, the telluric absorption, as well as the telescope\nand instrument properties. Exploring this parameter space through end-to-end\nsimulations to predict potential science cases and to optimize future\ninstrument designs is very time-consuming, making it difficult to draw\nconclusions. We propose to define an efficient methodology for such an\nanalysis. Explicit expressions of the estimates of signal and noise are\nderived, and they are validated through comparisons with end-to-end\nsimulations. They provide an understanding of the instrumental dependencies,\nand help to discuss optimal instrumental choices with regard to the targets of\ninterest. They are applied in the case of ELT/HARMONI, as a tool to predict the\ncontrast performance in various observational cases. We confirm the potential\nof molecular mapping for high-contrast detections, especially for cool planets\nat short separations. We provide guidelines based on quantified estimates for\ndesign trade-offs of future instruments. We discuss the planet detection\nperformances of HARMONI observing modes. While they nicely cover the\nappropriate requirements for high detection capability of warm exoplanets, a\ntransmission extended down to J band would be beneficial. A contrast of a few\n1E-7 at 50mas should be within reach on bright targets in photon noise regime\nwith molecular mapping.","publication_date":1700646718,"paper_link":"http://arxiv.org/pdf/2311.13275v1","categories":["Physics"],"abstract":"The combination of high-contrast imaging and medium to high spectral resolution spectroscopy offers new possibilities for the detection and characterization of exoplanets. The molecular mapping technique uses the difference between the planetary and stellar spectra. While traditional post-processing techniques are quickly limited by speckle noise at short angular separation, it efficiently suppresses speckles. Its performance depends on multiple parameters such as the star magnitude, the adaptive optics residual halo, the companion spectrum, the telluric absorption, as well as the telescope and instrument properties. Exploring this parameter space through end-to-end simulations to predict potential science cases and to optimize future instrument designs is very time-consuming, making it difficult to draw conclusions. We propose to define an efficient methodology for such an analysis. Explicit expressions of the estimates of signal and noise are derived, and they are validated through comparisons with end-to-end simulations. They provide an understanding of the instrumental dependencies, and help to discuss optimal instrumental choices with regard to the targets of interest. They are applied in the case of ELT/HARMONI, as a tool to predict the contrast performance in various observational cases. We confirm the potential of molecular mapping for high-contrast detections, especially for cool planets at short separations. We provide guidelines based on quantified estimates for design trade-offs of future instruments. We discuss the planet detection performances of HARMONI observing modes. While they nicely cover the appropriate requirements for high detection capability of warm exoplanets, a transmission extended down to J band would be beneficial. A contrast of a few 1E-7 at 50mas should be within reach on bright targets in photon noise regime with molecular mapping."}
{"title":"Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting","authors":["Daphne van Zandvoort","Laura Wiersema","Tom Huibers","Sandra van Dulmen","Sjaak Brinkkemper"],"raw_abstract":"Customized medical prompts enable Large Language Models (LLM) to effectively\naddress medical dialogue summarization. The process of medical reporting is\noften time-consuming for healthcare professionals. Implementing medical\ndialogue summarization techniques presents a viable solution to alleviate this\ntime constraint by generating automated medical reports. The effectiveness of\nLLMs in this process is significantly influenced by the formulation of the\nprompt, which plays a crucial role in determining the quality and relevance of\nthe generated reports. In this research, we used a combination of two distinct\nprompting strategies, known as shot prompting and pattern prompting to enhance\nthe performance of automated medical reporting. The evaluation of the automated\nmedical reports is carried out using the ROUGE score and a human evaluation\nwith the help of an expert panel. The two-shot prompting approach in\ncombination with scope and domain context outperforms other methods and\nachieves the highest score when compared to the human reference set by a\ngeneral practitioner. However, the automated reports are approximately twice as\nlong as the human references, due to the addition of both redundant and\nrelevant statements that are added to the report.","publication_date":1700646713,"paper_link":"http://arxiv.org/pdf/2311.13274v1","categories":["Computer Science"],"abstract":"Customized medical prompts enable Large Language Models (LLM) to effectively address medical dialogue summarization. The process of medical reporting is often time-consuming for healthcare professionals. Implementing medical dialogue summarization techniques presents a viable solution to alleviate this time constraint by generating automated medical reports. The effectiveness of LLMs in this process is significantly influenced by the formulation of the prompt, which plays a crucial role in determining the quality and relevance of the generated reports. In this research, we used a combination of two distinct prompting strategies, known as shot prompting and pattern prompting to enhance the performance of automated medical reporting. The evaluation of the automated medical reports is carried out using the ROUGE score and a human evaluation with the help of an expert panel. The two-shot prompting approach in combination with scope and domain context outperforms other methods and achieves the highest score when compared to the human reference set by a general practitioner. However, the automated reports are approximately twice as long as the human references, due to the addition of both redundant and relevant statements that are added to the report."}
{"title":"Comparative Experimentation of Accuracy Metrics in Automated Medical Reporting: The Case of Otitis Consultations","authors":["Wouter Faber","Renske Eline Bootsma","Tom Huibers","Sandra van Dulmen","Sjaak Brinkkemper"],"raw_abstract":"Generative Artificial Intelligence (AI) can be used to automatically generate\nmedical reports based on transcripts of medical consultations. The aim is to\nreduce the administrative burden that healthcare professionals face. The\naccuracy of the generated reports needs to be established to ensure their\ncorrectness and usefulness. There are several metrics for measuring the\naccuracy of AI generated reports, but little work has been done towards the\napplication of these metrics in medical reporting. A comparative\nexperimentation of 10 accuracy metrics has been performed on AI generated\nmedical reports against their corresponding General Practitioner's (GP) medical\nreports concerning Otitis consultations. The number of missing, incorrect, and\nadditional statements of the generated reports have been correlated with the\nmetric scores. In addition, we introduce and define a Composite Accuracy Score\nwhich produces a single score for comparing the metrics within the field of\nautomated medical reporting. Findings show that based on the correlation study\nand the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics\nare the preferred metrics, which is not in line with previous work. These\nfindings help determine the accuracy of an AI generated medical report, which\naids the development of systems that generate medical reports for GPs to reduce\nthe administrative burden.","publication_date":1700646703,"paper_link":"http://arxiv.org/pdf/2311.13273v1","categories":["Computer Science"],"abstract":"Generative Artificial Intelligence (AI) can be used to automatically generate medical reports based on transcripts of medical consultations. The aim is to reduce the administrative burden that healthcare professionals face. The accuracy of the generated reports needs to be established to ensure their correctness and usefulness. There are several metrics for measuring the accuracy of AI generated reports, but little work has been done towards the application of these metrics in medical reporting. A comparative experimentation of 10 accuracy metrics has been performed on AI generated medical reports against their corresponding General Practitioner's (GP) medical reports concerning Otitis consultations. The number of missing, incorrect, and additional statements of the generated reports have been correlated with the metric scores. In addition, we introduce and define a Composite Accuracy Score which produces a single score for comparing the metrics within the field of automated medical reporting. Findings show that based on the correlation study and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics are the preferred metrics, which is not in line with previous work. These findings help determine the accuracy of an AI generated medical report, which aids the development of systems that generate medical reports for GPs to reduce the administrative burden."}
{"title":"Spin-orbit interaction driven terahertz nonlinear dynamics in transition metals","authors":["Ruslan Salikhov","Markus Lysne","Philipp Werner","Igor Ilyakov","Michael Sch\u00fcler","Thales V. A. G. de Oliveira","Alexey Ponomaryov","Atiqa Arshad","Gulloo Lal Prajapati","Jan-Christoph Deinert","Pavlo Makushko","Denys Makarov","Thomas Cowan","J\u00fcrgen Fassbender","J\u00fcrgen Lindner","Aleksandra Lindner","Carmine Ortix","Sergey Kovalev"],"raw_abstract":"The interplay of electric charge, spin, and orbital polarizations, coherently\ndriven by picosecond long oscillations of light fields in spin-orbit coupled\nsystems, is the foundation of emerging terahertz spintronics and orbitronics.\nThe essential rules for how terahertz light interacts with these systems in a\nnonlinear way are still not understood. In this work, we demonstrate a\nuniversally applicable electronic nonlinearity originating from spin-orbit\ninteractions in conducting materials, wherein the interplay of light-induced\nspin and orbital textures manifests. We utilized terahertz harmonic generation\nspectroscopy to investigate the nonlinear dynamics over picosecond timescales\nin various transition metal films. We found that the terahertz harmonic\ngeneration efficiency scales with the spin Hall conductivity in the studied\nfilms, while the phase takes two possible values (shifted by {\\pi}), depending\non the d-shell filling. These findings elucidate the fundamental mechanisms\ngoverning non-equilibrium spin and orbital polarization dynamics at terahertz\nfrequencies, which is relevant for potential applications of terahertz spin-\nand orbital-based devices.","publication_date":1700646655,"paper_link":"http://arxiv.org/pdf/2311.13272v1","categories":["Physics"],"abstract":"The interplay of electric charge, spin, and orbital polarizations, coherently driven by picosecond long oscillations of light fields in spin-orbit coupled systems, is the foundation of emerging terahertz spintronics and orbitronics. The essential rules for how terahertz light interacts with these systems in a nonlinear way are still not understood. In this work, we demonstrate a universally applicable electronic nonlinearity originating from spin-orbit interactions in conducting materials, wherein the interplay of light-induced spin and orbital textures manifests. We utilized terahertz harmonic generation spectroscopy to investigate the nonlinear dynamics over picosecond timescales in various transition metal films. We found that the terahertz harmonic generation efficiency scales with the spin Hall conductivity in the studied films, while the phase takes two possible values (shifted by {\\pi}), depending on the d-shell filling. These findings elucidate the fundamental mechanisms governing non-equilibrium spin and orbital polarization dynamics at terahertz frequencies, which is relevant for potential applications of terahertz spin- and orbital-based devices."}
{"title":"FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning","authors":["Seongyoon Kim","Gihun Lee","Jaehoon Oh","Se-Young Yun"],"raw_abstract":"Federated Learning (FL) is a collaborative method for training models while\npreserving data privacy in decentralized settings. However, FL encounters\nchallenges related to data heterogeneity, which can result in performance\ndegradation. In our study, we observe that as data heterogeneity increases,\nfeature representation in the FedAVG model deteriorates more significantly\ncompared to classifier weight. Additionally, we observe that as data\nheterogeneity increases, the gap between higher feature norms for observed\nclasses, obtained from local models, and feature norms of unobserved classes\nwidens, in contrast to the behavior of classifier weight norms. This widening\ngap extends to encompass the feature norm disparities between local and the\nglobal models. To address these issues, we introduce Federated Averaging with\nFeature Normalization Update (FedFN), a straightforward learning method. We\ndemonstrate the superior performance of FedFN through extensive experiments,\neven when applied to pretrained ResNet18. Subsequently, we confirm the\napplicability of FedFN to foundation models.","publication_date":1700645853,"paper_link":"http://arxiv.org/pdf/2311.13267v1","categories":["Computer Science"],"abstract":"Federated Learning (FL) is a collaborative method for training models while preserving data privacy in decentralized settings. However, FL encounters challenges related to data heterogeneity, which can result in performance degradation. In our study, we observe that as data heterogeneity increases, feature representation in the FedAVG model deteriorates more significantly compared to classifier weight. Additionally, we observe that as data heterogeneity increases, the gap between higher feature norms for observed classes, obtained from local models, and feature norms of unobserved classes widens, in contrast to the behavior of classifier weight norms. This widening gap extends to encompass the feature norm disparities between local and the global models. To address these issues, we introduce Federated Averaging with Feature Normalization Update (FedFN), a straightforward learning method. We demonstrate the superior performance of FedFN through extensive experiments, even when applied to pretrained ResNet18. Subsequently, we confirm the applicability of FedFN to foundation models."}
{"title":"Spontaneous polarization in NaNbO$_{3}$ film on NdGaO$_{3}$ and DyScO$_{3}$ substrates","authors":["Kisung Kang","Saud Bin Anooz","Jutta Schwarzkopf","Christian Carbogno"],"raw_abstract":"Pure NaNbO$_{3}$ is an antiferroelectric material at room temperature that\nirreversibly transforms to a ferroelectric polar state when subjected to an\nexternal electrical field or lattice strain. Experimentally, it has been\nobserved that NaNbO$_{3}$ films grown on NdGaO$_{3}$ exhibit an electrical\npolarization along the [001]$_{\\mathrm{PC}}$ direction, whereas films on\nDyScO$_{3}$ substrates exhibit a polarization along the [011]$_{\\mathrm{PC}}$\ndirection. These effects have been attributed to the realization of different\nlattice symmetries in the films due to the incorporation of lattice strain\nimposed by the use of oxide substrates with different lattice parameters.\nHowever, the underlying atomistic mechanisms of the resulting phase symmetry in\nthe films are hardly clear, given that NaNbO$_{3}$ features a diverse and\ncomplex phase diagram. In turn, these also impede a straightforward tailoring\nand optimization of the resulting macroscopic properties on different\nsubstrates. To clarify this issue, we perform all-electron first-principles\ncalculations for several potential NaNbO$_{3}$ polymorphs under stress and\nstrain. The computed properties, including the ferroelectric polarization,\nreveal that an orthorhombic $Pmc2_{1}$ phase is realized on NdGaO$_{3}$\nsubstrates since this is the only phase with an out-of-plane polarization under\na compressive strain. Conversely, the monoclinic $Pm$ phase is consistent for\nthe samples grown on DyScO$_{3}$ substrate, since this phase exhibits a\nspontaneous in-plane polarization along [011]$_{\\mathrm{PC}}$ under tensile\nstrain.","publication_date":1700645827,"paper_link":"http://arxiv.org/pdf/2311.13266v1","categories":["Physics"],"abstract":"Pure NaNbO__FORMULA__ is an antiferroelectric material at room temperature that irreversibly transforms to a ferroelectric polar state when subjected to an external electrical field or lattice strain. Experimentally, it has been observed that NaNbO__FORMULA__ films grown on NdGaO__FORMULA__ exhibit an electrical polarization along the [001]__FORMULA__ direction, whereas films on DyScO__FORMULA__ substrates exhibit a polarization along the [011]__FORMULA__ direction. These effects have been attributed to the realization of different lattice symmetries in the films due to the incorporation of lattice strain imposed by the use of oxide substrates with different lattice parameters. However, the underlying atomistic mechanisms of the resulting phase symmetry in the films are hardly clear, given that NaNbO__FORMULA__ features a diverse and complex phase diagram. In turn, these also impede a straightforward tailoring and optimization of the resulting macroscopic properties on different substrates. To clarify this issue, we perform all-electron first-principles calculations for several potential NaNbO__FORMULA__ polymorphs under stress and strain. The computed properties, including the ferroelectric polarization, reveal that an orthorhombic __FORMULA__ phase is realized on NdGaO__FORMULA__ substrates since this is the only phase with an out-of-plane polarization under a compressive strain. Conversely, the monoclinic __FORMULA__ phase is consistent for the samples grown on DyScO__FORMULA__ substrate, since this phase exhibits a spontaneous in-plane polarization along [011]__FORMULA__ under tensile strain."}
{"title":"Gravitational repulsive effects in 3D regular black holes","authors":["Orlando Luongo","Hernando Quevedo","S. N. Sajadi"],"raw_abstract":"In this work, we consider the effects of repulsive gravity in an invariant\nway for four static 3D regular black holes, using the eigenvalues of the\nRiemann curvature tensor, the Ricci scalar, and the strong energy conditions.\nThe eigenvalues of the solutions are non-vanishing asymptotically (in\nasymptotically AdS) and increase as the source of gravity is approached,\nproviding a radius at which the passage from attractive to repulsive gravity\nmight occur. We compute the onsets and the regions of repulsive gravity and\nconclude that the regular behavior of the solutions at the origin of\ncoordinates can be interpreted as due to the presence of repulsive gravity,\nwhich also turns out to be related with the violation of the strong energy\ncondition. We showed that in all of the solutions for the allowed region of\nparameters, gravity changes its sign, but the repulsive regions only for the\nnon-logarithmic solution are affected by the mass that generates the regular\nblack hole. The repulsive regions for the logarithmic solutions are dependent\non electric charge and the AdS$_{3}$ length. The implications and physical\nconsequences of these results are discussed in detail.","publication_date":1700645295,"paper_link":"http://arxiv.org/pdf/2311.13264v1","categories":["Physics"],"abstract":"In this work, we consider the effects of repulsive gravity in an invariant way for four static 3D regular black holes, using the eigenvalues of the Riemann curvature tensor, the Ricci scalar, and the strong energy conditions. The eigenvalues of the solutions are non-vanishing asymptotically (in asymptotically AdS) and increase as the source of gravity is approached, providing a radius at which the passage from attractive to repulsive gravity might occur. We compute the onsets and the regions of repulsive gravity and conclude that the regular behavior of the solutions at the origin of coordinates can be interpreted as due to the presence of repulsive gravity, which also turns out to be related with the violation of the strong energy condition. We showed that in all of the solutions for the allowed region of parameters, gravity changes its sign, but the repulsive regions only for the non-logarithmic solution are affected by the mass that generates the regular black hole. The repulsive regions for the logarithmic solutions are dependent on electric charge and the AdS__FORMULA__ length. The implications and physical consequences of these results are discussed in detail."}
{"title":"CMFDFormer: Transformer-based Copy-Move Forgery Detection with Continual Learning","authors":["Yaqi Liu","Chao Xia","Song Xiao","Qingxiao Guan","Wenqian Dong","Yifan Zhang","Nenghai Yu"],"raw_abstract":"Copy-move forgery detection aims at detecting duplicated regions in a\nsuspected forged image, and deep learning based copy-move forgery detection\nmethods are in the ascendant. These deep learning based methods heavily rely on\nsynthetic training data, and the performance will degrade when facing new\ntasks. In this paper, we propose a Transformer-style copy-move forgery\ndetection network named as CMFDFormer, and provide a novel PCSD (Pooled Cube\nand Strip Distillation) continual learning framework to help CMFDFormer handle\nnew tasks. CMFDFormer consists of a MiT (Mix Transformer) backbone network and\na PHD (Pluggable Hybrid Decoder) mask prediction network. The MiT backbone\nnetwork is a Transformer-style network which is adopted on the basis of\ncomprehensive analyses with CNN-style and MLP-style backbones. The PHD network\nis constructed based on self-correlation computation, hierarchical feature\nintegration, a multi-scale cycle fully-connected block and a mask\nreconstruction block. The PHD network is applicable to feature extractors of\ndifferent styles for hierarchical multi-scale information extraction, achieving\ncomparable performance. Last but not least, we propose a PCSD continual\nlearning framework to improve the forgery detectability and avoid catastrophic\nforgetting when handling new tasks. Our continual learning framework restricts\nintermediate features from the PHD network, and takes advantage of both cube\npooling and strip pooling. Extensive experiments on publicly available datasets\ndemonstrate the good performance of CMFDFormer and the effectiveness of the\nPCSD continual learning framework.","publication_date":1700645266,"paper_link":"http://arxiv.org/pdf/2311.13263v1","categories":["Computer Science"],"abstract":"Copy-move forgery detection aims at detecting duplicated regions in a suspected forged image, and deep learning based copy-move forgery detection methods are in the ascendant. These deep learning based methods heavily rely on synthetic training data, and the performance will degrade when facing new tasks. In this paper, we propose a Transformer-style copy-move forgery detection network named as CMFDFormer, and provide a novel PCSD (Pooled Cube and Strip Distillation) continual learning framework to help CMFDFormer handle new tasks. CMFDFormer consists of a MiT (Mix Transformer) backbone network and a PHD (Pluggable Hybrid Decoder) mask prediction network. The MiT backbone network is a Transformer-style network which is adopted on the basis of comprehensive analyses with CNN-style and MLP-style backbones. The PHD network is constructed based on self-correlation computation, hierarchical feature integration, a multi-scale cycle fully-connected block and a mask reconstruction block. The PHD network is applicable to feature extractors of different styles for hierarchical multi-scale information extraction, achieving comparable performance. Last but not least, we propose a PCSD continual learning framework to improve the forgery detectability and avoid catastrophic forgetting when handling new tasks. Our continual learning framework restricts intermediate features from the PHD network, and takes advantage of both cube pooling and strip pooling. Extensive experiments on publicly available datasets demonstrate the good performance of CMFDFormer and the effectiveness of the PCSD continual learning framework."}
{"title":"Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides","authors":["Maren H\u00f8ib\u00f8","Andr\u00e9 Pedersen","Vibeke Grotnes Dale","Sissel Marie Berget","Borgny Ytterhus","Cecilia Lindskog","Elisabeth Wik","Lars A. Akslen","Ingerid Reinertsen","Erik Smistad","Marit Valla"],"raw_abstract":"Digital pathology enables automatic analysis of histopathological sections\nusing artificial intelligence (AI). Automatic evaluation could improve\ndiagnostic efficiency and help find associations between morphological features\nand clinical outcome. For development of such prediction models, identifying\ninvasive epithelial cells, and separating these from benign epithelial cells\nand in situ lesions would be the first step. In this study, we aimed to develop\nan AI model for segmentation of epithelial cells in sections from breast\ncancer. We generated epithelial ground truth masks by restaining hematoxylin\nand eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists'\nannotations. HE/CK image pairs were used to train a convolutional neural\nnetwork, and data augmentation was used to make the model more robust. Tissue\nmicroarrays (TMAs) from 839 patients, and whole slide images from two patients\nwere used for training and evaluation of the models. The sections were derived\nfrom four cohorts of breast cancer patients. TMAs from 21 patients from a fifth\ncohort was used as a second test set. In quantitative evaluation, a mean Dice\nscore of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial\ncells, and in situ lesions, respectively, were achieved. In qualitative scoring\n(0-5) by pathologists, results were best for all epithelium and invasive\nepithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in\nsitu lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in\nHE stained breast cancer slides well, but further work is needed for accurate\ndivision between the classes. Immunohistochemistry, together with pathologists'\nannotations, enabled the creation of accurate ground truths. The model is made\nfreely available in FastPathology and the code is available at\nhttps://github.com/AICAN-Research/breast-epithelium-segmentation","publication_date":1700645108,"paper_link":"http://arxiv.org/pdf/2311.13261v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Digital pathology enables automatic analysis of histopathological sections using artificial intelligence (AI). Automatic evaluation could improve diagnostic efficiency and help find associations between morphological features and clinical outcome. For development of such prediction models, identifying invasive epithelial cells, and separating these from benign epithelial cells and in situ lesions would be the first step. In this study, we aimed to develop an AI model for segmentation of epithelial cells in sections from breast cancer. We generated epithelial ground truth masks by restaining hematoxylin and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists' annotations. HE/CK image pairs were used to train a convolutional neural network, and data augmentation was used to make the model more robust. Tissue microarrays (TMAs) from 839 patients, and whole slide images from two patients were used for training and evaluation of the models. The sections were derived from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth cohort was used as a second test set. In quantitative evaluation, a mean Dice score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial cells, and in situ lesions, respectively, were achieved. In qualitative scoring (0-5) by pathologists, results were best for all epithelium and invasive epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in HE stained breast cancer slides well, but further work is needed for accurate division between the classes. Immunohistochemistry, together with pathologists' annotations, enabled the creation of accurate ground truths. The model is made freely available in FastPathology and the code is available at https://github.com/AICAN-Research/breast-epithelium-segmentation"}
{"title":"Complement to Higher Bernstein Polynomials and Multiple Poles of 1 $\u0393$($\u03bb$) X |f | 2$\u03bb$ f --h $\u03c1$$\u03c9$ $\\land$ $\u03c9$'","authors":["Daniel Barlet"],"raw_abstract":"We give, using higher Bernstein polynomials defined in our paper [2], a\nstronger version of our previous result in [1] whose converse is proved in [2]\nand we give some complements to the results in [2] which help to compute these\nhigher order Bernstein polynomials. Then we show some non trivial examples\nwhere we determine the root of the second Bernstein polynomial which is not a\ndouble root of the full Bernstein polynomial and where the main theorem of [2]\napplies and localizes where a double pole exists for the meromorphic extension\nof the (conjugate) analytic functional given by polar parts of $\\omega$ '\n$\\rightarrow$ |f | 2$\\lambda$ f --h $\\rho$$\\omega$ $\\land$ $\\omega$' when h\n$\\in$ N is large enough.","publication_date":1700645053,"paper_link":"http://arxiv.org/pdf/2311.13259v1","categories":["Mathematics"],"abstract":"We give, using higher Bernstein polynomials defined in our paper [2], a stronger version of our previous result in [1] whose converse is proved in [2] and we give some complements to the results in [2] which help to compute these higher order Bernstein polynomials. Then we show some non trivial examples where we determine the root of the second Bernstein polynomial which is not a double root of the full Bernstein polynomial and where the main theorem of [2] applies and localizes where a double pole exists for the meromorphic extension of the (conjugate) analytic functional given by polar parts of __FORMULA__ ' __FORMULA__ |f | 2__FORMULA__ f --h __FORMULA____FORMULA__ __FORMULA__ __FORMULA__' when h __FORMULA__ N is large enough."}
{"title":"ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation","authors":["Yangyi Chen","Xingyao Wang","Manling Li","Derek Hoiem","Heng Ji"],"raw_abstract":"State-of-the-art vision-language models (VLMs) still have limited performance\nin structural knowledge extraction, such as relations between objects. In this\nwork, we present ViStruct, a training framework to learn VLMs for effective\nvisual structural knowledge extraction. Two novel designs are incorporated.\nFirst, we propose to leverage the inherent structure of programming language to\ndepict visual structural information. This approach enables explicit and\nconsistent representation of visual structural information of multiple\ngranularities, such as concepts, relations, and events, in a well-organized\nstructured format. Second, we introduce curriculum-based learning for VLMs to\nprogressively comprehend visual structures, from fundamental visual concepts to\nintricate event structures. Our intuition is that lower-level knowledge may\ncontribute to complex visual structure understanding. Furthermore, we compile\nand release a collection of datasets tailored for visual structural knowledge\nextraction. We adopt a weakly-supervised approach to directly generate visual\nevent structures from captions for ViStruct training, capitalizing on abundant\nimage-caption pairs from the web. In experiments, we evaluate ViStruct on\nvisual structure prediction tasks, demonstrating its effectiveness in improving\nthe understanding of visual structures. The code is public at\n\\url{https://github.com/Yangyi-Chen/vi-struct}.","publication_date":1700645014,"paper_link":"http://arxiv.org/pdf/2311.13258v1","categories":["Computer Science"],"abstract":"State-of-the-art vision-language models (VLMs) still have limited performance in structural knowledge extraction, such as relations between objects. In this work, we present ViStruct, a training framework to learn VLMs for effective visual structural knowledge extraction. Two novel designs are incorporated. First, we propose to leverage the inherent structure of programming language to depict visual structural information. This approach enables explicit and consistent representation of visual structural information of multiple granularities, such as concepts, relations, and events, in a well-organized structured format. Second, we introduce curriculum-based learning for VLMs to progressively comprehend visual structures, from fundamental visual concepts to intricate event structures. Our intuition is that lower-level knowledge may contribute to complex visual structure understanding. Furthermore, we compile and release a collection of datasets tailored for visual structural knowledge extraction. We adopt a weakly-supervised approach to directly generate visual event structures from captions for ViStruct training, capitalizing on abundant image-caption pairs from the web. In experiments, we evaluate ViStruct on visual structure prediction tasks, demonstrating its effectiveness in improving the understanding of visual structures. The code is public at https://github.com/Yangyi-Chen/vi-struct."}
{"title":"Direct observation of the morphological changes in a carbon fiber composite by means of in-situ micro-CT compression experiment","authors":["M. I. Santos","K. Koschek","L. Pursche","J. M. de Souza e Silva"],"raw_abstract":"Carbon fiber reinforced polymers are widely used due to their lightweight and\nstrong structural properties and understanding their mechanical behavior is\ncrucial for their reliable use in structural components. In this study, we\nevaluated the mechanical properties of a carbon fiber reinforced composite made\nof a dynamic polymer based on benzoxazine and polyetheramine Jeffamine(R) ED\n600. Standard compression tests were performed on specimens with layers\noriented longitudinally or transversely to the loading direction. In-situ\nmechanical testing using micro-computed tomography (micro-CT) imaging was\nconducted to capture detailed images of the internal microstructure during\ndeformation. Changes in porosity and pore shape were observed as the composite\nunderwent compression. Digital Volume Correlation (DVC) analysis was applied to\nquantify displacements and strains within the material and enabled the\nvisualization and quantification of strain patterns, which agree with kinking\nfailure.","publication_date":1700644862,"paper_link":"http://arxiv.org/pdf/2311.13257v1","categories":["Physics"],"abstract":"Carbon fiber reinforced polymers are widely used due to their lightweight and strong structural properties and understanding their mechanical behavior is crucial for their reliable use in structural components. In this study, we evaluated the mechanical properties of a carbon fiber reinforced composite made of a dynamic polymer based on benzoxazine and polyetheramine Jeffamine(R) ED 600. Standard compression tests were performed on specimens with layers oriented longitudinally or transversely to the loading direction. In-situ mechanical testing using micro-computed tomography (micro-CT) imaging was conducted to capture detailed images of the internal microstructure during deformation. Changes in porosity and pore shape were observed as the composite underwent compression. Digital Volume Correlation (DVC) analysis was applied to quantify displacements and strains within the material and enabled the visualization and quantification of strain patterns, which agree with kinking failure."}
{"title":"DA-STC: Domain Adaptive Video Semantic Segmentation via Spatio-Temporal Consistency","authors":["Zhe Zhang","Gaochang Wu","Jing Zhang","Chunhua Shen","Dacheng Tao","Tianyou Chai"],"raw_abstract":"Video semantic segmentation is a pivotal aspect of video representation\nlearning. However, significant domain shifts present a challenge in effectively\nlearning invariant spatio-temporal features across the labeled source domain\nand unlabeled target domain for video semantic segmentation. To solve the\nchallenge, we propose a novel DA-STC method for domain adaptive video semantic\nsegmentation, which incorporates a bidirectional multi-level spatio-temporal\nfusion module and a category-aware spatio-temporal feature alignment module to\nfacilitate consistent learning for domain-invariant features. Firstly, we\nperform bidirectional spatio-temporal fusion at the image sequence level and\nshallow feature level, leading to the construction of two fused intermediate\nvideo domains. This prompts the video semantic segmentation model to\nconsistently learn spatio-temporal features of shared patch sequences which are\ninfluenced by domain-specific contexts, thereby mitigating the feature gap\nbetween the source and target domain. Secondly, we propose a category-aware\nfeature alignment module to promote the consistency of spatio-temporal\nfeatures, facilitating adaptation to the target domain. Specifically, we\nadaptively aggregate the domain-specific deep features of each category along\nspatio-temporal dimensions, which are further constrained to achieve\ncross-domain intra-class feature alignment and inter-class feature separation.\nExtensive experiments demonstrate the effectiveness of our method, which\nachieves state-of-the-art mIOUs on multiple challenging benchmarks.\nFurthermore, we extend the proposed DA-STC to the image domain, where it also\nexhibits superior performance for domain adaptive semantic segmentation. The\nsource code and models will be made available at\n\\url{https://github.com/ZHE-SAPI/DA-STC}.","publication_date":1700644729,"paper_link":"http://arxiv.org/pdf/2311.13254v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Video semantic segmentation is a pivotal aspect of video representation learning. However, significant domain shifts present a challenge in effectively learning invariant spatio-temporal features across the labeled source domain and unlabeled target domain for video semantic segmentation. To solve the challenge, we propose a novel DA-STC method for domain adaptive video semantic segmentation, which incorporates a bidirectional multi-level spatio-temporal fusion module and a category-aware spatio-temporal feature alignment module to facilitate consistent learning for domain-invariant features. Firstly, we perform bidirectional spatio-temporal fusion at the image sequence level and shallow feature level, leading to the construction of two fused intermediate video domains. This prompts the video semantic segmentation model to consistently learn spatio-temporal features of shared patch sequences which are influenced by domain-specific contexts, thereby mitigating the feature gap between the source and target domain. Secondly, we propose a category-aware feature alignment module to promote the consistency of spatio-temporal features, facilitating adaptation to the target domain. Specifically, we adaptively aggregate the domain-specific deep features of each category along spatio-temporal dimensions, which are further constrained to achieve cross-domain intra-class feature alignment and inter-class feature separation. Extensive experiments demonstrate the effectiveness of our method, which achieves state-of-the-art mIOUs on multiple challenging benchmarks. Furthermore, we extend the proposed DA-STC to the image domain, where it also exhibits superior performance for domain adaptive semantic segmentation. The source code and models will be made available at https://github.com/ZHE-SAPI/DA-STC."}
{"title":"The Tully-Fisher relation from SDSS-MaNGA: Physical causes of scatter and variation at different radii","authors":["Andrei Ristea","Luca Cortese","Amelia Fraser-McKelvie","Barbara Catinella","Jesse van de Sande","Scott M. Croom","Mark Swinbank"],"raw_abstract":"The stellar mass Tully-Fisher relation (STFR) and its scatter encode valuable\ninformation about the processes shaping galaxy evolution across cosmic time.\nHowever, we are still missing a proper quantification of the STFR slope and\nscatter dependence on the baryonic tracer used to quantify rotational velocity,\non the velocity measurement radius and on galaxy integrated properties. We\npresent a catalogue of stellar and ionised gas (traced by H$\\alpha$ emission)\nkinematic measurements for a sample of galaxies drawn from the MaNGA Galaxy\nSurvey, providing an ideal tool for galaxy formation model calibration and for\ncomparison with high-redshift studies. We compute the STFRs for stellar and gas\nrotation at 1, 1.3 and 2 effective radii ($R_e$). The relations for both\nbaryonic components become shallower at 2$R_e$ compared to 1$R_e$ and 1.3$R_e$.\nWe report a steeper STFR for the stars in the inner parts ($\\leq 1.3 R_e$)\ncompared to the gas. At 2$R_e$, the relations for the two components are\nconsistent. When accounting for covariances with integrated v/$\\sigma$, scatter\nin the stellar and gas STFRs shows no strong correlation with: optical\nmorphology, star formation rate surface density, tidal interaction strength or\ngas accretion signatures. Our results suggest that the STFR scatter is driven\nby an increase in stellar/gas dispersional support, from either external\n(mergers) or internal (feedback) processes. No correlation between STFR scatter\nand environment is found. Nearby Universe galaxies have their stars and gas in\nstatistically different states of dynamical equilibrium in the inner parts\n($\\leq 1.3 R_e$), while at 2$R_{e}$ the two components are dynamically coupled.","publication_date":1700644461,"paper_link":"http://arxiv.org/pdf/2311.13251v1","categories":["Physics"],"abstract":"The stellar mass Tully-Fisher relation (STFR) and its scatter encode valuable information about the processes shaping galaxy evolution across cosmic time. However, we are still missing a proper quantification of the STFR slope and scatter dependence on the baryonic tracer used to quantify rotational velocity, on the velocity measurement radius and on galaxy integrated properties. We present a catalogue of stellar and ionised gas (traced by H__FORMULA__ emission) kinematic measurements for a sample of galaxies drawn from the MaNGA Galaxy Survey, providing an ideal tool for galaxy formation model calibration and for comparison with high-redshift studies. We compute the STFRs for stellar and gas rotation at 1, 1.3 and 2 effective radii (__FORMULA__). The relations for both baryonic components become shallower at 2__FORMULA__ compared to 1__FORMULA__ and 1.3__FORMULA__. We report a steeper STFR for the stars in the inner parts (__FORMULA__) compared to the gas. At 2__FORMULA__, the relations for the two components are consistent. When accounting for covariances with integrated v/__FORMULA__, scatter in the stellar and gas STFRs shows no strong correlation with: optical morphology, star formation rate surface density, tidal interaction strength or gas accretion signatures. Our results suggest that the STFR scatter is driven by an increase in stellar/gas dispersional support, from either external (mergers) or internal (feedback) processes. No correlation between STFR scatter and environment is found. Nearby Universe galaxies have their stars and gas in statistically different states of dynamical equilibrium in the inner parts (__FORMULA__), while at 2__FORMULA__ the two components are dynamically coupled."}
{"title":"Towards Hetero-Client Federated Multi-Task Learning","authors":["Yuxiang Lu","Suizhi Huang","Yuwen Yang","Shalayiding Sirejiding","Yue Ding","Hongtao Lu"],"raw_abstract":"Federated Learning (FL) enables joint training across distributed clients\nusing their local data privately. Federated Multi-Task Learning (FMTL) builds\non FL to handle multiple tasks, assuming model congruity that identical model\narchitecture is deployed in each client. To relax this assumption and thus\nextend real-world applicability, we introduce a novel problem setting,\nHetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse\ntask setups. The main challenge of HC-FMTL is the model incongruity issue that\ninvalidates conventional aggregation methods. It also escalates the\ndifficulties in accurate model aggregation to deal with data and task\nheterogeneity inherent in FMTL. To address these challenges, we propose the\nFedHCA$^2$ framework, which allows for federated training of personalized\nmodels by modeling relationships among heterogeneous clients. Drawing on our\ntheoretical insights into the difference between multi-task and federated\noptimization, we propose the Hyper Conflict-Averse Aggregation scheme to\nmitigate conflicts during encoder updates. Additionally, inspired by task\ninteraction in MTL, the Hyper Cross Attention Aggregation scheme uses\nlayer-wise cross attention to enhance decoder interactions while alleviating\nmodel incongruity. Moreover, we employ learnable Hyper Aggregation Weights for\neach client to customize personalized parameter updates. Extensive experiments\ndemonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios\ncompared to representative methods. Our code will be made publicly available.","publication_date":1700644370,"paper_link":"http://arxiv.org/pdf/2311.13250v1","categories":["Computer Science"],"abstract":"Federated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in accurate model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA__FORMULA__ framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference between multi-task and federated optimization, we propose the Hyper Conflict-Averse Aggregation scheme to mitigate conflicts during encoder updates. Additionally, inspired by task interaction in MTL, the Hyper Cross Attention Aggregation scheme uses layer-wise cross attention to enhance decoder interactions while alleviating model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for each client to customize personalized parameter updates. Extensive experiments demonstrate the superior performance of FedHCA__FORMULA__ in various HC-FMTL scenarios compared to representative methods. Our code will be made publicly available."}
{"title":"The Journey to Serverless Migration: An Empirical Analysis of Intentions, Strategies, and Challenges","authors":["Muhammad Hamza","Muhammad Azeem Akbar","Kari Smolander"],"raw_abstract":"Serverless is an emerging cloud computing paradigm that facilitates\ndevelopers to focus solely on the application logic rather than provisioning\nand managing the underlying infrastructure. The inherent characteristics such\nas scalability, flexibility, and cost efficiency of serverless computing,\nattracted many companies to migrate their legacy applications toward this\nparadigm. However, the stateless nature of serverless requires careful\nmigration planning, consideration of its subsequent implications, and potential\nchallenges. To this end, this study investigates the intentions, strategies,\nand technical and organizational challenges while migrating to a serverless\narchitecture. We investigated the migration processes of 11 systems across\ndiverse domains by conducting 15 in-depth interviews with professionals from 11\norganizations. we also presented a detailed discussion of each migration case.\nOur findings reveal that large enterprises primarily migrate to enhance\nscalability and operational efficiency, while smaller organizations intend to\nreduce the cost. Furthermore, organizations use a domain-driven design approach\nto identify the use case and gradually migrate to serverless using a strangler\npattern. However, migration encounters technical challenges i.e., testing\nevent-driven architecture, integrating with the legacy system, lack of\nstandardization, and organizational challenges i.e., mindset change and hiring\nskilled serverless developers as a prominent. The findings of this study\nprovide a comprehensive understanding that can guide future implementations and\nadvancements in the context of serverless migration.","publication_date":1700644219,"paper_link":"http://arxiv.org/pdf/2311.13249v1","categories":["Computer Science"],"abstract":"Serverless is an emerging cloud computing paradigm that facilitates developers to focus solely on the application logic rather than provisioning and managing the underlying infrastructure. The inherent characteristics such as scalability, flexibility, and cost efficiency of serverless computing, attracted many companies to migrate their legacy applications toward this paradigm. However, the stateless nature of serverless requires careful migration planning, consideration of its subsequent implications, and potential challenges. To this end, this study investigates the intentions, strategies, and technical and organizational challenges while migrating to a serverless architecture. We investigated the migration processes of 11 systems across diverse domains by conducting 15 in-depth interviews with professionals from 11 organizations. we also presented a detailed discussion of each migration case. Our findings reveal that large enterprises primarily migrate to enhance scalability and operational efficiency, while smaller organizations intend to reduce the cost. Furthermore, organizations use a domain-driven design approach to identify the use case and gradually migrate to serverless using a strangler pattern. However, migration encounters technical challenges i.e., testing event-driven architecture, integrating with the legacy system, lack of standardization, and organizational challenges i.e., mindset change and hiring skilled serverless developers as a prominent. The findings of this study provide a comprehensive understanding that can guide future implementations and advancements in the context of serverless migration."}
{"title":"Microstructured organic cavities with high-reflective flat reflectors fabricated by using a nanoimprint-bonding process","authors":["Takuya Enna","Yuji Adachi","Tsukasa Hirao","Shun Takahashi","Yohei Yamamoto","Kenichi Yamashita"],"raw_abstract":"The integration of photonic microstructure into organic microcavities\nrepresents an effective strategy for manipulating eigenstates of cavity or\npolariton modes. However, well-established fabrication processes for\nmicrostructured organic microcavities are still lacking. In this study, we\npropose a nanoimprint-bonding process as a novel fabrication method for\nmicrostructured organic microcavities. This process relies on a UV nanoimprint\ntechnique utilizing two different photopolymer resins, enabling the independent\nfabrication of highly reflective reflectors and photonic microstructures\nwithout compromising the accuracy of each. The resulting organic microcavities\ndemonstrate spatially localized photonic modes within dot structures and their\nnonlinear responses on the pumping fluence. Furthermore, a highly precise\nphotonic band is confirmed within a honeycomb lattice structure, which is owing\nto the high quality factor of the cavity achievable with the\nnanoimprint-bonding process. Additionally, a topological edge state is also\nobservable within a zigzag lattice structure. These results highlight the\nsignificant potential of our fabrication method for advancing organic-based\nphotonic devices, including lasers and polariton devices.","publication_date":1700644142,"paper_link":"http://arxiv.org/pdf/2311.13248v1","categories":["Physics"],"abstract":"The integration of photonic microstructure into organic microcavities represents an effective strategy for manipulating eigenstates of cavity or polariton modes. However, well-established fabrication processes for microstructured organic microcavities are still lacking. In this study, we propose a nanoimprint-bonding process as a novel fabrication method for microstructured organic microcavities. This process relies on a UV nanoimprint technique utilizing two different photopolymer resins, enabling the independent fabrication of highly reflective reflectors and photonic microstructures without compromising the accuracy of each. The resulting organic microcavities demonstrate spatially localized photonic modes within dot structures and their nonlinear responses on the pumping fluence. Furthermore, a highly precise photonic band is confirmed within a honeycomb lattice structure, which is owing to the high quality factor of the cavity achievable with the nanoimprint-bonding process. Additionally, a topological edge state is also observable within a zigzag lattice structure. These results highlight the significant potential of our fabrication method for advancing organic-based photonic devices, including lasers and polariton devices."}
{"title":"A projected nonlinear state-space model for forecasting time series signals","authors":["Christian Donner","Anuj Mishra","Hideaki Shimazaki"],"raw_abstract":"Learning and forecasting stochastic time series is essential in various\nscientific fields. However, despite the proposals of nonlinear filters and\ndeep-learning methods, it remains challenging to capture nonlinear dynamics\nfrom a few noisy samples and predict future trajectories with uncertainty\nestimates while maintaining computational efficiency. Here, we propose a fast\nalgorithm to learn and forecast nonlinear dynamics from noisy time series data.\nA key feature of the proposed model is kernel functions applied to projected\nlines, enabling fast and efficient capture of nonlinearities in the latent\ndynamics. Through empirical case studies and benchmarking, the model\ndemonstrates its effectiveness in learning and forecasting complex nonlinear\ndynamics, offering a valuable tool for researchers and practitioners in time\nseries analysis.","publication_date":1700643937,"paper_link":"http://arxiv.org/pdf/2311.13247v1","categories":["Statistics"],"abstract":"Learning and forecasting stochastic time series is essential in various scientific fields. However, despite the proposals of nonlinear filters and deep-learning methods, it remains challenging to capture nonlinear dynamics from a few noisy samples and predict future trajectories with uncertainty estimates while maintaining computational efficiency. Here, we propose a fast algorithm to learn and forecast nonlinear dynamics from noisy time series data. A key feature of the proposed model is kernel functions applied to projected lines, enabling fast and efficient capture of nonlinearities in the latent dynamics. Through empirical case studies and benchmarking, the model demonstrates its effectiveness in learning and forecasting complex nonlinear dynamics, offering a valuable tool for researchers and practitioners in time series analysis."}
{"title":"Automatic Instruction Optimization for Open-source LLM Instruction Tuning","authors":["Yilun Liu","Shimin Tao","Xiaofeng Zhao","Ming Zhu","Wenbing Ma","Junhao Zhu","Chang Su","Yutai Hou","Miao Zhang","Min Zhang","Hongxia Ma","Li Zhang","Hao Yang","Yanfei Jiang"],"raw_abstract":"Instruction tuning is crucial for enabling Language Learning Models (LLMs) in\nresponding to human instructions. The quality of instruction pairs used for\ntuning greatly affects the performance of LLMs. However, the manual creation of\nhigh-quality instruction datasets is costly, leading to the adoption of\nautomatic generation of instruction pairs by LLMs as a popular alternative in\nthe training of open-source LLMs. To ensure the high quality of LLM-generated\ninstruction datasets, several approaches have been proposed. Nevertheless,\nexisting methods either compromise dataset integrity by filtering a large\nproportion of samples, or are unsuitable for industrial applications. In this\npaper, instead of discarding low-quality samples, we propose CoachLM, a novel\napproach to enhance the quality of instruction datasets through automatic\nrevisions on samples in the dataset. CoachLM is trained from the samples\nrevised by human experts and significantly increases the proportion of\nhigh-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of\nCoachLM is further assessed on various real-world instruction test sets. The\nresults show that CoachLM improves the instruction-following capabilities of\nthe instruction-tuned LLM by an average of 29.9%, which even surpasses larger\nLLMs with nearly twice the number of parameters. Furthermore, CoachLM is\nsuccessfully deployed in a data management system for LLMs at Huawei, resulting\nin an efficiency improvement of up to 20% in the cleaning of 40k real-world\ninstruction pairs. We release the training data and code of CoachLM\n(https://github.com/lunyiliu/CoachLM).","publication_date":1700643897,"paper_link":"http://arxiv.org/pdf/2311.13246v1","categories":["Computer Science"],"abstract":"Instruction tuning is crucial for enabling Language Learning Models (LLMs) in responding to human instructions. The quality of instruction pairs used for tuning greatly affects the performance of LLMs. However, the manual creation of high-quality instruction datasets is costly, leading to the adoption of automatic generation of instruction pairs by LLMs as a popular alternative in the training of open-source LLMs. To ensure the high quality of LLM-generated instruction datasets, several approaches have been proposed. Nevertheless, existing methods either compromise dataset integrity by filtering a large proportion of samples, or are unsuitable for industrial applications. In this paper, instead of discarding low-quality samples, we propose CoachLM, a novel approach to enhance the quality of instruction datasets through automatic revisions on samples in the dataset. CoachLM is trained from the samples revised by human experts and significantly increases the proportion of high-quality samples in the dataset from 17.7% to 78.9%. The effectiveness of CoachLM is further assessed on various real-world instruction test sets. The results show that CoachLM improves the instruction-following capabilities of the instruction-tuned LLM by an average of 29.9%, which even surpasses larger LLMs with nearly twice the number of parameters. Furthermore, CoachLM is successfully deployed in a data management system for LLMs at Huawei, resulting in an efficiency improvement of up to 20% in the cleaning of 40k real-world instruction pairs. We release the training data and code of CoachLM (https://github.com/lunyiliu/CoachLM)."}
{"title":"Understanding Cost Dynamics of Serverless Computing: An Empirical Study","authors":["Muhammad Hamza","Muhammad Azeem Akbar","Rafael Capilla"],"raw_abstract":"The advent of serverless computing has revolutionized the landscape of cloud\ncomputing, offering a new paradigm that enables developers to focus solely on\ntheir applications rather than managing and provisioning the underlying\ninfrastructure. These applications involve integrating individual functions\ninto a cohesive workflow for complex tasks. The pay-per-use model and\nnontransparent reporting by cloud providers make it difficult to estimate\nserverless costs, imped-ing informed business decisions. Existing research\nstudies on serverless compu-ting focus on performance optimization and state\nmanagement, both from empir-ical and technical perspectives. However, the\nstate-of-the-art shows a lack of em-pirical investigations on the understanding\nof the cost dynamics of serverless computing over traditional cloud computing.\nTherefore, this study delves into how organizations anticipate the costs of\nadopting serverless. It also aims to com-prehend workload suitability and\nidentify best practices for cost optimization of serverless applications. To\nthis end, we conducted a qualitative (interviews) study with 15 experts from 8\ncompanies involved in the migration and development of serverless systems. The\nfindings revealed that, while serverless computing is highly suitable for\nunpredictable workloads, it may not be cost-effective for cer-tain high-scale\napplications. The study also introduces a taxonomy for comparing the cost of\nadopting serverless versus traditional cloud.","publication_date":1700643683,"paper_link":"http://arxiv.org/pdf/2311.13242v1","categories":["Computer Science"],"abstract":"The advent of serverless computing has revolutionized the landscape of cloud computing, offering a new paradigm that enables developers to focus solely on their applications rather than managing and provisioning the underlying infrastructure. These applications involve integrating individual functions into a cohesive workflow for complex tasks. The pay-per-use model and nontransparent reporting by cloud providers make it difficult to estimate serverless costs, imped-ing informed business decisions. Existing research studies on serverless compu-ting focus on performance optimization and state management, both from empir-ical and technical perspectives. However, the state-of-the-art shows a lack of em-pirical investigations on the understanding of the cost dynamics of serverless computing over traditional cloud computing. Therefore, this study delves into how organizations anticipate the costs of adopting serverless. It also aims to com-prehend workload suitability and identify best practices for cost optimization of serverless applications. To this end, we conducted a qualitative (interviews) study with 15 experts from 8 companies involved in the migration and development of serverless systems. The findings revealed that, while serverless computing is highly suitable for unpredictable workloads, it may not be cost-effective for cer-tain high-scale applications. The study also introduces a taxonomy for comparing the cost of adopting serverless versus traditional cloud."}
{"title":"On the Calibration of Large Language Models and Alignment","authors":["Chiwei Zhu","Benfeng Xu","Quan Wang","Yongdong Zhang","Zhendong Mao"],"raw_abstract":"As large language models attract increasing attention and find widespread\napplication, concurrent challenges of reliability also arise at the same time.\nConfidence calibration, an effective analysis method for gauging the\nreliability of deep models, serves as a crucial tool for assessing and\nimproving their reliability. However, such investigation has been comparatively\nunderexplored. In this work, we conduct a systematic examination of the\ncalibration of aligned language models throughout the entire construction\nprocess, including pretraining and alignment training. At each stage, we\ninvestigate how different training settings, such as parameter scales and\ntraining data, affect model calibration. To thoroughly assess model\ncalibration, we evaluate models on three most concerned aspects: generation,\nfactuality and understanding. Our work sheds light on whether popular LLMs are\nwell-calibrated and how the training process influences model calibration.","publication_date":1700643475,"paper_link":"http://arxiv.org/pdf/2311.13240v1","categories":["Computer Science"],"abstract":"As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration."}
{"title":"TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer","authors":["Huimin Xiong","Kunle Li","Kaiyuan Tan","Yang Feng","Joey Tianyi Zhou","Jin Hao","Haochao Ying","Jian Wu","Zuozhu Liu"],"raw_abstract":"Optical Intraoral Scanners (IOS) are widely used in digital dentistry to\nprovide detailed 3D information of dental crowns and the gingiva. Accurate 3D\ntooth segmentation in IOSs is critical for various dental applications, while\nprevious methods are error-prone at complicated boundaries and exhibit\nunsatisfactory results across patients. In this paper, we propose TSegFormer\nwhich captures both local and global dependencies among different teeth and the\ngingiva in the IOS point clouds with a multi-task 3D transformer architecture.\nMoreover, we design a geometry-guided loss based on a novel point curvature to\nrefine boundaries in an end-to-end manner, avoiding time-consuming\npost-processing to reach clinically applicable segmentation. In addition, we\ncreate a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of\nour knowledge. The experimental results demonstrate that our TSegFormer\nconsistently surpasses existing state-of-the-art baselines. The superiority of\nTSegFormer is corroborated by extensive analysis, visualizations and real-world\nclinical applicability tests. Our code is available at\nhttps://github.com/huiminxiong/TSegFormer.","publication_date":1700642701,"paper_link":"http://arxiv.org/pdf/2311.13234v1","categories":["Computer Science"],"abstract":"Optical Intraoral Scanners (IOS) are widely used in digital dentistry to provide detailed 3D information of dental crowns and the gingiva. Accurate 3D tooth segmentation in IOSs is critical for various dental applications, while previous methods are error-prone at complicated boundaries and exhibit unsatisfactory results across patients. In this paper, we propose TSegFormer which captures both local and global dependencies among different teeth and the gingiva in the IOS point clouds with a multi-task 3D transformer architecture. Moreover, we design a geometry-guided loss based on a novel point curvature to refine boundaries in an end-to-end manner, avoiding time-consuming post-processing to reach clinically applicable segmentation. In addition, we create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of our knowledge. The experimental results demonstrate that our TSegFormer consistently surpasses existing state-of-the-art baselines. The superiority of TSegFormer is corroborated by extensive analysis, visualizations and real-world clinical applicability tests. Our code is available at https://github.com/huiminxiong/TSegFormer."}
{"title":"A Survey of Adversarial CAPTCHAs on its History, Classification and Generation","authors":["Zisheng Xu","Qiao Yan","F. Richard Yu","Victor C. M. Leung"],"raw_abstract":"Completely Automated Public Turing test to tell Computers and Humans Apart,\nshort for CAPTCHA, is an essential and relatively easy way to defend against\nmalicious attacks implemented by bots. The security and usability trade-off\nlimits the use of massive geometric transformations to interfere deep model\nrecognition and deep models even outperformed humans in complex CAPTCHAs. The\ndiscovery of adversarial examples provides an ideal solution to the security\nand usability trade-off by integrating adversarial examples and CAPTCHAs to\ngenerate adversarial CAPTCHAs that can fool the deep models. In this paper, we\nextend the definition of adversarial CAPTCHAs and propose a classification\nmethod for adversarial CAPTCHAs. Then we systematically review some commonly\nused methods to generate adversarial examples and methods that are successfully\nused to generate adversarial CAPTCHAs. Also, we analyze some defense methods\nthat can be used to defend adversarial CAPTCHAs, indicating potential threats\nto adversarial CAPTCHAs. Finally, we discuss some possible future research\ndirections for adversarial CAPTCHAs at the end of this paper.","publication_date":1700642698,"paper_link":"http://arxiv.org/pdf/2311.13233v1","categories":["Computer Science"],"abstract":"Completely Automated Public Turing test to tell Computers and Humans Apart, short for CAPTCHA, is an essential and relatively easy way to defend against malicious attacks implemented by bots. The security and usability trade-off limits the use of massive geometric transformations to interfere deep model recognition and deep models even outperformed humans in complex CAPTCHAs. The discovery of adversarial examples provides an ideal solution to the security and usability trade-off by integrating adversarial examples and CAPTCHAs to generate adversarial CAPTCHAs that can fool the deep models. In this paper, we extend the definition of adversarial CAPTCHAs and propose a classification method for adversarial CAPTCHAs. Then we systematically review some commonly used methods to generate adversarial examples and methods that are successfully used to generate adversarial CAPTCHAs. Also, we analyze some defense methods that can be used to defend adversarial CAPTCHAs, indicating potential threats to adversarial CAPTCHAs. Finally, we discuss some possible future research directions for adversarial CAPTCHAs at the end of this paper."}
{"title":"Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model","authors":["Kai Yang","Jian Tao","Jiafei Lyu","Chunjiang Ge","Jiaxin Chen","Qimai Li","Weihan Shen","Xiaolong Zhu","Xiu Li"],"raw_abstract":"Using reinforcement learning with human feedback (RLHF) has shown significant\npromise in fine-tuning diffusion models. Previous methods start by training a\nreward model that aligns with human preferences, then leverage RL techniques to\nfine-tune the underlying models. However, crafting an efficient reward model\ndemands extensive datasets, optimal architecture, and manual hyperparameter\ntuning, making the process both time and cost-intensive. The direct preference\noptimization (DPO) method, effective in fine-tuning large language models,\neliminates the necessity for a reward model. However, the extensive GPU memory\nrequirement of the diffusion model's denoising process hinders the direct\napplication of the DPO method. To address this issue, we introduce the Direct\nPreference for Denoising Diffusion Policy Optimization (D3PO) method to\ndirectly fine-tune diffusion models. The theoretical analysis demonstrates that\nalthough D3PO omits training a reward model, it effectively functions as the\noptimal reward model trained using human feedback data to guide the learning\nprocess. This approach requires no training of a reward model, proving to be\nmore direct, cost-effective, and minimizing computational overhead. In\nexperiments, our method uses the relative scale of objectives as a proxy for\nhuman preference, delivering comparable results to methods using ground-truth\nrewards. Moreover, D3PO demonstrates the ability to reduce image distortion\nrates and generate safer images, overcoming challenges lacking robust reward\nmodels.","publication_date":1700642566,"paper_link":"http://arxiv.org/pdf/2311.13231v1","categories":["Computer Science"],"abstract":"Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models."}
{"title":"Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus","authors":["Tianhang Zhang","Lin Qiu","Qipeng Guo","Cheng Deng","Yue Zhang","Zheng Zhang","Chenghu Zhou","Xinbing Wang","Luoyi Fu"],"raw_abstract":"Large Language Models (LLMs) have gained significant popularity for their\nimpressive performance across diverse fields. However, LLMs are prone to\nhallucinate untruthful or nonsensical outputs that fail to meet user\nexpectations in many real-world applications. Existing works for detecting\nhallucinations in LLMs either rely on external knowledge for reference\nretrieval or require sampling multiple responses from the LLM for consistency\nverification, making these methods costly and inefficient. In this paper, we\npropose a novel reference-free, uncertainty-based method for detecting\nhallucinations in LLMs. Our approach imitates human focus in factuality\nchecking from three aspects: 1) focus on the most informative and important\nkeywords in the given text; 2) focus on the unreliable tokens in historical\ncontext which may lead to a cascade of hallucinations; and 3) focus on the\ntoken properties such as token type and token frequency. Experimental results\non relevant datasets demonstrate the effectiveness of our proposed method,\nwhich achieves state-of-the-art performance across all the evaluation metrics\nand eliminates the need for additional information.","publication_date":1700642357,"paper_link":"http://arxiv.org/pdf/2311.13230v1","categories":["Computer Science"],"abstract":"Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information."}
{"title":"Strain mediated phase crossover in Ruddlesden Popper nickelates","authors":["Ting Cui","Songhee Choi","Ting Lin","Chen Liu","Gang Wang","Ningning Wang","Shengru Chen","Haitao Hong","Dongke Rong","Qianying Wang","Qiao Jin","Jia-Ou Wang","Lin Gu","Chen Ge","Can Wang","Jin Guang Cheng","Qinghua Zhang","Liang Si","Kui-juan Jin","Er-Jia Guo"],"raw_abstract":"Recent progress on the signatures of pressure-induced high temperature\nsuperconductivity in Ruddlesden Popper (RP) nickelates (Lan+1NinO3n+1) has\nattracted growing interest in both theoretical calculations and experimental\nefforts. The fabrication of high-quality single crystalline RP nickelate thin\nfilms is critical for possible reducing the superconducting transition pressure\nand advancing applications in microelectronics in the future. In this study, we\nreport the observations of an active phase transition in RP nickelate films\ninduced by misfit strain. We found that RP nickelate films favor the perovskite\nstructure (n = infinite) under tensile strains, while compressive strains\nstabilize the La3Ni2O7 (n = 2) phase. The selection of distinct phases is\ngoverned by the strain dependent formation energy and electronic configuration.\nIn compressively strained La3Ni2O7, we experimentally determined splitting\nenergy is ~0.2 eV and electrons prefer to occupy in-plane orbitals. First\nprinciples calculations unveil a robust coupling between strain effects and the\nvalence state of Ni ions in RP nickelates, suggesting a dual driving force for\nthe inevitable phase co-existence transition in RP nickelates. Our work\nunderscores the sensitivity of RP nickelate formation to epitaxial strain,\npresenting a significant challenge in fabricating pure-phase RP nickelate\nfilms. Therefore, special attention to stacking defects and grain boundaries\nbetween different RP phases is essential when discussing the pressure-induced\nsuperconductivity in RP nickelates.","publication_date":1700641981,"paper_link":"http://arxiv.org/pdf/2311.13228v1","categories":["Physics"],"abstract":"Recent progress on the signatures of pressure-induced high temperature superconductivity in Ruddlesden Popper (RP) nickelates (Lan+1NinO3n+1) has attracted growing interest in both theoretical calculations and experimental efforts. The fabrication of high-quality single crystalline RP nickelate thin films is critical for possible reducing the superconducting transition pressure and advancing applications in microelectronics in the future. In this study, we report the observations of an active phase transition in RP nickelate films induced by misfit strain. We found that RP nickelate films favor the perovskite structure (n = infinite) under tensile strains, while compressive strains stabilize the La3Ni2O7 (n = 2) phase. The selection of distinct phases is governed by the strain dependent formation energy and electronic configuration. In compressively strained La3Ni2O7, we experimentally determined splitting energy is ~0.2 eV and electrons prefer to occupy in-plane orbitals. First principles calculations unveil a robust coupling between strain effects and the valence state of Ni ions in RP nickelates, suggesting a dual driving force for the inevitable phase co-existence transition in RP nickelates. Our work underscores the sensitivity of RP nickelate formation to epitaxial strain, presenting a significant challenge in fabricating pure-phase RP nickelate films. Therefore, special attention to stacking defects and grain boundaries between different RP phases is essential when discussing the pressure-induced superconductivity in RP nickelates."}
{"title":"NeutronOrch: Rethinking Sample-based GNN Training under CPU-GPU Heterogeneous Environments","authors":["Xin Ai","Qiange Wang","Chunyu Cao","Yanfeng Zhang","Chaoyi Chen","Hao Yuan","Yu Gu","Ge Yu"],"raw_abstract":"Graph Neural Networks (GNNs) have demonstrated outstanding performance in\nvarious applications. Existing frameworks utilize CPU-GPU heterogeneous\nenvironments to train GNN models and integrate mini-batch and sampling\ntechniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous\nenvironments, we can divide sample-based GNN training into three steps: sample,\ngather, and train. Existing GNN systems use different task orchestrating\nmethods to employ each step on CPU or GPU. After extensive experiments and\nanalysis, we find that existing task orchestrating methods fail to fully\nutilize the heterogeneous resources, limited by inefficient CPU processing or\nGPU resource contention. In this paper, we propose NeutronOrch, a system for\nsample-based GNN training that incorporates a layer-based task orchestrating\nmethod and ensures balanced utilization of the CPU and GPU. NeutronOrch\ndecouples the training process by layer and pushes down the training task of\nthe bottom layer to the CPU. This significantly reduces the computational load\nand memory footprint of GPU training. To avoid inefficient CPU processing,\nNeutronOrch only offloads the training of frequently accessed vertices to the\nCPU and lets GPU reuse their embeddings with bounded staleness. Furthermore,\nNeutronOrch provides a fine-grained pipeline design for the layer-based task\norchestrating method, fully overlapping different tasks on heterogeneous\nresources while strictly guaranteeing bounded staleness. The experimental\nresults show that compared with the state-of-the-art GNN systems, NeutronOrch\ncan achieve up to 4.61x performance speedup.","publication_date":1700641602,"paper_link":"http://arxiv.org/pdf/2311.13225v1","categories":["Computer Science"],"abstract":"Graph Neural Networks (GNNs) have demonstrated outstanding performance in various applications. Existing frameworks utilize CPU-GPU heterogeneous environments to train GNN models and integrate mini-batch and sampling techniques to overcome the GPU memory limitation. In CPU-GPU heterogeneous environments, we can divide sample-based GNN training into three steps: sample, gather, and train. Existing GNN systems use different task orchestrating methods to employ each step on CPU or GPU. After extensive experiments and analysis, we find that existing task orchestrating methods fail to fully utilize the heterogeneous resources, limited by inefficient CPU processing or GPU resource contention. In this paper, we propose NeutronOrch, a system for sample-based GNN training that incorporates a layer-based task orchestrating method and ensures balanced utilization of the CPU and GPU. NeutronOrch decouples the training process by layer and pushes down the training task of the bottom layer to the CPU. This significantly reduces the computational load and memory footprint of GPU training. To avoid inefficient CPU processing, NeutronOrch only offloads the training of frequently accessed vertices to the CPU and lets GPU reuse their embeddings with bounded staleness. Furthermore, NeutronOrch provides a fine-grained pipeline design for the layer-based task orchestrating method, fully overlapping different tasks on heterogeneous resources while strictly guaranteeing bounded staleness. The experimental results show that compared with the state-of-the-art GNN systems, NeutronOrch can achieve up to 4.61x performance speedup."}
{"title":"Nanoporous gold thin films as substrates to analyze liquids by cryo-atom probe tomography","authors":["E. V. Woods","A. Saksena","A. A. El-Zoka","L. T. Stephenson","T. M. Schwarz","M. P. Singh","L. S. Aota","S. -H. Kim","J. Schneider","B. Gault"],"raw_abstract":"Cryogenic atom probe tomography (cryo-APT) is being developed to enable\nnanoscale compositional analyses of frozen liquids. Yet, the availability of\nreadily available substrates that allow for the fixation of liquids while\nproviding sufficient strength to their interface, is still an issue. Here we\npropose the use of 1-2 microns thick binary alloy film of gold-silver (AuAg)\nsputtered onto flat silicon, with sufficient adhesion without an additional\nlayer. Through chemical dealloying, we successfully fabricate a nanoporous\nsubstrate, with open-pore structure, which is mounted on a microarray of Si\nposts by lift out in the focused-ion beam, allowing for cryogenic fixation of\nliquids. We present cryo-APT results obtained after cryogenic sharpening,\nvacuum cryo-transfer and analysis of pure water on top and inside the\nnanoporous film. We demonstrate that this new substrate has the requisite\ncharacteristics for facilitating cryo-APT of frozen liquids, with a relatively\nlower volume of precious metals. This complete workflow represents an improved\napproach for frozen liquid analysis, from preparation of the films to the\nsuccessful fixation of the liquid in the porous network, to cryo-atom probe\ntomography.","publication_date":1700641596,"paper_link":"http://arxiv.org/pdf/2311.13224v1","categories":["Physics"],"abstract":"Cryogenic atom probe tomography (cryo-APT) is being developed to enable nanoscale compositional analyses of frozen liquids. Yet, the availability of readily available substrates that allow for the fixation of liquids while providing sufficient strength to their interface, is still an issue. Here we propose the use of 1-2 microns thick binary alloy film of gold-silver (AuAg) sputtered onto flat silicon, with sufficient adhesion without an additional layer. Through chemical dealloying, we successfully fabricate a nanoporous substrate, with open-pore structure, which is mounted on a microarray of Si posts by lift out in the focused-ion beam, allowing for cryogenic fixation of liquids. We present cryo-APT results obtained after cryogenic sharpening, vacuum cryo-transfer and analysis of pure water on top and inside the nanoporous film. We demonstrate that this new substrate has the requisite characteristics for facilitating cryo-APT of frozen liquids, with a relatively lower volume of precious metals. This complete workflow represents an improved approach for frozen liquid analysis, from preparation of the films to the successful fixation of the liquid in the porous network, to cryo-atom probe tomography."}
{"title":"Design Recommendations Based on Speech Analysis for Disability-Friendly Interfaces for the Control of a Home Automation Environment","authors":["Nadine Vigouroux","Fr\u00e9d\u00e9ric Vella","Ga\u00eblle Lepage","\u00c9ric Campo"],"raw_abstract":"The objective of this paper is to describe the study on speech interaction\nmode for home automation control of equipment by impaired people for an\ninclusive housing. The study is related to the HIP HOPE project concerning a\nbuilding of 19 inclusive housing units. 7 participants with different types of\ndisabilities were invited to carry out use cases using voice and touch control.\nOnly the results obtained on the voice interaction mode through the Amazon\nvoice assistant are reported here. The results show, according to the type of\nhandicap, the success rates in the speech recognition of the command emitted on\nthe equipment and highlight the errors related to the formulation, the noisy\nenvironment, the intelligible speech, the speech segmentation and the bad\nsynchronization of the audio channel opening.","publication_date":1700641559,"paper_link":"http://arxiv.org/pdf/2311.13223v1","categories":["Computer Science"],"abstract":"The objective of this paper is to describe the study on speech interaction mode for home automation control of equipment by impaired people for an inclusive housing. The study is related to the HIP HOPE project concerning a building of 19 inclusive housing units. 7 participants with different types of disabilities were invited to carry out use cases using voice and touch control. Only the results obtained on the voice interaction mode through the Amazon voice assistant are reported here. The results show, according to the type of handicap, the success rates in the speech recognition of the command emitted on the equipment and highlight the errors related to the formulation, the noisy environment, the intelligible speech, the speech segmentation and the bad synchronization of the audio channel opening."}
{"title":"Towards Detecting, Recognizing, and Parsing the Address Information from Bangla Signboard: A Deep Learning-based Approach","authors":["Hasan Murad","Mohammed Eunus Ali"],"raw_abstract":"Retrieving textual information from natural scene images is an active\nresearch area in the field of computer vision with numerous practical\napplications. Detecting text regions and extracting text from signboards is a\nchallenging problem due to special characteristics like reflecting lights,\nuneven illumination, or shadows found in real-life natural scene images. With\nthe advent of deep learning-based methods, different sophisticated techniques\nhave been proposed for text detection and text recognition from the natural\nscene. Though a significant amount of effort has been devoted to extracting\nnatural scene text for resourceful languages like English, little has been done\nfor low-resource languages like Bangla. In this research work, we have proposed\nan end-to-end system with deep learning-based models for efficiently detecting,\nrecognizing, correcting, and parsing address information from Bangla\nsignboards. We have created manually annotated datasets and synthetic datasets\nto train signboard detection, address text detection, address text recognition,\naddress text correction, and address text parser models. We have conducted a\ncomparative study among different CTC-based and Encoder-Decoder model\narchitectures for Bangla address text recognition. Moreover, we have designed a\nnovel address text correction model using a sequence-to-sequence\ntransformer-based network to improve the performance of Bangla address text\nrecognition model by post-correction. Finally, we have developed a Bangla\naddress text parser using the state-of-the-art transformer-based pre-trained\nlanguage model.","publication_date":1700641515,"paper_link":"http://arxiv.org/pdf/2311.13222v1","categories":["Computer Science"],"abstract":"Retrieving textual information from natural scene images is an active research area in the field of computer vision with numerous practical applications. Detecting text regions and extracting text from signboards is a challenging problem due to special characteristics like reflecting lights, uneven illumination, or shadows found in real-life natural scene images. With the advent of deep learning-based methods, different sophisticated techniques have been proposed for text detection and text recognition from the natural scene. Though a significant amount of effort has been devoted to extracting natural scene text for resourceful languages like English, little has been done for low-resource languages like Bangla. In this research work, we have proposed an end-to-end system with deep learning-based models for efficiently detecting, recognizing, correcting, and parsing address information from Bangla signboards. We have created manually annotated datasets and synthetic datasets to train signboard detection, address text detection, address text recognition, address text correction, and address text parser models. We have conducted a comparative study among different CTC-based and Encoder-Decoder model architectures for Bangla address text recognition. Moreover, we have designed a novel address text correction model using a sequence-to-sequence transformer-based network to improve the performance of Bangla address text recognition model by post-correction. Finally, we have developed a Bangla address text parser using the state-of-the-art transformer-based pre-trained language model."}
{"title":"Asymptotically compatible energy and dissipation law of the nonuniform L2-$1_\u03c3$ scheme for time fractional Allen-Cahn model","authors":["Hong-lin Liao","Xiaohan Zhu","Hong Sun"],"raw_abstract":"We build an asymptotically compatible energy of the variable-step\nL2-$1_{\\sigma}$ scheme for the time-fractional Allen-Cahn model with the\nCaputo's fractional derivative of order $\\alpha\\in(0,1)$, under a weak\nstep-ratio constraint $\\tau_k/\\tau_{k-1}\\geq r_{\\star}(\\alpha)$ for $k\\ge2$,\nwhere $\\tau_k$ is the $k$-th time-step size and\n$r_{\\star}(\\alpha)\\in(0.3865,0.4037)$ for $\\alpha\\in(0,1)$. It provides a\npositive answer to the open problem in [J. Comput. Phys., 414:109473], and, to\nthe best of our knowledge, it is the first second-order nonuniform\ntime-stepping scheme to preserve both the maximum bound principle and the\nenergy dissipation law of time-fractional Allen-Cahn model. The compatible\ndiscrete energy is constructed via a novel discrete gradient structure of the\nsecond-order L2-$1_{\\sigma}$ formula by a local-nonlocal splitting technique.\nIt splits the discrete fractional derivative into two parts: one is a local\nterm analogue to the trapezoid rule of the first derivative and the other is a\nnonlocal summation analogue to the L1 formula of Caputo derivative. Numerical\nexamples with an adaptive time-stepping strategy are provided to show the\neffectiveness of our scheme and the asymptotic properties of the associated\nmodified energy.","publication_date":1700640204,"paper_link":"http://arxiv.org/pdf/2311.13216v1","categories":["Mathematics"],"abstract":"We build an asymptotically compatible energy of the variable-step L2-__FORMULA__ scheme for the time-fractional Allen-Cahn model with the Caputo's fractional derivative of order __FORMULA__, under a weak step-ratio constraint __FORMULA__ for __FORMULA__, where __FORMULA__ is the __FORMULA__-th time-step size and __FORMULA__ for __FORMULA__. It provides a positive answer to the open problem in [J. Comput. Phys., 414:109473], and, to the best of our knowledge, it is the first second-order nonuniform time-stepping scheme to preserve both the maximum bound principle and the energy dissipation law of time-fractional Allen-Cahn model. The compatible discrete energy is constructed via a novel discrete gradient structure of the second-order L2-__FORMULA__ formula by a local-nonlocal splitting technique. It splits the discrete fractional derivative into two parts: one is a local term analogue to the trapezoid rule of the first derivative and the other is a nonlocal summation analogue to the L1 formula of Caputo derivative. Numerical examples with an adaptive time-stepping strategy are provided to show the effectiveness of our scheme and the asymptotic properties of the associated modified energy."}
{"title":"Engineering magnetic domain wall energies in multiferroic BiFeO$_3$ via epitaxial strain","authors":["Sebastian Meyer","Bin Xu","Laurent Bellaiche","Bertrand Dup\u00e9"],"raw_abstract":"Epitaxial strain has emerged as a powerful tool to tune magnetic and\nferroelectric properties in functional materials such as in multiferroic\nperovskite oxides. Here, we use first-principles calculations to explore the\nevolution of magnetic interactions in the antiferromagnetic multiferroic\nBiFeO$_3$ (BFO), one of the most promising multiferroics for future technology.\nThe epitaxial strain in BFO(001) oriented film is varied between\n$\\varepsilon_{xx,yy}$ $\\in$ $[-2\\%, +2\\%]$. We find that both strengths of the\nexchange interaction and Dzyaloshinskii-Moriya interaction (DMI) decrease\nlinearly from compressive to tensile strain whereas the uniaxial\nmagnetocrystalline anisotropy follows a parabolic behavior which lifts the\nenergy degeneracy of the (111) easy plane of bulk BFO. From the trends of the\nmagnetic interactions we can explain the destruction of cycloidal order in\ncompressive strain as observed in experiments due to the increasing anisotropy\nenergy. For tensile strain, we predict that the ground state remains unchanged\nas a function of strain. By using the domain wall (DW) energy, we envision the\nregion where isolated chiral magnetic texture might occur as function of strain\ni.e. where the DW and the spin spiral energy are equal. This transition between\n$-1.5\\%$ and $-0.5\\%$ of strain should allow topologically stable magnetic\nstates such as antiferromagnetic skyrmions and merons to occur. Hence, our work\nshould trigger experimental and theoretical investigations in this range of\nstrain.","publication_date":1700640102,"paper_link":"http://arxiv.org/pdf/2311.13215v1","categories":["Physics"],"abstract":"Epitaxial strain has emerged as a powerful tool to tune magnetic and ferroelectric properties in functional materials such as in multiferroic perovskite oxides. Here, we use first-principles calculations to explore the evolution of magnetic interactions in the antiferromagnetic multiferroic BiFeO__FORMULA__ (BFO), one of the most promising multiferroics for future technology. The epitaxial strain in BFO(001) oriented film is varied between __FORMULA__ __FORMULA__ __FORMULA__. We find that both strengths of the exchange interaction and Dzyaloshinskii-Moriya interaction (DMI) decrease linearly from compressive to tensile strain whereas the uniaxial magnetocrystalline anisotropy follows a parabolic behavior which lifts the energy degeneracy of the (111) easy plane of bulk BFO. From the trends of the magnetic interactions we can explain the destruction of cycloidal order in compressive strain as observed in experiments due to the increasing anisotropy energy. For tensile strain, we predict that the ground state remains unchanged as a function of strain. By using the domain wall (DW) energy, we envision the region where isolated chiral magnetic texture might occur as function of strain i.e. where the DW and the spin spiral energy are equal. This transition between __FORMULA__ and __FORMULA__ of strain should allow topologically stable magnetic states such as antiferromagnetic skyrmions and merons to occur. Hence, our work should trigger experimental and theoretical investigations in this range of strain."}
{"title":"Artificial Intelligence in the Service of Entrepreneurial Finance: Knowledge Structure and the Foundational Algorithmic Paradigm","authors":["Robert Kudeli\u0107","Tamara \u0160maguc","Sherry Robinson"],"raw_abstract":"While the application of Artificial Intelligence in Finance has a long\ntradition, its potential in Entrepreneurship has been intensively explored only\nrecently. In this context, Entrepreneurial Finance is a particularly fertile\nground for future Artificial Intelligence proliferation. To support the latter,\nthe study provides a bibliometric review of Artificial Intelligence\napplications in (1) entrepreneurial finance literature, and (2) corporate\nfinance literature with implications for Entrepreneurship. Rigorous search and\nscreening procedures of the scientific database Web of Science Core Collection\nresulted in the identification of 1890 relevant journal articles subjected to\nanalysis. The bibliometric analysis gives a rich insight into the knowledge\nfield's conceptual, intellectual, and social structure, indicating nascent and\nunderdeveloped research directions. As far as we were able to identify, this is\nthe first study to map and bibliometrically analyze the academic field\nconcerning the relationship between Artificial Intelligence, Entrepreneurship,\nand Finance, and the first review that deals with Artificial Intelligence\nmethods in Entrepreneurship. According to the results, Artificial Neural\nNetwork, Deep Neural Network and Support Vector Machine are highly represented\nin almost all identified topic niches. At the same time, applying Topic\nModeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is\nquite rare. As an element of the research, and before final remarks, the\narticle deals as well with a discussion of certain gaps in the relationship\nbetween Computer Science and Economics. These gaps do represent problems in the\napplication of Artificial Intelligence in Economic Science. As a way to at\nleast in part remedy this situation, the foundational paradigm and the bespoke\ndemonstration of the Monte Carlo randomized algorithm are presented.","publication_date":1700639926,"paper_link":"http://arxiv.org/pdf/2311.13213v1","categories":["Computer Science"],"abstract":"While the application of Artificial Intelligence in Finance has a long tradition, its potential in Entrepreneurship has been intensively explored only recently. In this context, Entrepreneurial Finance is a particularly fertile ground for future Artificial Intelligence proliferation. To support the latter, the study provides a bibliometric review of Artificial Intelligence applications in (1) entrepreneurial finance literature, and (2) corporate finance literature with implications for Entrepreneurship. Rigorous search and screening procedures of the scientific database Web of Science Core Collection resulted in the identification of 1890 relevant journal articles subjected to analysis. The bibliometric analysis gives a rich insight into the knowledge field's conceptual, intellectual, and social structure, indicating nascent and underdeveloped research directions. As far as we were able to identify, this is the first study to map and bibliometrically analyze the academic field concerning the relationship between Artificial Intelligence, Entrepreneurship, and Finance, and the first review that deals with Artificial Intelligence methods in Entrepreneurship. According to the results, Artificial Neural Network, Deep Neural Network and Support Vector Machine are highly represented in almost all identified topic niches. At the same time, applying Topic Modeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is quite rare. As an element of the research, and before final remarks, the article deals as well with a discussion of certain gaps in the relationship between Computer Science and Economics. These gaps do represent problems in the application of Artificial Intelligence in Economic Science. As a way to at least in part remedy this situation, the foundational paradigm and the bespoke demonstration of the Monte Carlo randomized algorithm are presented."}
{"title":"Coherent and incoherent excitation pathways in time-resolved photoemission orbital tomography of CuPc/Cu(001)-2O","authors":["Alexa Adamkiewicz","Miriam Raths","Monja Stettner","Marcel Theilen","Lasse M\u00fcnster","Sabine Wenzel","Mark Hutter","Sergey Soubatch","Christian Kumpf","Fran\u00e7ois C. Bocquet","Robert Wallauer","Frank Stefan Tautz","Ulrich H\u00f6fer"],"raw_abstract":"Time-resolved photoemission orbital tomography (tr-POT) offers unique\npossibilities for tracing molecular electron dynamics. The recorded\npump-induced changes of the angle-resolved photoemission intensities allow to\ncharacterize unoccupied molecular states in momentum space and to deduce the\nincoherent temporal evolution of their population. Here, we show for the\nexample of CuPc/Cu(001)-2O that the method also gives access to the coherent\nregime and that different excitation pathways can be disentangled by a careful\nanalysis of the time-dependent change of the photoemission momentum pattern. In\nparticular, we demonstrate by varying photon energy and polarization of the\npump light, how the incoherent temporal evolution of the LUMO distribution can\nbe distinguished from coherent contributions of the projected HOMO. Moreover,\nwe report the selective excitation of molecules with a specific orientation at\nnormal incidence by aligning the electric field of the pump light along the\nmolecular axis.","publication_date":1700639517,"paper_link":"http://arxiv.org/pdf/2311.13212v1","categories":["Physics"],"abstract":"Time-resolved photoemission orbital tomography (tr-POT) offers unique possibilities for tracing molecular electron dynamics. The recorded pump-induced changes of the angle-resolved photoemission intensities allow to characterize unoccupied molecular states in momentum space and to deduce the incoherent temporal evolution of their population. Here, we show for the example of CuPc/Cu(001)-2O that the method also gives access to the coherent regime and that different excitation pathways can be disentangled by a careful analysis of the time-dependent change of the photoemission momentum pattern. In particular, we demonstrate by varying photon energy and polarization of the pump light, how the incoherent temporal evolution of the LUMO distribution can be distinguished from coherent contributions of the projected HOMO. Moreover, we report the selective excitation of molecules with a specific orientation at normal incidence by aligning the electric field of the pump light along the molecular axis."}
{"title":"Test-time Adaptive Vision-and-Language Navigation","authors":["Junyu Gao","Xuan Yao","Changsheng Xu"],"raw_abstract":"Vision-and-Language Navigation (VLN) has witnessed significant advancements\nin recent years, largely attributed to meticulously curated datasets and\nproficiently trained models. Nevertheless, when tested in diverse environments,\nthe trained models inevitably encounter significant shifts in data\ndistribution, highlighting that relying solely on pre-trained and fixed\nnavigation models is insufficient. To enhance models' generalization ability,\ntest-time adaptation (TTA) demonstrates significant potential in the computer\nvision field by leveraging unlabeled test samples for model updates. However,\nsimply applying existing TTA methods to the VLN task cannot well handle the\nadaptability-stability dilemma of VLN models, i.e., frequent updates can result\nin drastic changes in model parameters, while occasional updates can make the\nmodels ill-equipped to handle dynamically changing environments. Therefore, we\npropose a Fast-Slow Test-Time Adaptation (FSTTA) approach for VLN by performing\ndecomposition-accumulation analysis for both gradients and parameters in a\nunified framework. Specifically, in the fast update phase, gradients generated\nduring the recent multi-step navigation process are decomposed into components\nwith varying levels of consistency. Then, these components are adaptively\naccumulated to pinpoint a concordant direction for fast model adaptation. In\nthe slow update phase, historically recorded parameters are gathered, and a\nsimilar decomposition-accumulation analysis is conducted to revert the model to\na stable state. Extensive experiments show that our method obtains impressive\nperformance gains on four popular benchmarks.","publication_date":1700639259,"paper_link":"http://arxiv.org/pdf/2311.13209v1","categories":["Computer Science"],"abstract":"Vision-and-Language Navigation (VLN) has witnessed significant advancements in recent years, largely attributed to meticulously curated datasets and proficiently trained models. Nevertheless, when tested in diverse environments, the trained models inevitably encounter significant shifts in data distribution, highlighting that relying solely on pre-trained and fixed navigation models is insufficient. To enhance models' generalization ability, test-time adaptation (TTA) demonstrates significant potential in the computer vision field by leveraging unlabeled test samples for model updates. However, simply applying existing TTA methods to the VLN task cannot well handle the adaptability-stability dilemma of VLN models, i.e., frequent updates can result in drastic changes in model parameters, while occasional updates can make the models ill-equipped to handle dynamically changing environments. Therefore, we propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for VLN by performing decomposition-accumulation analysis for both gradients and parameters in a unified framework. Specifically, in the fast update phase, gradients generated during the recent multi-step navigation process are decomposed into components with varying levels of consistency. Then, these components are adaptively accumulated to pinpoint a concordant direction for fast model adaptation. In the slow update phase, historically recorded parameters are gathered, and a similar decomposition-accumulation analysis is conducted to revert the model to a stable state. Extensive experiments show that our method obtains impressive performance gains on four popular benchmarks."}
{"title":"Electrified Fracture of Nanotube Films","authors":["Jinbo Bian","Shijun Wang","Zhaokuan Yu","Zhong Zhang","Zhiping Xu"],"raw_abstract":"Strong and conductive carbon nanotube films are ideal candidates for\nlightning-strike protection. Understanding their failure mechanisms by\nconsidering the anisotropic and single-fiber nature is essential to improve\nperformance. Our experimental studies show that the single-layer,\nnanometer-thick films fail under electrification by crack nucleation and\npropagation, reminiscent of brittle and ductile fracture of materials under\nmechanical loads. Sharp and diffuse patterns of fracture are identified in\naligned and non-woven films, respectively, signaling the strong effect of\nmaterial anisotropy that is absent in common engineering materials. The\nfracture is driven by local Joule heating concentrated at the crack fronts\ninstead of force-induced breakage, which is validated by experimental\ncharacterization and simulation results at both continuum and atomistic levels.","publication_date":1700639123,"paper_link":"http://arxiv.org/pdf/2311.13208v1","categories":["Physics"],"abstract":"Strong and conductive carbon nanotube films are ideal candidates for lightning-strike protection. Understanding their failure mechanisms by considering the anisotropic and single-fiber nature is essential to improve performance. Our experimental studies show that the single-layer, nanometer-thick films fail under electrification by crack nucleation and propagation, reminiscent of brittle and ductile fracture of materials under mechanical loads. Sharp and diffuse patterns of fracture are identified in aligned and non-woven films, respectively, signaling the strong effect of material anisotropy that is absent in common engineering materials. The fracture is driven by local Joule heating concentrated at the crack fronts instead of force-induced breakage, which is validated by experimental characterization and simulation results at both continuum and atomistic levels."}
{"title":"Robust Multi-Model Subset Selection","authors":["Anthony-Alexander Christidis","Gabriela Cohen-Freue"],"raw_abstract":"Modern datasets in biology and chemistry are often characterized by the\npresence of a large number of variables and outlying samples due to measurement\nerrors or rare biological and chemical profiles. To handle the characteristics\nof such datasets we introduce a method to learn a robust ensemble comprised of\na small number of sparse, diverse and robust models, the first of its kind in\nthe literature. The degree to which the models are sparse, diverse and\nresistant to data contamination is driven directly by the data based on a\ncross-validation criterion. We establish the finite-sample breakdown of the\nensembles and the models that comprise them, and we develop a tailored\ncomputing algorithm to learn the ensembles by leveraging recent developments in\nl0 optimization. Our extensive numerical experiments on synthetic and\nartificially contaminated real datasets from genomics and cheminformatics\ndemonstrate the competitive advantage of our method over state-of-the-art\nsparse and robust methods. We also demonstrate the applicability of our\nproposal on a cardiac allograft vasculopathy dataset.","publication_date":1700637077,"paper_link":"http://arxiv.org/pdf/2311.13202v1","categories":["Statistics"],"abstract":"Modern datasets in biology and chemistry are often characterized by the presence of a large number of variables and outlying samples due to measurement errors or rare biological and chemical profiles. To handle the characteristics of such datasets we introduce a method to learn a robust ensemble comprised of a small number of sparse, diverse and robust models, the first of its kind in the literature. The degree to which the models are sparse, diverse and resistant to data contamination is driven directly by the data based on a cross-validation criterion. We establish the finite-sample breakdown of the ensembles and the models that comprise them, and we develop a tailored computing algorithm to learn the ensembles by leveraging recent developments in l0 optimization. Our extensive numerical experiments on synthetic and artificially contaminated real datasets from genomics and cheminformatics demonstrate the competitive advantage of our method over state-of-the-art sparse and robust methods. We also demonstrate the applicability of our proposal on a cardiac allograft vasculopathy dataset."}
{"title":"Entanglement Phase Transition in Holographic Pseudo Entropy","authors":["Hiroki Kanda","Taishi Kawamoto","Yu-ki Suzuki","Tadashi Takayanagi","Kenya Tasuki","Zixia Wei"],"raw_abstract":"In this paper, we present holographic descriptions of entanglement phase\ntransition using AdS/BCFT. First, we analytically calculate the holographic\npseudo entropy in the AdS/BCFT model with a brane localized scalar field and\nshow the entanglement phase transition behavior where the time evolution of\nentropy changes from the linear growth to the trivial one via a critical\nlogarithmic evolution. In this model, the imaginary valued scalar field\nlocalized on the brane controls the phase transition, which is analogous to the\namount of projections in the measurement induced phase transition. Next, we\nstudy the AdS/BCFT model with a brane localized gauge field, where the phase\ntransition looks different in that there is no logarithmically evolving\ncritical point. Finally, we discuss a bulk analog of the above model by\nconsidering a double Wick rotation of the Janus solution. We compute the\nholographic pseudo entropy in this model and show that the entropy grows\nlogarithmically.","publication_date":1700636959,"paper_link":"http://arxiv.org/pdf/2311.13201v1","categories":["Physics"],"abstract":"In this paper, we present holographic descriptions of entanglement phase transition using AdS/BCFT. First, we analytically calculate the holographic pseudo entropy in the AdS/BCFT model with a brane localized scalar field and show the entanglement phase transition behavior where the time evolution of entropy changes from the linear growth to the trivial one via a critical logarithmic evolution. In this model, the imaginary valued scalar field localized on the brane controls the phase transition, which is analogous to the amount of projections in the measurement induced phase transition. Next, we study the AdS/BCFT model with a brane localized gauge field, where the phase transition looks different in that there is no logarithmically evolving critical point. Finally, we discuss a bulk analog of the above model by considering a double Wick rotation of the Janus solution. We compute the holographic pseudo entropy in this model and show that the entropy grows logarithmically."}
{"title":"Self-guided Few-shot Semantic Segmentation for Remote Sensing Imagery Based on Large Vision Models","authors":["Xiyu Qi","Yifan Wu","Yongqiang Mao","Wenhui Zhang","Yidan Zhang"],"raw_abstract":"The Segment Anything Model (SAM) exhibits remarkable versatility and\nzero-shot learning abilities, owing largely to its extensive training data\n(SA-1B). Recognizing SAM's dependency on manual guidance given its\ncategory-agnostic nature, we identified unexplored potential within few-shot\nsemantic segmentation tasks for remote sensing imagery. This research\nintroduces a structured framework designed for the automation of few-shot\nsemantic segmentation. It utilizes the SAM model and facilitates a more\nefficient generation of semantically discernible segmentation outcomes. Central\nto our methodology is a novel automatic prompt learning approach, leveraging\nprior guided masks to produce coarse pixel-wise prompts for SAM. Extensive\nexperiments on the DLRSD datasets underline the superiority of our approach,\noutperforming other available few-shot methodologies.","publication_date":1700636875,"paper_link":"http://arxiv.org/pdf/2311.13200v1","categories":["Computer Science"],"abstract":"The Segment Anything Model (SAM) exhibits remarkable versatility and zero-shot learning abilities, owing largely to its extensive training data (SA-1B). Recognizing SAM's dependency on manual guidance given its category-agnostic nature, we identified unexplored potential within few-shot semantic segmentation tasks for remote sensing imagery. This research introduces a structured framework designed for the automation of few-shot semantic segmentation. It utilizes the SAM model and facilitates a more efficient generation of semantically discernible segmentation outcomes. Central to our methodology is a novel automatic prompt learning approach, leveraging prior guided masks to produce coarse pixel-wise prompts for SAM. Extensive experiments on the DLRSD datasets underline the superiority of our approach, outperforming other available few-shot methodologies."}
{"title":"DRIFu: Differentiable Rendering and Implicit Function-based Single-View 3D Reconstruction","authors":["Zijian Kuang","Lihang Ying","Shi Jin"],"raw_abstract":"The Differentiable Rendering and Implicit Function-based model (DRIFu) draws\nits roots from the Pixel-aligned Implicit Function (PIFU), a pioneering 3D\ndigitization technique initially designed for clothed human bodies. PIFU excels\nin capturing nuanced body shape variations within a low-dimensional space and\nhas been extensively trained on human 3D scans. However, the application of\nPIFU to live animals poses significant challenges, primarily due to the\ninherent difficulty in obtaining the cooperation of animals for 3D scanning. In\nresponse to this challenge, we introduce the DRIFu model, specifically tailored\nfor animal digitization. To train DRIFu, we employ a curated set of synthetic\n3D animal models, encompassing diverse shapes, sizes, and even accounting for\nvariations such as baby birds. Our innovative alignment tools play a pivotal\nrole in mapping these diverse synthetic animal models onto a unified template,\nfacilitating precise predictions of animal shape and texture. Crucially, our\ntemplate alignment strategy establishes a shared shape space, allowing for the\nseamless sampling of new animal shapes, posing them realistically, animating\nthem, and aligning them with real-world data. This groundbreaking approach\nrevolutionizes our capacity to comprehensively understand and represent avian\nforms. For further details and access to the project, the project website can\nbe found at https://github.com/kuangzijian/drifu-for-animals","publication_date":1700636798,"paper_link":"http://arxiv.org/pdf/2311.13199v1","categories":["Computer Science"],"abstract":"The Differentiable Rendering and Implicit Function-based model (DRIFu) draws its roots from the Pixel-aligned Implicit Function (PIFU), a pioneering 3D digitization technique initially designed for clothed human bodies. PIFU excels in capturing nuanced body shape variations within a low-dimensional space and has been extensively trained on human 3D scans. However, the application of PIFU to live animals poses significant challenges, primarily due to the inherent difficulty in obtaining the cooperation of animals for 3D scanning. In response to this challenge, we introduce the DRIFu model, specifically tailored for animal digitization. To train DRIFu, we employ a curated set of synthetic 3D animal models, encompassing diverse shapes, sizes, and even accounting for variations such as baby birds. Our innovative alignment tools play a pivotal role in mapping these diverse synthetic animal models onto a unified template, facilitating precise predictions of animal shape and texture. Crucially, our template alignment strategy establishes a shared shape space, allowing for the seamless sampling of new animal shapes, posing them realistically, animating them, and aligning them with real-world data. This groundbreaking approach revolutionizes our capacity to comprehensively understand and represent avian forms. For further details and access to the project, the project website can be found at https://github.com/kuangzijian/drifu-for-animals"}
{"title":"DoubleAUG: Single-domain Generalized Object Detector in Urban via Color Perturbation and Dual-style Memory","authors":["Lei Qi","Peng Dong","Tan Xiong","Hui Xue","Xin Geng"],"raw_abstract":"Object detection in urban scenarios is crucial for autonomous driving in\nintelligent traffic systems. However, unlike conventional object detection\ntasks, urban-scene images vary greatly in style. For example, images taken on\nsunny days differ significantly from those taken on rainy days. Therefore,\nmodels trained on sunny day images may not generalize well to rainy day images.\nIn this paper, we aim to solve the single-domain generalizable object detection\ntask in urban scenarios, meaning that a model trained on images from one\nweather condition should be able to perform well on images from any other\nweather conditions. To address this challenge, we propose a novel Double\nAUGmentation (DoubleAUG) method that includes image- and feature-level\naugmentation schemes. In the image-level augmentation, we consider the\nvariation in color information across different weather conditions and propose\na Color Perturbation (CP) method that randomly exchanges the RGB channels to\ngenerate various images. In the feature-level augmentation, we propose to\nutilize a Dual-Style Memory (DSM) to explore the diverse style information on\nthe entire dataset, further enhancing the model's generalization capability.\nExtensive experiments demonstrate that our proposed method outperforms\nstate-of-the-art methods. Furthermore, ablation studies confirm the\neffectiveness of each module in our proposed method. Moreover, our method is\nplug-and-play and can be integrated into existing methods to further improve\nmodel performance.","publication_date":1700636754,"paper_link":"http://arxiv.org/pdf/2311.13198v1","categories":["Computer Science"],"abstract":"Object detection in urban scenarios is crucial for autonomous driving in intelligent traffic systems. However, unlike conventional object detection tasks, urban-scene images vary greatly in style. For example, images taken on sunny days differ significantly from those taken on rainy days. Therefore, models trained on sunny day images may not generalize well to rainy day images. In this paper, we aim to solve the single-domain generalizable object detection task in urban scenarios, meaning that a model trained on images from one weather condition should be able to perform well on images from any other weather conditions. To address this challenge, we propose a novel Double AUGmentation (DoubleAUG) method that includes image- and feature-level augmentation schemes. In the image-level augmentation, we consider the variation in color information across different weather conditions and propose a Color Perturbation (CP) method that randomly exchanges the RGB channels to generate various images. In the feature-level augmentation, we propose to utilize a Dual-Style Memory (DSM) to explore the diverse style information on the entire dataset, further enhancing the model's generalization capability. Extensive experiments demonstrate that our proposed method outperforms state-of-the-art methods. Furthermore, ablation studies confirm the effectiveness of each module in our proposed method. Moreover, our method is plug-and-play and can be integrated into existing methods to further improve model performance."}
{"title":"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs","authors":["Yonghui Wang","Wengang Zhou","Hao Feng","Keyi Zhou","Houqiang Li"],"raw_abstract":"In the field of document understanding, significant advances have been made\nin the fine-tuning of Multimodal Large Language Models (MLLMs) with\ninstruction-following data. Nevertheless, the potential of text-grounding\ncapability within text-rich scenarios remains underexplored. In this paper, we\npresent a text-grounding document understanding model, termed TGDoc, which\naddresses this deficiency by enhancing MLLMs with the ability to discern the\nspatial positioning of text within images. Empirical evidence suggests that\ntext-grounding improves the model's interpretation of textual content, thereby\nelevating its proficiency in comprehending text-rich images. Specifically, we\ncompile a dataset containing 99K PowerPoint presentations sourced from the\ninternet. We formulate instruction tuning tasks including text detection,\nrecognition, and spotting to facilitate the cohesive alignment between the\nvisual encoder and large language model. Moreover, we curate a collection of\ntext-rich images and prompt the text-only GPT-4 to generate 12K high-quality\nconversations, featuring textual locations within text-rich scenarios. By\nintegrating text location data into the instructions, TGDoc is adept at\ndiscerning text locations during the visual question process. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple text-rich benchmarks, validating the effectiveness of our\nmethod.","publication_date":1700635597,"paper_link":"http://arxiv.org/pdf/2311.13194v1","categories":["Computer Science"],"abstract":"In the field of document understanding, significant advances have been made in the fine-tuning of Multimodal Large Language Models (MLLMs) with instruction-following data. Nevertheless, the potential of text-grounding capability within text-rich scenarios remains underexplored. In this paper, we present a text-grounding document understanding model, termed TGDoc, which addresses this deficiency by enhancing MLLMs with the ability to discern the spatial positioning of text within images. Empirical evidence suggests that text-grounding improves the model's interpretation of textual content, thereby elevating its proficiency in comprehending text-rich images. Specifically, we compile a dataset containing 99K PowerPoint presentations sourced from the internet. We formulate instruction tuning tasks including text detection, recognition, and spotting to facilitate the cohesive alignment between the visual encoder and large language model. Moreover, we curate a collection of text-rich images and prompt the text-only GPT-4 to generate 12K high-quality conversations, featuring textual locations within text-rich scenarios. By integrating text location data into the instructions, TGDoc is adept at discerning text locations during the visual question process. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple text-rich benchmarks, validating the effectiveness of our method."}
{"title":"Direct Observation of sp-d Exchange Interaction in Mn$^2$$^+$doped All-inorganic Perovskite Quantum Dots (CsPbX$_3$: X= Cl, Br)","authors":["Prasenjit Mandal","Ranjani Viswanatha"],"raw_abstract":"The field of lead halide perovskite nanocrystal doping has witnessed notable\nprogress in recent times, leading to the creation of innovative materials that\nshowcase compelling physical characteristics and hold substantial technological\npromise. The true characteristics of these materials lie in the presence of\ndopant-carrier magnetic exchange interactions. This work presents the first\ndirect observation of such exchange interactions in colloidal Mn-doped\nCsPbX$_3$ (X= Cl, Br) quantum dots (QDs). Here, we employ magnetic circular\ndichroism (MCD) spectroscopy to unambiguously demonstrate the successful doping\nand the presence of giant excitonic Zeeman splitting in CsPbX$_3$ (X= Cl, Br)\nQDs doped with Mn$^2$$^+$. The controllable tuning of effective exciton\ng-factors (g$_e$$_f$$_f$) within the range of 2.1 to (-)314 has been achieved\nthrough the process of doping with 6.9 % Mn in CsPbCl$_3$, which will\nfacilitate their application towards future spintronics","publication_date":1700635304,"paper_link":"http://arxiv.org/pdf/2311.13190v1","categories":["Physics"],"abstract":"The field of lead halide perovskite nanocrystal doping has witnessed notable progress in recent times, leading to the creation of innovative materials that showcase compelling physical characteristics and hold substantial technological promise. The true characteristics of these materials lie in the presence of dopant-carrier magnetic exchange interactions. This work presents the first direct observation of such exchange interactions in colloidal Mn-doped CsPbX__FORMULA__ (X= Cl, Br) quantum dots (QDs). Here, we employ magnetic circular dichroism (MCD) spectroscopy to unambiguously demonstrate the successful doping and the presence of giant excitonic Zeeman splitting in CsPbX__FORMULA__ (X= Cl, Br) QDs doped with Mn__FORMULA____FORMULA__. The controllable tuning of effective exciton g-factors (g__FORMULA____FORMULA____FORMULA__) within the range of 2.1 to (-)314 has been achieved through the process of doping with 6.9 % Mn in CsPbCl__FORMULA__, which will facilitate their application towards future spintronics"}
{"title":"NeISF: Neural Incident Stokes Field for Geometry and Material Estimation","authors":["Chenhao Li","Taishi Ono","Takeshi Uemori","Hajime Mihara","Alexander Gatto","Hajime Nagahara","Yuseke Moriuchi"],"raw_abstract":"Multi-view inverse rendering is the problem of estimating the scene\nparameters such as shapes, materials, or illuminations from a sequence of\nimages captured under different viewpoints. Many approaches, however, assume\nsingle light bounce and thus fail to recover challenging scenarios like\ninter-reflections. On the other hand, simply extending those methods to\nconsider multi-bounced light requires more assumptions to alleviate the\nambiguity. To address this problem, we propose Neural Incident Stokes Fields\n(NeISF), a multi-view inverse rendering framework that reduces ambiguities\nusing polarization cues. The primary motivation for using polarization cues is\nthat it is the accumulation of multi-bounced light, providing rich information\nabout geometry and material. Based on this knowledge, the proposed incident\nStokes field efficiently models the accumulated polarization effect with the\naid of an original physically-based differentiable polarimetric renderer.\nLastly, experimental results show that our method outperforms the existing\nworks in synthetic and real scenarios.","publication_date":1700634510,"paper_link":"http://arxiv.org/pdf/2311.13187v1","categories":["Computer Science"],"abstract":"Multi-view inverse rendering is the problem of estimating the scene parameters such as shapes, materials, or illuminations from a sequence of images captured under different viewpoints. Many approaches, however, assume single light bounce and thus fail to recover challenging scenarios like inter-reflections. On the other hand, simply extending those methods to consider multi-bounced light requires more assumptions to alleviate the ambiguity. To address this problem, we propose Neural Incident Stokes Fields (NeISF), a multi-view inverse rendering framework that reduces ambiguities using polarization cues. The primary motivation for using polarization cues is that it is the accumulation of multi-bounced light, providing rich information about geometry and material. Based on this knowledge, the proposed incident Stokes field efficiently models the accumulated polarization effect with the aid of an original physically-based differentiable polarimetric renderer. Lastly, experimental results show that our method outperforms the existing works in synthetic and real scenarios."}
{"title":"Applications of Spiking Neural Networks in Visual Place Recognition","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"raw_abstract":"In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). First, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Our SNNs are compact and small, comprising only 1500\nneurons and 474k synapses, which makes them ideally suited for ensembling due\nto this small size. Lastly, we investigate the role of sequence matching in\nSNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We analyze the responsiveness of SNNs to ensembling and sequence\nmatching compared to other VPR techniques. Our contributions highlight the\nviability of SNNs for VPR, offering scalable and robust solutions, paving the\nway for their application in various energy-sensitive robotic tasks.","publication_date":1700634384,"paper_link":"http://arxiv.org/pdf/2311.13186v1","categories":["Computer Science"],"abstract":"In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for their largely-unrealized potential energy efficiency and low latency particularly when implemented on neuromorphic hardware. Our paper highlights three advancements for SNNs in Visual Place Recognition (VPR). First, we propose Modular SNNs, where each SNN represents a set of non-overlapping geographically distinct places, enabling scalable networks for large environments. Secondly, we present Ensembles of Modular SNNs, where multiple networks represent the same place, significantly enhancing accuracy compared to single-network models. Our SNNs are compact and small, comprising only 1500 neurons and 474k synapses, which makes them ideally suited for ensembling due to this small size. Lastly, we investigate the role of sequence matching in SNN-based VPR, a technique where consecutive images are used to refine place recognition. We analyze the responsiveness of SNNs to ensembling and sequence matching compared to other VPR techniques. Our contributions highlight the viability of SNNs for VPR, offering scalable and robust solutions, paving the way for their application in various energy-sensitive robotic tasks."}
{"title":"AS-LLM: When Algorithm Selection Meets Large Language Model","authors":["Xingyu Wu","Yan Zhong","Jibin Wu","Kay Chen Tan"],"raw_abstract":"Algorithm selection aims to identify the most suitable algorithm for solving\na specific problem before execution, which has become a critical process of the\nAutoML. Current mainstream algorithm selection techniques rely heavily on\nfeature representations of various problems and employ the performance of each\nalgorithm as supervised information. However, there is a significant research\ngap concerning the consideration of algorithm features. This gap is primarily\nattributed to the inherent complexity of algorithms, making it particularly\nchallenging to find a universally effective feature extraction method that is\napplicable across a diverse range of algorithms. Unfortunately, neglecting this\naspect undoubtedly impacts the accuracy of algorithm selection and indirectly\nnecessitates an increased volume of problem data for training purposes. This\npaper takes a significant stride towards addressing this gap by proposing an\napproach that integrates algorithm representation into the algorithm selection\nprocess. Specifically, our proposed model employs distinct modules to extract\nrepresentations of both problems and algorithms, where the algorithm\nrepresentation leverages the capabilities of pre-trained LLMs in the realm of\ncode comprehension. Following the extraction of embedding vectors for both\nalgorithms and problems, the most suitable algorithm is determined through\ncalculations of matching degrees. Our experiments not only validate the\neffectiveness of the proposed model but also showcase the performance of\ndifferent embedded pre-trained LLMs, which suggests that the proposed algorithm\nselection framework holds the potential to serve as a baseline task for\nevaluating the code representation capabilities of LLMs.","publication_date":1700634198,"paper_link":"http://arxiv.org/pdf/2311.13184v1","categories":["Computer Science"],"abstract":"Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm selection process. Specifically, our proposed model employs distinct modules to extract representations of both problems and algorithms, where the algorithm representation leverages the capabilities of pre-trained LLMs in the realm of code comprehension. Following the extraction of embedding vectors for both algorithms and problems, the most suitable algorithm is determined through calculations of matching degrees. Our experiments not only validate the effectiveness of the proposed model but also showcase the performance of different embedded pre-trained LLMs, which suggests that the proposed algorithm selection framework holds the potential to serve as a baseline task for evaluating the code representation capabilities of LLMs."}
{"title":"Differentiable Radio Frequency Ray Tracing for Millimeter-Wave Sensing","authors":["Xingyu Chen","Xinyu Zhang","Qiyue Xia","Xinmin Fang","Chris Xiaoxuan Lu","Zhengxiong Li"],"raw_abstract":"Millimeter wave (mmWave) sensing is an emerging technology with applications\nin 3D object characterization and environment mapping. However, realizing\nprecise 3D reconstruction from sparse mmWave signals remains challenging.\nExisting methods rely on data-driven learning, constrained by dataset\navailability and difficulty in generalization. We propose DiffSBR, a\ndifferentiable framework for mmWave-based 3D reconstruction. DiffSBR\nincorporates a differentiable ray tracing engine to simulate radar point clouds\nfrom virtual 3D models. A gradient-based optimizer refines the model parameters\nto minimize the discrepancy between simulated and real point clouds.\nExperiments using various radar hardware validate DiffSBR's capability for\nfine-grained 3D reconstruction, even for novel objects unseen by the radar\npreviously. By integrating physics-based simulation with gradient optimization,\nDiffSBR transcends the limitations of data-driven approaches and pioneers a new\nparadigm for mmWave sensing.","publication_date":1700633619,"paper_link":"http://arxiv.org/pdf/2311.13182v1","categories":["Computer Science"],"abstract":"Millimeter wave (mmWave) sensing is an emerging technology with applications in 3D object characterization and environment mapping. However, realizing precise 3D reconstruction from sparse mmWave signals remains challenging. Existing methods rely on data-driven learning, constrained by dataset availability and difficulty in generalization. We propose DiffSBR, a differentiable framework for mmWave-based 3D reconstruction. DiffSBR incorporates a differentiable ray tracing engine to simulate radar point clouds from virtual 3D models. A gradient-based optimizer refines the model parameters to minimize the discrepancy between simulated and real point clouds. Experiments using various radar hardware validate DiffSBR's capability for fine-grained 3D reconstruction, even for novel objects unseen by the radar previously. By integrating physics-based simulation with gradient optimization, DiffSBR transcends the limitations of data-driven approaches and pioneers a new paradigm for mmWave sensing."}
{"title":"Volumetric Reconstruction Resolves Off-Resonance Artifacts in Static and Dynamic PROPELLER MRI","authors":["Annesha Ghosh","Gordon Wetzstein","Mert Pilanci","Sara Fridovich-Keil"],"raw_abstract":"Off-resonance artifacts in magnetic resonance imaging (MRI) are visual\ndistortions that occur when the actual resonant frequencies of spins within the\nimaging volume differ from the expected frequencies used to encode spatial\ninformation. These discrepancies can be caused by a variety of factors,\nincluding magnetic field inhomogeneities, chemical shifts, or susceptibility\ndifferences within the tissues. Such artifacts can manifest as blurring,\nghosting, or misregistration of the reconstructed image, and they often\ncompromise its diagnostic quality. We propose to resolve these artifacts by\nlifting the 2D MRI reconstruction problem to 3D, introducing an additional\n\"spectral\" dimension to model this off-resonance. Our approach is inspired by\nrecent progress in modeling radiance fields, and is capable of reconstructing\nboth static and dynamic MR images as well as separating fat and water, which is\nof independent clinical interest. We demonstrate our approach in the context of\nPROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced\nReconstruction) MRI acquisitions, which are popular for their robustness to\nmotion artifacts. Our method operates in a few minutes on a single GPU, and to\nour knowledge is the first to correct for chemical shift in gradient echo\nPROPELLER MRI reconstruction without additional measurements or pretraining\ndata.","publication_date":1700631891,"paper_link":"http://arxiv.org/pdf/2311.13177v1","categories":["Physics"],"abstract":"Off-resonance artifacts in magnetic resonance imaging (MRI) are visual distortions that occur when the actual resonant frequencies of spins within the imaging volume differ from the expected frequencies used to encode spatial information. These discrepancies can be caused by a variety of factors, including magnetic field inhomogeneities, chemical shifts, or susceptibility differences within the tissues. Such artifacts can manifest as blurring, ghosting, or misregistration of the reconstructed image, and they often compromise its diagnostic quality. We propose to resolve these artifacts by lifting the 2D MRI reconstruction problem to 3D, introducing an additional \"spectral\" dimension to model this off-resonance. Our approach is inspired by recent progress in modeling radiance fields, and is capable of reconstructing both static and dynamic MR images as well as separating fat and water, which is of independent clinical interest. We demonstrate our approach in the context of PROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced Reconstruction) MRI acquisitions, which are popular for their robustness to motion artifacts. Our method operates in a few minutes on a single GPU, and to our knowledge is the first to correct for chemical shift in gradient echo PROPELLER MRI reconstruction without additional measurements or pretraining data."}
{"title":"Vector bundle automorphisms preserving Morse-Bott foliations","authors":["Sergiy Maksymenko"],"raw_abstract":"Let $M$ be a smooth manifold and $\\mathcal{F}$ a Morse-Bott foliation on $M$\nwith a compact critical manifold $\\Sigma$. Denote by $\\mathcal{D}(\\mathcal{F})$\nthe group of diffeomorphisms of $M$ leaving invariant each leaf of\n$\\mathcal{F}$. Under certain assumptions on $\\mathcal{F}$ it is shown that the\ncomputation of the homotopy type of $\\mathcal{D}(\\mathcal{F})$ reduces to three\nrather independent groups: the group of diffeomorphisms of $\\Sigma$, the group\nof vector bundle automorphisms of some regular neighborhood of $\\Sigma$, and\nthe subgroup of $\\mathcal{D}(\\mathcal{F})$ consisting of diffeomorphisms fixed\nnear $\\Sigma$. Examples of computations of homotopy types of groups\n$\\mathcal{D}(\\mathcal{F})$ for such foliations are also presented.","publication_date":1700631734,"paper_link":"http://arxiv.org/pdf/2311.13176v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be a smooth manifold and __FORMULA__ a Morse-Bott foliation on __FORMULA__ with a compact critical manifold __FORMULA__. Denote by __FORMULA__ the group of diffeomorphisms of __FORMULA__ leaving invariant each leaf of __FORMULA__. Under certain assumptions on __FORMULA__ it is shown that the computation of the homotopy type of __FORMULA__ reduces to three rather independent groups: the group of diffeomorphisms of __FORMULA__, the group of vector bundle automorphisms of some regular neighborhood of __FORMULA__, and the subgroup of __FORMULA__ consisting of diffeomorphisms fixed near __FORMULA__. Examples of computations of homotopy types of groups __FORMULA__ for such foliations are also presented."}
{"title":"Smoothed Particle Hydrodynamics simulations of integral multi-mode and fractional viscoelastic models","authors":["Luca Santelli","Adolfo Vazquez-Quesada","Marco Ellero"],"raw_abstract":"To capture specific characteristics of non-Newtonian fluids, during the past\nyears fractional constitutive models have become increasingly popular. These\nmodels are able to capture in a simple and compact way the complex behaviour of\nviscoelastic materials, such as the change in power-law relaxation pattern\nduring the relaxation process of some materials. Using the Lagrangian\nSmoothed-Particle Hydrodynamics (SPH) method we can easily track particle\nhistory; this allows us to solve integral constitutive models in a novel way,\nwithout relying on complex tasks. Hence, we develop here a SPH integral\nviscoelastic method which is first validated for simple Maxwell or Oldroyd-B\nmodels under Small Amplitude Oscillatory Shear flows (SAOS). By exploiting the\nstructure of the integral method, a multi-mode Maxwell model is then\nimplemented. Finally, the method is extended to include fractional constitutive\nmodels, validating the approach by comparing results with theory under SAOS.","publication_date":1700631164,"paper_link":"http://arxiv.org/pdf/2311.13173v1","categories":["Physics"],"abstract":"To capture specific characteristics of non-Newtonian fluids, during the past years fractional constitutive models have become increasingly popular. These models are able to capture in a simple and compact way the complex behaviour of viscoelastic materials, such as the change in power-law relaxation pattern during the relaxation process of some materials. Using the Lagrangian Smoothed-Particle Hydrodynamics (SPH) method we can easily track particle history; this allows us to solve integral constitutive models in a novel way, without relying on complex tasks. Hence, we develop here a SPH integral viscoelastic method which is first validated for simple Maxwell or Oldroyd-B models under Small Amplitude Oscillatory Shear flows (SAOS). By exploiting the structure of the integral method, a multi-mode Maxwell model is then implemented. Finally, the method is extended to include fractional constitutive models, validating the approach by comparing results with theory under SAOS."}
{"title":"Learning to Complement with Multiple Humans (LECOMH): Integrating Multi-rater and Noisy-Label Learning into Human-AI Collaboration","authors":["Zheng Zhang","Kevin Wells","Gustavo Carneiro"],"raw_abstract":"The advent of learning with noisy labels (LNL), multi-rater learning, and\nhuman-AI collaboration has revolutionised the development of robust\nclassifiers, enabling them to address the challenges posed by different types\nof data imperfections and complex decision processes commonly encountered in\nreal-world applications. While each of these methodologies has individually\nmade significant strides in addressing their unique challenges, the development\nof techniques that can simultaneously tackle these three problems remains\nunderexplored. This paper addresses this research gap by integrating\nnoisy-label learning, multi-rater learning, and human-AI collaboration with new\nbenchmarks and the innovative Learning to Complement with Multiple Humans\n(LECOMH) approach. LECOMH optimises the level of human collaboration during\ntesting, aiming to optimise classification accuracy while minimising\ncollaboration costs that vary from 0 to M, where M is the maximum number of\nhuman collaborators. We quantitatively compare LECOMH with leading human-AI\ncollaboration methods using our proposed benchmarks. LECOMH consistently\noutperforms the competition, with accuracy improving as collaboration costs\nincrease. Notably, LECOMH is the only method enhancing human labeller\nperformance across all benchmarks.","publication_date":1700631066,"paper_link":"http://arxiv.org/pdf/2311.13172v1","categories":["Computer Science"],"abstract":"The advent of learning with noisy labels (LNL), multi-rater learning, and human-AI collaboration has revolutionised the development of robust classifiers, enabling them to address the challenges posed by different types of data imperfections and complex decision processes commonly encountered in real-world applications. While each of these methodologies has individually made significant strides in addressing their unique challenges, the development of techniques that can simultaneously tackle these three problems remains underexplored. This paper addresses this research gap by integrating noisy-label learning, multi-rater learning, and human-AI collaboration with new benchmarks and the innovative Learning to Complement with Multiple Humans (LECOMH) approach. LECOMH optimises the level of human collaboration during testing, aiming to optimise classification accuracy while minimising collaboration costs that vary from 0 to M, where M is the maximum number of human collaborators. We quantitatively compare LECOMH with leading human-AI collaboration methods using our proposed benchmarks. LECOMH consistently outperforms the competition, with accuracy improving as collaboration costs increase. Notably, LECOMH is the only method enhancing human labeller performance across all benchmarks."}
{"title":"ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization","authors":["Prateek Yadav","Leshem Choshen","Colin Raffel","Mohit Bansal"],"raw_abstract":"Parameter-efficient fine-tuning (PEFT) techniques make it possible to\nefficiently adapt a language model to create \"expert\" models that specialize to\nnew tasks or domains. Recent techniques in model merging and compositional\ngeneralization leverage these expert models by dynamically composing modules to\nimprove zero/few-shot generalization. Despite the efficiency of PEFT methods,\nthe size of expert models can make it onerous to retrieve expert models per\nquery over high-latency networks like the Internet or serve multiple experts on\na single GPU. To address these issues, we present ComPEFT, a novel method for\ncompressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT\nemploys sparsification and ternary quantization to reduce the size of the PEFT\nmodule without performing any additional retraining while preserving or\nenhancing model performance. In extensive evaluation across T5, T0, and\nLLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression\nratios of 8x - 50x. In particular, we show that ComPEFT improves with scale -\nstronger models exhibit higher compressibility and better performance. For\nexample, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on\nMMLU with a storage size reduction of up to 26x. In addition, we show that the\ncompressed experts produced by ComPEFT maintain few-shot compositional\ngeneralization capabilities, facilitate efficient communication and\ncomputation, and exhibit enhanced performance when merged. Lastly, we provide\nan analysis of different method components, compare it with other PEFT methods,\nand test ComPEFT's efficacy for compressing the residual of full-finetuning.\nOur code is available at https://github.com/prateeky2806/compeft.","publication_date":1700630939,"paper_link":"http://arxiv.org/pdf/2311.13171v1","categories":["Computer Science"],"abstract":"Parameter-efficient fine-tuning (PEFT) techniques make it possible to efficiently adapt a language model to create \"expert\" models that specialize to new tasks or domains. Recent techniques in model merging and compositional generalization leverage these expert models by dynamically composing modules to improve zero/few-shot generalization. Despite the efficiency of PEFT methods, the size of expert models can make it onerous to retrieve expert models per query over high-latency networks like the Internet or serve multiple experts on a single GPU. To address these issues, we present ComPEFT, a novel method for compressing fine-tuning residuals (task vectors) of PEFT based models. ComPEFT employs sparsification and ternary quantization to reduce the size of the PEFT module without performing any additional retraining while preserving or enhancing model performance. In extensive evaluation across T5, T0, and LLaMA-based models with 200M - 65B parameters, ComPEFT achieves compression ratios of 8x - 50x. In particular, we show that ComPEFT improves with scale - stronger models exhibit higher compressibility and better performance. For example, we show that ComPEFT applied to LLaMA outperforms QLoRA by 4.16% on MMLU with a storage size reduction of up to 26x. In addition, we show that the compressed experts produced by ComPEFT maintain few-shot compositional generalization capabilities, facilitate efficient communication and computation, and exhibit enhanced performance when merged. Lastly, we provide an analysis of different method components, compare it with other PEFT methods, and test ComPEFT's efficacy for compressing the residual of full-finetuning. Our code is available at https://github.com/prateeky2806/compeft."}
{"title":"An iterative deep learning procedure for determining electron scattering cross-sections from transport coefficients","authors":["Dale L Muccignat","Gregory G Boyle","Nathan A Garland","Peter W Stokes","Ronald D White"],"raw_abstract":"We propose improvements to the Artificial Neural Network (ANN) method of\ndetermining electron scattering cross-sections from swarm data proposed by\ncoauthors. A limitation inherent to this problem, known as the inverse swarm\nproblem, is the non-unique nature of its solutions, particularly when there\nexists multiple cross-sections that each describe similar scattering processes.\nConsidering this, prior methods leveraged existing knowledge of a particular\ncross-section set to reduce the solution space of the problem. To reduce the\nneed for prior knowledge, we propose the following modifications to the ANN\nmethod. First, we propose a Multi-Branch ANN (MBANN) that assigns an\nindependent branch of hidden layers to each cross-section output. We show that\nin comparison with an equivalent conventional ANN, the MBANN architecture\nenables an efficient and physics informed feature map of each cross-section.\nAdditionally, we show that the MBANN solution can be improved upon by\nsuccessive networks that are each trained using perturbations of the previous\nregression. Crucially, the method requires much less input data and fewer\nrestrictive assumptions, and only assumes knowledge of energy loss thresholds\nand the number of cross-sections present.","publication_date":1700630882,"paper_link":"http://arxiv.org/pdf/2311.13170v1","categories":["Physics"],"abstract":"We propose improvements to the Artificial Neural Network (ANN) method of determining electron scattering cross-sections from swarm data proposed by coauthors. A limitation inherent to this problem, known as the inverse swarm problem, is the non-unique nature of its solutions, particularly when there exists multiple cross-sections that each describe similar scattering processes. Considering this, prior methods leveraged existing knowledge of a particular cross-section set to reduce the solution space of the problem. To reduce the need for prior knowledge, we propose the following modifications to the ANN method. First, we propose a Multi-Branch ANN (MBANN) that assigns an independent branch of hidden layers to each cross-section output. We show that in comparison with an equivalent conventional ANN, the MBANN architecture enables an efficient and physics informed feature map of each cross-section. Additionally, we show that the MBANN solution can be improved upon by successive networks that are each trained using perturbations of the previous regression. Crucially, the method requires much less input data and fewer restrictive assumptions, and only assumes knowledge of energy loss thresholds and the number of cross-sections present."}
{"title":"SiGeo: Sub-One-Shot NAS via Information Theory and Geometry of Loss Landscape","authors":["Hua Zheng","Kuang-Hung Liu","Igor Fedorov","Xin Zhang","Wen-Yen Chen","Wei Wen"],"raw_abstract":"Neural Architecture Search (NAS) has become a widely used tool for automating\nneural network design. While one-shot NAS methods have successfully reduced\ncomputational requirements, they often require extensive training. On the other\nhand, zero-shot NAS utilizes training-free proxies to evaluate a candidate\narchitecture's test performance but has two limitations: (1) inability to use\nthe information gained as a network improves with training and (2) unreliable\nperformance, particularly in complex domains like RecSys, due to the\nmulti-modal data inputs and complex architecture configurations. To synthesize\nthe benefits of both methods, we introduce a \"sub-one-shot\" paradigm that\nserves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the\nsupernet is trained using only a small subset of the training data, a phase we\nrefer to as \"warm-up.\" Within this framework, we present SiGeo, a proxy founded\non a novel theoretical framework that connects the supernet warm-up with the\nefficacy of the proxy. Extensive experiments have shown that SiGeo, with the\nbenefit of warm-up, consistently outperforms state-of-the-art NAS proxies on\nvarious established NAS benchmarks. When a supernet is warmed up, it can\nachieve comparable performance to weight-sharing one-shot NAS methods, but with\na significant reduction ($\\sim 60$\\%) in computational costs.","publication_date":1700630724,"paper_link":"http://arxiv.org/pdf/2311.13169v1","categories":["Computer Science"],"abstract":"Neural Architecture Search (NAS) has become a widely used tool for automating neural network design. While one-shot NAS methods have successfully reduced computational requirements, they often require extensive training. On the other hand, zero-shot NAS utilizes training-free proxies to evaluate a candidate architecture's test performance but has two limitations: (1) inability to use the information gained as a network improves with training and (2) unreliable performance, particularly in complex domains like RecSys, due to the multi-modal data inputs and complex architecture configurations. To synthesize the benefits of both methods, we introduce a \"sub-one-shot\" paradigm that serves as a bridge between zero-shot and one-shot NAS. In sub-one-shot NAS, the supernet is trained using only a small subset of the training data, a phase we refer to as \"warm-up.\" Within this framework, we present SiGeo, a proxy founded on a novel theoretical framework that connects the supernet warm-up with the efficacy of the proxy. Extensive experiments have shown that SiGeo, with the benefit of warm-up, consistently outperforms state-of-the-art NAS proxies on various established NAS benchmarks. When a supernet is warmed up, it can achieve comparable performance to weight-sharing one-shot NAS methods, but with a significant reduction (__FORMULA__\\%) in computational costs."}
{"title":"3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh Rasterization","authors":["Jianwei Feng","Prateek Singhal"],"raw_abstract":"Style transfer for human face has been widely researched in recent years.\nMajority of the existing approaches work in 2D image domain and have 3D\ninconsistency issue when applied on different viewpoints of the same face. In\nthis paper, we tackle the problem of 3D face style transfer which aims at\ngenerating stylized novel views of a 3D human face with multi-view consistency.\nWe propose to use a neural radiance field (NeRF) to represent 3D human face and\ncombine it with 2D style transfer to stylize the 3D face. We find that directly\ntraining a NeRF on stylized images from 2D style transfer brings in 3D\ninconsistency issue and causes blurriness. On the other hand, training a NeRF\njointly with 2D style transfer objectives shows poor convergence due to the\nidentity and head pose gap between style image and content image. It also poses\nchallenge in training time and memory due to the need of volume rendering for\nfull image to apply style transfer loss functions. We therefore propose a\nhybrid framework of NeRF and mesh rasterization to combine the benefits of high\nfidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our\nframework consists of three stages: 1. Training a NeRF model on input face\nimages to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF\nmodel and optimizing it with style transfer objectives via differentiable\nrasterization; 3. Training a new color network in NeRF conditioned on a style\nembedding to enable arbitrary style transfer to the 3D face. Experiment results\nshow that our approach generates high quality face style transfer with great 3D\nconsistency, while also enabling a flexible style control.","publication_date":1700630675,"paper_link":"http://arxiv.org/pdf/2311.13168v1","categories":["Computer Science"],"abstract":"Style transfer for human face has been widely researched in recent years. Majority of the existing approaches work in 2D image domain and have 3D inconsistency issue when applied on different viewpoints of the same face. In this paper, we tackle the problem of 3D face style transfer which aims at generating stylized novel views of a 3D human face with multi-view consistency. We propose to use a neural radiance field (NeRF) to represent 3D human face and combine it with 2D style transfer to stylize the 3D face. We find that directly training a NeRF on stylized images from 2D style transfer brings in 3D inconsistency issue and causes blurriness. On the other hand, training a NeRF jointly with 2D style transfer objectives shows poor convergence due to the identity and head pose gap between style image and content image. It also poses challenge in training time and memory due to the need of volume rendering for full image to apply style transfer loss functions. We therefore propose a hybrid framework of NeRF and mesh rasterization to combine the benefits of high fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our framework consists of three stages: 1. Training a NeRF model on input face images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF model and optimizing it with style transfer objectives via differentiable rasterization; 3. Training a new color network in NeRF conditioned on a style embedding to enable arbitrary style transfer to the 3D face. Experiment results show that our approach generates high quality face style transfer with great 3D consistency, while also enabling a flexible style control."}
{"title":"AdaptiveFL: Adaptive Heterogeneous Federated Learning for Resource-Constrained AIoT Systems","authors":["Chentao Jia","Ming Hu","Zekai Chen","Yanxin Yang","Xiaofei Xie","Yang Liu","Mingsong Chen"],"raw_abstract":"Although Federated Learning (FL) is promising to enable collaborative\nlearning among Artificial Intelligence of Things (AIoT) devices, it suffers\nfrom the problem of low classification performance due to various heterogeneity\nfactors (e.g., computing capacity, memory size) of devices and uncertain\noperating environments. To address these issues, this paper introduces an\neffective FL approach named AdaptiveFL based on a novel fine-grained width-wise\nmodel pruning strategy, which can generate various heterogeneous local models\nfor heterogeneous AIoT devices. By using our proposed reinforcement\nlearning-based device selection mechanism, AdaptiveFL can adaptively dispatch\nsuitable heterogeneous models to corresponding AIoT devices on the fly based on\ntheir available resources for local training. Experimental results show that,\ncompared to state-of-the-art methods, AdaptiveFL can achieve up to 16.83%\ninference improvements for both IID and non-IID scenarios.","publication_date":1700630262,"paper_link":"http://arxiv.org/pdf/2311.13166v1","categories":["Computer Science"],"abstract":"Although Federated Learning (FL) is promising to enable collaborative learning among Artificial Intelligence of Things (AIoT) devices, it suffers from the problem of low classification performance due to various heterogeneity factors (e.g., computing capacity, memory size) of devices and uncertain operating environments. To address these issues, this paper introduces an effective FL approach named AdaptiveFL based on a novel fine-grained width-wise model pruning strategy, which can generate various heterogeneous local models for heterogeneous AIoT devices. By using our proposed reinforcement learning-based device selection mechanism, AdaptiveFL can adaptively dispatch suitable heterogeneous models to corresponding AIoT devices on the fly based on their available resources for local training. Experimental results show that, compared to state-of-the-art methods, AdaptiveFL can achieve up to 16.83% inference improvements for both IID and non-IID scenarios."}
{"title":"Have Your Cake and Eat It Too: Toward Efficient and Accurate Split Federated Learning","authors":["Dengke Yan","Ming Hu","Zeke Xia","Yanxin Yang","Jun Xia","Xiaofei Xie","Mingsong Chen"],"raw_abstract":"Due to its advantages in resource constraint scenarios, Split Federated\nLearning (SFL) is promising in AIoT systems. However, due to data heterogeneity\nand stragglers, SFL suffers from the challenges of low inference accuracy and\nlow efficiency. To address these issues, this paper presents a novel SFL\napproach, named Sliding Split Federated Learning (S$^2$FL), which adopts an\nadaptive sliding model split strategy and a data balance-based training\nmechanism. By dynamically dispatching different model portions to AIoT devices\naccording to their computing capability, S$^2$FL can alleviate the low training\nefficiency caused by stragglers. By combining features uploaded by devices with\ndifferent data distributions to generate multiple larger batches with a uniform\ndistribution for back-propagation, S$^2$FL can alleviate the performance\ndegradation caused by data heterogeneity. Experimental results demonstrate\nthat, compared to conventional SFL, S$^2$FL can achieve up to 16.5\\% inference\naccuracy improvement and 3.54X training acceleration.","publication_date":1700629790,"paper_link":"http://arxiv.org/pdf/2311.13163v1","categories":["Computer Science"],"abstract":"Due to its advantages in resource constraint scenarios, Split Federated Learning (SFL) is promising in AIoT systems. However, due to data heterogeneity and stragglers, SFL suffers from the challenges of low inference accuracy and low efficiency. To address these issues, this paper presents a novel SFL approach, named Sliding Split Federated Learning (S__FORMULA__FL), which adopts an adaptive sliding model split strategy and a data balance-based training mechanism. By dynamically dispatching different model portions to AIoT devices according to their computing capability, S__FORMULA__FL can alleviate the low training efficiency caused by stragglers. By combining features uploaded by devices with different data distributions to generate multiple larger batches with a uniform distribution for back-propagation, S__FORMULA__FL can alleviate the performance degradation caused by data heterogeneity. Experimental results demonstrate that, compared to conventional SFL, S__FORMULA__FL can achieve up to 16.5\\% inference accuracy improvement and 3.54X training acceleration."}
{"title":"Test-Time Augmentation for 3D Point Cloud Classification and Segmentation","authors":["Tuan-Anh Vu","Srinjay Sarkar","Zhiyuan Zhang","Binh-Son Hua","Sai-Kit Yeung"],"raw_abstract":"Data augmentation is a powerful technique to enhance the performance of a\ndeep learning task but has received less attention in 3D deep learning. It is\nwell known that when 3D shapes are sparsely represented with low point density,\nthe performance of the downstream tasks drops significantly. This work explores\ntest-time augmentation (TTA) for 3D point clouds. We are inspired by the recent\nrevolution of learning implicit representation and point cloud upsampling,\nwhich can produce high-quality 3D surface reconstruction and\nproximity-to-surface, respectively. Our idea is to leverage the implicit field\nreconstruction or point cloud upsampling techniques as a systematic way to\naugment point cloud data. Mainly, we test both strategies by sampling points\nfrom the reconstructed results and using the sampled point cloud as test-time\naugmented data. We show that both strategies are effective in improving\naccuracy. We observed that point cloud upsampling for test-time augmentation\ncan lead to more significant performance improvement on downstream tasks such\nas object classification and segmentation on the ModelNet40, ShapeNet,\nScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds.","publication_date":1700627469,"paper_link":"http://arxiv.org/pdf/2311.13152v1","categories":["Computer Science"],"abstract":"Data augmentation is a powerful technique to enhance the performance of a deep learning task but has received less attention in 3D deep learning. It is well known that when 3D shapes are sparsely represented with low point density, the performance of the downstream tasks drops significantly. This work explores test-time augmentation (TTA) for 3D point clouds. We are inspired by the recent revolution of learning implicit representation and point cloud upsampling, which can produce high-quality 3D surface reconstruction and proximity-to-surface, respectively. Our idea is to leverage the implicit field reconstruction or point cloud upsampling techniques as a systematic way to augment point cloud data. Mainly, we test both strategies by sampling points from the reconstructed results and using the sampled point cloud as test-time augmented data. We show that both strategies are effective in improving accuracy. We observed that point cloud upsampling for test-time augmentation can lead to more significant performance improvement on downstream tasks such as object classification and segmentation on the ModelNet40, ShapeNet, ScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds."}
{"title":"Quantum spin dynamics due to strong Kitaev interactions in the triangular-lattice antiferromagnet CsCeSe$_2$","authors":["Tao Xie","S. Goze","Jie Xing","N. Zhao","S. M. Avdoshenko","L. Wu","Athena S. Sefat","A. L. Chernyshev","A. M. L\u00e4uchli","A. Podlesnyak","S. E. Nikitin"],"raw_abstract":"The extraordinary properties of the Kitaev model have motivated an intense\nsearch for new physics in materials that combine geometrical and bond\nfrustration. In this work, we employ inelastic neutron scattering, spin wave\ntheory, and exact diagonalization to study the spin dynamics in the perfect\ntriangular-lattice antiferromagnet (TLAF) CsCeSe$_2$. This material orders into\na stripe phase, which is demonstrated to arise as a consequence of the\noff-diagonal bond-dependent terms in the spin Hamiltonian. By studying the spin\ndynamics at intermediate fields, we identify an interaction between the\nsingle-magnon state and the two-magnon continuum that causes decay of coherent\nmagnon excitations, level repulsion, and transfer of spectral weight to the\ncontinuum that are controlled by the strength of the magnetic field. Our\nresults provide a microscopic mechanism for the stabilization of the stripe\nphase in TLAF and show how complex many-body physics can be present in the spin\ndynamics in a magnet with strong Kitaev coupling even in an ordered ground\nstate.","publication_date":1700626622,"paper_link":"http://arxiv.org/pdf/2311.13146v1","categories":["Physics"],"abstract":"The extraordinary properties of the Kitaev model have motivated an intense search for new physics in materials that combine geometrical and bond frustration. In this work, we employ inelastic neutron scattering, spin wave theory, and exact diagonalization to study the spin dynamics in the perfect triangular-lattice antiferromagnet (TLAF) CsCeSe__FORMULA__. This material orders into a stripe phase, which is demonstrated to arise as a consequence of the off-diagonal bond-dependent terms in the spin Hamiltonian. By studying the spin dynamics at intermediate fields, we identify an interaction between the single-magnon state and the two-magnon continuum that causes decay of coherent magnon excitations, level repulsion, and transfer of spectral weight to the continuum that are controlled by the strength of the magnetic field. Our results provide a microscopic mechanism for the stabilization of the stripe phase in TLAF and show how complex many-body physics can be present in the spin dynamics in a magnet with strong Kitaev coupling even in an ordered ground state."}
{"title":"Single Image Compressed Sensing MRI via a Self-Supervised Deep Denoising Approach","authors":["Marlon Bran Lorenzana","Feng Liu","Shekhar S. Chandra"],"raw_abstract":"Popular methods in compressed sensing (CS) are dependent on deep learning\n(DL), where large amounts of data are used to train non-linear reconstruction\nmodels. However, ensuring generalisability over and access to multiple datasets\nis challenging to realise for real-world applications. To address these\nconcerns, this paper proposes a single image, self-supervised (SS) CS-MRI\nframework that enables a joint deep and sparse regularisation of CS artefacts.\nThe approach effectively dampens structured CS artefacts, which can be\ndifficult to remove assuming sparse reconstruction, or relying solely on the\ninductive biases of CNN to produce noise-free images. Image quality is thereby\nimproved compared to either approach alone. Metrics are evaluated using\nCartesian 1D masks on a brain and knee dataset, with PSNR improving by 2-4dB on\naverage.","publication_date":1700626482,"paper_link":"http://arxiv.org/pdf/2311.13144v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Popular methods in compressed sensing (CS) are dependent on deep learning (DL), where large amounts of data are used to train non-linear reconstruction models. However, ensuring generalisability over and access to multiple datasets is challenging to realise for real-world applications. To address these concerns, this paper proposes a single image, self-supervised (SS) CS-MRI framework that enables a joint deep and sparse regularisation of CS artefacts. The approach effectively dampens structured CS artefacts, which can be difficult to remove assuming sparse reconstruction, or relying solely on the inductive biases of CNN to produce noise-free images. Image quality is thereby improved compared to either approach alone. Metrics are evaluated using Cartesian 1D masks on a brain and knee dataset, with PSNR improving by 2-4dB on average."}
{"title":"Stripe magnetic order and field-induced quantum criticality in the perfect triangular-lattice antiferromagnet CsCeSe$_2$","authors":["Tao Xie","N. Zhao","S. Goze","Jie Xing","S. M. Avdoshenko","K. M. Taddei","A. I. Kolesnikov","N. Harrison","C. dela Cruz","L. Wu","Athena S. Sefat","A. L. Chernyshev","A. M. L\u00e4uchli","A. Podlesnyak","S. E. Nikitin"],"raw_abstract":"The two-dimensional triangular-lattice antiferromagnet (TLAF) is a textbook\nexample of frustrated magnetic systems. Despite its simplicity, the TLAF model\nexhibits a highly rich and complex magnetic phase diagram, featuring numerous\ndistinct ground states that can be stabilized through frustrated next-nearest\nneighbor couplings or anisotropy. In this paper, we report low-temperature\nmagnetic properties of the TLAF material CsCeSe$_2$. The inelastic neutron\nscattering (INS) together with specific heat measurements and density\nfunctional theory calculations of crystalline electric field suggest that the\nground state of Ce ions is a Kramers doublet with strong easy-plane anisotropy.\nElastic neutron scattering measurements demonstrate the presence of stripe-$yz$\nmagnetic order that develops below $T_{\\rm N} = 0.35$ K, with the zero-field\nordered moment of $m_{\\rm Ce} \\approx 0.65~\\mu_{\\rm B}$. Application of\nmagnetic field first increases the ordering temperature by about 20% at the\nintermediate field region and eventually suppresses the stripe order in favor\nof the field-polarized ferromagnetic state via a continuous quantum phase\ntransition (QPT). The field-induced response demonstrates sizable anisotropy\nfor different in-plane directions, $\\mathbf{B}\\parallel{}\\mathbf{a}$ and\n$\\mathbf{B}\\perp{}\\mathbf{a}$, which indicates the presence of bond-dependent\ncoupling in the spin Hamiltonian. We further show theoretically that the\npresence of anisotropic bond-dependent interactions can change the universality\nclass of QPT for $\\mathbf{B}\\parallel{}\\mathbf{a}$ and\n$\\mathbf{B}\\perp{}\\mathbf{a}$.","publication_date":1700626274,"paper_link":"http://arxiv.org/pdf/2311.13143v1","categories":["Physics"],"abstract":"The two-dimensional triangular-lattice antiferromagnet (TLAF) is a textbook example of frustrated magnetic systems. Despite its simplicity, the TLAF model exhibits a highly rich and complex magnetic phase diagram, featuring numerous distinct ground states that can be stabilized through frustrated next-nearest neighbor couplings or anisotropy. In this paper, we report low-temperature magnetic properties of the TLAF material CsCeSe__FORMULA__. The inelastic neutron scattering (INS) together with specific heat measurements and density functional theory calculations of crystalline electric field suggest that the ground state of Ce ions is a Kramers doublet with strong easy-plane anisotropy. Elastic neutron scattering measurements demonstrate the presence of stripe-__FORMULA__ magnetic order that develops below __FORMULA__ K, with the zero-field ordered moment of __FORMULA__. Application of magnetic field first increases the ordering temperature by about 20% at the intermediate field region and eventually suppresses the stripe order in favor of the field-polarized ferromagnetic state via a continuous quantum phase transition (QPT). The field-induced response demonstrates sizable anisotropy for different in-plane directions, __FORMULA__ and __FORMULA__, which indicates the presence of bond-dependent coupling in the spin Hamiltonian. We further show theoretically that the presence of anisotropic bond-dependent interactions can change the universality class of QPT for __FORMULA__ and __FORMULA__."}
{"title":"Diffusion360: Seamless 360 Degree Panoramic Image Generation based on Diffusion Models","authors":["Mengyang Feng","Jinlin Liu","Miaomiao Cui","Xuansong Xie"],"raw_abstract":"This is a technical report on the 360-degree panoramic image generation task\nbased on diffusion models. Unlike ordinary 2D images, 360-degree panoramic\nimages capture the entire $360^\\circ\\times 180^\\circ$ field of view. So the\nrightmost and the leftmost sides of the 360 panoramic image should be\ncontinued, which is the main challenge in this field. However, the current\ndiffusion pipeline is not appropriate for generating such a seamless 360-degree\npanoramic image. To this end, we propose a circular blending strategy on both\nthe denoising and VAE decoding stages to maintain the geometry continuity.\nBased on this, we present two models for \\textbf{Text-to-360-panoramas} and\n\\textbf{Single-Image-to-360-panoramas} tasks. The code has been released as an\nopen-source project at\n\\href{https://github.com/ArcherFMY/SD-T2I-360PanoImage}{https://github.com/ArcherFMY/SD-T2I-360PanoImage}\nand\n\\href{https://www.modelscope.cn/models/damo/cv_diffusion_text-to-360panorama-image_generation/summary}{ModelScope}","publication_date":1700625999,"paper_link":"http://arxiv.org/pdf/2311.13141v1","categories":["Computer Science"],"abstract":"This is a technical report on the 360-degree panoramic image generation task based on diffusion models. Unlike ordinary 2D images, 360-degree panoramic images capture the entire __FORMULA__ field of view. So the rightmost and the leftmost sides of the 360 panoramic image should be continued, which is the main challenge in this field. However, the current diffusion pipeline is not appropriate for generating such a seamless 360-degree panoramic image. To this end, we propose a circular blending strategy on both the denoising and VAE decoding stages to maintain the geometry continuity. Based on this, we present two models for Text-to-360-panoramas and Single-Image-to-360-panoramas tasks. The code has been released as an open-source project at https://github.com/ArcherFMY/SD-T2I-360PanoImage{https://github.com/ArcherFMY/SD-T2I-360PanoImage} and https://www.modelscope.cn/models/damo/cv_diffusion_text-to-360panorama-image_generation/summary{ModelScope}"}
{"title":"Lightweight High-Speed Photography Built on Coded Exposure and Implicit Neural Representation of Videos","authors":["Zhihong Zhang","Runzhao Yang","Jinli Suo","Yuxiao Cheng","Qionghai Dai"],"raw_abstract":"The compact cameras recording high-speed scenes with high resolution are\nhighly demanded, but the required high bandwidth often leads to bulky, heavy\nsystems, which limits their applications on low-capacity platforms. Adopting a\ncoded exposure setup to encode a frame sequence into a blurry snapshot and\nretrieve the latent sharp video afterward can serve as a lightweight solution.\nHowever, restoring motion from blur is quite challenging due to the high\nill-posedness of motion blur decomposition, intrinsic ambiguity in motion\ndirection, and diverse motions in natural videos. In this work, by leveraging\nclassical coded exposure imaging technique and emerging implicit neural\nrepresentation for videos, we tactfully embed the motion direction cues into\nthe blurry image during the imaging process and develop a novel self-recursive\nneural network to sequentially retrieve the latent video sequence from the\nblurry image utilizing the embedded motion direction cues. To validate the\neffectiveness and efficiency of the proposed framework, we conduct extensive\nexperiments on benchmark datasets and real-captured blurry images. The results\ndemonstrate that our proposed framework significantly outperforms existing\nmethods in quality and flexibility. The code for our work is available at\nhttps://github.com/zhihongz/BDINR","publication_date":1700624473,"paper_link":"http://arxiv.org/pdf/2311.13134v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The compact cameras recording high-speed scenes with high resolution are highly demanded, but the required high bandwidth often leads to bulky, heavy systems, which limits their applications on low-capacity platforms. Adopting a coded exposure setup to encode a frame sequence into a blurry snapshot and retrieve the latent sharp video afterward can serve as a lightweight solution. However, restoring motion from blur is quite challenging due to the high ill-posedness of motion blur decomposition, intrinsic ambiguity in motion direction, and diverse motions in natural videos. In this work, by leveraging classical coded exposure imaging technique and emerging implicit neural representation for videos, we tactfully embed the motion direction cues into the blurry image during the imaging process and develop a novel self-recursive neural network to sequentially retrieve the latent video sequence from the blurry image utilizing the embedded motion direction cues. To validate the effectiveness and efficiency of the proposed framework, we conduct extensive experiments on benchmark datasets and real-captured blurry images. The results demonstrate that our proposed framework significantly outperforms existing methods in quality and flexibility. The code for our work is available at https://github.com/zhihongz/BDINR"}
{"title":"LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms","authors":["Aditi Jha","Sam Havens","Jeremey Dohmann","Alex Trott","Jacob Portes"],"raw_abstract":"Large Language Models are traditionally finetuned on large instruction\ndatasets. However recent studies suggest that small, high-quality datasets can\nsuffice for general purpose instruction following. This lack of consensus\nsurrounding finetuning best practices is in part due to rapidly diverging\napproaches to LLM evaluation. In this study, we ask whether a small amount of\ndiverse finetuning samples can improve performance on both traditional\nperplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We\nfinetune open-source MPT-7B and MPT-30B models on instruction finetuning\ndatasets of various sizes ranging from 1k to 60k samples. We find that subsets\nof 1k-6k instruction finetuning samples are sufficient to achieve good\nperformance on both (1) traditional NLP benchmarks and (2) model-based\nevaluation. Finally, we show that mixing textbook-style and open-ended QA\nfinetuning datasets optimizes performance on both evaluation paradigms.","publication_date":1700624221,"paper_link":"http://arxiv.org/pdf/2311.13133v1","categories":["Computer Science"],"abstract":"Large Language Models are traditionally finetuned on large instruction datasets. However recent studies suggest that small, high-quality datasets can suffice for general purpose instruction following. This lack of consensus surrounding finetuning best practices is in part due to rapidly diverging approaches to LLM evaluation. In this study, we ask whether a small amount of diverse finetuning samples can improve performance on both traditional perplexity-based NLP benchmarks, and on open-ended, model-based evaluation. We finetune open-source MPT-7B and MPT-30B models on instruction finetuning datasets of various sizes ranging from 1k to 60k samples. We find that subsets of 1k-6k instruction finetuning samples are sufficient to achieve good performance on both (1) traditional NLP benchmarks and (2) model-based evaluation. Finally, we show that mixing textbook-style and open-ended QA finetuning datasets optimizes performance on both evaluation paradigms."}
{"title":"Orientable Burning Number of Graphs","authors":["Julien Courtiel","Paul Dorbec","Tatsuya Gima","Romain Lecoq","Yota Otachi"],"raw_abstract":"In this paper, we introduce the problem of finding an orientation of a given\nundirected graph that maximizes the burning number of the resulting directed\ngraph. We show that the problem is polynomial-time solvable on\nK\\H{o}nig-Egerv\\'{a}ry graphs (and thus on bipartite graphs) and that an almost\noptimal solution can be computed in polynomial time for perfect graphs. On the\nother hand, we show that the problem is NP-hard in general and W[1]-hard\nparameterized by the target burning number. The hardness results are\ncomplemented by several fixed-parameter tractable results parameterized by\nstructural parameters. Our main result in this direction shows that the problem\nis fixed-parameter tractable parameterized by cluster vertex deletion number\nplus clique number (and thus also by vertex cover number).","publication_date":1700624116,"paper_link":"http://arxiv.org/pdf/2311.13132v1","categories":["Computer Science"],"abstract":"In this paper, we introduce the problem of finding an orientation of a given undirected graph that maximizes the burning number of the resulting directed graph. We show that the problem is polynomial-time solvable on Konig-Egerv\\'{a}ry graphs (and thus on bipartite graphs) and that an almost optimal solution can be computed in polynomial time for perfect graphs. On the other hand, we show that the problem is NP-hard in general and W[1]-hard parameterized by the target burning number. The hardness results are complemented by several fixed-parameter tractable results parameterized by structural parameters. Our main result in this direction shows that the problem is fixed-parameter tractable parameterized by cluster vertex deletion number plus clique number (and thus also by vertex cover number)."}
{"title":"Inducing a Tunable Skyrmion-Antiskyrmion System through Ion Beam Modification of FeGe Films","authors":["M. B. Venuti","Xiyue S. Zhang","Eric J Lang","Sadhvikas J. Addamane","Hanjong Paik","Portia Allen","Peter Sharma","David Muller","Khalid Hattar","Tzu-Ming Lu","Serena Eley"],"raw_abstract":"Skyrmions and antiskyrmions are nanoscale swirling textures of magnetic\nmoments formed by chiral interactions between atomic spins in magnetic\nnon-centrosymmetric materials and multilayer films with broken inversion\nsymmetry. These quasiparticles are of interest for use as information carriers\nin next-generation, low-energy spintronic applications. To develop\nskyrmion-based memory and logic, we must understand skyrmion-defect\ninteractions with two main goals -- determining how skyrmions navigate\nintrinsic material defects and determining how to engineer disorder for optimal\ndevice operation. Here, we introduce a tunable means of creating a\nskyrmion-antiskyrmion system by engineering the disorder landscape in FeGe\nusing ion irradiation. Specifically, we irradiate epitaxial B20-phase FeGe\nfilms with 2.8 MeV Au$^{4+}$ ions at varying fluences, inducing amorphous\nregions within the crystalline matrix. Using low-temperature electrical\ntransport and magnetization measurements, we observe a strong topological Hall\neffect with a double-peak feature that serves as a signature of skyrmions and\nantiskyrmions. These results are a step towards the development of information\nstorage devices that use skyrmions and anitskyrmions as storage bits and our\nsystem may serve as a testbed for theoretically predicted phenomena in\nskyrmion-antiskyrmion crystals.","publication_date":1700624058,"paper_link":"http://arxiv.org/pdf/2311.13130v1","categories":["Physics"],"abstract":"Skyrmions and antiskyrmions are nanoscale swirling textures of magnetic moments formed by chiral interactions between atomic spins in magnetic non-centrosymmetric materials and multilayer films with broken inversion symmetry. These quasiparticles are of interest for use as information carriers in next-generation, low-energy spintronic applications. To develop skyrmion-based memory and logic, we must understand skyrmion-defect interactions with two main goals -- determining how skyrmions navigate intrinsic material defects and determining how to engineer disorder for optimal device operation. Here, we introduce a tunable means of creating a skyrmion-antiskyrmion system by engineering the disorder landscape in FeGe using ion irradiation. Specifically, we irradiate epitaxial B20-phase FeGe films with 2.8 MeV Au__FORMULA__ ions at varying fluences, inducing amorphous regions within the crystalline matrix. Using low-temperature electrical transport and magnetization measurements, we observe a strong topological Hall effect with a double-peak feature that serves as a signature of skyrmions and antiskyrmions. These results are a step towards the development of information storage devices that use skyrmions and anitskyrmions as storage bits and our system may serve as a testbed for theoretically predicted phenomena in skyrmion-antiskyrmion crystals."}
{"title":"P2RBox: A Single Point is All You Need for Oriented Object Detection","authors":["Guangming Cao","Xuehui Yu","Wenwen Yu","Xumeng Han","Xue Yang","Guorong Li","Jianbin Jiao","Zhenjun Han"],"raw_abstract":"Oriented object detection, a specialized subfield in computer vision, finds\napplications across diverse scenarios, excelling particularly when dealing with\nobjects of arbitrary orientations. Conversely, point annotation, which treats\nobjects as single points, offers a cost-effective alternative to rotated and\nhorizontal bounding boxes but sacrifices performance due to the loss of size\nand orientation information. In this study, we introduce the P2RBox network,\nwhich leverages point annotations and a mask generator to create mask\nproposals, followed by filtration through our Inspector Module and Constrainer\nModule. This process selects high-quality masks, which are subsequently\nconverted into rotated box annotations for training a fully supervised\ndetector. Specifically, we've thoughtfully crafted an Inspector Module rooted\nin multi-instance learning principles to evaluate the semantic score of masks.\nWe've also proposed a more robust mask quality assessment in conjunction with\nthe Constrainer Module. Furthermore, we've introduced a Symmetry Axis\nEstimation (SAE) Module inspired by the spectral theorem for symmetric matrices\nto transform the top-performing mask proposal into rotated bounding boxes.\nP2RBox performs well with three fully supervised rotated object detectors:\nRetinaNet, Rotated FCOS, and Oriented R-CNN. By combining with Oriented R-CNN,\nP2RBox achieves 62.26% on DOTA-v1.0 test dataset. As far as we know, this is\nthe first attempt at training an oriented object detector with point\nsupervision.","publication_date":1700623980,"paper_link":"http://arxiv.org/pdf/2311.13128v1","categories":["Computer Science"],"abstract":"Oriented object detection, a specialized subfield in computer vision, finds applications across diverse scenarios, excelling particularly when dealing with objects of arbitrary orientations. Conversely, point annotation, which treats objects as single points, offers a cost-effective alternative to rotated and horizontal bounding boxes but sacrifices performance due to the loss of size and orientation information. In this study, we introduce the P2RBox network, which leverages point annotations and a mask generator to create mask proposals, followed by filtration through our Inspector Module and Constrainer Module. This process selects high-quality masks, which are subsequently converted into rotated box annotations for training a fully supervised detector. Specifically, we've thoughtfully crafted an Inspector Module rooted in multi-instance learning principles to evaluate the semantic score of masks. We've also proposed a more robust mask quality assessment in conjunction with the Constrainer Module. Furthermore, we've introduced a Symmetry Axis Estimation (SAE) Module inspired by the spectral theorem for symmetric matrices to transform the top-performing mask proposal into rotated bounding boxes. P2RBox performs well with three fully supervised rotated object detectors: RetinaNet, Rotated FCOS, and Oriented R-CNN. By combining with Oriented R-CNN, P2RBox achieves 62.26% on DOTA-v1.0 test dataset. As far as we know, this is the first attempt at training an oriented object detector with point supervision."}
{"title":"Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis","authors":["Yixin Liu","Chenrui Fan","Yutong Dai","Xun Chen","Pan Zhou","Lichao Sun"],"raw_abstract":"Text-to-image diffusion models allow seamless generation of personalized\nimages from scant reference photos. Yet, these tools, in the wrong hands, can\nfabricate misleading or harmful content, endangering individuals. To address\nthis problem, existing poisoning-based approaches perturb user images in an\nimperceptible way to render them \"unlearnable\" from malicious uses. We identify\ntwo limitations of these defending approaches: i) sub-optimal due to the\nhand-crafted heuristics for solving the intractable bilevel optimization and\nii) lack of robustness against simple data transformations like Gaussian\nfiltering. To solve these challenges, we propose MetaCloak, which solves the\nbi-level poisoning problem with a meta-learning framework with an additional\ntransformation sampling process to craft transferable and robust perturbation.\nSpecifically, we employ a pool of surrogate diffusion models to craft\ntransferable and model-agnostic perturbation. Furthermore, by incorporating an\nadditional transformation process, we design a simple denoising-error\nmaximization loss that is sufficient for causing transformation-robust semantic\ndistortion and degradation in a personalized generation. Extensive experiments\non the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing\napproaches. Notably, MetaCloak can successfully fool online training services\nlike Replicate, in a black-box manner, demonstrating the effectiveness of\nMetaCloak in real-world scenarios. Our code is available at\nhttps://github.com/liuyixin-louis/MetaCloak.","publication_date":1700623891,"paper_link":"http://arxiv.org/pdf/2311.13127v1","categories":["Computer Science"],"abstract":"Text-to-image diffusion models allow seamless generation of personalized images from scant reference photos. Yet, these tools, in the wrong hands, can fabricate misleading or harmful content, endangering individuals. To address this problem, existing poisoning-based approaches perturb user images in an imperceptible way to render them \"unlearnable\" from malicious uses. We identify two limitations of these defending approaches: i) sub-optimal due to the hand-crafted heuristics for solving the intractable bilevel optimization and ii) lack of robustness against simple data transformations like Gaussian filtering. To solve these challenges, we propose MetaCloak, which solves the bi-level poisoning problem with a meta-learning framework with an additional transformation sampling process to craft transferable and robust perturbation. Specifically, we employ a pool of surrogate diffusion models to craft transferable and model-agnostic perturbation. Furthermore, by incorporating an additional transformation process, we design a simple denoising-error maximization loss that is sufficient for causing transformation-robust semantic distortion and degradation in a personalized generation. Extensive experiments on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing approaches. Notably, MetaCloak can successfully fool online training services like Replicate, in a black-box manner, demonstrating the effectiveness of MetaCloak in real-world scenarios. Our code is available at https://github.com/liuyixin-louis/MetaCloak."}
{"title":"Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper","authors":["Chengyu Wang","Junbing Yan","Wei Zhang","Jun Huang"],"raw_abstract":"This paper delves into the pressing need in Parameter-Efficient Fine-Tuning\n(PEFT) for Large Language Models (LLMs). While LLMs possess remarkable\ncapabilities, their extensive parameter requirements and associated\ncomputational demands hinder their practicality and scalability for real-world\napplications. Our position paper highlights current states and the necessity of\nfurther studying into the topic, and recognizes significant challenges and open\nissues that must be addressed to fully harness the powerful abilities of LLMs.\nThese challenges encompass novel efficient PEFT architectures, PEFT for\ndifferent learning settings, PEFT combined with model compression techniques,\nand the exploration of PEFT for multi-modal LLMs. By presenting this position\npaper, we aim to stimulate further research and foster discussions surrounding\nmore efficient and accessible PEFT for LLMs.","publication_date":1700623714,"paper_link":"http://arxiv.org/pdf/2311.13126v1","categories":["Computer Science"],"abstract":"This paper delves into the pressing need in Parameter-Efficient Fine-Tuning (PEFT) for Large Language Models (LLMs). While LLMs possess remarkable capabilities, their extensive parameter requirements and associated computational demands hinder their practicality and scalability for real-world applications. Our position paper highlights current states and the necessity of further studying into the topic, and recognizes significant challenges and open issues that must be addressed to fully harness the powerful abilities of LLMs. These challenges encompass novel efficient PEFT architectures, PEFT for different learning settings, PEFT combined with model compression techniques, and the exploration of PEFT for multi-modal LLMs. By presenting this position paper, we aim to stimulate further research and foster discussions surrounding more efficient and accessible PEFT for LLMs."}
{"title":"DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation","authors":["Zhiqin Chen","Qimin Chen","Hang Zhou","Hao Zhang"],"raw_abstract":"We present an unsupervised 3D shape co-segmentation method which learns a set\nof deformable part templates from a shape collection. To accommodate structural\nvariations in the collection, our network composes each shape by a selected\nsubset of template parts which are affine-transformed. To maximize the\nexpressive power of the part templates, we introduce a per-part deformation\nnetwork to enable the modeling of diverse parts with substantial geometry\nvariations, while imposing constraints on the deformation capacity to ensure\nfidelity to the originally represented parts. We also propose a training scheme\nto effectively overcome local minima. Architecturally, our network is a\nbranched autoencoder, with a CNN encoder taking a voxel shape as input and\nproducing per-part transformation matrices, latent codes, and part existence\nscores, and the decoder outputting point occupancies to define the\nreconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder,\ncan achieve unsupervised 3D shape co-segmentation that yields fine-grained,\ncompact, and meaningful parts that are consistent across diverse shapes. We\nconduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an\nanimal subset of Objaverse to show superior performance over prior methods.","publication_date":1700623567,"paper_link":"http://arxiv.org/pdf/2311.13125v1","categories":["Computer Science"],"abstract":"We present an unsupervised 3D shape co-segmentation method which learns a set of deformable part templates from a shape collection. To accommodate structural variations in the collection, our network composes each shape by a selected subset of template parts which are affine-transformed. To maximize the expressive power of the part templates, we introduce a per-part deformation network to enable the modeling of diverse parts with substantial geometry variations, while imposing constraints on the deformation capacity to ensure fidelity to the originally represented parts. We also propose a training scheme to effectively overcome local minima. Architecturally, our network is a branched autoencoder, with a CNN encoder taking a voxel shape as input and producing per-part transformation matrices, latent codes, and part existence scores, and the decoder outputting point occupancies to define the reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder, can achieve unsupervised 3D shape co-segmentation that yields fine-grained, compact, and meaningful parts that are consistent across diverse shapes. We conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an animal subset of Objaverse to show superior performance over prior methods."}
{"title":"Fast Parallel Algorithms for Submodular $p$-Superseparable Maximization","authors":["Philip Cervenjak","Junhao Gan","Anthony Wirth"],"raw_abstract":"Maximizing a non-negative, monontone, submodular function $f$ over $n$\nelements under a cardinality constraint $k$ (SMCC) is a well-studied NP-hard\nproblem. It has important applications in, e.g., machine learning and influence\nmaximization. Though the theoretical problem admits polynomial-time\napproximation algorithms, solving it in practice often involves frequently\nquerying submodular functions that are expensive to compute. This has motivated\nsignificant research into designing parallel approximation algorithms in the\nadaptive complexity model; adaptive complexity (adaptivity) measures the number\nof sequential rounds of $\\text{poly}(n)$ function queries an algorithm\nrequires. The state-of-the-art algorithms can achieve\n$(1-\\frac{1}{e}-\\varepsilon)$-approximate solutions with\n$O(\\frac{1}{\\varepsilon^2}\\log n)$ adaptivity, which approaches the known\nadaptivity lower-bounds. However, the $O(\\frac{1}{\\varepsilon^2} \\log n)$\nadaptivity only applies to maximizing worst-case functions that are unlikely to\nappear in practice. Thus, in this paper, we consider the special class of\n$p$-superseparable submodular functions, which places a reasonable constraint\non $f$, based on the parameter $p$, and is more amenable to maximization, while\nalso having real-world applicability. Our main contribution is the algorithm\nLS+GS, a finer-grained version of the existing LS+PGB algorithm, designed for\ninstances of SMCC when $f$ is $p$-superseparable; it achieves an expected\n$(1-\\frac{1}{e}-\\varepsilon)$-approximate solution with\n$O(\\frac{1}{\\varepsilon^2}\\log(p k))$ adaptivity independent of $n$.\nAdditionally, unrelated to $p$-superseparability, our LS+GS algorithm uses only\n$O(\\frac{n}{\\varepsilon} + \\frac{\\log n}{\\varepsilon^2})$ oracle queries, which\nhas an improved dependence on $\\varepsilon^{-1}$ over the state-of-the-art\nLS+PGB; this is achieved through the design of a novel thresholding subroutine.","publication_date":1700621433,"paper_link":"http://arxiv.org/pdf/2311.13123v1","categories":["Computer Science"],"abstract":"Maximizing a non-negative, monontone, submodular function __FORMULA__ over __FORMULA__ elements under a cardinality constraint __FORMULA__ (SMCC) is a well-studied NP-hard problem. It has important applications in, e.g., machine learning and influence maximization. Though the theoretical problem admits polynomial-time approximation algorithms, solving it in practice often involves frequently querying submodular functions that are expensive to compute. This has motivated significant research into designing parallel approximation algorithms in the adaptive complexity model; adaptive complexity (adaptivity) measures the number of sequential rounds of __FORMULA__ function queries an algorithm requires. The state-of-the-art algorithms can achieve __FORMULA__-approximate solutions with __FORMULA__ adaptivity, which approaches the known adaptivity lower-bounds. However, the __FORMULA__ adaptivity only applies to maximizing worst-case functions that are unlikely to appear in practice. Thus, in this paper, we consider the special class of __FORMULA__-superseparable submodular functions, which places a reasonable constraint on __FORMULA__, based on the parameter __FORMULA__, and is more amenable to maximization, while also having real-world applicability. Our main contribution is the algorithm LS+GS, a finer-grained version of the existing LS+PGB algorithm, designed for instances of SMCC when __FORMULA__ is __FORMULA__-superseparable; it achieves an expected __FORMULA__-approximate solution with __FORMULA__ adaptivity independent of __FORMULA__. Additionally, unrelated to __FORMULA__-superseparability, our LS+GS algorithm uses only __FORMULA__ oracle queries, which has an improved dependence on __FORMULA__ over the state-of-the-art LS+PGB; this is achieved through the design of a novel thresholding subroutine."}
{"title":"Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer","authors":["Zhen Zhao","Can Huang","Binghong Wu","Chunhui Lin","Hao Liu","Zhizhong Zhang","Xin Tan","Jingqun Tang","Yuan Xie"],"raw_abstract":"Scene text recognition (STR) in the wild frequently encounters challenges\nwhen coping with domain variations, font diversity, shape deformations, etc. A\nstraightforward solution is performing model fine-tuning tailored to a specific\nscenario, but it is computationally intensive and requires multiple model\ncopies for various scenarios. Recent studies indicate that large language\nmodels (LLMs) can learn from a few demonstration examples in a training-free\nmanner, termed \"In-Context Learning\" (ICL). Nevertheless, applying LLMs as a\ntext recognizer is unacceptably resource-consuming. Moreover, our pilot\nexperiments on LLMs show that ICL fails in STR, mainly attributed to the\ninsufficient incorporation of contextual information from diverse samples in\nthe training stage. To this end, we introduce E$^2$STR, a STR model trained\nwith context-rich scene text sequences, where the sequences are generated via\nour proposed in-context training strategy. E$^2$STR demonstrates that a\nregular-sized model is sufficient to achieve effective ICL capabilities in STR.\nExtensive experiments show that E$^2$STR exhibits remarkable training-free\nadaptation in various scenarios and outperforms even the fine-tuned\nstate-of-the-art approaches on public benchmarks.","publication_date":1700621217,"paper_link":"http://arxiv.org/pdf/2311.13120v1","categories":["Computer Science"],"abstract":"Scene text recognition (STR) in the wild frequently encounters challenges when coping with domain variations, font diversity, shape deformations, etc. A straightforward solution is performing model fine-tuning tailored to a specific scenario, but it is computationally intensive and requires multiple model copies for various scenarios. Recent studies indicate that large language models (LLMs) can learn from a few demonstration examples in a training-free manner, termed \"In-Context Learning\" (ICL). Nevertheless, applying LLMs as a text recognizer is unacceptably resource-consuming. Moreover, our pilot experiments on LLMs show that ICL fails in STR, mainly attributed to the insufficient incorporation of contextual information from diverse samples in the training stage. To this end, we introduce E__FORMULA__STR, a STR model trained with context-rich scene text sequences, where the sequences are generated via our proposed in-context training strategy. E__FORMULA__STR demonstrates that a regular-sized model is sufficient to achieve effective ICL capabilities in STR. Extensive experiments show that E__FORMULA__STR exhibits remarkable training-free adaptation in various scenarios and outperforms even the fine-tuned state-of-the-art approaches on public benchmarks."}
{"title":"Quantum optimization of coherent chaotic systems: A case for buses of Kathmandu","authors":["Kiran Adhikari","Aman Ganeju","Iva Kumari Lamichhane","Rohit Bhattarai","Manghang Limbu","Nishma Bhattarai","Christian Deppe"],"raw_abstract":"In this paper, we propose a novel quantum computing approach to solve the\nreal-world problem of optimizing transportation in bustling Kathmandu city. The\ntransportation system in Kathmandu is chaotic, with no central authority\ncontrolling the transportation. We leverage this chaotic feature in our quantum\noptimization procedure. The quantum chaos theory's Wigner-Dyson distribution\nsurfaced as the most effective bus spacing distribution for a bus driver to\nmaximize their profit. We investigate the statistical properties of the buses\nwith real-time GPS bus location data and optimize bus spacing and interval\ndistribution around the 27 km circular ring road in Kathmandu. Using tools like\nquantum simulation, eigenvalue distributions, and output wave function\nanalysis, we show that such optimal bus spacing distribution could be achieved.","publication_date":1700621138,"paper_link":"http://arxiv.org/pdf/2311.13119v1","categories":["Physics"],"abstract":"In this paper, we propose a novel quantum computing approach to solve the real-world problem of optimizing transportation in bustling Kathmandu city. The transportation system in Kathmandu is chaotic, with no central authority controlling the transportation. We leverage this chaotic feature in our quantum optimization procedure. The quantum chaos theory's Wigner-Dyson distribution surfaced as the most effective bus spacing distribution for a bus driver to maximize their profit. We investigate the statistical properties of the buses with real-time GPS bus location data and optimize bus spacing and interval distribution around the 27 km circular ring road in Kathmandu. Using tools like quantum simulation, eigenvalue distributions, and output wave function analysis, we show that such optimal bus spacing distribution could be achieved."}
{"title":"Combatting Human Trafficking in the Cyberspace: A Natural Language Processing-Based Methodology to Analyze the Language in Online Advertisements","authors":["Alejandro Rodriguez Perez","Pablo Rivas"],"raw_abstract":"This project tackles the pressing issue of human trafficking in online C2C\nmarketplaces through advanced Natural Language Processing (NLP) techniques. We\nintroduce a novel methodology for generating pseudo-labeled datasets with\nminimal supervision, serving as a rich resource for training state-of-the-art\nNLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and\nOrganized Activity Detection (OAD), we employ cutting-edge Transformer models\nfor analysis. A key contribution is the implementation of an interpretability\nframework using Integrated Gradients, providing explainable insights crucial\nfor law enforcement. This work not only fills a critical gap in the literature\nbut also offers a scalable, machine learning-driven approach to combat human\nexploitation online. It serves as a foundation for future research and\npractical applications, emphasizing the role of machine learning in addressing\ncomplex social issues.","publication_date":1700621101,"paper_link":"http://arxiv.org/pdf/2311.13118v1","categories":["Computer Science"],"abstract":"This project tackles the pressing issue of human trafficking in online C2C marketplaces through advanced Natural Language Processing (NLP) techniques. We introduce a novel methodology for generating pseudo-labeled datasets with minimal supervision, serving as a rich resource for training state-of-the-art NLP models. Focusing on tasks like Human Trafficking Risk Prediction (HTRP) and Organized Activity Detection (OAD), we employ cutting-edge Transformer models for analysis. A key contribution is the implementation of an interpretability framework using Integrated Gradients, providing explainable insights crucial for law enforcement. This work not only fills a critical gap in the literature but also offers a scalable, machine learning-driven approach to combat human exploitation online. It serves as a foundation for future research and practical applications, emphasizing the role of machine learning in addressing complex social issues."}
{"title":"Magneto-structural phase transitions and two-dimensional spin waves in graphite","authors":["Nadina Gheorghiu","Charles R. Ebbing","Timothy J. Haugan"],"raw_abstract":"We have previously found experimental evidence for several quantum phenomena\nin oxygen-ion implanted of hydrogenated graphite: ferromagnetism,\nantiferromagnetism, paramagentism, triplet superconductivity, Andreev states,\nLittle-Parks oscillations, Lamb shift, Casimir effect, colossal\nmagnetoresistance, and topologically-protected flat-energy bands [1-6]. Triplet\nsuperconductivity results in the formation of Josephson junctions, thus with\npotential of being used for spintronics applications in the critical area of\nquantum computing. In this paper, we are showing new experimental evidence for\nthe formation of two-dimensional (2D) spin waves in oxygen-ion enriched and in\nhydrogenated highly oriented pyrolytic graphite. The temperature evolution of\nthe remanent magnetization Mrem(T) data confirms the formation of spin waves\nthat follow the 2D Heisenberg model with a weak uniaxial anisotropy. In\naddition, the step-like features also found in the temperature dependence of\nthe electrical resistivity between insulating and metallic states suggest\nseveral outstanding possibilities, such as a structural transition, triplet\nsuperconductivity, and chiral properties.","publication_date":1700620672,"paper_link":"http://arxiv.org/pdf/2311.13116v1","categories":["Physics"],"abstract":"We have previously found experimental evidence for several quantum phenomena in oxygen-ion implanted of hydrogenated graphite: ferromagnetism, antiferromagnetism, paramagentism, triplet superconductivity, Andreev states, Little-Parks oscillations, Lamb shift, Casimir effect, colossal magnetoresistance, and topologically-protected flat-energy bands [1-6]. Triplet superconductivity results in the formation of Josephson junctions, thus with potential of being used for spintronics applications in the critical area of quantum computing. In this paper, we are showing new experimental evidence for the formation of two-dimensional (2D) spin waves in oxygen-ion enriched and in hydrogenated highly oriented pyrolytic graphite. The temperature evolution of the remanent magnetization Mrem(T) data confirms the formation of spin waves that follow the 2D Heisenberg model with a weak uniaxial anisotropy. In addition, the step-like features also found in the temperature dependence of the electrical resistivity between insulating and metallic states suggest several outstanding possibilities, such as a structural transition, triplet superconductivity, and chiral properties."}
{"title":"Quantum geodesics reflecting the internal structure of stars composed of shells","authors":["Sojeong Cheong","Wontae Kim"],"raw_abstract":"In general relativity, an external observer cannot distinguish distinct\ninternal structures between two spherically symmetric stars that have the same\ntotal mass $M$. However, when quantum corrections are taken into account, the\nexternal metrics of the stars will receive quantum corrections depending on\ntheir internal structures. In this paper, we obtain the quantum-corrected\nmetrics at second order in curvature for two spherically symmetric shells\ncharacterized by different internal structures: one with an empty interior and\nthe other with $N$ internal shells. The dependence on the internal structures\nin the corrected metrics tells us that geodesics on these backgrounds would be\ndeformed according to the internal structures. We conduct numerical\ncomputations to find out the angle of geodesic precession and show that the\npresence of internal structures amplifies the precession angle reflecting the\ndiscrepancy between the radial and orbital periods within the geodesic orbit.\nThe amount of the precession angle increases monotonically as the number of\ninternal shells increases and it eventually converges to a certain value for $N\n\\to \\infty$.","publication_date":1700620240,"paper_link":"http://arxiv.org/pdf/2311.13113v1","categories":["Physics"],"abstract":"In general relativity, an external observer cannot distinguish distinct internal structures between two spherically symmetric stars that have the same total mass __FORMULA__. However, when quantum corrections are taken into account, the external metrics of the stars will receive quantum corrections depending on their internal structures. In this paper, we obtain the quantum-corrected metrics at second order in curvature for two spherically symmetric shells characterized by different internal structures: one with an empty interior and the other with __FORMULA__ internal shells. The dependence on the internal structures in the corrected metrics tells us that geodesics on these backgrounds would be deformed according to the internal structures. We conduct numerical computations to find out the angle of geodesic precession and show that the presence of internal structures amplifies the precession angle reflecting the discrepancy between the radial and orbital periods within the geodesic orbit. The amount of the precession angle increases monotonically as the number of internal shells increases and it eventually converges to a certain value for __FORMULA__."}
{"title":"Reconfigurable Image Processing Metasurfaces with Phase-Change Materials","authors":["Michele Cotrufo","Shaban B. Sulejman","Lukas Wesemann","Md. Ataur Rahman","Madhu Bhaskaran","Ann Roberts","Andrea Al\u00f9"],"raw_abstract":"Optical metasurfaces have been enabling reduced footprint and power\nconsumption, as well as faster speeds, in the context of analog computing and\nimage processing. While various image processing and optical computing\nfunctionalities have been recently demonstrated using metasurfaces, most of the\nconsidered devices are static and lack reconfigurability. Yet, the ability to\ndynamically reconfigure processing operations is key for metasurfaces to be\nable to compete with practical computing systems. Here, we demonstrate a\npassive edge-detection metasurface operating in the near-infrared regime whose\nimage processing response can be drastically modified by temperature variations\nsmaller than 10{\\deg} C around a CMOS-compatible temperature of 65{\\deg} C.\nSuch reconfigurability is achieved by leveraging the insulator-to-metal phase\ntransition of a thin buried layer of vanadium dioxide which, in turn, strongly\nalters the nonlocal response of the metasurface. Importantly, this\nreconfigurability is accompanied by performance metrics - such as high\nnumerical aperture, high efficiency, isotropy, and polarization-independence -\nclose to optimal, and it is combined with a simple geometry compatible with\nlarge-scale manufacturing. Our work paves the way to a new generation of\nultra-compact, tunable, passive devices for all-optical computation, with\npotential applications in augmented reality, remote sensing and bio-medical\nimaging.","publication_date":1700619777,"paper_link":"http://arxiv.org/pdf/2311.13109v1","categories":["Physics"],"abstract":"Optical metasurfaces have been enabling reduced footprint and power consumption, as well as faster speeds, in the context of analog computing and image processing. While various image processing and optical computing functionalities have been recently demonstrated using metasurfaces, most of the considered devices are static and lack reconfigurability. Yet, the ability to dynamically reconfigure processing operations is key for metasurfaces to be able to compete with practical computing systems. Here, we demonstrate a passive edge-detection metasurface operating in the near-infrared regime whose image processing response can be drastically modified by temperature variations smaller than 10{\\deg} C around a CMOS-compatible temperature of 65{\\deg} C. Such reconfigurability is achieved by leveraging the insulator-to-metal phase transition of a thin buried layer of vanadium dioxide which, in turn, strongly alters the nonlocal response of the metasurface. Importantly, this reconfigurability is accompanied by performance metrics - such as high numerical aperture, high efficiency, isotropy, and polarization-independence - close to optimal, and it is combined with a simple geometry compatible with large-scale manufacturing. Our work paves the way to a new generation of ultra-compact, tunable, passive devices for all-optical computation, with potential applications in augmented reality, remote sensing and bio-medical imaging."}
{"title":"Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language","authors":["Pablo Loyola","Edison Marrese-Taylor","Andres Hoyos-Idobro"],"raw_abstract":"The need for grounding in language understanding is an active research topic.\nPrevious work has suggested that color perception and color language appear as\na suitable test bed to empirically study the problem, given its cognitive\nsignificance and showing that there is considerable alignment between a defined\ncolor space and the feature space defined by a language model. To further study\nthis issue, we collect a large scale source of colors and their descriptions,\ncontaining almost a 1 million examples , and perform an empirical analysis to\ncompare two kinds of alignments: (i) inter-space, by learning a mapping between\nembedding space and color space, and (ii) intra-space, by means of prompting\ncomparatives between color descriptions. Our results show that while color\nspace alignment holds for monolexemic, highly pragmatic color descriptions,\nthis alignment drops considerably in the presence of examples that exhibit\nelements of real linguistic usage such as subjectivity and abstractedness,\nsuggesting that grounding may be required in such cases.","publication_date":1700619156,"paper_link":"http://arxiv.org/pdf/2311.13105v1","categories":["Computer Science"],"abstract":"The need for grounding in language understanding is an active research topic. Previous work has suggested that color perception and color language appear as a suitable test bed to empirically study the problem, given its cognitive significance and showing that there is considerable alignment between a defined color space and the feature space defined by a language model. To further study this issue, we collect a large scale source of colors and their descriptions, containing almost a 1 million examples , and perform an empirical analysis to compare two kinds of alignments: (i) inter-space, by learning a mapping between embedding space and color space, and (ii) intra-space, by means of prompting comparatives between color descriptions. Our results show that while color space alignment holds for monolexemic, highly pragmatic color descriptions, this alignment drops considerably in the presence of examples that exhibit elements of real linguistic usage such as subjectivity and abstractedness, suggesting that grounding may be required in such cases."}
{"title":"AC Power Flow Informed Parameter Learning for DC Power Flow Network Equivalents","authors":["Babak Taheri","Daniel K. Molzahn"],"raw_abstract":"This paper presents an algorithm to optimize the parameters of power systems\nequivalents to enhance the accuracy of the DC power flow approximation in\nreduced networks. Based on a zonal division of the network, the algorithm\nproduces a reduced power system equivalent that captures inter-zonal flows with\naggregated buses and equivalent transmission lines. The algorithm refines\ncoefficient and bias parameters for the DC power flow model of the reduced\nnetwork, aiming to minimize discrepancies between inter-zonal flows in DC and\nAC power flow results. Using optimization methods like BFGS, L-BFGS, and TNC in\nan offline training phase, these parameters boost the accuracy of online DC\npower flow computations. In contrast to existing network equivalencing methods,\nthe proposed algorithm optimizes accuracy over a specified range of operation\nas opposed to only considering a single nominal point. Numerical tests\ndemonstrate substantial accuracy improvements over traditional equivalencing\nand approximation methods.","publication_date":1700619039,"paper_link":"http://arxiv.org/pdf/2311.13104v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper presents an algorithm to optimize the parameters of power systems equivalents to enhance the accuracy of the DC power flow approximation in reduced networks. Based on a zonal division of the network, the algorithm produces a reduced power system equivalent that captures inter-zonal flows with aggregated buses and equivalent transmission lines. The algorithm refines coefficient and bias parameters for the DC power flow model of the reduced network, aiming to minimize discrepancies between inter-zonal flows in DC and AC power flow results. Using optimization methods like BFGS, L-BFGS, and TNC in an offline training phase, these parameters boost the accuracy of online DC power flow computations. In contrast to existing network equivalencing methods, the proposed algorithm optimizes accuracy over a specified range of operation as opposed to only considering a single nominal point. Numerical tests demonstrate substantial accuracy improvements over traditional equivalencing and approximation methods."}
{"title":"The signaling dimension in generalized probabilistic theories","authors":["Michele Dall'Arno","Alessandro Tosini","Francesco Buscemi"],"raw_abstract":"The signaling dimension of a given physical system quantifies the minimum\ndimension of a classical system required to reproduce all input/output\ncorrelations of the given system. Thus, unlike other dimension measures - such\nas the dimension of the linear space or the maximum number of (jointly or\npairwise) perfectly discriminable states - which examine the correlation space\nonly along a single direction, the signaling dimension does not depend on the\narbitrary choice of a specific operational task. In this sense, the signaling\ndimension summarizes the structure of the entire set of input/output\ncorrelations consistent with a given system in a single scalar quantity. For\nquantum theory, it was recently proved by Frenkel and Weiner in a seminal\nresult that the signaling dimension coincides with the Hilbert space dimension.\n  Here, we derive analytical and algorithmic techniques to compute the\nsignaling dimension for any given system of any given generalized probabilistic\ntheory. We prove that it suffices to consider extremal measurements with\nray-extremal effects, and we bound the number of elements of any such\nmeasurement in terms of the linear dimension. For systems with a finite number\nof extremal effects, we recast the problem of characterizing the extremal\nmeasurements with ray-extremal effects as the problem of deriving the vertex\ndescription of a polytope given its face description, which can be conveniently\nsolved by standard techniques. For each such measurement, we recast the\ncomputation of the signaling dimension as a linear program, and we propose a\ncombinatorial branch and bound algorithm to reduce its size. We apply our\nresults to derive the extremal measurements with ray-extremal effects of a\ncomposition of two square bits (or squits) and prove that their signaling\ndimension is five, even though each squit has a signaling dimension equal to\ntwo.","publication_date":1700618956,"paper_link":"http://arxiv.org/pdf/2311.13103v1","categories":["Physics"],"abstract":"The signaling dimension of a given physical system quantifies the minimum dimension of a classical system required to reproduce all input/output correlations of the given system. Thus, unlike other dimension measures - such as the dimension of the linear space or the maximum number of (jointly or pairwise) perfectly discriminable states - which examine the correlation space only along a single direction, the signaling dimension does not depend on the arbitrary choice of a specific operational task. In this sense, the signaling dimension summarizes the structure of the entire set of input/output correlations consistent with a given system in a single scalar quantity. For quantum theory, it was recently proved by Frenkel and Weiner in a seminal result that the signaling dimension coincides with the Hilbert space dimension.   Here, we derive analytical and algorithmic techniques to compute the signaling dimension for any given system of any given generalized probabilistic theory. We prove that it suffices to consider extremal measurements with ray-extremal effects, and we bound the number of elements of any such measurement in terms of the linear dimension. For systems with a finite number of extremal effects, we recast the problem of characterizing the extremal measurements with ray-extremal effects as the problem of deriving the vertex description of a polytope given its face description, which can be conveniently solved by standard techniques. For each such measurement, we recast the computation of the signaling dimension as a linear program, and we propose a combinatorial branch and bound algorithm to reduce its size. We apply our results to derive the extremal measurements with ray-extremal effects of a composition of two square bits (or squits) and prove that their signaling dimension is five, even though each squit has a signaling dimension equal to two."}
{"title":"Detecting out-of-distribution text using topological features of transformer-based language models","authors":["Andres Pollano","Anupam Chaudhuri","Anj Simmons"],"raw_abstract":"We attempt to detect out-of-distribution (OOD) text samples though applying\nTopological Data Analysis (TDA) to attention maps in transformer-based language\nmodels. We evaluate our proposed TDA-based approach for out-of-distribution\ndetection on BERT, a transformer-based language model, and compare the to a\nmore traditional OOD approach based on BERT CLS embeddings. We found that our\nTDA approach outperforms the CLS embedding approach at distinguishing\nin-distribution data (politics and entertainment news articles from HuffPost)\nfrom far out-of-domain samples (IMDB reviews), but its effectiveness\ndeteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business\nnews articles from HuffPost) datasets.","publication_date":1700618675,"paper_link":"http://arxiv.org/pdf/2311.13102v1","categories":["Mathematics"],"abstract":"We attempt to detect out-of-distribution (OOD) text samples though applying Topological Data Analysis (TDA) to attention maps in transformer-based language models. We evaluate our proposed TDA-based approach for out-of-distribution detection on BERT, a transformer-based language model, and compare the to a more traditional OOD approach based on BERT CLS embeddings. We found that our TDA approach outperforms the CLS embedding approach at distinguishing in-distribution data (politics and entertainment news articles from HuffPost) from far out-of-domain samples (IMDB reviews), but its effectiveness deteriorates with near out-of-domain (CNN/Dailymail) or same-domain (business news articles from HuffPost) datasets."}
{"title":"Automated Measurement of Pericoronary Adipose Tissue Attenuation and Volume in CT Angiography","authors":["Andrew M. Nguyen","Tejas Sudharshan Mathai","Liangchen Liu","Jianfei Liu","Ronald M. Summers"],"raw_abstract":"Pericoronary adipose tissue (PCAT) is the deposition of fat in the vicinity\nof the coronary arteries. It is an indicator of coronary inflammation and\nassociated with coronary artery disease. Non-invasive coronary CT angiography\n(CCTA) is presently used to obtain measures of the thickness, volume, and\nattenuation of fat deposition. However, prior works solely focus on measuring\nPCAT using semi-automated approaches at the right coronary artery (RCA) over\nthe left coronary artery (LCA). In this pilot work, we developed a fully\nautomated approach for the measurement of PCAT mean attenuation and volume in\nthe region around both coronary arteries. First, we used a large subset of\npatients from the public ImageCAS dataset (n = 735) to train a 3D full\nresolution nnUNet to segment LCA and RCA. Then, we automatically measured PCAT\nin the surrounding arterial regions. We evaluated our method on a held-out test\nset of patients (n = 183) from the same dataset. A mean Dice score of 83% and\nPCAT attenuation of -73.81 $\\pm$ 12.69 HU was calculated for the RCA, while a\nmean Dice score of 81% and PCAT attenuation of -77.51 $\\pm$ 7.94 HU was\ncomputed for the LCA. To the best of our knowledge, we are the first to develop\na fully automated method to measure PCAT attenuation and volume at both the RCA\nand LCA. Our work underscores how automated PCAT measurement holds promise as a\nbiomarker for identification of inflammation and cardiac disease.","publication_date":1700618359,"paper_link":"http://arxiv.org/pdf/2311.13100v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Pericoronary adipose tissue (PCAT) is the deposition of fat in the vicinity of the coronary arteries. It is an indicator of coronary inflammation and associated with coronary artery disease. Non-invasive coronary CT angiography (CCTA) is presently used to obtain measures of the thickness, volume, and attenuation of fat deposition. However, prior works solely focus on measuring PCAT using semi-automated approaches at the right coronary artery (RCA) over the left coronary artery (LCA). In this pilot work, we developed a fully automated approach for the measurement of PCAT mean attenuation and volume in the region around both coronary arteries. First, we used a large subset of patients from the public ImageCAS dataset (n = 735) to train a 3D full resolution nnUNet to segment LCA and RCA. Then, we automatically measured PCAT in the surrounding arterial regions. We evaluated our method on a held-out test set of patients (n = 183) from the same dataset. A mean Dice score of 83% and PCAT attenuation of -73.81 __FORMULA__ 12.69 HU was calculated for the RCA, while a mean Dice score of 81% and PCAT attenuation of -77.51 __FORMULA__ 7.94 HU was computed for the LCA. To the best of our knowledge, we are the first to develop a fully automated method to measure PCAT attenuation and volume at both the RCA and LCA. Our work underscores how automated PCAT measurement holds promise as a biomarker for identification of inflammation and cardiac disease."}
{"title":"PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF","authors":["Yutao Feng","Yintong Shang","Xuan Li","Tianjia Shao","Chenfanfu Jiang","Yin Yang"],"raw_abstract":"We show that physics-based simulations can be seamlessly integrated with NeRF\nto generate high-quality elastodynamics of real-world objects. Unlike existing\nmethods, we discretize nonlinear hyperelasticity in a meshless way, obviating\nthe necessity for intermediate auxiliary shape proxies like a tetrahedral mesh\nor voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed\nto capture nonlinear dynamics and large deformation on the implicit model. Such\nmeshless integration enables versatile simulations of complex and codimensional\nshapes. We adaptively place the least-square kernels according to the NeRF\ndensity field to significantly reduce the complexity of the nonlinear\nsimulation. As a result, physically realistic animations can be conveniently\nsynthesized using our method for a wide range of hyperelastic materials at an\ninteractive rate. For more information, please visit our project page at\nhttps://fytalon.github.io/pienerf/.","publication_date":1700618306,"paper_link":"http://arxiv.org/pdf/2311.13099v1","categories":["Computer Science"],"abstract":"We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalon.github.io/pienerf/."}
{"title":"Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications","authors":["Ha-Thanh Nguyen","Wachara Fungwacharakorn","Ken Satoh"],"raw_abstract":"Language serves as a vehicle for conveying thought, enabling communication\namong individuals. The ability to distinguish between diverse concepts,\nidentify fairness and injustice, and comprehend a range of legal notions\nfundamentally relies on logical reasoning. Large Language Models (LLMs) attempt\nto emulate human language understanding and generation, but their competency in\nlogical reasoning remains limited. This paper seeks to address the\nphilosophical question: How can we effectively teach logical reasoning to LLMs\nwhile maintaining a deep understanding of the intricate relationship between\nlanguage and logic? By focusing on bolstering LLMs' capabilities in logical\nreasoning, we aim to expand their applicability in law and other\nlogic-intensive disciplines. To this end, we propose a Reinforcement Learning\nfrom Logical Feedback (RLLF) approach, which serves as a potential framework\nfor refining LLMs' reasoning capacities. Through RLLF and a revised evaluation\nmethodology, we explore new avenues for research in this domain and contribute\nto the development of LLMs capable of handling complex legal reasoning tasks\nwhile acknowledging the fundamental connection between language and logic.","publication_date":1700617910,"paper_link":"http://arxiv.org/pdf/2311.13095v1","categories":["Computer Science"],"abstract":"Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic."}
{"title":"Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise","authors":["Yixin Liu","Kaidi Xu","Xun Chen","Lichao Sun"],"raw_abstract":"The open source of large amounts of image data promotes the development of\ndeep learning techniques. Along with this comes the privacy risk of these\nopen-source image datasets being exploited by unauthorized third parties to\ntrain deep learning models for commercial or illegal purposes. To avoid the\nabuse of public data, a poisoning-based technique, the unlearnable example, is\nproposed to significantly degrade the generalization performance of models by\nadding a kind of imperceptible noise to the data. To further enhance its\nrobustness against adversarial training, existing works leverage iterative\nadversarial training on both the defensive noise and the surrogate model.\nHowever, it still remains unknown whether the robustness of unlearnable\nexamples primarily comes from the effect of enhancement in the surrogate model\nor the defensive noise. Observing that simply removing the adversarial noise on\nthe training process of the defensive noise can improve the performance of\nrobust unlearnable examples, we identify that solely the surrogate model's\nrobustness contributes to the performance. Furthermore, we found a negative\ncorrelation exists between the robustness of defensive noise and the protection\nperformance, indicating defensive noise's instability issue. Motivated by this,\nto further boost the robust unlearnable example, we introduce stable\nerror-minimizing noise (SEM), which trains the defensive noise against random\nperturbation instead of the time-consuming adversarial perturbation to improve\nthe stability of defensive noise. Through extensive experiments, we demonstrate\nthat SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,\nand ImageNet Subset in terms of both effectiveness and efficiency. The code is\navailable at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.","publication_date":1700617437,"paper_link":"http://arxiv.org/pdf/2311.13091v1","categories":["Computer Science"],"abstract":"The open source of large amounts of image data promotes the development of deep learning techniques. Along with this comes the privacy risk of these open-source image datasets being exploited by unauthorized third parties to train deep learning models for commercial or illegal purposes. To avoid the abuse of public data, a poisoning-based technique, the unlearnable example, is proposed to significantly degrade the generalization performance of models by adding a kind of imperceptible noise to the data. To further enhance its robustness against adversarial training, existing works leverage iterative adversarial training on both the defensive noise and the surrogate model. However, it still remains unknown whether the robustness of unlearnable examples primarily comes from the effect of enhancement in the surrogate model or the defensive noise. Observing that simply removing the adversarial noise on the training process of the defensive noise can improve the performance of robust unlearnable examples, we identify that solely the surrogate model's robustness contributes to the performance. Furthermore, we found a negative correlation exists between the robustness of defensive noise and the protection performance, indicating defensive noise's instability issue. Motivated by this, to further boost the robust unlearnable example, we introduce stable error-minimizing noise (SEM), which trains the defensive noise against random perturbation instead of the time-consuming adversarial perturbation to improve the stability of defensive noise. Through extensive experiments, we demonstrate that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet Subset in terms of both effectiveness and efficiency. The code is available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example."}
{"title":"On the Limitation of Diffusion Models for Synthesizing Training Datasets","authors":["Shin'ya Yamaguchi","Takuma Fukuda"],"raw_abstract":"Synthetic samples from diffusion models are promising for leveraging in\ntraining discriminative models as replications of real training datasets.\nHowever, we found that the synthetic datasets degrade classification\nperformance over real datasets even when using state-of-the-art diffusion\nmodels. This means that modern diffusion models do not perfectly represent the\ndata distribution for the purpose of replicating datasets for training\ndiscriminative tasks. This paper investigates the gap between synthetic and\nreal samples by analyzing the synthetic samples reconstructed from real samples\nthrough the diffusion and reverse process. By varying the time steps starting\nthe reverse process in the reconstruction, we can control the trade-off between\nthe information in the original real data and the information added by\ndiffusion models. Through assessing the reconstructed samples and trained\nmodels, we found that the synthetic data are concentrated in modes of the\ntraining data distribution as the reverse step increases, and thus, they are\ndifficult to cover the outer edges of the distribution. Our findings imply that\nmodern diffusion models are insufficient to replicate training data\ndistribution perfectly, and there is room for the improvement of generative\nmodeling in the replication of training datasets.","publication_date":1700617343,"paper_link":"http://arxiv.org/pdf/2311.13090v1","categories":["Computer Science"],"abstract":"Synthetic samples from diffusion models are promising for leveraging in training discriminative models as replications of real training datasets. However, we found that the synthetic datasets degrade classification performance over real datasets even when using state-of-the-art diffusion models. This means that modern diffusion models do not perfectly represent the data distribution for the purpose of replicating datasets for training discriminative tasks. This paper investigates the gap between synthetic and real samples by analyzing the synthetic samples reconstructed from real samples through the diffusion and reverse process. By varying the time steps starting the reverse process in the reconstruction, we can control the trade-off between the information in the original real data and the information added by diffusion models. Through assessing the reconstructed samples and trained models, we found that the synthetic data are concentrated in modes of the training data distribution as the reverse step increases, and thus, they are difficult to cover the outer edges of the distribution. Our findings imply that modern diffusion models are insufficient to replicate training data distribution perfectly, and there is room for the improvement of generative modeling in the replication of training datasets."}
{"title":"Assessing the Probability of Extremely Low Wind Energy Production in Europe at Sub-seasonal to Seasonal Time Scales","authors":["Bastien Cozian","Corentin Herbert","Freddy Bouchet"],"raw_abstract":"The European energy system will undergo major transformations in the coming\ndecades to implement mitigation measures and comply with the Paris Agreement.\nIn particular, the share of weather-dependent wind generation will increase\nsignificantly in the European energy mix. The most extreme fluctuations of the\nproduction at all time scales need to be taken into account in the design of\nthe power system. In particular, extreme long-lasting low wind energy\nproduction events constitute a specific challenge, as most flexibility\nsolutions do not apply at time scales beyond a few days. However, the\nprobability and amplitude of such events has to a large extent eluded\nquantitative study so far due to lack of sufficiently long data. In this\nletter, using a 1000-year climate simulation, we study rare events of wind\nenergy production that last from a few weeks to a few months over the\nJanuary-February period, at the scale of a continent (Europe) and a country\n(France). The results show that the fluctuations of the capacity factor over\nEurope exhibit nearly Gaussian statistics at all time scales. A similar result\nholds over France for events longer than about two weeks and return times up to\na few decades. In that case, the return time curves follow a universal curve.\nFurthermore, a simple Gaussian process with the same covariance structure as\nthe data gives good estimates of the amplitude of the most extreme events. This\nmethod allows to estimate return times for rare events from shorter but more\naccurate data sources. We demonstrate this possibility with reanalysis data.","publication_date":1700672448,"paper_link":"http://arxiv.org/pdf/2311.13526v1","categories":["Physics"],"abstract":"The European energy system will undergo major transformations in the coming decades to implement mitigation measures and comply with the Paris Agreement. In particular, the share of weather-dependent wind generation will increase significantly in the European energy mix. The most extreme fluctuations of the production at all time scales need to be taken into account in the design of the power system. In particular, extreme long-lasting low wind energy production events constitute a specific challenge, as most flexibility solutions do not apply at time scales beyond a few days. However, the probability and amplitude of such events has to a large extent eluded quantitative study so far due to lack of sufficiently long data. In this letter, using a 1000-year climate simulation, we study rare events of wind energy production that last from a few weeks to a few months over the January-February period, at the scale of a continent (Europe) and a country (France). The results show that the fluctuations of the capacity factor over Europe exhibit nearly Gaussian statistics at all time scales. A similar result holds over France for events longer than about two weeks and return times up to a few decades. In that case, the return time curves follow a universal curve. Furthermore, a simple Gaussian process with the same covariance structure as the data gives good estimates of the amplitude of the most extreme events. This method allows to estimate return times for rare events from shorter but more accurate data sources. We demonstrate this possibility with reanalysis data."}
{"title":"Benchmarking Toxic Molecule Classification using Graph Neural Networks and Few Shot Learning","authors":["Bhavya Mehta","Kush Kothari","Reshmika Nambiar","Seema Shrawne"],"raw_abstract":"Traditional methods like Graph Convolutional Networks (GCNs) face challenges\nwith limited data and class imbalance, leading to suboptimal performance in\ngraph classification tasks during toxicity prediction of molecules as a whole.\nTo address these issues, we harness the power of Graph Isomorphic Networks,\nMulti Headed Attention and Free Large-scale Adversarial Augmentation separately\non Graphs for precisely capturing the structural data of molecules and their\ntoxicological properties. Additionally, we incorporate Few-Shot Learning to\nimprove the model's generalization with limited annotated samples. Extensive\nexperiments on a diverse toxicology dataset demonstrate that our method\nachieves an impressive state-of-art AUC-ROC value of 0.816, surpassing the\nbaseline GCN model by 11.4%. This highlights the significance of our proposed\nmethodology and Few Shot Learning in advancing Toxic Molecular Classification,\nwith the potential to enhance drug discovery and environmental risk assessment\nprocesses.","publication_date":1700669252,"paper_link":"http://arxiv.org/pdf/2311.13490v1","categories":["Quantitative Biology"],"abstract":"Traditional methods like Graph Convolutional Networks (GCNs) face challenges with limited data and class imbalance, leading to suboptimal performance in graph classification tasks during toxicity prediction of molecules as a whole. To address these issues, we harness the power of Graph Isomorphic Networks, Multi Headed Attention and Free Large-scale Adversarial Augmentation separately on Graphs for precisely capturing the structural data of molecules and their toxicological properties. Additionally, we incorporate Few-Shot Learning to improve the model's generalization with limited annotated samples. Extensive experiments on a diverse toxicology dataset demonstrate that our method achieves an impressive state-of-art AUC-ROC value of 0.816, surpassing the baseline GCN model by 11.4%. This highlights the significance of our proposed methodology and Few Shot Learning in advancing Toxic Molecular Classification, with the potential to enhance drug discovery and environmental risk assessment processes."}
{"title":"Accelerating Inference in Molecular Diffusion Models with Latent Representations of Protein Structure","authors":["Ian Dunn","David Ryan Koes"],"raw_abstract":"Diffusion generative models have emerged as a powerful framework for\naddressing problems in structural biology and structure-based drug design.\nThese models operate directly on 3D molecular structures. Due to the\nunfavorable scaling of graph neural networks (GNNs) with graph size as well as\nthe relatively slow inference speeds inherent to diffusion models, many\nexisting molecular diffusion models rely on coarse-grained representations of\nprotein structure to make training and inference feasible. However, such\ncoarse-grained representations discard essential information for modeling\nmolecular interactions and impair the quality of generated structures. In this\nwork, we present a novel GNN-based architecture for learning latent\nrepresentations of molecular structure. When trained end-to-end with a\ndiffusion model for de novo ligand design, our model achieves comparable\nperformance to one with an all-atom protein representation while exhibiting a\n3-fold reduction in inference time.","publication_date":1700667151,"paper_link":"http://arxiv.org/pdf/2311.13466v1","categories":["Quantitative Biology"],"abstract":"Diffusion generative models have emerged as a powerful framework for addressing problems in structural biology and structure-based drug design. These models operate directly on 3D molecular structures. Due to the unfavorable scaling of graph neural networks (GNNs) with graph size as well as the relatively slow inference speeds inherent to diffusion models, many existing molecular diffusion models rely on coarse-grained representations of protein structure to make training and inference feasible. However, such coarse-grained representations discard essential information for modeling molecular interactions and impair the quality of generated structures. In this work, we present a novel GNN-based architecture for learning latent representations of molecular structure. When trained end-to-end with a diffusion model for de novo ligand design, our model achieves comparable performance to one with an all-atom protein representation while exhibiting a 3-fold reduction in inference time."}
{"title":"Exploring a Modification of $d_p$ Convergence","authors":["Brian Allen","Edward Bryden"],"raw_abstract":"In the work by M. C. Lee, A. Naber, and R. Neumayer \\cite{LNN} a beautiful\n$\\varepsilon$-regularity theorem is proved under small negative scalar\ncurvature and entropy bounds. In that paper, the $d_p$ distance for Riemannian\nmanifolds is introduced and the quantitative stability results are given in\nterms of this notion of distance, with important examples showing why other\nexisting notions of convergence are not adequate in their setting. Due to the\npresence of an entropy bound, the possibility of long, thin splines forming\nalong a sequence of Riemannian manifolds whose scalar curvature is becoming\nalmost positive is ruled out. It is important to rule out such examples since\nthe $d_p$ distance is not well behaved in the presences of splines that persist\nin the limit. Since there are many geometric stability conjectures\n\\cite{GromovFour, SormaniIAS} where we want to allow for the presence of\nsplines that persist in the limit, it is crucial to be able to modify the $d_p$\ndistance to retain its positive qualities and prevent it from being sensitive\nto splines. In this paper we explore one such modification of the $d_p$\ndistance and give a theorem which allows one to estimate the modified $d_p$\ndistance, which we expect to be useful in practice.","publication_date":1700666101,"paper_link":"http://arxiv.org/pdf/2311.13450v1","categories":["Mathematics"],"abstract":"In the work by M. C. Lee, A. Naber, and R. Neumayer LNN a beautiful __FORMULA__-regularity theorem is proved under small negative scalar curvature and entropy bounds. In that paper, the __FORMULA__ distance for Riemannian manifolds is introduced and the quantitative stability results are given in terms of this notion of distance, with important examples showing why other existing notions of convergence are not adequate in their setting. Due to the presence of an entropy bound, the possibility of long, thin splines forming along a sequence of Riemannian manifolds whose scalar curvature is becoming almost positive is ruled out. It is important to rule out such examples since the __FORMULA__ distance is not well behaved in the presences of splines that persist in the limit. Since there are many geometric stability conjectures GromovFour, SormaniIAS where we want to allow for the presence of splines that persist in the limit, it is crucial to be able to modify the __FORMULA__ distance to retain its positive qualities and prevent it from being sensitive to splines. In this paper we explore one such modification of the __FORMULA__ distance and give a theorem which allows one to estimate the modified __FORMULA__ distance, which we expect to be useful in practice."}
{"title":"Reproducible image-based profiling with Pycytominer","authors":["Erik Serrano","Srinivas Niranj Chandrasekaran","Dave Bunten","Kenneth I. Brewer","Jenna Tomkinson","Roshan Kern","Michael Bornholdt","Stephen Fleming","Ruifan Pei","John Arevalo","Hillary Tsang","Vincent Rubinetti","Callum Tromans-Coia","Tim Becker","Erin Weisbart","Charlotte Bunne","Alexandr A. Kalinin","Rebecca Senft","Stephen J. Taylor","Nasim Jamali","Adeniyi Adeboye","Hamdah Shafqat Abbasi","Allen Goodman","Juan C. Caicedo","Anne E. Carpenter","Beth A. Cimini","Shantanu Singh","Gregory P. Way"],"raw_abstract":"Technological advances in high-throughput microscopy have facilitated the\nacquisition of cell images at a rapid pace, and data pipelines can now extract\nand process thousands of image-based features from microscopy images. These\nfeatures represent valuable single-cell phenotypes that contain information\nabout cell state and biological processes. The use of these features for\nbiological discovery is known as image-based or morphological profiling.\nHowever, these raw features need processing before use and image-based\nprofiling lacks scalable and reproducible open-source software. Inconsistent\nprocessing across studies makes it difficult to compare datasets and processing\nsteps, further delaying the development of optimal pipelines, methods, and\nanalyses. To address these issues, we present Pycytominer, an open-source\nsoftware package with a vibrant community that establishes an image-based\nprofiling standard. Pycytominer has a simple, user-friendly Application\nProgramming Interface (API) that implements image-based profiling functions for\nprocessing high-dimensional morphological features extracted from microscopy\nimages of cells. Establishing Pycytominer as a standard image-based profiling\ntoolkit ensures consistent data processing pipelines with data provenance,\ntherefore minimizing potential inconsistencies and enabling researchers to\nconfidently derive accurate conclusions and discover novel insights from their\ndata, thus driving progress in our field.","publication_date":1700663208,"paper_link":"http://arxiv.org/pdf/2311.13417v1","categories":["Quantitative Biology"],"abstract":"Technological advances in high-throughput microscopy have facilitated the acquisition of cell images at a rapid pace, and data pipelines can now extract and process thousands of image-based features from microscopy images. These features represent valuable single-cell phenotypes that contain information about cell state and biological processes. The use of these features for biological discovery is known as image-based or morphological profiling. However, these raw features need processing before use and image-based profiling lacks scalable and reproducible open-source software. Inconsistent processing across studies makes it difficult to compare datasets and processing steps, further delaying the development of optimal pipelines, methods, and analyses. To address these issues, we present Pycytominer, an open-source software package with a vibrant community that establishes an image-based profiling standard. Pycytominer has a simple, user-friendly Application Programming Interface (API) that implements image-based profiling functions for processing high-dimensional morphological features extracted from microscopy images of cells. Establishing Pycytominer as a standard image-based profiling toolkit ensures consistent data processing pipelines with data provenance, therefore minimizing potential inconsistencies and enabling researchers to confidently derive accurate conclusions and discover novel insights from their data, thus driving progress in our field."}
{"title":"Space-like Electromagnetic Form Factors of Lambda- and Sigma-Baryons from Quark-Diquark Faddeev Equations","authors":["Langtian Liu","Christian S. Fischer"],"raw_abstract":"An important goal of ongoing and future experiments is to explore spectra and\ntransition form factors of baryons with non-zero strangeness\n\\cite{Dudek:2012vr,Adamczewski-Musch2021a}. Of particular interest is the\ntransition form factor$\\gamma^{(*)} \\Sigma^0 \\rightarrow \\Lambda$ in the\ntime-like momentum region that can be extracted from Dalitz decays. On the road\ntowards a theoretical description of these form factors we extend a covariant\ndynamical quark-diquark model for the baryon Faddeev equation to the\nstrange-quark sector. Based on an excellent description of the mass spectrum of\nselected baryon octet and decuplet states and reasonable results for the\nnucleon form factors we determine the elastic electromagnetic form factors of\n$\\Lambda$ and $\\Sigma^+, \\Sigma^0, \\Sigma^-$ hyperons in the space-like region\nas well as the ones for the octet transition $\\gamma^{(*)} \\Sigma^0 \\rightarrow\n\\Lambda$.\n  We discuss qualitative and quantitative features of the diquark-quark picture\nand compare systematically with previous results from a three-body Faddeev\napproach and lattice data where available.","publication_date":1700646184,"paper_link":"http://arxiv.org/pdf/2311.13269v1","categories":["Physics"],"abstract":"An important goal of ongoing and future experiments is to explore spectra and transition form factors of baryons with non-zero strangeness Dudek:2012vr,Adamczewski-Musch2021a. Of particular interest is the transition form factor__FORMULA__ in the time-like momentum region that can be extracted from Dalitz decays. On the road towards a theoretical description of these form factors we extend a covariant dynamical quark-diquark model for the baryon Faddeev equation to the strange-quark sector. Based on an excellent description of the mass spectrum of selected baryon octet and decuplet states and reasonable results for the nucleon form factors we determine the elastic electromagnetic form factors of __FORMULA__ and __FORMULA__ hyperons in the space-like region as well as the ones for the octet transition __FORMULA__.   We discuss qualitative and quantitative features of the diquark-quark picture and compare systematically with previous results from a three-body Faddeev approach and lattice data where available."}
{"title":"Effect of Constraint Relaxation on the Minimum Vertex Cover Problem in Random Graphs","authors":["Aki Dote","Koji Hukushima"],"raw_abstract":"A statistical-mechanical study of the effect of constraint relaxation on the\nminimum vertex cover problem in Erd\\H{o}s-R\\'enyi random graphs is presented.\nUsing a penalty-method formulation for constraint relaxation, typical\nproperties of solutions, including infeasible solutions that violate the\nconstraints, are analyzed by means of the replica method and cavity method. The\nproblem involves a competition between reducing the number of vertices to be\ncovered and satisfying the edge constraints. The analysis under the\nreplica-symmetric (RS) ansatz clarifies that the competition leads to\ndegeneracies in the vertex and edge states, which determine the quantitative\nproperties of the system, such as the cover and penalty ratios. A precise\nanalysis of these effects improves the accuracy of RS approximation for the\nminimum cover ratio in the replica symmetry breaking (RSB) region. Furthermore,\nthe analysis based on the RS cavity method indicates that the RS/RSB boundary\nof the ground states with respect to the mean degree of the graphs is expanded,\nand the critical temperature is lowered by constraint relaxation.","publication_date":1700643191,"paper_link":"http://arxiv.org/pdf/2311.13237v1","categories":["Mathematics","Physics"],"abstract":"A statistical-mechanical study of the effect of constraint relaxation on the minimum vertex cover problem in Erdos-R\\'enyi random graphs is presented. Using a penalty-method formulation for constraint relaxation, typical properties of solutions, including infeasible solutions that violate the constraints, are analyzed by means of the replica method and cavity method. The problem involves a competition between reducing the number of vertices to be covered and satisfying the edge constraints. The analysis under the replica-symmetric (RS) ansatz clarifies that the competition leads to degeneracies in the vertex and edge states, which determine the quantitative properties of the system, such as the cover and penalty ratios. A precise analysis of these effects improves the accuracy of RS approximation for the minimum cover ratio in the replica symmetry breaking (RSB) region. Furthermore, the analysis based on the RS cavity method indicates that the RS/RSB boundary of the ground states with respect to the mean degree of the graphs is expanded, and the critical temperature is lowered by constraint relaxation."}
{"title":"Validation of Consumer-grade Digital Camera-based Human Activity Evaluation for Upper Limb Exercises and Development of a Therapist-guided, Automated Telerehabilitation Framework and Platform for Stroke Rehabilitation","authors":["Elton H. L. Yeung","Yingxian Chen","Wilton W. T. Fok","Gary K. K. Lau"],"raw_abstract":"Timely and adequate rehabilitation is critical in facilitating post-stroke\nrecovery. However, the organization and delivery of rehabilitation are\nresource-demanding, and are only available to approximately 25% of stroke\nsurvivors in low-to-middle-income countries. Improving access to stroke\nrehabilitation services through innovative solutions is therefore urgently\nrequired. Tele-rehabilitation, which transits care to home- and community\nsettings, has emerged as a promising solution. However, current approaches\nusing video tutorial, teleconference, or other specialized devices face\ninherent shortfalls that limit their uptake. In this study, we proposed and\nvalidated the use of an open-source, markerless motion capture model with\nconsumer-grade devices to overcome these challenges. Our solution enables\nreliable measurement of the end range of motion during upper limb exercises\nwith near-perfect waveform similarity and intraclass correlation to that of the\ngold standard Kinect approach. Our multidisciplinary team developed an\nautomated telerehabilitation framework incorporating the validated markerless\ntechnique to facilitate a seamless telerehabilitation process. It enables\npersonalized rehabilitation plans with real-time feedback, and individual\nprogress reports using objective quantitative and qualitative features to\nimprove patient monitoring and management, and home-based rehabilitation\nservice uptake and compliance. This study serves as a proof-of-concept in\npreparation for the future development of a detailed model of care, and\nfeasibility, usability, and cost-effectiveness studies of an automated\ntelerehabilitation platform and framework in improving the state of post-stroke\nrehabilitation and functional outcome.","publication_date":1700617118,"paper_link":"http://arxiv.org/pdf/2311.13088v1","categories":["Quantitative Biology"],"abstract":"Timely and adequate rehabilitation is critical in facilitating post-stroke recovery. However, the organization and delivery of rehabilitation are resource-demanding, and are only available to approximately 25% of stroke survivors in low-to-middle-income countries. Improving access to stroke rehabilitation services through innovative solutions is therefore urgently required. Tele-rehabilitation, which transits care to home- and community settings, has emerged as a promising solution. However, current approaches using video tutorial, teleconference, or other specialized devices face inherent shortfalls that limit their uptake. In this study, we proposed and validated the use of an open-source, markerless motion capture model with consumer-grade devices to overcome these challenges. Our solution enables reliable measurement of the end range of motion during upper limb exercises with near-perfect waveform similarity and intraclass correlation to that of the gold standard Kinect approach. Our multidisciplinary team developed an automated telerehabilitation framework incorporating the validated markerless technique to facilitate a seamless telerehabilitation process. It enables personalized rehabilitation plans with real-time feedback, and individual progress reports using objective quantitative and qualitative features to improve patient monitoring and management, and home-based rehabilitation service uptake and compliance. This study serves as a proof-of-concept in preparation for the future development of a detailed model of care, and feasibility, usability, and cost-effectiveness studies of an automated telerehabilitation platform and framework in improving the state of post-stroke rehabilitation and functional outcome."}
{"title":"Intrinsic Image Decomposition via Ordinal Shading","authors":["Chris Careaga","Ya\u011f\u0131z Aksoy"],"raw_abstract":"Intrinsic decomposition is a fundamental mid-level vision problem that plays\na crucial role in various inverse rendering and computational photography\npipelines. Generating highly accurate intrinsic decompositions is an inherently\nunder-constrained task that requires precisely estimating continuous-valued\nshading and albedo. In this work, we achieve high-resolution intrinsic\ndecomposition by breaking the problem into two parts. First, we present a dense\nordinal shading formulation using a shift- and scale-invariant loss in order to\nestimate ordinal shading cues without restricting the predictions to obey the\nintrinsic model. We then combine low- and high-resolution ordinal estimations\nusing a second network to generate a shading estimate with both global\ncoherency and local details. We encourage the model to learn an accurate\ndecomposition by computing losses on the estimated shading as well as the\nalbedo implied by the intrinsic model. We develop a straightforward method for\ngenerating dense pseudo ground truth using our model's predictions and\nmulti-illumination data, enabling generalization to in-the-wild imagery. We\npresent an exhaustive qualitative and quantitative analysis of our predicted\nintrinsic components against state-of-the-art methods. Finally, we demonstrate\nthe real-world applicability of our estimations by performing otherwise\ndifficult editing tasks such as recoloring and relighting.","publication_date":1700593081,"paper_link":"http://arxiv.org/pdf/2311.12792v1","categories":["Quantitative Biology"],"abstract":"Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. We develop a straightforward method for generating dense pseudo ground truth using our model's predictions and multi-illumination data, enabling generalization to in-the-wild imagery. We present an exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting."}
{"title":"Quantifying Impairment and Disease Severity Using AI Models Trained on Healthy Subjects","authors":["Boyang Yu","Aakash Kaku","Kangning Liu","Avinash Parnandi","Emily Fokas","Anita Venkatesan","Natasha Pandit","Rajesh Ranganath","Heidi Schambra","Carlos Fernandez-Granda"],"raw_abstract":"Automatic assessment of impairment and disease severity is a key challenge in\ndata-driven medicine. We propose a novel framework to address this challenge,\nwhich leverages AI models trained exclusively on healthy individuals. The\nCOnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the\ndecrease in confidence of these models when presented with impaired or diseased\npatients to quantify their deviation from the healthy population. We applied\nthe COBRA score to address a key limitation of current clinical evaluation of\nupper-body impairment in stroke patients. The gold-standard Fugl-Meyer\nAssessment (FMA) requires in-person administration by a trained assessor for\n30-45 minutes, which restricts monitoring frequency and precludes physicians\nfrom adapting rehabilitation protocols to the progress of each patient. The\nCOBRA score, computed automatically in under one minute, is shown to be\nstrongly correlated with the FMA on an independent test cohort for two\ndifferent data modalities: wearable sensors ($\\rho = 0.845$, 95% CI\n[0.743,0.908]) and video ($\\rho = 0.746$, 95% C.I [0.594, 0.847]). To\ndemonstrate the generalizability of the approach to other conditions, the COBRA\nscore was also applied to quantify severity of knee osteoarthritis from\nmagnetic-resonance imaging scans, again achieving significant correlation with\nan independent clinical assessment ($\\rho = 0.644$, 95% C.I [0.585,0.696]).","publication_date":1700592352,"paper_link":"http://arxiv.org/pdf/2311.12781v1","categories":["Quantitative Biology"],"abstract":"Automatic assessment of impairment and disease severity is a key challenge in data-driven medicine. We propose a novel framework to address this challenge, which leverages AI models trained exclusively on healthy individuals. The COnfidence-Based chaRacterization of Anomalies (COBRA) score exploits the decrease in confidence of these models when presented with impaired or diseased patients to quantify their deviation from the healthy population. We applied the COBRA score to address a key limitation of current clinical evaluation of upper-body impairment in stroke patients. The gold-standard Fugl-Meyer Assessment (FMA) requires in-person administration by a trained assessor for 30-45 minutes, which restricts monitoring frequency and precludes physicians from adapting rehabilitation protocols to the progress of each patient. The COBRA score, computed automatically in under one minute, is shown to be strongly correlated with the FMA on an independent test cohort for two different data modalities: wearable sensors (__FORMULA__, 95% CI [0.743,0.908]) and video (__FORMULA__, 95% C.I [0.594, 0.847]). To demonstrate the generalizability of the approach to other conditions, the COBRA score was also applied to quantify severity of knee osteoarthritis from magnetic-resonance imaging scans, again achieving significant correlation with an independent clinical assessment (__FORMULA__, 95% C.I [0.585,0.696])."}
{"title":"Towards a more inductive world for drug repurposing approaches","authors":["Jesus de la Fuente","Guillermo Serrano","Ux\u00eda Veleiro","Mikel Casals","Laura Vera","Marija Pizurica","Antonio Pineda-Lucena","Idoia Ochoa","Silve Vicent","Olivier Gevaert","Mikel Hernaez"],"raw_abstract":"Drug-target interaction (DTI) prediction is a challenging, albeit essential\ntask in drug repurposing. Learning on graph models have drawn special attention\nas they can significantly reduce drug repurposing costs and time commitment.\nHowever, many current approaches require high-demanding additional information\nbesides DTIs that complicates their evaluation process and usability.\nAdditionally, structural differences in the learning architecture of current\nmodels hinder their fair benchmarking. In this work, we first perform an\nin-depth evaluation of current DTI datasets and prediction models through a\nrobust benchmarking process, and show that DTI prediction methods based on\ntransductive models lack generalization and lead to inflated performance when\nevaluated as previously done in the literature, hence not being suited for drug\nrepurposing approaches. We then propose a novel biologically-driven strategy\nfor negative edge subsampling and show through in vitro validation that newly\ndiscovered interactions are indeed true. We envision this work as the\nunderpinning for future fair benchmarking and robust model design. All\ngenerated resources and tools are publicly available as a python package.","publication_date":1700580524,"paper_link":"http://arxiv.org/pdf/2311.12670v1","categories":["Quantitative Biology"],"abstract":"Drug-target interaction (DTI) prediction is a challenging, albeit essential task in drug repurposing. Learning on graph models have drawn special attention as they can significantly reduce drug repurposing costs and time commitment. However, many current approaches require high-demanding additional information besides DTIs that complicates their evaluation process and usability. Additionally, structural differences in the learning architecture of current models hinder their fair benchmarking. In this work, we first perform an in-depth evaluation of current DTI datasets and prediction models through a robust benchmarking process, and show that DTI prediction methods based on transductive models lack generalization and lead to inflated performance when evaluated as previously done in the literature, hence not being suited for drug repurposing approaches. We then propose a novel biologically-driven strategy for negative edge subsampling and show through in vitro validation that newly discovered interactions are indeed true. We envision this work as the underpinning for future fair benchmarking and robust model design. All generated resources and tools are publicly available as a python package."}
{"title":"Carbohydrate NMR chemical shift predictions using E(3) equivariant graph neural networks","authors":["Maria B\u00e5nkestad","Keven M. Dorst","G\u00f6ran Widmalm","Jerk R\u00f6nnols"],"raw_abstract":"Carbohydrates, vital components of biological systems, are well-known for\ntheir structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays\na crucial role in understanding their intricate molecular arrangements and is\nessential in assessing and verifying the molecular structure of organic\nmolecules. An important part of this process is to predict the NMR chemical\nshift from the molecular structure. This work introduces a novel approach that\nleverages E(3) equivariant graph neural networks to predict carbohydrate NMR\nspectra. Notably, our model achieves a substantial reduction in mean absolute\nerror, up to threefold, compared to traditional models that rely solely on\ntwo-dimensional molecular structure. Even with limited data, the model excels,\nhighlighting its robustness and generalization capabilities. The implications\nare far-reaching and go beyond an advanced understanding of carbohydrate\nstructures and spectral interpretation. For example, it could accelerate\nresearch in pharmaceutical applications, biochemistry, and structural biology,\noffering a faster and more reliable analysis of molecular structures.\nFurthermore, our approach is a key step towards a new data-driven era in\nspectroscopy, potentially influencing spectroscopic techniques beyond NMR.","publication_date":1700578874,"paper_link":"http://arxiv.org/pdf/2311.12657v1","categories":["Physics"],"abstract":"Carbohydrates, vital components of biological systems, are well-known for their structural diversity. Nuclear Magnetic Resonance (NMR) spectroscopy plays a crucial role in understanding their intricate molecular arrangements and is essential in assessing and verifying the molecular structure of organic molecules. An important part of this process is to predict the NMR chemical shift from the molecular structure. This work introduces a novel approach that leverages E(3) equivariant graph neural networks to predict carbohydrate NMR spectra. Notably, our model achieves a substantial reduction in mean absolute error, up to threefold, compared to traditional models that rely solely on two-dimensional molecular structure. Even with limited data, the model excels, highlighting its robustness and generalization capabilities. The implications are far-reaching and go beyond an advanced understanding of carbohydrate structures and spectral interpretation. For example, it could accelerate research in pharmaceutical applications, biochemistry, and structural biology, offering a faster and more reliable analysis of molecular structures. Furthermore, our approach is a key step towards a new data-driven era in spectroscopy, potentially influencing spectroscopic techniques beyond NMR."}
{"title":"Poisson approximation of fixed-degree nodes in weighted random connection models","authors":["Christian Hirsch","Benedikt Jahnel","Sanjoy Kumar Jhawar","Peter Juhasz"],"raw_abstract":"We present a process-level Poisson-approximation result for the degree-k\nvertices in a high-density weighted random connection model with\npreferential-attachment kernel in the unit volume. Our main focus lies on the\nimpact of the left tails of the weight distribution for which we establish\ngeneral criteria based on their small-weight quantiles. To illustrate that our\nconditions are broadly applicable, we verify them for weight distributions with\npolynomial and stretched exponential left tails. The proofs rest on truncation\narguments and a recently established quantitative Poisson approximation result\nfor functionals of Poisson point processes.","publication_date":1700577880,"paper_link":"http://arxiv.org/pdf/2311.12643v1","categories":["Mathematics"],"abstract":"We present a process-level Poisson-approximation result for the degree-k vertices in a high-density weighted random connection model with preferential-attachment kernel in the unit volume. Our main focus lies on the impact of the left tails of the weight distribution for which we establish general criteria based on their small-weight quantiles. To illustrate that our conditions are broadly applicable, we verify them for weight distributions with polynomial and stretched exponential left tails. The proofs rest on truncation arguments and a recently established quantitative Poisson approximation result for functionals of Poisson point processes."}
{"title":"Trustworthy AI: Deciding What to Decide","authors":["Caesar Wu","Yuan-Fang Li","Jian Li","Jingjing Xu","Bouvry Pascal"],"raw_abstract":"When engaging in strategic decision-making, we are frequently confronted with\noverwhelming information and data. The situation can be further complicated\nwhen certain pieces of evidence contradict each other or become paradoxical.\nThe primary challenge is how to determine which information can be trusted when\nwe adopt Artificial Intelligence (AI) systems for decision-making. This issue\nis known as deciding what to decide or Trustworthy AI. However, the AI system\nitself is often considered an opaque black box. We propose a new approach to\naddress this issue by introducing a novel framework of Trustworthy AI (TAI)\nencompassing three crucial components of AI: representation space, loss\nfunction, and optimizer. Each component is loosely coupled with four TAI\nproperties. Altogether, the framework consists of twelve TAI properties. We aim\nto use this framework to conduct the TAI experiments by quantitive and\nqualitative research methods to satisfy TAI properties for the decision-making\ncontext. The framework allows us to formulate an optimal prediction model\ntrained by the given dataset for applying the strategic investment decision of\ncredit default swaps (CDS) in the technology sector. Finally, we provide our\nview of the future direction of TAI research","publication_date":1700574238,"paper_link":"http://arxiv.org/pdf/2311.12604v1","categories":["Quantitative Biology"],"abstract":"When engaging in strategic decision-making, we are frequently confronted with overwhelming information and data. The situation can be further complicated when certain pieces of evidence contradict each other or become paradoxical. The primary challenge is how to determine which information can be trusted when we adopt Artificial Intelligence (AI) systems for decision-making. This issue is known as deciding what to decide or Trustworthy AI. However, the AI system itself is often considered an opaque black box. We propose a new approach to address this issue by introducing a novel framework of Trustworthy AI (TAI) encompassing three crucial components of AI: representation space, loss function, and optimizer. Each component is loosely coupled with four TAI properties. Altogether, the framework consists of twelve TAI properties. We aim to use this framework to conduct the TAI experiments by quantitive and qualitative research methods to satisfy TAI properties for the decision-making context. The framework allows us to formulate an optimal prediction model trained by the given dataset for applying the strategic investment decision of credit default swaps (CDS) in the technology sector. Finally, we provide our view of the future direction of TAI research"}
{"title":"Deep learning-based detection of morphological features associated with hypoxia in H&E breast cancer whole slide images","authors":["Petru Manescu","Joseph Geradts","Delmiro Fernandez-Reyes"],"raw_abstract":"Hypoxia occurs when tumour cells outgrow their blood supply, leading to\nregions of low oxygen levels within the tumour. Calculating hypoxia levels can\nbe an important step in understanding the biology of tumours, their clinical\nprogression and response to treatment. This study demonstrates a novel\napplication of deep learning to evaluate hypoxia in the context of breast\ncancer histomorphology. More precisely, we show that Weakly Supervised Deep\nLearning (WSDL) models can accurately detect hypoxia associated features in\nroutine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and\nevaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue\nfrom breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on\na left-out test set. We also showed significant differences between features of\nhypoxic and normoxic tissue regions as distinguished by the WSDL models. Such\nDL hypoxia H&E WSI detection models could potentially be extended to other\ntumour types and easily integrated into the pathology workflow without\nrequiring additional costly assays.","publication_date":1700574160,"paper_link":"http://arxiv.org/pdf/2311.12601v1","categories":["Quantitative Biology"],"abstract":"Hypoxia occurs when tumour cells outgrow their blood supply, leading to regions of low oxygen levels within the tumour. Calculating hypoxia levels can be an important step in understanding the biology of tumours, their clinical progression and response to treatment. This study demonstrates a novel application of deep learning to evaluate hypoxia in the context of breast cancer histomorphology. More precisely, we show that Weakly Supervised Deep Learning (WSDL) models can accurately detect hypoxia associated features in routine Hematoxylin and Eosin (H&E) whole slide images (WSI). We trained and evaluated a deep Multiple Instance Learning model on tiles from WSI H&E tissue from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on a left-out test set. We also showed significant differences between features of hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such DL hypoxia H&E WSI detection models could potentially be extended to other tumour types and easily integrated into the pathology workflow without requiring additional costly assays."}
{"title":"Multiscale interpolative construction of quantized tensor trains","authors":["Michael Lindsey"],"raw_abstract":"Quantized tensor trains (QTTs) have recently emerged as a framework for the\nnumerical discretization of continuous functions, with the potential for\nwidespread applications in numerical analysis. However, the theory of QTT\napproximation is not fully understood. In this work, we advance this theory\nfrom the point of view of multiscale polynomial interpolation. This perspective\nclarifies why QTT ranks decay with increasing depth, quantitatively controls\nQTT rank in terms of smoothness of the target function, and explains why\ncertain functions with sharp features and poor quantitative smoothness can\nstill be well approximated by QTTs. The perspective also motivates new\npractical and efficient algorithms for the construction of QTTs from function\nevaluations on multiresolution grids.","publication_date":1700568429,"paper_link":"http://arxiv.org/pdf/2311.12554v1","categories":["Mathematics"],"abstract":"Quantized tensor trains (QTTs) have recently emerged as a framework for the numerical discretization of continuous functions, with the potential for widespread applications in numerical analysis. However, the theory of QTT approximation is not fully understood. In this work, we advance this theory from the point of view of multiscale polynomial interpolation. This perspective clarifies why QTT ranks decay with increasing depth, quantitatively controls QTT rank in terms of smoothness of the target function, and explains why certain functions with sharp features and poor quantitative smoothness can still be well approximated by QTTs. The perspective also motivates new practical and efficient algorithms for the construction of QTTs from function evaluations on multiresolution grids."}
{"title":"MetaStore: High-Performance Metagenomic Analysis via In-Storage Computing","authors":["Nika Mansouri Ghiasi","Mohammad Sadrosadati","Harun Mustafa","Arvid Gollwitzer","Can Firtina","Julien Eudine","Haiyu Ma","Jo\u00ebl Lindegger","Meryem Banu Cavlak","Mohammed Alser","Jisung Park","Onur Mutlu"],"raw_abstract":"Metagenomics has led to significant advancements in many fields. Metagenomic\nanalysis commonly involves the key tasks of determining the species present in\na sample and their relative abundances. These tasks require searching large\nmetagenomic databases containing information on different species' genomes.\nMetagenomic analysis suffers from significant data movement overhead due to\nmoving large amounts of low-reuse data from the storage system to the rest of\nthe system. In-storage processing can be a fundamental solution for reducing\ndata movement overhead. However, designing an in-storage processing system for\nmetagenomics is challenging because none of the existing approaches can be\ndirectly implemented in storage effectively due to the hardware limitations of\nmodern SSDs. We propose MetaStore, the first in-storage processing system\ndesigned to significantly reduce the data movement overhead of end-to-end\nmetagenomic analysis. MetaStore is enabled by our lightweight and cooperative\ndesign that effectively leverages and orchestrates processing inside and\noutside the storage system. Through our detailed analysis of the end-to-end\nmetagenomic analysis pipeline and careful hardware/software co-design, we\naddress in-storage processing challenges for metagenomics via specialized and\nefficient 1) task partitioning, 2) data/computation flow coordination, 3)\nstorage technology-aware algorithmic optimizations, 4) light-weight in-storage\naccelerators, and 5) data mapping. Our evaluation shows that MetaStore\noutperforms the state-of-the-art performance- and accuracy-optimized software\nmetagenomic tools by 2.7-37.2$\\times$ and 6.9-100.2$\\times$, respectively,\nwhile matching the accuracy of the accuracy-optimized tool. MetaStore achieves\n1.5-5.1$\\times$ speedup compared to the state-of-the-art metagenomic\nhardware-accelerated tool, while achieving significantly higher accuracy.","publication_date":1700565302,"paper_link":"http://arxiv.org/pdf/2311.12527v1","categories":["Quantitative Biology"],"abstract":"Metagenomics has led to significant advancements in many fields. Metagenomic analysis commonly involves the key tasks of determining the species present in a sample and their relative abundances. These tasks require searching large metagenomic databases containing information on different species' genomes. Metagenomic analysis suffers from significant data movement overhead due to moving large amounts of low-reuse data from the storage system to the rest of the system. In-storage processing can be a fundamental solution for reducing data movement overhead. However, designing an in-storage processing system for metagenomics is challenging because none of the existing approaches can be directly implemented in storage effectively due to the hardware limitations of modern SSDs. We propose MetaStore, the first in-storage processing system designed to significantly reduce the data movement overhead of end-to-end metagenomic analysis. MetaStore is enabled by our lightweight and cooperative design that effectively leverages and orchestrates processing inside and outside the storage system. Through our detailed analysis of the end-to-end metagenomic analysis pipeline and careful hardware/software co-design, we address in-storage processing challenges for metagenomics via specialized and efficient 1) task partitioning, 2) data/computation flow coordination, 3) storage technology-aware algorithmic optimizations, 4) light-weight in-storage accelerators, and 5) data mapping. Our evaluation shows that MetaStore outperforms the state-of-the-art performance- and accuracy-optimized software metagenomic tools by 2.7-37.2__FORMULA__ and 6.9-100.2__FORMULA__, respectively, while matching the accuracy of the accuracy-optimized tool. MetaStore achieves 1.5-5.1__FORMULA__ speedup compared to the state-of-the-art metagenomic hardware-accelerated tool, while achieving significantly higher accuracy."}
{"title":"From Microbes to Methane: AI-Based Predictive Modeling of Feed Additive Efficacy in Dairy Cows","authors":["Yaniv Altshuler","Tzruya Calvao Chebach","Shalom Cohen"],"raw_abstract":"In an era of increasing pressure to achieve sustainable agriculture, the\noptimization of livestock feed for enhancing yield and minimizing environmental\nimpact is a paramount objective. This study presents a pioneering approach\ntowards this goal, using rumen microbiome data to predict the efficacy of feed\nadditives in dairy cattle.\n  We collected an extensive dataset that includes methane emissions from 2,190\nHolstein cows distributed across 34 distinct sites. The cows were divided into\ncontrol and experimental groups in a double-blind, unbiased manner, accounting\nfor variables such as age, days in lactation, and average milk yield. The\nexperimental groups were administered one of four leading commercial feed\nadditives: Agolin, Kexxtone, Allimax, and Relyon. Methane emissions were\nmeasured individually both before the administration of additives and over a\nsubsequent 12-week period. To develop our predictive model for additive\nefficacy, rumen microbiome samples were collected from 510 cows from the same\nherds prior to the study's onset. These samples underwent deep metagenomic\nshotgun sequencing, yielding an average of 15.7 million reads per sample.\nUtilizing innovative artificial intelligence techniques we successfully\nestimated the efficacy of these feed additives across different farms. The\nmodel's robustness was further confirmed through validation with independent\ncohorts, affirming its generalizability and reliability.\n  Our results underscore the transformative capability of using targeted feed\nadditive strategies to both optimize dairy yield and milk composition, and to\nsignificantly reduce methane emissions. Specifically, our predictive model\ndemonstrates a scenario where its application could guide the assignment of\nadditives to farms where they are most effective. In doing so, we could achieve\nan average potential reduction of over 27\\% in overall emissions.","publication_date":1700564248,"paper_link":"http://arxiv.org/pdf/2311.12901v1","categories":["Quantitative Biology"],"abstract":"In an era of increasing pressure to achieve sustainable agriculture, the optimization of livestock feed for enhancing yield and minimizing environmental impact is a paramount objective. This study presents a pioneering approach towards this goal, using rumen microbiome data to predict the efficacy of feed additives in dairy cattle.   We collected an extensive dataset that includes methane emissions from 2,190 Holstein cows distributed across 34 distinct sites. The cows were divided into control and experimental groups in a double-blind, unbiased manner, accounting for variables such as age, days in lactation, and average milk yield. The experimental groups were administered one of four leading commercial feed additives: Agolin, Kexxtone, Allimax, and Relyon. Methane emissions were measured individually both before the administration of additives and over a subsequent 12-week period. To develop our predictive model for additive efficacy, rumen microbiome samples were collected from 510 cows from the same herds prior to the study's onset. These samples underwent deep metagenomic shotgun sequencing, yielding an average of 15.7 million reads per sample. Utilizing innovative artificial intelligence techniques we successfully estimated the efficacy of these feed additives across different farms. The model's robustness was further confirmed through validation with independent cohorts, affirming its generalizability and reliability.   Our results underscore the transformative capability of using targeted feed additive strategies to both optimize dairy yield and milk composition, and to significantly reduce methane emissions. Specifically, our predictive model demonstrates a scenario where its application could guide the assignment of additives to farms where they are most effective. In doing so, we could achieve an average potential reduction of over 27\\% in overall emissions."}
{"title":"Testing the Accuracy of Surface Code Decoders","authors":["Arshpreet Singh Maan","Alexandru Paler"],"raw_abstract":"Large-scale, fault-tolerant quantum computations will be enabled by quantum\nerror-correcting codes (QECC). This work presents the first systematic\ntechnique to test the accuracy and effectiveness of different QECC decoding\nschemes by comparing a look-up table decoder to solutions generated using\nalgorithmic decoders. Specifically, we examine the results of\nminimum-weight-perfect-matching and belief-propagation decoders against\nexhaustive look-up tables for surface codes up to distance seven and categorise\nwhere errors are accurately corrected in both decoding schemes. While our\nresults are preliminary, we show that significant quantitative results can be\ngenerated, comparing how actual error channels are successfully or\nunsuccessfully decoded. We show that different decoding schemes perform very\ndifferently under the same QECC scheme and error model, and detail how decoders\ncan be tested and classified with respect to errors that are successfully\ndecodable. This work paves the way to the data driven tuning of decoder\nensembles and will enable tailored design of hybrid decoding schemes that allow\nfor real-time decoding, while maintaining the high theoretical thresholds\nallowed by specific quantum error correction codes.","publication_date":1700562128,"paper_link":"http://arxiv.org/pdf/2311.12503v1","categories":["Physics"],"abstract":"Large-scale, fault-tolerant quantum computations will be enabled by quantum error-correcting codes (QECC). This work presents the first systematic technique to test the accuracy and effectiveness of different QECC decoding schemes by comparing a look-up table decoder to solutions generated using algorithmic decoders. Specifically, we examine the results of minimum-weight-perfect-matching and belief-propagation decoders against exhaustive look-up tables for surface codes up to distance seven and categorise where errors are accurately corrected in both decoding schemes. While our results are preliminary, we show that significant quantitative results can be generated, comparing how actual error channels are successfully or unsuccessfully decoded. We show that different decoding schemes perform very differently under the same QECC scheme and error model, and detail how decoders can be tested and classified with respect to errors that are successfully decodable. This work paves the way to the data driven tuning of decoder ensembles and will enable tailored design of hybrid decoding schemes that allow for real-time decoding, while maintaining the high theoretical thresholds allowed by specific quantum error correction codes."}
{"title":"How Far Have We Gone in Vulnerability Detection Using Large Language Models","authors":["Zeyu Gao","Hao Wang","Yuchen Zhou","Wenyu Zhu","Chao Zhang"],"raw_abstract":"As software becomes increasingly complex and prone to vulnerabilities,\nautomated vulnerability detection is critically important, yet challenging.\nGiven the significant successes of Large Language Models (LLMs) in various\ntasks, there is growing anticipation of their efficacy in vulnerability\ndetection. However, a quantitative understanding of their potential in\nvulnerability detection is still missing. To bridge this gap, we introduce a\ncomprehensive vulnerability benchmark VulBench. This benchmark aggregates\nhigh-quality data from a wide range of CTF (Capture-the-Flag) challenges and\nreal-world applications, with annotations for each vulnerable function\ndetailing the vulnerability type and its root cause. Through our experiments\nencompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models\nand static analyzers, we find that several LLMs outperform traditional deep\nlearning approaches in vulnerability detection, revealing an untapped potential\nin LLMs. This work contributes to the understanding and utilization of LLMs for\nenhanced software security.","publication_date":1700554839,"paper_link":"http://arxiv.org/pdf/2311.12420v1","categories":["Quantitative Biology"],"abstract":"As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of Large Language Models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security."}
{"title":"Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval","authors":["Xiu-Shen Wei","Yang Shen","Xuhao Sun","Peng Wang","Yuxin Peng"],"raw_abstract":"Our work focuses on tackling large-scale fine-grained image retrieval as\nranking the images depicting the concept of interests (i.e., the same\nsub-category labels) highest based on the fine-grained details in the query. It\nis desirable to alleviate the challenges of both fine-grained nature of small\ninter-class variations with large intra-class variations and explosive growth\nof fine-grained data for such a practical task. In this paper, we propose\nattribute-aware hashing networks with self-consistency for generating\nattribute-aware hash codes to not only make the retrieval process efficient,\nbut also establish explicit correspondences between hash codes and visual\nattributes. Specifically, based on the captured visual representations by\nattention, we develop an encoder-decoder structure network of a reconstruction\ntask to unsupervisedly distill high-level attribute-specific vectors from the\nappearance-specific visual representations without attribute annotations. Our\nmodels are also equipped with a feature decorrelation constraint upon these\nattribute vectors to strengthen their representative abilities. Then, driven by\npreserving original entities' similarity, the required hash codes can be\ngenerated from these attribute-specific vectors and thus become\nattribute-aware. Furthermore, to combat simplicity bias in deep hashing, we\nconsider the model design from the perspective of the self-consistency\nprinciple and propose to further enhance models' self-consistency by equipping\nan additional image reconstruction path. Comprehensive quantitative experiments\nunder diverse empirical settings on six fine-grained retrieval datasets and two\ngeneric retrieval datasets show the superiority of our models over competing\nmethods.","publication_date":1700554838,"paper_link":"http://arxiv.org/pdf/2311.12894v1","categories":["Quantitative Biology"],"abstract":"Our work focuses on tackling large-scale fine-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the fine-grained details in the query. It is desirable to alleviate the challenges of both fine-grained nature of small inter-class variations with large intra-class variations and explosive growth of fine-grained data for such a practical task. In this paper, we propose attribute-aware hashing networks with self-consistency for generating attribute-aware hash codes to not only make the retrieval process efficient, but also establish explicit correspondences between hash codes and visual attributes. Specifically, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsupervisedly distill high-level attribute-specific vectors from the appearance-specific visual representations without attribute annotations. Our models are also equipped with a feature decorrelation constraint upon these attribute vectors to strengthen their representative abilities. Then, driven by preserving original entities' similarity, the required hash codes can be generated from these attribute-specific vectors and thus become attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we consider the model design from the perspective of the self-consistency principle and propose to further enhance models' self-consistency by equipping an additional image reconstruction path. Comprehensive quantitative experiments under diverse empirical settings on six fine-grained retrieval datasets and two generic retrieval datasets show the superiority of our models over competing methods."}
{"title":"nach0: Multimodal Natural and Chemical Languages Foundation Model","authors":["Micha Livne","Zulfat Miftahutdinov","Elena Tutubalina","Maksim Kuznetsov","Daniil Polykovskiy","Annika Brundyn","Aastha Jhunjhunwala","Anthony Costa","Alex Aliper","Alex Zhavoronkov"],"raw_abstract":"Large Language Models (LLMs) have substantially driven scientific progress in\nvarious domains, and many papers have demonstrated their ability to tackle\ncomplex problems with creative solutions. Our paper introduces a new foundation\nmodel, nach0, capable of solving various chemical and biological tasks:\nbiomedical question answering, named entity recognition, molecular generation,\nmolecular synthesis, attributes prediction, and others. nach0 is a multi-domain\nand multi-task encoder-decoder LLM pre-trained on unlabeled text from\nscientific literature, patents, and molecule strings to incorporate a range of\nchemical and linguistic knowledge. We employed instruction tuning, where\nspecific task-related instructions are utilized to fine-tune nach0 for the\nfinal set of tasks. To train nach0 effectively, we leverage the NeMo framework,\nenabling efficient parallel optimization of both base and large model versions.\nExtensive experiments demonstrate that our model outperforms state-of-the-art\nbaselines on single-domain and cross-domain tasks. Furthermore, it can generate\nhigh-quality outputs in molecular and textual formats, showcasing its\neffectiveness in multi-domain setups.","publication_date":1700553390,"paper_link":"http://arxiv.org/pdf/2311.12410v1","categories":["Quantitative Biology"],"abstract":"Large Language Models (LLMs) have substantially driven scientific progress in various domains, and many papers have demonstrated their ability to tackle complex problems with creative solutions. Our paper introduces a new foundation model, nach0, capable of solving various chemical and biological tasks: biomedical question answering, named entity recognition, molecular generation, molecular synthesis, attributes prediction, and others. nach0 is a multi-domain and multi-task encoder-decoder LLM pre-trained on unlabeled text from scientific literature, patents, and molecule strings to incorporate a range of chemical and linguistic knowledge. We employed instruction tuning, where specific task-related instructions are utilized to fine-tune nach0 for the final set of tasks. To train nach0 effectively, we leverage the NeMo framework, enabling efficient parallel optimization of both base and large model versions. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on single-domain and cross-domain tasks. Furthermore, it can generate high-quality outputs in molecular and textual formats, showcasing its effectiveness in multi-domain setups."}
{"title":"Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations","authors":["Yushi Du","Ruihai Wu","Yan Shen","Hao Dong"],"raw_abstract":"Articulated objects (e.g., doors and drawers) exist everywhere in our life.\nDifferent from rigid objects, articulated objects have higher degrees of\nfreedom and are rich in geometries, semantics, and part functions. Modeling\ndifferent kinds of parts and articulations with nerual networks plays an\nessential role in articulated object understanding and manipulation, and will\nfurther benefit 3D vision and robotics communities. To model articulated\nobjects, most previous works directly encode articulated objects into feature\nrepresentations, without specific designs for parts, articulations and part\nmotions. In this paper, we introduce a novel framework that explicitly\ndisentangles the part motion of articulated objects by predicting the\ntransformation matrix of points on the part surface, using spatially continuous\nneural implicit representations to model the part motion smoothly in the space.\nMore importantly, while many methods could only model a certain kind of joint\nmotion (such as the revolution in the clockwise order), our proposed framework\nis generic to different kinds of joint motions in that transformation matrix\ncan model diverse kinds of joint motions in the space. Quantitative and\nqualitative results of experiments over diverse categories of articulated\nobjects demonstrate the effectiveness of our proposed framework.","publication_date":1700553280,"paper_link":"http://arxiv.org/pdf/2311.12407v1","categories":["Quantitative Biology"],"abstract":"Articulated objects (e.g., doors and drawers) exist everywhere in our life. Different from rigid objects, articulated objects have higher degrees of freedom and are rich in geometries, semantics, and part functions. Modeling different kinds of parts and articulations with nerual networks plays an essential role in articulated object understanding and manipulation, and will further benefit 3D vision and robotics communities. To model articulated objects, most previous works directly encode articulated objects into feature representations, without specific designs for parts, articulations and part motions. In this paper, we introduce a novel framework that explicitly disentangles the part motion of articulated objects by predicting the transformation matrix of points on the part surface, using spatially continuous neural implicit representations to model the part motion smoothly in the space. More importantly, while many methods could only model a certain kind of joint motion (such as the revolution in the clockwise order), our proposed framework is generic to different kinds of joint motions in that transformation matrix can model diverse kinds of joint motions in the space. Quantitative and qualitative results of experiments over diverse categories of articulated objects demonstrate the effectiveness of our proposed framework."}
{"title":"Quantitative Profilometric Measurement of Magnetostriction in Thin-Films","authors":["Hamish Greenall","Benjamin J. Carey","Douglas Bulla","James S. Bennett","Glen I. Harris","Fernando Gotardo","Scott Foster","Warwick P. Bowen"],"raw_abstract":"A DC non-contact method for measuring the magnetostrictive strain in\nthin-films is demonstrated, achieving a state-of-the-art sensitivity of 0.1\nppm. In this method, an optical profilometer is used to measure the curvature\ninduced in a magnetostrictively coated coverslip under a DC field through\nphase-sensitive interferometry. From this the magnetostrictive stress and\nstrain are calculated using Stoney's formula. This addresses limitations of\nconventional techniques that measure magnetostriction based on the deflection\nof a cantilever under an AC field, which require complex dedicated set-ups and\nare sensitive to vibrational noise. Further, it reveals information about the\nanisotropy of the film and allows for the possibility of measuring multiple\nsamples simultaneously. The theoretical sensitivity limits are derived,\npredicting a shot-noise-limit of 0.01 ppm. The method is implemented to measure\nthe magnetostrictive hysteresis and piezomagnetic coupling of thin-film\ngalfenol. Degradation in film performance is observed above a thickness of 206\nnm, alongside a change in coercivity. This prompts investigation into the\ngrowth and optimization of galfenol films for use in devices.","publication_date":1700548887,"paper_link":"http://arxiv.org/pdf/2311.12378v1","categories":["Physics"],"abstract":"A DC non-contact method for measuring the magnetostrictive strain in thin-films is demonstrated, achieving a state-of-the-art sensitivity of 0.1 ppm. In this method, an optical profilometer is used to measure the curvature induced in a magnetostrictively coated coverslip under a DC field through phase-sensitive interferometry. From this the magnetostrictive stress and strain are calculated using Stoney's formula. This addresses limitations of conventional techniques that measure magnetostriction based on the deflection of a cantilever under an AC field, which require complex dedicated set-ups and are sensitive to vibrational noise. Further, it reveals information about the anisotropy of the film and allows for the possibility of measuring multiple samples simultaneously. The theoretical sensitivity limits are derived, predicting a shot-noise-limit of 0.01 ppm. The method is implemented to measure the magnetostrictive hysteresis and piezomagnetic coupling of thin-film galfenol. Degradation in film performance is observed above a thickness of 206 nm, alongside a change in coercivity. This prompts investigation into the growth and optimization of galfenol films for use in devices."}
{"title":"LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis","authors":["Peiang Zhao","Han Li","Ruiyang Jin","S. Kevin Zhou"],"raw_abstract":"Recent text-to-image diffusion models have reached an unprecedented level in\ngenerating high-quality images. However, their exclusive reliance on textual\nprompts often falls short in accurately conveying fine-grained spatial\ncompositions. In this paper, we propose LoCo, a training-free approach for\nlayout-to-image synthesis that excels in producing high-quality images aligned\nwith both textual prompts and spatial layouts. Our method introduces a\nLocalized Attention Constraint to refine cross-attention for individual\nobjects, ensuring their precise placement in designated regions. We further\npropose a Padding Token Constraint to leverage the semantic information\nembedded in previously neglected padding tokens, thereby preventing the\nundesired fusion of synthesized objects. LoCo seamlessly integrates into\nexisting text-to-image and layout-to-image models, significantly amplifying\ntheir performance and effectively addressing semantic failures observed in\nprior methods. Through extensive experiments, we showcase the superiority of\nour approach, surpassing existing state-of-the-art training-free\nlayout-to-image methods both qualitatively and quantitatively across multiple\nbenchmarks.","publication_date":1700540892,"paper_link":"http://arxiv.org/pdf/2311.12342v1","categories":["Quantitative Biology"],"abstract":"Recent text-to-image diffusion models have reached an unprecedented level in generating high-quality images. However, their exclusive reliance on textual prompts often falls short in accurately conveying fine-grained spatial compositions. In this paper, we propose LoCo, a training-free approach for layout-to-image synthesis that excels in producing high-quality images aligned with both textual prompts and spatial layouts. Our method introduces a Localized Attention Constraint to refine cross-attention for individual objects, ensuring their precise placement in designated regions. We further propose a Padding Token Constraint to leverage the semantic information embedded in previously neglected padding tokens, thereby preventing the undesired fusion of synthesized objects. LoCo seamlessly integrates into existing text-to-image and layout-to-image models, significantly amplifying their performance and effectively addressing semantic failures observed in prior methods. Through extensive experiments, we showcase the superiority of our approach, surpassing existing state-of-the-art training-free layout-to-image methods both qualitatively and quantitatively across multiple benchmarks."}
{"title":"Identifying DNA Sequence Motifs Using Deep Learning","authors":["Asmita Poddar","Vladimir Uzun","Elizabeth Tunbridge","Wilfried Haerty","Alejo Nevado-Holgado"],"raw_abstract":"Splice sites play a crucial role in gene expression, and accurate prediction\nof these sites in DNA sequences is essential for diagnosing and treating\ngenetic disorders. We address the challenge of splice site prediction by\nintroducing DeepDeCode, an attention-based deep learning sequence model to\ncapture the long-term dependencies in the nucleotides in DNA sequences. We\nfurther propose using visualization techniques for accurate identification of\nsequence motifs, which enhance the interpretability and trustworthiness of\nDeepDeCode. We compare DeepDeCode to other state-of-the-art methods for splice\nsite prediction and demonstrate its accuracy, explainability and efficiency.\nGiven the results of our methodology, we expect that it can used for healthcare\napplications to reason about genomic processes and be extended to discover new\nsplice sites and genomic regulatory elements.","publication_date":1700522068,"paper_link":"http://arxiv.org/pdf/2311.12884v1","categories":["Quantitative Biology"],"abstract":"Splice sites play a crucial role in gene expression, and accurate prediction of these sites in DNA sequences is essential for diagnosing and treating genetic disorders. We address the challenge of splice site prediction by introducing DeepDeCode, an attention-based deep learning sequence model to capture the long-term dependencies in the nucleotides in DNA sequences. We further propose using visualization techniques for accurate identification of sequence motifs, which enhance the interpretability and trustworthiness of DeepDeCode. We compare DeepDeCode to other state-of-the-art methods for splice site prediction and demonstrate its accuracy, explainability and efficiency. Given the results of our methodology, we expect that it can used for healthcare applications to reason about genomic processes and be extended to discover new splice sites and genomic regulatory elements."}
{"title":"Uncertainty Estimation in Contrast-Enhanced MR Image Translation with Multi-Axis Fusion","authors":["Ivo M. Baltruschat","Parvaneh Janbakhshi","Melanie Dohmen","Matthias Lenga"],"raw_abstract":"In recent years, deep learning has been applied to a wide range of medical\nimaging and image processing tasks. In this work, we focus on the estimation of\nepistemic uncertainty for 3D medical image-to-image translation. We propose a\nnovel model uncertainty quantification method, Multi-Axis Fusion (MAF), which\nrelies on the integration of complementary information derived from multiple\nviews on volumetric image data. The proposed approach is applied to the task of\nsynthesizing contrast enhanced T1-weighted images based on native T1, T2 and\nT2-FLAIR scans. The quantitative findings indicate a strong correlation\n($\\rho_{\\text healthy} = 0.89$) between the mean absolute image synthetization\nerror and the mean uncertainty score for our MAF method. Hence, we consider MAF\nas a promising approach to solve the highly relevant task of detecting\nsynthetization failures at inference time.","publication_date":1700510988,"paper_link":"http://arxiv.org/pdf/2311.12153v1","categories":["Electrical Engineering and Systems Science"],"abstract":"In recent years, deep learning has been applied to a wide range of medical imaging and image processing tasks. In this work, we focus on the estimation of epistemic uncertainty for 3D medical image-to-image translation. We propose a novel model uncertainty quantification method, Multi-Axis Fusion (MAF), which relies on the integration of complementary information derived from multiple views on volumetric image data. The proposed approach is applied to the task of synthesizing contrast enhanced T1-weighted images based on native T1, T2 and T2-FLAIR scans. The quantitative findings indicate a strong correlation (__FORMULA__) between the mean absolute image synthetization error and the mean uncertainty score for our MAF method. Hence, we consider MAF as a promising approach to solve the highly relevant task of detecting synthetization failures at inference time."}
{"title":"Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models","authors":["Rohit Gandikota","Joanna Materzynska","Tingrui Zhou","Antonio Torralba","David Bau"],"raw_abstract":"We present a method to create interpretable concept sliders that enable\nprecise control over attributes in image generations from diffusion models. Our\napproach identifies a low-rank parameter direction corresponding to one concept\nwhile minimizing interference with other attributes. A slider is created using\na small set of prompts or sample images; thus slider directions can be created\nfor either textual or visual concepts. Concept Sliders are plug-and-play: they\ncan be composed efficiently and continuously modulated, enabling precise\ncontrol over image generation. In quantitative experiments comparing to\nprevious editing techniques, our sliders exhibit stronger targeted edits with\nlower interference. We showcase sliders for weather, age, styles, and\nexpressions, as well as slider compositions. We show how sliders can transfer\nlatents from StyleGAN for intuitive editing of visual concepts for which\ntextual description is difficult. We also find that our method can help address\npersistent quality issues in Stable Diffusion XL including repair of object\ndeformations and fixing distorted hands. Our code, data, and trained sliders\nare available at https://sliders.baulab.info/","publication_date":1700506741,"paper_link":"http://arxiv.org/pdf/2311.12092v1","categories":["Quantitative Biology"],"abstract":"We present a method to create interpretable concept sliders that enable precise control over attributes in image generations from diffusion models. Our approach identifies a low-rank parameter direction corresponding to one concept while minimizing interference with other attributes. A slider is created using a small set of prompts or sample images; thus slider directions can be created for either textual or visual concepts. Concept Sliders are plug-and-play: they can be composed efficiently and continuously modulated, enabling precise control over image generation. In quantitative experiments comparing to previous editing techniques, our sliders exhibit stronger targeted edits with lower interference. We showcase sliders for weather, age, styles, and expressions, as well as slider compositions. We show how sliders can transfer latents from StyleGAN for intuitive editing of visual concepts for which textual description is difficult. We also find that our method can help address persistent quality issues in Stable Diffusion XL including repair of object deformations and fixing distorted hands. Our code, data, and trained sliders are available at https://sliders.baulab.info/"}
{"title":"The allosteric lever: towards a principle of specific allosteric response","authors":["Maximilian Vossel","Bert L. de Groot","Alja\u017e Godec"],"raw_abstract":"Allostery, the phenomenon by which the perturbation of a molecule at one site\nalters its behavior at a remote functional site, enables control over\nbiomolecular function. Allosteric modulation is a promising avenue for drug\ndiscovery and is employed in the design of mechanical metamaterials. However, a\ngeneral principle of allostery, i.e. a set of quantitative and transferable\n\"ground rules\", remained elusive. It is neither a set of structural motifs nor\nintrinsic motions. Focusing on elastic network models, we here show that an\nallosteric lever--a mode-coupling pattern induced by the perturbation--governs\nthe directional, source-to-target, allosteric communication: a structural\nperturbation of an allosteric site couples the excitation of localized hard\nelastic modes with concerted long range soft-mode relaxation. Perturbations of\nnon-allosteric sites instead couple hard and soft modes uniformly. The\nallosteric response is shown to be generally non-linear and non-reciprocal, and\nallows for minimal structural distortions to be efficiently transmitted to\nspecific changes at distant sites. Allosteric levers exist in proteins and\n\"pseudoproteins\"--networks designed to display an allosteric response.\nInterestingly, protein sequences that constitute allosteric transmission\nchannels are shown to be evolutionarily conserved. To illustrate how the\nresults may be applied in drug design, we use them to successfully predict\nknown allosteric sites in proteins.","publication_date":1700506684,"paper_link":"http://arxiv.org/pdf/2311.12025v1","categories":["Quantitative Biology","Physics"],"abstract":"Allostery, the phenomenon by which the perturbation of a molecule at one site alters its behavior at a remote functional site, enables control over biomolecular function. Allosteric modulation is a promising avenue for drug discovery and is employed in the design of mechanical metamaterials. However, a general principle of allostery, i.e. a set of quantitative and transferable \"ground rules\", remained elusive. It is neither a set of structural motifs nor intrinsic motions. Focusing on elastic network models, we here show that an allosteric lever--a mode-coupling pattern induced by the perturbation--governs the directional, source-to-target, allosteric communication: a structural perturbation of an allosteric site couples the excitation of localized hard elastic modes with concerted long range soft-mode relaxation. Perturbations of non-allosteric sites instead couple hard and soft modes uniformly. The allosteric response is shown to be generally non-linear and non-reciprocal, and allows for minimal structural distortions to be efficiently transmitted to specific changes at distant sites. Allosteric levers exist in proteins and \"pseudoproteins\"--networks designed to display an allosteric response. Interestingly, protein sequences that constitute allosteric transmission channels are shown to be evolutionarily conserved. To illustrate how the results may be applied in drug design, we use them to successfully predict known allosteric sites in proteins."}
{"title":"GPQA: A Graduate-Level Google-Proof Q&A Benchmark","authors":["David Rein","Betty Li Hou","Asa Cooper Stickland","Jackson Petty","Richard Yuanzhe Pang","Julien Dirani","Julian Michael","Samuel R. Bowman"],"raw_abstract":"We present GPQA, a challenging dataset of 448 multiple-choice questions\nwritten by domain experts in biology, physics, and chemistry. We ensure that\nthe questions are high-quality and extremely difficult: experts who have or are\npursuing PhDs in the corresponding domains reach 65% accuracy (74% when\ndiscounting clear mistakes the experts identified in retrospect), while highly\nskilled non-expert validators only reach 34% accuracy, despite spending on\naverage over 30 minutes with unrestricted access to the web (i.e., the\nquestions are \"Google-proof\"). The questions are also difficult for\nstate-of-the-art AI systems, with our strongest GPT-4 based baseline achieving\n39% accuracy. If we are to use future AI systems to help us answer very hard\nquestions, for example, when developing new scientific knowledge, we need to\ndevelop scalable oversight methods that enable humans to supervise their\noutputs, which may be difficult even if the supervisors are themselves skilled\nand knowledgeable. The difficulty of GPQA both for skilled non-experts and\nfrontier AI systems should enable realistic scalable oversight experiments,\nwhich we hope can help devise ways for human experts to reliably get truthful\ninformation from AI systems that surpass human capabilities.","publication_date":1700506654,"paper_link":"http://arxiv.org/pdf/2311.12022v1","categories":["Quantitative Biology"],"abstract":"We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65% accuracy (74% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are \"Google-proof\"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities."}
{"title":"Weak existence for SDEs with singular drifts and fractional Brownian or Levy noise beyond the subcritical regime","authors":["Oleg Butkovsky","Samuel Gallay"],"raw_abstract":"We study a multidimensional stochastic differential equation with additive\nnoise: $$ d X_t=b(t, X_t) dt +d \\xi_t, $$ where the drift $b$ is integrable in\nspace and time, and $\\xi$ is either a fractional Brownian motion or an\n$\\alpha$-stable process. We show weak existence of solutions to this equation\nunder the optimal condition on integrability indices of $b$, going beyond the\nsubcritical Krylov-R\\\"ockner (Prodi-Serrin-Ladyzhenskaya) regime. This extends\nthe recent results of Krylov (2020) to the fractional Brownian and L\\'evy\ncases. We also construct a counterexample to demonstrate the optimality of this\ncondition. Our methods are built upon a version of the stochastic sewing lemma\nand the quantitative John--Nirenberg inequality of L\\^e.","publication_date":1700506216,"paper_link":"http://arxiv.org/pdf/2311.12013v1","categories":["Mathematics"],"abstract":"We study a multidimensional stochastic differential equation with additive noise: $__FORMULA____FORMULA__b__FORMULA__\\xi__FORMULA__\\alpha__FORMULA__b$, going beyond the subcritical Krylov-R\\\"ockner (Prodi-Serrin-Ladyzhenskaya) regime. This extends the recent results of Krylov (2020) to the fractional Brownian and L\\'evy cases. We also construct a counterexample to demonstrate the optimality of this condition. Our methods are built upon a version of the stochastic sewing lemma and the quantitative John--Nirenberg inequality of L\\^e."}
{"title":"FrePolad: Frequency-Rectified Point Latent Diffusion for Point Cloud Generation","authors":["Chenliang Zhou","Fangcheng Zhong","Param Hanji","Zhilin Guo","Kyle Fogarty","Alejandro Sztrajman","Hongyun Gao","Cengiz Oztireli"],"raw_abstract":"We propose FrePolad: frequency-rectified point latent diffusion, a point\ncloud generation pipeline integrating a variational autoencoder (VAE) with a\ndenoising diffusion probabilistic model (DDPM) for the latent distribution.\nFrePolad simultaneously achieves high quality, diversity, and flexibility in\npoint cloud cardinality for generation tasks while maintaining high\ncomputational efficiency. The improvement in generation quality and diversity\nis achieved through (1) a novel frequency rectification module via spherical\nharmonics designed to retain high-frequency content while learning the point\ncloud distribution; and (2) a latent DDPM to learn the regularized yet complex\nlatent distribution. In addition, FrePolad supports variable point cloud\ncardinality by formulating the sampling of points as conditional distributions\nover a latent shape distribution. Finally, the low-dimensional latent space\nencoded by the VAE contributes to FrePolad's fast and scalable sampling. Our\nquantitative and qualitative results demonstrate the state-of-the-art\nperformance of FrePolad in terms of quality, diversity, and computational\nefficiency.","publication_date":1700505811,"paper_link":"http://arxiv.org/pdf/2311.12090v1","categories":["Quantitative Biology"],"abstract":"We propose FrePolad: frequency-rectified point latent diffusion, a point cloud generation pipeline integrating a variational autoencoder (VAE) with a denoising diffusion probabilistic model (DDPM) for the latent distribution. FrePolad simultaneously achieves high quality, diversity, and flexibility in point cloud cardinality for generation tasks while maintaining high computational efficiency. The improvement in generation quality and diversity is achieved through (1) a novel frequency rectification module via spherical harmonics designed to retain high-frequency content while learning the point cloud distribution; and (2) a latent DDPM to learn the regularized yet complex latent distribution. In addition, FrePolad supports variable point cloud cardinality by formulating the sampling of points as conditional distributions over a latent shape distribution. Finally, the low-dimensional latent space encoded by the VAE contributes to FrePolad's fast and scalable sampling. Our quantitative and qualitative results demonstrate the state-of-the-art performance of FrePolad in terms of quality, diversity, and computational efficiency."}
{"title":"Sweetwater: An interpretable and adaptive autoencoder for efficient tissue deconvolution","authors":["Jesus de la Fuente","Naroa Legarra","Guillermo Serrano","Ana Garc\u00eda Osta","Krishna R. Kalari","Carlos Fernandez-Granda","Idoia Ochoa","Mikel Hernaez"],"raw_abstract":"Bulk RNA sequencing (RNA-seq) has revolutionized gene expression analysis,\nyet struggles with cellular heterogeneity. Traditional methods lack the ability\nto examine diverse cell types simultaneously, while single-cell RNA sequencing\n(scRNA-seq) is costly and complex, especially for tissues like the brain.\nRecent methodologies have emerged to estimate cell type proportions from\nRNA-seq samples, leveraging scRNA-seq matrices. Nevertheless, existing\ndeconvolution approaches face challenges, including being black-box methods\nwith unclear feature importance and not adequately addressing the\ndistributional shift between bulk and scRNA-seq data. This work presents\nSweetwater, an interpretable data-driven deconvolution model. Using an\nautoencoder-based approach with real and simulated bulk samples, Sweetwater\ncreates a common low-dimensional embedding, minimizing platform-specific\nvariations. Moreover, interpretation analysis reveals Sweetwater's\neffectiveness in identifying cell type marker genes, offering a transparent and\npowerful tool for dissecting intricate cellular landscapes.","publication_date":1700504603,"paper_link":"http://arxiv.org/pdf/2311.11991v1","categories":["Quantitative Biology"],"abstract":"Bulk RNA sequencing (RNA-seq) has revolutionized gene expression analysis, yet struggles with cellular heterogeneity. Traditional methods lack the ability to examine diverse cell types simultaneously, while single-cell RNA sequencing (scRNA-seq) is costly and complex, especially for tissues like the brain. Recent methodologies have emerged to estimate cell type proportions from RNA-seq samples, leveraging scRNA-seq matrices. Nevertheless, existing deconvolution approaches face challenges, including being black-box methods with unclear feature importance and not adequately addressing the distributional shift between bulk and scRNA-seq data. This work presents Sweetwater, an interpretable data-driven deconvolution model. Using an autoencoder-based approach with real and simulated bulk samples, Sweetwater creates a common low-dimensional embedding, minimizing platform-specific variations. Moreover, interpretation analysis reveals Sweetwater's effectiveness in identifying cell type marker genes, offering a transparent and powerful tool for dissecting intricate cellular landscapes."}
{"title":"MiniAnDE: a reduced AnDE ensemble to deal with microarray data","authors":["Pablo Torrijos","Jos\u00e9 A. G\u00e1mez","Jos\u00e9 M. Puerta"],"raw_abstract":"This article focuses on the supervised classification of datasets with a\nlarge number of variables and a small number of instances. This is the case,\nfor example, for microarray data sets commonly used in bioinformatics. Complex\nclassifiers that require estimating statistics over many variables are not\nsuitable for this type of data. Probabilistic classifiers with low-order\nprobability tables, e.g. NB and AODE, are good alternatives for dealing with\nthis type of data. AODE usually improves NB in accuracy, but suffers from high\nspatial complexity since $k$ models, each with $n+1$ variables, are included in\nthe AODE ensemble. In this paper, we propose MiniAnDE, an algorithm that\nincludes only a small number of heterogeneous base classifiers in the ensemble,\ni.e., each model only includes a different subset of the $k$ predictive\nvariables. Experimental evaluation shows that using MiniAnDE classifiers on\nmicroarray data is feasible and outperforms NB and other ensembles such as\nbagging and random forest.","publication_date":1700503975,"paper_link":"http://arxiv.org/pdf/2311.12879v1","categories":["Quantitative Biology"],"abstract":"This article focuses on the supervised classification of datasets with a large number of variables and a small number of instances. This is the case, for example, for microarray data sets commonly used in bioinformatics. Complex classifiers that require estimating statistics over many variables are not suitable for this type of data. Probabilistic classifiers with low-order probability tables, e.g. NB and AODE, are good alternatives for dealing with this type of data. AODE usually improves NB in accuracy, but suffers from high spatial complexity since __FORMULA__ models, each with __FORMULA__ variables, are included in the AODE ensemble. In this paper, we propose MiniAnDE, an algorithm that includes only a small number of heterogeneous base classifiers in the ensemble, i.e., each model only includes a different subset of the __FORMULA__ predictive variables. Experimental evaluation shows that using MiniAnDE classifiers on microarray data is feasible and outperforms NB and other ensembles such as bagging and random forest."}
{"title":"Decorrelation estimates for translated measures under diagonal flows","authors":["Michael Bj\u00f6rklund","Reynold Fregoli","Alexander Gorodnik"],"raw_abstract":"A profound link between Homogeneous Dynamics and Diophantine Approximation is\nbased on an observation that Diophantine properties of a real matrix $B$ are\nencoded by the corresponding lattice $\\Lambda_B$ translated by a\nmulti-parameter semigroup $a(t)$. We establish quantitative decorrelation\nestimates for measures supported on leaves $a(t)\\Lambda_B$ with the error terms\ndepending only on the minimum of the pairwise distances between the parameters.\nThe proof involves a careful analysis of the translated measures in the\nproducts of the spaces of unimodular lattices and establishes quantitative\nequidistributions to measures supported on various intermediate homogeneous\nsubspaces.","publication_date":1700501215,"paper_link":"http://arxiv.org/pdf/2311.11942v1","categories":["Mathematics"],"abstract":"A profound link between Homogeneous Dynamics and Diophantine Approximation is based on an observation that Diophantine properties of a real matrix __FORMULA__ are encoded by the corresponding lattice __FORMULA__ translated by a multi-parameter semigroup __FORMULA__. We establish quantitative decorrelation estimates for measures supported on leaves __FORMULA__ with the error terms depending only on the minimum of the pairwise distances between the parameters. The proof involves a careful analysis of the translated measures in the products of the spaces of unimodular lattices and establishes quantitative equidistributions to measures supported on various intermediate homogeneous subspaces."}
{"title":"Towards Proactive Safe Human-Robot Collaborations via Data-Efficient Conditional Behavior Prediction","authors":["Ravi Pandya","Zhuoyuan Wang","Yorie Nakahira","Changliu Liu"],"raw_abstract":"We focus on the problem of how we can enable a robot to collaborate\nseamlessly with a human partner, specifically in scenarios like collaborative\nmanufacturing where prexisting data is sparse. Much prior work in human-robot\ncollaboration uses observational models of humans (i.e. models that treat the\nrobot purely as an observer) to choose the robot's behavior, but such models do\nnot account for the influence the robot has on the human's actions, which may\nlead to inefficient interactions. We instead formulate the problem of optimally\nchoosing a collaborative robot's behavior based on a conditional model of the\nhuman that depends on the robot's future behavior. First, we propose a novel\nmodel-based formulation of conditional behavior prediction that allows the\nrobot to infer the human's intentions based on its future plan in data-sparse\nenvironments. We then show how to utilize a conditional model for proactive\ngoal selection and path generation around human collaborators. Finally, we use\nour proposed proactive controller in a collaborative task with real users to\nshow that it can improve users' interactions with a robot collaborator\nquantitatively and qualitatively.","publication_date":1700497615,"paper_link":"http://arxiv.org/pdf/2311.11893v1","categories":["Quantitative Biology"],"abstract":"We focus on the problem of how we can enable a robot to collaborate seamlessly with a human partner, specifically in scenarios like collaborative manufacturing where prexisting data is sparse. Much prior work in human-robot collaboration uses observational models of humans (i.e. models that treat the robot purely as an observer) to choose the robot's behavior, but such models do not account for the influence the robot has on the human's actions, which may lead to inefficient interactions. We instead formulate the problem of optimally choosing a collaborative robot's behavior based on a conditional model of the human that depends on the robot's future behavior. First, we propose a novel model-based formulation of conditional behavior prediction that allows the robot to infer the human's intentions based on its future plan in data-sparse environments. We then show how to utilize a conditional model for proactive goal selection and path generation around human collaborators. Finally, we use our proposed proactive controller in a collaborative task with real users to show that it can improve users' interactions with a robot collaborator quantitatively and qualitatively."}
{"title":"AMES: A Differentiable Embedding Space Selection Framework for Latent Graph Inference","authors":["Yuan Lu","Haitz S\u00e1ez de Oc\u00e1riz Borde","Pietro Li\u00f2"],"raw_abstract":"In real-world scenarios, although data entities may possess inherent\nrelationships, the specific graph illustrating their connections might not be\ndirectly accessible. Latent graph inference addresses this issue by enabling\nGraph Neural Networks (GNNs) to operate on point cloud data, dynamically\nlearning the necessary graph structure. These graphs are often derived from a\nlatent embedding space, which can be modeled using Euclidean, hyperbolic,\nspherical, or product spaces. However, currently, there is no principled\ndifferentiable method for determining the optimal embedding space. In this\nwork, we introduce the Attentional Multi-Embedding Selection (AMES) framework,\na differentiable method for selecting the best embedding space for latent graph\ninference through backpropagation, considering a downstream task. Our framework\nconsistently achieves comparable or superior results compared to previous\nmethods for latent graph inference across five benchmark datasets. Importantly,\nour approach eliminates the need for conducting multiple experiments to\nidentify the optimal embedding space. Furthermore, we explore interpretability\ntechniques that track the gradient contributions of different latent graphs,\nshedding light on how our attention-based, fully differentiable approach learns\nto choose the appropriate latent space. In line with previous works, our\nexperiments emphasize the advantages of hyperbolic spaces in enhancing\nperformance. More importantly, our interpretability framework provides a\ngeneral approach for quantitatively comparing embedding spaces across different\ntasks based on their contributions, a dimension that has been overlooked in\nprevious literature on latent graph inference.","publication_date":1700497463,"paper_link":"http://arxiv.org/pdf/2311.11891v1","categories":["Statistics"],"abstract":"In real-world scenarios, although data entities may possess inherent relationships, the specific graph illustrating their connections might not be directly accessible. Latent graph inference addresses this issue by enabling Graph Neural Networks (GNNs) to operate on point cloud data, dynamically learning the necessary graph structure. These graphs are often derived from a latent embedding space, which can be modeled using Euclidean, hyperbolic, spherical, or product spaces. However, currently, there is no principled differentiable method for determining the optimal embedding space. In this work, we introduce the Attentional Multi-Embedding Selection (AMES) framework, a differentiable method for selecting the best embedding space for latent graph inference through backpropagation, considering a downstream task. Our framework consistently achieves comparable or superior results compared to previous methods for latent graph inference across five benchmark datasets. Importantly, our approach eliminates the need for conducting multiple experiments to identify the optimal embedding space. Furthermore, we explore interpretability techniques that track the gradient contributions of different latent graphs, shedding light on how our attention-based, fully differentiable approach learns to choose the appropriate latent space. In line with previous works, our experiments emphasize the advantages of hyperbolic spaces in enhancing performance. More importantly, our interpretability framework provides a general approach for quantitatively comparing embedding spaces across different tasks based on their contributions, a dimension that has been overlooked in previous literature on latent graph inference."}
{"title":"Generalized super-resolution 4D Flow MRI $\\unicode{x2013}$ using ensemble learning to extend across the cardiovascular system","authors":["Leon Ericsson","Adam Hjalmarsson","Muhammad Usman Akbar","Edward Ferdian","Mia Bonini","Brandon Hardy","Jonas Schollenberger","Maria Aristova","Patrick Winter","Nicholas Burris","Alexander Fyrdahl","Andreas Sigfridsson","Susanne Schnell","C. Alberto Figueroa","David Nordsletten","Alistair A. Young","David Marlevi"],"raw_abstract":"4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive\nmeasurement technique capable of quantifying blood flow across the\ncardiovascular system. While practical use is limited by spatial resolution and\nimage noise, incorporation of trained super-resolution (SR) networks has\npotential to enhance image quality post-scan. However, these efforts have\npredominantly been restricted to narrowly defined cardiovascular domains, with\nlimited exploration of how SR performance extends across the cardiovascular\nsystem; a task aggravated by contrasting hemodynamic conditions apparent across\nthe cardiovasculature. The aim of our study was to explore the generalizability\nof SR 4D Flow MRI using a combination of heterogeneous training sets and\ndedicated ensemble learning. With synthetic training data generated across\nthree disparate domains (cardiac, aortic, cerebrovascular), varying\nconvolutional base and ensemble learners were evaluated as a function of domain\nand architecture, quantifying performance on both in-silico and acquired\nin-vivo data from the same three domains. Results show that both bagging and\nstacking ensembling enhance SR performance across domains, accurately\npredicting high-resolution velocities from low-resolution input data in-silico.\nLikewise, optimized networks successfully recover native resolution velocities\nfrom downsampled in-vivo data, as well as show qualitative potential in\ngenerating denoised SR-images from clinical level input data. In conclusion,\nour work presents a viable approach for generalized SR 4D Flow MRI, with\nensemble learning extending utility across various clinical areas of interest.","publication_date":1700492140,"paper_link":"http://arxiv.org/pdf/2311.11819v2","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive measurement technique capable of quantifying blood flow across the cardiovascular system. While practical use is limited by spatial resolution and image noise, incorporation of trained super-resolution (SR) networks has potential to enhance image quality post-scan. However, these efforts have predominantly been restricted to narrowly defined cardiovascular domains, with limited exploration of how SR performance extends across the cardiovascular system; a task aggravated by contrasting hemodynamic conditions apparent across the cardiovasculature. The aim of our study was to explore the generalizability of SR 4D Flow MRI using a combination of heterogeneous training sets and dedicated ensemble learning. With synthetic training data generated across three disparate domains (cardiac, aortic, cerebrovascular), varying convolutional base and ensemble learners were evaluated as a function of domain and architecture, quantifying performance on both in-silico and acquired in-vivo data from the same three domains. Results show that both bagging and stacking ensembling enhance SR performance across domains, accurately predicting high-resolution velocities from low-resolution input data in-silico. Likewise, optimized networks successfully recover native resolution velocities from downsampled in-vivo data, as well as show qualitative potential in generating denoised SR-images from clinical level input data. In conclusion, our work presents a viable approach for generalized SR 4D Flow MRI, with ensemble learning extending utility across various clinical areas of interest."}
{"title":"DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding","authors":["Hao Feng","Qi Liu","Hao Liu","Wengang Zhou","Houqiang Li","Can Huang"],"raw_abstract":"This work presents DocPedia, a novel large multimodal model (LMM) for\nversatile OCR-free document understanding, capable of parsing images up to\n2,560$\\times$2,560 resolution. Unlike existing work either struggle with\nhigh-resolution documents or give up the large language model thus vision or\nlanguage ability constrained, our DocPedia directly processes visual input in\nthe frequency domain rather than the pixel space. The unique characteristic\nenables DocPedia to capture a greater amount of visual and textual information\nusing a limited number of visual tokens. To consistently enhance both\nperception and comprehension abilities of our model, we develop a dual-stage\ntraining strategy and enrich instructions/annotations of all training tasks\ncovering multiple document types. Extensive quantitative and qualitative\nexperiments conducted on various publicly available benchmarks confirm the\nmutual benefits of jointly learning perception and comprehension tasks. The\nresults provide further evidence of the effectiveness and superior performance\nof our DocPedia over other methods.","publication_date":1700491345,"paper_link":"http://arxiv.org/pdf/2311.11810v1","categories":["Quantitative Biology"],"abstract":"This work presents DocPedia, a novel large multimodal model (LMM) for versatile OCR-free document understanding, capable of parsing images up to 2,560__FORMULA__2,560 resolution. Unlike existing work either struggle with high-resolution documents or give up the large language model thus vision or language ability constrained, our DocPedia directly processes visual input in the frequency domain rather than the pixel space. The unique characteristic enables DocPedia to capture a greater amount of visual and textual information using a limited number of visual tokens. To consistently enhance both perception and comprehension abilities of our model, we develop a dual-stage training strategy and enrich instructions/annotations of all training tasks covering multiple document types. Extensive quantitative and qualitative experiments conducted on various publicly available benchmarks confirm the mutual benefits of jointly learning perception and comprehension tasks. The results provide further evidence of the effectiveness and superior performance of our DocPedia over other methods."}
{"title":"Correlation-induced viscous dissipation in concentrated electrolytes","authors":["Paul Robin"],"raw_abstract":"Electrostatic correlations between ions dissolved in water are known to\nimpact their transport properties in numerous ways, from conductivity to ion\nselectivity. The effects of these correlations on the solvent itself remain,\nhowever, much less clear. In particular, the addition of salt has been\nconsistently reported to affect the solution's viscosity -- but most modelling\nattempts fail to reproduce experimental data even at moderate salt\nconcentration. Here, we use an approach based on stochastic density functional\ntheory, which accurately captures charge fluctuations and correlations. We\nderive a simple analytical expression for the viscosity correction in\nconcentrated electrolytes, by directly linking it to the liquid's structure\nfactor. Our prediction compares quantitatively to experimental data at all\ntemperatures and all salt concentrations up to the saturation limit. This\nuniversal link between microscopic structure and viscosity allows to shed light\non the nanoscale dynamics of water and ions in highly concentrated and\ncorrelated conditions.","publication_date":1700489436,"paper_link":"http://arxiv.org/pdf/2311.11784v2","categories":["Physics"],"abstract":"Electrostatic correlations between ions dissolved in water are known to impact their transport properties in numerous ways, from conductivity to ion selectivity. The effects of these correlations on the solvent itself remain, however, much less clear. In particular, the addition of salt has been consistently reported to affect the solution's viscosity -- but most modelling attempts fail to reproduce experimental data even at moderate salt concentration. Here, we use an approach based on stochastic density functional theory, which accurately captures charge fluctuations and correlations. We derive a simple analytical expression for the viscosity correction in concentrated electrolytes, by directly linking it to the liquid's structure factor. Our prediction compares quantitatively to experimental data at all temperatures and all salt concentrations up to the saturation limit. This universal link between microscopic structure and viscosity allows to shed light on the nanoscale dynamics of water and ions in highly concentrated and correlated conditions."}
{"title":"Cut-and-Paste: Subject-Driven Video Editing with Attention Control","authors":["Zhichao Zuo","Zhao Zhang","Yan Luo","Yang Zhao","Haijun Zhang","Yi Yang","Meng Wang"],"raw_abstract":"This paper presents a novel framework termed Cut-and-Paste for real-word\nsemantic video editing under the guidance of text prompt and additional\nreference image. While the text-driven video editing has demonstrated\nremarkable ability to generate highly diverse videos following given text\nprompts, the fine-grained semantic edits are hard to control by plain textual\nprompt only in terms of object details and edited region, and cumbersome long\ntext descriptions are usually needed for the task. We therefore investigate\nsubject-driven video editing for more precise control of both edited regions\nand background preservation, and fine-grained semantic generation. We achieve\nthis goal by introducing an reference image as supplementary input to the\ntext-driven video editing, which avoids racking your brain to come up with a\ncumbersome text prompt describing the detailed appearance of the object. To\nlimit the editing area, we refer to a method of cross attention control in\nimage editing and successfully extend it to video editing by fusing the\nattention map of adjacent frames, which strikes a balance between maintaining\nvideo background and spatio-temporal consistency. Compared with current\nmethods, the whole process of our method is like ``cut\" the source object to be\nedited and then ``paste\" the target object provided by reference image. We\ndemonstrate that our method performs favorably over prior arts for video\nediting under the guidance of text prompt and extra reference image, as\nmeasured by both quantitative and subjective evaluations.","publication_date":1700481606,"paper_link":"http://arxiv.org/pdf/2311.11697v1","categories":["Quantitative Biology"],"abstract":"This paper presents a novel framework termed Cut-and-Paste for real-word semantic video editing under the guidance of text prompt and additional reference image. While the text-driven video editing has demonstrated remarkable ability to generate highly diverse videos following given text prompts, the fine-grained semantic edits are hard to control by plain textual prompt only in terms of object details and edited region, and cumbersome long text descriptions are usually needed for the task. We therefore investigate subject-driven video editing for more precise control of both edited regions and background preservation, and fine-grained semantic generation. We achieve this goal by introducing an reference image as supplementary input to the text-driven video editing, which avoids racking your brain to come up with a cumbersome text prompt describing the detailed appearance of the object. To limit the editing area, we refer to a method of cross attention control in image editing and successfully extend it to video editing by fusing the attention map of adjacent frames, which strikes a balance between maintaining video background and spatio-temporal consistency. Compared with current methods, the whole process of our method is like ``cut\" the source object to be edited and then ``paste\" the target object provided by reference image. We demonstrate that our method performs favorably over prior arts for video editing under the guidance of text prompt and extra reference image, as measured by both quantitative and subjective evaluations."}
{"title":"Refactoring Programs Using Large Language Models with Few-Shot Examples","authors":["Atsushi Shirafuji","Yusuke Oda","Jun Suzuki","Makoto Morishita","Yutaka Watanobe"],"raw_abstract":"A less complex and more straightforward program is a crucial factor that\nenhances its maintainability and makes writing secure and bug-free programs\neasier. However, due to its heavy workload and the risks of breaking the\nworking programs, programmers are reluctant to do code refactoring, and thus,\nit also causes the loss of potential learning experiences. To mitigate this, we\ndemonstrate the application of using a large language model (LLM), GPT-3.5, to\nsuggest less complex versions of the user-written Python program, aiming to\nencourage users to learn how to write better programs. We propose a method to\nleverage the prompting with few-shot examples of the LLM by selecting the\nbest-suited code refactoring examples for each target programming problem based\non the prior evaluation of prompting with the one-shot example. The\nquantitative evaluation shows that 95.68% of programs can be refactored by\ngenerating 10 candidates each, resulting in a 17.35% reduction in the average\ncyclomatic complexity and a 25.84% decrease in the average number of lines\nafter filtering only generated programs that are semantically correct.\nFurthermore, the qualitative evaluation shows outstanding capability in code\nformatting, while unnecessary behaviors such as deleting or translating\ncomments are also observed.","publication_date":1700480625,"paper_link":"http://arxiv.org/pdf/2311.11690v1","categories":["Quantitative Biology"],"abstract":"A less complex and more straightforward program is a crucial factor that enhances its maintainability and makes writing secure and bug-free programs easier. However, due to its heavy workload and the risks of breaking the working programs, programmers are reluctant to do code refactoring, and thus, it also causes the loss of potential learning experiences. To mitigate this, we demonstrate the application of using a large language model (LLM), GPT-3.5, to suggest less complex versions of the user-written Python program, aiming to encourage users to learn how to write better programs. We propose a method to leverage the prompting with few-shot examples of the LLM by selecting the best-suited code refactoring examples for each target programming problem based on the prior evaluation of prompting with the one-shot example. The quantitative evaluation shows that 95.68% of programs can be refactored by generating 10 candidates each, resulting in a 17.35% reduction in the average cyclomatic complexity and a 25.84% decrease in the average number of lines after filtering only generated programs that are semantically correct. Furthermore, the qualitative evaluation shows outstanding capability in code formatting, while unnecessary behaviors such as deleting or translating comments are also observed."}
{"title":"Enhanced Spatio-Temporal Context for Temporally Consistent Robust 3D Human Motion Recovery from Monocular Videos","authors":["Sushovan Chanda","Amogh Tiwari","Lokender Tiwari","Brojeshwar Bhowmick","Avinash Sharma","Hrishav Barua"],"raw_abstract":"Recovering temporally consistent 3D human body pose, shape and motion from a\nmonocular video is a challenging task due to (self-)occlusions, poor lighting\nconditions, complex articulated body poses, depth ambiguity, and limited\navailability of annotated data. Further, doing a simple perframe estimation is\ninsufficient as it leads to jittery and implausible results. In this paper, we\npropose a novel method for temporally consistent motion estimation from a\nmonocular video. Instead of using generic ResNet-like features, our method uses\na body-aware feature representation and an independent per-frame pose and\ncamera initialization over a temporal window followed by a novel\nspatio-temporal feature aggregation by using a combination of self-similarity\nand self-attention over the body-aware features and the perframe\ninitialization. Together, they yield enhanced spatiotemporal context for every\nframe by considering remaining past and future frames. These features are used\nto predict the pose and shape parameters of the human body model, which are\nfurther refined using an LSTM. Experimental results on the publicly available\nbenchmark data show that our method attains significantly lower acceleration\nerror and outperforms the existing state-of-the-art methods over all key\nquantitative evaluation metrics, including complex scenarios like partial\nocclusion, complex poses and even relatively low illumination.","publication_date":1700477639,"paper_link":"http://arxiv.org/pdf/2311.11662v1","categories":["Quantitative Biology"],"abstract":"Recovering temporally consistent 3D human body pose, shape and motion from a monocular video is a challenging task due to (self-)occlusions, poor lighting conditions, complex articulated body poses, depth ambiguity, and limited availability of annotated data. Further, doing a simple perframe estimation is insufficient as it leads to jittery and implausible results. In this paper, we propose a novel method for temporally consistent motion estimation from a monocular video. Instead of using generic ResNet-like features, our method uses a body-aware feature representation and an independent per-frame pose and camera initialization over a temporal window followed by a novel spatio-temporal feature aggregation by using a combination of self-similarity and self-attention over the body-aware features and the perframe initialization. Together, they yield enhanced spatiotemporal context for every frame by considering remaining past and future frames. These features are used to predict the pose and shape parameters of the human body model, which are further refined using an LSTM. Experimental results on the publicly available benchmark data show that our method attains significantly lower acceleration error and outperforms the existing state-of-the-art methods over all key quantitative evaluation metrics, including complex scenarios like partial occlusion, complex poses and even relatively low illumination."}
{"title":"Exploring the Relationship Between COVID-19 Induced Economic Downturn and Women's Nutritional Health Disparities","authors":["Alaa M. Sadeq"],"raw_abstract":"This study explores how the COVID-19 pandemic's economic impact has\nexacerbated nutritional health disparities among women. It sought to understand\nthe effects of economic challenges on women's dietary choices and access to\nnutritious food across different socioeconomic groups. Using a mixed-methods\napproach, the research combined quantitative data from health and economic\nrecords with qualitative insights from interviews with diverse women. The study\nanalyzed trends in nutritional health and economic factors before and after the\npandemic and gathered personal accounts regarding nutrition and economic\ndifficulties during this period. Findings showed a clear link between the\neconomic downturn and deteriorating nutritional health, particularly in\nlow-income and marginalized groups. These women reported decreased access to\nhealthy foods and an increased dependence on less nutritious options due to\nbudget constraints, leading to a decline in dietary quality. This trend was\nless evident in higher-income groups, highlighting stark disparities. The\npandemic intensified pre-existing nutritional inequalities, with the most\nvulnerable groups facing greater adverse effects. However, community support\nand public health measures provided some relief. In summary, the pandemic's\neconomic repercussions have indirectly impaired women's nutritional health,\nespecially among the socioeconomically disadvantaged. This highlights the\nnecessity for tailored nutritional interventions and economic policies focused\non safeguarding women's health.","publication_date":1700471425,"paper_link":"http://arxiv.org/pdf/2311.12080v1","categories":["Quantitative Biology"],"abstract":"This study explores how the COVID-19 pandemic's economic impact has exacerbated nutritional health disparities among women. It sought to understand the effects of economic challenges on women's dietary choices and access to nutritious food across different socioeconomic groups. Using a mixed-methods approach, the research combined quantitative data from health and economic records with qualitative insights from interviews with diverse women. The study analyzed trends in nutritional health and economic factors before and after the pandemic and gathered personal accounts regarding nutrition and economic difficulties during this period. Findings showed a clear link between the economic downturn and deteriorating nutritional health, particularly in low-income and marginalized groups. These women reported decreased access to healthy foods and an increased dependence on less nutritious options due to budget constraints, leading to a decline in dietary quality. This trend was less evident in higher-income groups, highlighting stark disparities. The pandemic intensified pre-existing nutritional inequalities, with the most vulnerable groups facing greater adverse effects. However, community support and public health measures provided some relief. In summary, the pandemic's economic repercussions have indirectly impaired women's nutritional health, especially among the socioeconomically disadvantaged. This highlights the necessity for tailored nutritional interventions and economic policies focused on safeguarding women's health."}
{"title":"SpecHD: Hyperdimensional Computing Framework for FPGA-based Mass Spectrometry Clustering","authors":["Sumukh Pinge","Weihong Xu","Jaeyoung Kang","Tianqi Zhang","Neima Moshiri","Wout Bittremieux","Tajana Rosing"],"raw_abstract":"Mass spectrometry-based proteomics is a key enabler for personalized\nhealthcare, providing a deep dive into the complex protein compositions of\nbiological systems. This technology has vast applications in biotechnology and\nbiomedicine but faces significant computational bottlenecks. Current\nmethodologies often require multiple hours or even days to process extensive\ndatasets, particularly in the domain of spectral clustering. To tackle these\ninefficiencies, we introduce SpecHD, a hyperdimensional computing (HDC)\nframework supplemented by an FPGA-accelerated architecture with integrated\nnear-storage preprocessing. Utilizing streamlined binary operations in an HDC\nenvironment, SpecHD capitalizes on the low-latency and parallel capabilities of\nFPGAs. This approach markedly improves clustering speed and efficiency, serving\nas a catalyst for real-time, high-throughput data analysis in future healthcare\napplications. Our evaluations demonstrate that SpecHD not only maintains but\noften surpasses existing clustering quality metrics while drastically cutting\ncomputational time. Specifically, it can cluster a large-scale human proteome\ndataset-comprising 25 million MS/MS spectra and 131 GB of MS data-in just 5\nminutes. With energy efficiency exceeding 31x and a speedup factor that spans a\nrange of 6x to 54x over existing state of-the-art solutions, SpecHD emerges as\na promising solution for the rapid analysis of mass spectrometry data with\ngreat implications for personalized healthcare.","publication_date":1700462557,"paper_link":"http://arxiv.org/pdf/2311.12874v1","categories":["Quantitative Biology"],"abstract":"Mass spectrometry-based proteomics is a key enabler for personalized healthcare, providing a deep dive into the complex protein compositions of biological systems. This technology has vast applications in biotechnology and biomedicine but faces significant computational bottlenecks. Current methodologies often require multiple hours or even days to process extensive datasets, particularly in the domain of spectral clustering. To tackle these inefficiencies, we introduce SpecHD, a hyperdimensional computing (HDC) framework supplemented by an FPGA-accelerated architecture with integrated near-storage preprocessing. Utilizing streamlined binary operations in an HDC environment, SpecHD capitalizes on the low-latency and parallel capabilities of FPGAs. This approach markedly improves clustering speed and efficiency, serving as a catalyst for real-time, high-throughput data analysis in future healthcare applications. Our evaluations demonstrate that SpecHD not only maintains but often surpasses existing clustering quality metrics while drastically cutting computational time. Specifically, it can cluster a large-scale human proteome dataset-comprising 25 million MS/MS spectra and 131 GB of MS data-in just 5 minutes. With energy efficiency exceeding 31x and a speedup factor that spans a range of 6x to 54x over existing state of-the-art solutions, SpecHD emerges as a promising solution for the rapid analysis of mass spectrometry data with great implications for personalized healthcare."}
{"title":"Spectral function of Fermi polarons at finite temperature from a self-consistent many-body $T$-matrix approach in real frequency","authors":["Hui Hu","Xia-Ji Liu"],"raw_abstract":"We theoretically examine the finite-temperature spectral function of Fermi\npolarons in three dimensions, by using a self-consistent many-body $T$-matrix\ntheory in real frequency. In comparison with the previous results from a\nnon-self-consistent many-body $T$-matrix approach, we show that the treatment\nof self-consistency in the impurity Green function leads to notable changes in\nalmost all the dynamical quantities, including the vertex function, impurity\nself-energy and spectral function. Eventually, it gives rise to quantitatively\ndifferent predictions for the measurable radio-frequency spectrum and Raman\nspectrum at finite temperature. Using the recent spectroscopic measurements as\na benchmark, we find that the self-consistent many-body $T$-matrix theory\nsomehow provides a better explanation for the experimental data. The notable\ndifference in the predictions from the non-self-consistent and self-consistent\ntheories suggests that more accurate theoretical descriptions are needed, in\norder to fully account for the current spectroscopic observations on Fermi\npolarons.","publication_date":1700460786,"paper_link":"http://arxiv.org/pdf/2311.11554v1","categories":["Physics"],"abstract":"We theoretically examine the finite-temperature spectral function of Fermi polarons in three dimensions, by using a self-consistent many-body __FORMULA__-matrix theory in real frequency. In comparison with the previous results from a non-self-consistent many-body __FORMULA__-matrix approach, we show that the treatment of self-consistency in the impurity Green function leads to notable changes in almost all the dynamical quantities, including the vertex function, impurity self-energy and spectral function. Eventually, it gives rise to quantitatively different predictions for the measurable radio-frequency spectrum and Raman spectrum at finite temperature. Using the recent spectroscopic measurements as a benchmark, we find that the self-consistent many-body __FORMULA__-matrix theory somehow provides a better explanation for the experimental data. The notable difference in the predictions from the non-self-consistent and self-consistent theories suggests that more accurate theoretical descriptions are needed, in order to fully account for the current spectroscopic observations on Fermi polarons."}
{"title":"Indentation of an elastic arch on a frictional substrate: Pinning, unfolding and snapping","authors":["Keisuke Yoshida","Hirofumi Wada"],"raw_abstract":"We investigate the morphology and mechanics of a naturally curved elastic\narch loaded at its center and frictionally supported at both ends on a flat,\nrigid substrate. Through systematic numerical simulations, we classify the\nobserved behaviors of the arch into three distinct types of configurations in\nterms of the arch geometry and the coefficient of static friction with the\nsubstrate. A linear theory is developed based on a planar elastica model\ncombined with Amontons-Coulomb's frictional law, which quantitatively explains\nthe numerically constructed phase diagram. The snapping transition of a loaded\narch in a sufficiently large indentation regime, which involves a discontinuous\nforce jump, is numerically observed. The proposed model problem allows a fully\nanalytical investigation and demonstrates a rich variety of mechanical\nbehaviors owing to the interplay between elasticity, geometry, and friction.\nThis study provides a basis for understanding more common but complex systems,\nsuch as a cylindrical shell subjected to a concentrated load and simultaneously\nsupported by frictional contact with surrounding objects.","publication_date":1700449701,"paper_link":"http://arxiv.org/pdf/2311.11504v1","categories":["Physics"],"abstract":"We investigate the morphology and mechanics of a naturally curved elastic arch loaded at its center and frictionally supported at both ends on a flat, rigid substrate. Through systematic numerical simulations, we classify the observed behaviors of the arch into three distinct types of configurations in terms of the arch geometry and the coefficient of static friction with the substrate. A linear theory is developed based on a planar elastica model combined with Amontons-Coulomb's frictional law, which quantitatively explains the numerically constructed phase diagram. The snapping transition of a loaded arch in a sufficiently large indentation regime, which involves a discontinuous force jump, is numerically observed. The proposed model problem allows a fully analytical investigation and demonstrates a rich variety of mechanical behaviors owing to the interplay between elasticity, geometry, and friction. This study provides a basis for understanding more common but complex systems, such as a cylindrical shell subjected to a concentrated load and simultaneously supported by frictional contact with surrounding objects."}
{"title":"Decoding the Molecular Universe -- Workshop Report","authors":["Thomas O. Metz","Joshua N. Adkins","Peter B. Armentrout","Patrick Chain","Fanny Chu","Courtney D Corley","John R. Cort","Elizabeth Denis","Daniel Drell","Katherine R. Duncan","Robert G. Ewing","Facundo M. Fernandez","Oliver Fiehn","Neha Garg","Stefan Grimme","Christopher Henry","Robert L. Hettich","Tobias Kind","Roger G. Linington","Gary W. Miller","Trent Northen","Kirsten Overdahl","Ari Patrinos","Daniel Raftery","Paul Rigor","Richard D. Smith","Jon Sobus","Justin Teeguarden","Akos Vertes","Katrina Waters","Bobbie-Jo Webb-Robertson","Antony Williams","David Wishart"],"raw_abstract":"On August 9-10, 2023, a workshop was convened at the Pacific Northwest\nNational Laboratory (PNNL) in Richland, WA that brought together a group of\ninternationally recognized experts in metabolomics, natural products discovery,\nchemical ecology, chemical and biological threat assessment, cheminformatics,\ncomputational chemistry, cloud computing, artificial intelligence, and novel\ntechnology development. These experts were invited to assess the value and\nfeasibility of a grand-scale project to create new technologies that would\nallow the identification and quantification of all small molecules, or to\ndecode the molecular universe. The Decoding the Molecular Universe project\nwould extend and complement the success of the Human Genome Project by\ndeveloping new capabilities and technologies to measure small molecules\n(defined as non-protein, non-polymer molecules less than 1500 Daltons) of any\norigin and generated in biological systems or produced abiotically. Workshop\nattendees 1) explored what new understanding of biological and environmental\nsystems could be revealed through the lens of small molecules; 2) characterized\nthe similarities in current needs and technical challenges between each science\nor mission area for unambiguous and comprehensive determination of the\ncomposition and quantities of small molecules of any sample; 3) determined the\nextent to which technologies or methods currently exist for unambiguously and\ncomprehensively determining the small molecule composition of any sample and in\na reasonable time; and 4) identified the attributes of the ideal technology or\napproach for universal small molecule measurement and identification. The\nworkshop concluded with a discussion of how a project of this scale could be\nundertaken, possible thrusts for the project, early proof-of-principle\napplications, and similar efforts upon which the project could be modeled.","publication_date":1700432260,"paper_link":"http://arxiv.org/pdf/2311.11437v1","categories":["Quantitative Biology"],"abstract":"On August 9-10, 2023, a workshop was convened at the Pacific Northwest National Laboratory (PNNL) in Richland, WA that brought together a group of internationally recognized experts in metabolomics, natural products discovery, chemical ecology, chemical and biological threat assessment, cheminformatics, computational chemistry, cloud computing, artificial intelligence, and novel technology development. These experts were invited to assess the value and feasibility of a grand-scale project to create new technologies that would allow the identification and quantification of all small molecules, or to decode the molecular universe. The Decoding the Molecular Universe project would extend and complement the success of the Human Genome Project by developing new capabilities and technologies to measure small molecules (defined as non-protein, non-polymer molecules less than 1500 Daltons) of any origin and generated in biological systems or produced abiotically. Workshop attendees 1) explored what new understanding of biological and environmental systems could be revealed through the lens of small molecules; 2) characterized the similarities in current needs and technical challenges between each science or mission area for unambiguous and comprehensive determination of the composition and quantities of small molecules of any sample; 3) determined the extent to which technologies or methods currently exist for unambiguously and comprehensively determining the small molecule composition of any sample and in a reasonable time; and 4) identified the attributes of the ideal technology or approach for universal small molecule measurement and identification. The workshop concluded with a discussion of how a project of this scale could be undertaken, possible thrusts for the project, early proof-of-principle applications, and similar efforts upon which the project could be modeled."}
{"title":"Bell-INGARCH Model","authors":["Ying Wang","Shuang Chen","Lianyong Qian"],"raw_abstract":"Integer-valued time series exist widely in economics, finance, biology,\ncomputer science, medicine, insurance, and many other fields. In recent years,\nmany types of models have been proposed to model integer-valued time series\ndata, in which the integer autoregressive model and integer-valued GARCH model\nare the most representative. Although there have been many results of\ninteger-valued time series data, the parameters of integer-valued time series\nmodel structure are more complicated. This paper is dedicated to proposing a\nnew simple integer-valued GARCH model. First, the Bell integer-valued GARCH\nmodel is given based on Bell distribution. Then, the conditional maximum\nlikelihood estimation method is used to obtain the estimators of parameters.\nLater, numerical simulations confirm the finite sample properties of the\nestimation of unknown parameters. Finally, the model is applied in the two real\nexamples. Compared with the existing models, the proposed model is more simple\nand applicable.","publication_date":1700407633,"paper_link":"http://arxiv.org/pdf/2311.11352v1","categories":["Mathematics","Statistics"],"abstract":"Integer-valued time series exist widely in economics, finance, biology, computer science, medicine, insurance, and many other fields. In recent years, many types of models have been proposed to model integer-valued time series data, in which the integer autoregressive model and integer-valued GARCH model are the most representative. Although there have been many results of integer-valued time series data, the parameters of integer-valued time series model structure are more complicated. This paper is dedicated to proposing a new simple integer-valued GARCH model. First, the Bell integer-valued GARCH model is given based on Bell distribution. Then, the conditional maximum likelihood estimation method is used to obtain the estimators of parameters. Later, numerical simulations confirm the finite sample properties of the estimation of unknown parameters. Finally, the model is applied in the two real examples. Compared with the existing models, the proposed model is more simple and applicable."}
{"title":"Discrete approximations of Gaussian smoothing and Gaussian derivatives","authors":["Tony Lindeberg"],"raw_abstract":"This paper develops an in-depth treatment concerning the problem of\napproximating the Gaussian smoothing and Gaussian derivative computations in\nscale-space theory for application on discrete data. With close connections to\nprevious axiomatic treatments of continuous and discrete scale-space theory, we\nconsider three main ways discretizing these scale-space operations in terms of\nexplicit discrete convolutions, based on either (i) sampling the Gaussian\nkernels and the Gaussian derivative kernels, (ii) locally integrating the\nGaussian kernels and the Gaussian derivative kernels over each pixel support\nregion and (iii) basing the scale-space analysis on the discrete analogue of\nthe Gaussian kernel, and then computing derivative approximations by applying\nsmall-support central difference operators to the spatially smoothed image\ndata.\n  We study the properties of these three main discretization methods both\ntheoretically and experimentally, and characterize their performance by\nquantitative measures, including the results they give rise to with respect to\nthe task of scale selection, investigated for four different use cases, and\nwith emphasis on the behaviour at fine scales. The results show that the\nsampled Gaussian kernels and derivatives as well as the integrated Gaussian\nkernels and derivatives perform very poorly at very fine scales. At very fine\nscales, the discrete analogue of the Gaussian kernel with its corresponding\ndiscrete derivative approximations performs substantially better. The sampled\nGaussian kernel and the sampled Gaussian derivatives do, on the other hand,\nlead to numerically very good approximations of the corresponding continuous\nresults, when the scale parameter is sufficiently large, in the experiments\npresented in the paper, when the scale parameter is greater than a value of\nabout 1, in units of the grid spacing.","publication_date":1700399226,"paper_link":"http://arxiv.org/pdf/2311.11317v2","categories":["Quantitative Biology"],"abstract":"This paper develops an in-depth treatment concerning the problem of approximating the Gaussian smoothing and Gaussian derivative computations in scale-space theory for application on discrete data. With close connections to previous axiomatic treatments of continuous and discrete scale-space theory, we consider three main ways discretizing these scale-space operations in terms of explicit discrete convolutions, based on either (i) sampling the Gaussian kernels and the Gaussian derivative kernels, (ii) locally integrating the Gaussian kernels and the Gaussian derivative kernels over each pixel support region and (iii) basing the scale-space analysis on the discrete analogue of the Gaussian kernel, and then computing derivative approximations by applying small-support central difference operators to the spatially smoothed image data.   We study the properties of these three main discretization methods both theoretically and experimentally, and characterize their performance by quantitative measures, including the results they give rise to with respect to the task of scale selection, investigated for four different use cases, and with emphasis on the behaviour at fine scales. The results show that the sampled Gaussian kernels and derivatives as well as the integrated Gaussian kernels and derivatives perform very poorly at very fine scales. At very fine scales, the discrete analogue of the Gaussian kernel with its corresponding discrete derivative approximations performs substantially better. The sampled Gaussian kernel and the sampled Gaussian derivatives do, on the other hand, lead to numerically very good approximations of the corresponding continuous results, when the scale parameter is sufficiently large, in the experiments presented in the paper, when the scale parameter is greater than a value of about 1, in units of the grid spacing."}
{"title":"Shape-Sensitive Loss for Catheter and Guidewire Segmentation","authors":["Chayun Kongtongvattana","Baoru Huang","Jingxuan Kang","Hoan Nguyen","Olajide Olufemi","Anh Nguyen"],"raw_abstract":"We introduce a shape-sensitive loss function for catheter and guidewire\nsegmentation and utilize it in a vision transformer network to establish a new\nstate-of-the-art result on a large-scale X-ray images dataset. We transform\nnetwork-derived predictions and their corresponding ground truths into signed\ndistance maps, thereby enabling any networks to concentrate on the essential\nboundaries rather than merely the overall contours. These SDMs are subjected to\nthe vision transformer, efficiently producing high-dimensional feature vectors\nencapsulating critical image attributes. By computing the cosine similarity\nbetween these feature vectors, we gain a nuanced understanding of image\nsimilarity that goes beyond the limitations of traditional overlap-based\nmeasures. The advantages of our approach are manifold, ranging from scale and\ntranslation invariance to superior detection of subtle differences, thus\nensuring precise localization and delineation of the medical instruments within\nthe images. Comprehensive quantitative and qualitative analyses substantiate\nthe significant enhancement in performance over existing baselines,\ndemonstrating the promise held by our new shape-sensitive loss function for\nimproving catheter and guidewire segmentation.","publication_date":1700363121,"paper_link":"http://arxiv.org/pdf/2311.11205v1","categories":["Electrical Engineering and Systems Science"],"abstract":"We introduce a shape-sensitive loss function for catheter and guidewire segmentation and utilize it in a vision transformer network to establish a new state-of-the-art result on a large-scale X-ray images dataset. We transform network-derived predictions and their corresponding ground truths into signed distance maps, thereby enabling any networks to concentrate on the essential boundaries rather than merely the overall contours. These SDMs are subjected to the vision transformer, efficiently producing high-dimensional feature vectors encapsulating critical image attributes. By computing the cosine similarity between these feature vectors, we gain a nuanced understanding of image similarity that goes beyond the limitations of traditional overlap-based measures. The advantages of our approach are manifold, ranging from scale and translation invariance to superior detection of subtle differences, thus ensuring precise localization and delineation of the medical instruments within the images. Comprehensive quantitative and qualitative analyses substantiate the significant enhancement in performance over existing baselines, demonstrating the promise held by our new shape-sensitive loss function for improving catheter and guidewire segmentation."}
{"title":"Navigating Cultural Diversity: Barriers and Potentials in Multicultural Agile Software Development Teams","authors":["Daniel Welsch","Luisa Burk","David M\u00f6tefindt","Michael Neumann"],"raw_abstract":"Context: Social aspects are of high importance for being successful using\nagile methods in software development. People are influenced by their cultural\nimprint, as the underlying cultural values are guiding us in how we think and\nact. Thus, one may assume that in multicultural agile software development\nteams, cultural characteristics influence the result in terms of quality of the\nteam work and consequently, the product to be delivered. Objective: We aim to\nidentify barriers and potentials that may arise in multicultural agile software\ndevelopment teams to provide valuable strategies for both researchers and\npractitioners faced with barriers or unrealized potentials of cultural\ndiversity. Method: The study is designed as a single-case study with two units\nof analysis using a mixed-method design consisting quantitative and qualitative\nmethods. Results: First, our results suggest that the cultural characteristics\nat the team level need to be analyzed individually in intercultural teams,\nSecond, we identified key potentials regarding cultural characteristics\nproviding key potentials such as a individual team subculture that fits agile\nvalues like open communication. Third, we derived strategies supporting the\npotentials of cultural diversity in agile software development teams.\nConclusion: Our findings show, that a deeper understanding of cultural\ninfluences in multicultural agile software development teams is needed. Based\non the results, we already prepare future work to validate the results in other\nindustries.","publication_date":1700335668,"paper_link":"http://arxiv.org/pdf/2311.12061v1","categories":["Quantitative Biology"],"abstract":"Context: Social aspects are of high importance for being successful using agile methods in software development. People are influenced by their cultural imprint, as the underlying cultural values are guiding us in how we think and act. Thus, one may assume that in multicultural agile software development teams, cultural characteristics influence the result in terms of quality of the team work and consequently, the product to be delivered. Objective: We aim to identify barriers and potentials that may arise in multicultural agile software development teams to provide valuable strategies for both researchers and practitioners faced with barriers or unrealized potentials of cultural diversity. Method: The study is designed as a single-case study with two units of analysis using a mixed-method design consisting quantitative and qualitative methods. Results: First, our results suggest that the cultural characteristics at the team level need to be analyzed individually in intercultural teams, Second, we identified key potentials regarding cultural characteristics providing key potentials such as a individual team subculture that fits agile values like open communication. Third, we derived strategies supporting the potentials of cultural diversity in agile software development teams. Conclusion: Our findings show, that a deeper understanding of cultural influences in multicultural agile software development teams is needed. Based on the results, we already prepare future work to validate the results in other industries."}
{"title":"Evolutionary game selection creates cooperative environments","authors":["Onkar Sadekar","Andrea Civilini","Jes\u00fas G\u00f3mez-Garde\u00f1es","Vito Latora","Federico Battiston"],"raw_abstract":"The emergence of collective cooperation in competitive environments is a\nwell-known phenomenon in biology, economics and social systems. While most\nevolutionary game models focus on the evolution of strategies for a fixed game,\nhow strategic decisions co-evolve with the environment has so far mostly been\noverlooked. Here, we consider a game selection model where not only the\nstrategies but also the game can change over time following evolutionary\nprinciples. Our results show that co-evolutionary dynamics of games and\nstrategies can induce novel collective phenomena, fostering the emergence of\ncooperative environments. When the model is taken on structured populations the\narchitecture of the interaction network can significantly amplify pro-social\nbehaviour, with a critical role played by network heterogeneity and the\npresence of clustered groups, distinctive features observed in real-world\npopulations. By unveiling the link between the evolution of strategies and\ngames for different structured populations, our model sheds new light on the\norigin of social dilemmas ubiquitously observed in real-world social systems.","publication_date":1700328698,"paper_link":"http://arxiv.org/pdf/2311.11128v1","categories":["Mathematics","Quantitative Biology","Physics"],"abstract":"The emergence of collective cooperation in competitive environments is a well-known phenomenon in biology, economics and social systems. While most evolutionary game models focus on the evolution of strategies for a fixed game, how strategic decisions co-evolve with the environment has so far mostly been overlooked. Here, we consider a game selection model where not only the strategies but also the game can change over time following evolutionary principles. Our results show that co-evolutionary dynamics of games and strategies can induce novel collective phenomena, fostering the emergence of cooperative environments. When the model is taken on structured populations the architecture of the interaction network can significantly amplify pro-social behaviour, with a critical role played by network heterogeneity and the presence of clustered groups, distinctive features observed in real-world populations. By unveiling the link between the evolution of strategies and games for different structured populations, our model sheds new light on the origin of social dilemmas ubiquitously observed in real-world social systems."}
{"title":"Beyond Images: An Integrative Multi-modal Approach to Chest X-Ray Report Generation","authors":["Nurbanu Aksoy","Serge Sharoff","Selcuk Baser","Nishant Ravikumar","Alejandro F Frangi"],"raw_abstract":"Image-to-text radiology report generation aims to automatically produce\nradiology reports that describe the findings in medical images. Most existing\nmethods focus solely on the image data, disregarding the other patient\ninformation accessible to radiologists. In this paper, we present a novel\nmulti-modal deep neural network framework for generating chest X-rays reports\nby integrating structured patient data, such as vital signs and symptoms,\nalongside unstructured clinical notes.We introduce a conditioned\ncross-multi-head attention module to fuse these heterogeneous data modalities,\nbridging the semantic gap between visual and textual data. Experiments\ndemonstrate substantial improvements from using additional modalities compared\nto relying on images alone. Notably, our model achieves the highest reported\nperformance on the ROUGE-L metric compared to relevant state-of-the-art models\nin the literature. Furthermore, we employed both human evaluation and clinical\nsemantic similarity measurement alongside word-overlap metrics to improve the\ndepth of quantitative analysis. A human evaluation, conducted by a\nboard-certified radiologist, confirms the model's accuracy in identifying\nhigh-level findings, however, it also highlights that more improvement is\nneeded to capture nuanced details and clinical context.","publication_date":1700318273,"paper_link":"http://arxiv.org/pdf/2311.11090v1","categories":["Quantitative Biology"],"abstract":"Image-to-text radiology report generation aims to automatically produce radiology reports that describe the findings in medical images. Most existing methods focus solely on the image data, disregarding the other patient information accessible to radiologists. In this paper, we present a novel multi-modal deep neural network framework for generating chest X-rays reports by integrating structured patient data, such as vital signs and symptoms, alongside unstructured clinical notes.We introduce a conditioned cross-multi-head attention module to fuse these heterogeneous data modalities, bridging the semantic gap between visual and textual data. Experiments demonstrate substantial improvements from using additional modalities compared to relying on images alone. Notably, our model achieves the highest reported performance on the ROUGE-L metric compared to relevant state-of-the-art models in the literature. Furthermore, we employed both human evaluation and clinical semantic similarity measurement alongside word-overlap metrics to improve the depth of quantitative analysis. A human evaluation, conducted by a board-certified radiologist, confirms the model's accuracy in identifying high-level findings, however, it also highlights that more improvement is needed to capture nuanced details and clinical context."}
{"title":"A Survey of Simulators for Autonomous Driving: Taxonomy, Challenges, and Evaluation Metrics","authors":["Yueyuan Li","Wei Yuan","Weihao Yan","Qiyuan Shen","Chunxiang Wang","Ming Yang"],"raw_abstract":"Simulators have irreplaceable importance for the research and development of\nautonomous driving. Besides saving resources, labor, and time, simulation is\nthe only feasible way to reproduce many severe accident scenarios. Despite\ntheir widespread adoption across academia and industry, there is an absence in\nthe evolutionary trajectory of simulators and critical discourse on their\nlimitations.\n  To bridge the gap in research, this paper conducts an in-depth review of\nsimulators for autonomous driving. It delineates the three-decade development\ninto three stages: specialized development period, gap period, and\ncomprehensive development, from which it detects a trend of implementing\ncomprehensive functionalities and open-source accessibility. Then it classifies\nthe simulators by functions, identifying five categories: traffic flow\nsimulator, vehicle dynamics simulator, scenario editor, sensory data generator,\nand driving strategy validator. Simulators that amalgamate diverse features are\ndefined as comprehensive simulators. By investigating commercial and\nopen-source simulators, this paper reveals that the critical issues faced by\nsimulators primarily revolve around fidelity and efficiency concerns. This\npaper justifies that enhancing the realism of adverse weather simulation,\nautomated map reconstruction, and interactive traffic participants will bolster\ncredibility. Concurrently, headless simulation and multiple-speed simulation\ntechniques will exploit the theoretic advantages. Moreover, this paper delves\ninto potential solutions for the identified issues. It explores qualitative and\nquantitative evaluation metrics to assess the simulator's performance. This\npaper guides users to find suitable simulators efficiently and provides\ninstructive suggestions for developers to improve simulator efficacy\npurposefully.","publication_date":1700310641,"paper_link":"http://arxiv.org/pdf/2311.11056v1","categories":["Quantitative Biology"],"abstract":"Simulators have irreplaceable importance for the research and development of autonomous driving. Besides saving resources, labor, and time, simulation is the only feasible way to reproduce many severe accident scenarios. Despite their widespread adoption across academia and industry, there is an absence in the evolutionary trajectory of simulators and critical discourse on their limitations.   To bridge the gap in research, this paper conducts an in-depth review of simulators for autonomous driving. It delineates the three-decade development into three stages: specialized development period, gap period, and comprehensive development, from which it detects a trend of implementing comprehensive functionalities and open-source accessibility. Then it classifies the simulators by functions, identifying five categories: traffic flow simulator, vehicle dynamics simulator, scenario editor, sensory data generator, and driving strategy validator. Simulators that amalgamate diverse features are defined as comprehensive simulators. By investigating commercial and open-source simulators, this paper reveals that the critical issues faced by simulators primarily revolve around fidelity and efficiency concerns. This paper justifies that enhancing the realism of adverse weather simulation, automated map reconstruction, and interactive traffic participants will bolster credibility. Concurrently, headless simulation and multiple-speed simulation techniques will exploit the theoretic advantages. Moreover, this paper delves into potential solutions for the identified issues. It explores qualitative and quantitative evaluation metrics to assess the simulator's performance. This paper guides users to find suitable simulators efficiently and provides instructive suggestions for developers to improve simulator efficacy purposefully."}
{"title":"DenseNet and Support Vector Machine classifications of major depressive disorder using vertex-wise cortical features","authors":["Vladimir Belov","Tracy Erwin-Grabner","Ling-Li Zeng","Christopher R. K. Ching","Andre Aleman","Alyssa R. Amod","Zeynep Basgoze","Francesco Benedetti","Bianca Besteher","Katharina Brosch","Robin B\u00fclow","Romain Colle","Colm G. Connolly","Emmanuelle Corruble","Baptiste Couvy-Duchesne","Kathryn Cullen","Udo Dannlowski","Christopher G. Davey","Annemiek Dols","Jan Ernsting","Jennifer W. Evans","Lukas Fisch","Paola Fuentes-Claramonte","Ali Saffet Gonul","Ian H. Gotlib","Hans J. Grabe","Nynke A. Groenewold","Dominik Grotegerd","Tim Hahn","J. Paul Hamilton","Laura K. M. Han","Ben J Harrison","Tiffany C. Ho","Neda Jahanshad","Alec J. Jamieson","Andriana Karuk","Tilo Kircher","Bonnie Klimes-Dougan","Sheri-Michelle Koopowitz","Thomas Lancaster","Ramona Leenings","Meng Li","David E. J. Linden","Frank P. MacMaster","David M. A. Mehler","Susanne Meinert","Elisa Melloni","Bryon A. Mueller","Benson Mwangi","Igor Nenadi\u0107","Amar Ojha","Yasumasa Okamoto","Mardien L. Oudega","Brenda W. J. H. Penninx","Sara Poletti","Edith Pomarol-Clotet","Maria J. Portella","Elena Pozzi","Joaquim Radua","Elena Rodr\u00edguez-Cano","Matthew D. Sacchet","Raymond Salvador","Anouk Schrantee","Kang Sim","Jair C. Soares","Aleix Solanes","Dan J. Stein","Frederike Stein","Aleks Stolicyn","Sophia I. Thomopoulos","Yara J. Toenders","Aslihan Uyar-Demir","Eduard Vieta","Yolanda Vives-Gilabert","Henry V\u00f6lzke","Martin Walter","Heather C. Whalley","Sarah Whittle","Nils Winter","Katharina Wittfeld","Margaret J. Wright","Mon-Ju Wu","Tony T. Yang","Carlos Zarate","Dick J. Veltman","Lianne Schmaal","Paul M. Thompson","Roberto Goya-Maldonado"],"raw_abstract":"Major depressive disorder (MDD) is a complex psychiatric disorder that\naffects the lives of hundreds of millions of individuals around the globe. Even\ntoday, researchers debate if morphological alterations in the brain are linked\nto MDD, likely due to the heterogeneity of this disorder. The application of\ndeep learning tools to neuroimaging data, capable of capturing complex\nnon-linear patterns, has the potential to provide diagnostic and predictive\nbiomarkers for MDD. However, previous attempts to demarcate MDD patients and\nhealthy controls (HC) based on segmented cortical features via linear machine\nlearning approaches have reported low accuracies. In this study, we used\nglobally representative data from the ENIGMA-MDD working group containing an\nextensive sample of people with MDD (N=2,772) and HC (N=4,240), which allows a\ncomprehensive analysis with generalizable results. Based on the hypothesis that\nintegration of vertex-wise cortical features can improve classification\nperformance, we evaluated the classification of a DenseNet and a Support Vector\nMachine (SVM), with the expectation that the former would outperform the\nlatter. As we analyzed a multi-site sample, we additionally applied the ComBat\nharmonization tool to remove potential nuisance effects of site. We found that\nboth classifiers exhibited close to chance performance (balanced accuracy\nDenseNet: 51%; SVM: 53%), when estimated on unseen sites. Slightly higher\nclassification performance (balanced accuracy DenseNet: 58%; SVM: 55%) was\nfound when the cross-validation folds contained subjects from all sites,\nindicating site effect. In conclusion, the integration of vertex-wise\nmorphometric features and the use of the non-linear classifier did not lead to\nthe differentiability between MDD and HC. Our results support the notion that\nMDD classification on this combination of features and classifiers is\nunfeasible.","publication_date":1700307985,"paper_link":"http://arxiv.org/pdf/2311.11046v1","categories":["Quantitative Biology"],"abstract":"Major depressive disorder (MDD) is a complex psychiatric disorder that affects the lives of hundreds of millions of individuals around the globe. Even today, researchers debate if morphological alterations in the brain are linked to MDD, likely due to the heterogeneity of this disorder. The application of deep learning tools to neuroimaging data, capable of capturing complex non-linear patterns, has the potential to provide diagnostic and predictive biomarkers for MDD. However, previous attempts to demarcate MDD patients and healthy controls (HC) based on segmented cortical features via linear machine learning approaches have reported low accuracies. In this study, we used globally representative data from the ENIGMA-MDD working group containing an extensive sample of people with MDD (N=2,772) and HC (N=4,240), which allows a comprehensive analysis with generalizable results. Based on the hypothesis that integration of vertex-wise cortical features can improve classification performance, we evaluated the classification of a DenseNet and a Support Vector Machine (SVM), with the expectation that the former would outperform the latter. As we analyzed a multi-site sample, we additionally applied the ComBat harmonization tool to remove potential nuisance effects of site. We found that both classifiers exhibited close to chance performance (balanced accuracy DenseNet: 51%; SVM: 53%), when estimated on unseen sites. Slightly higher classification performance (balanced accuracy DenseNet: 58%; SVM: 55%) was found when the cross-validation folds contained subjects from all sites, indicating site effect. In conclusion, the integration of vertex-wise morphometric features and the use of the non-linear classifier did not lead to the differentiability between MDD and HC. Our results support the notion that MDD classification on this combination of features and classifiers is unfeasible."}
{"title":"3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing","authors":["Haoran Li","Long Ma","Yong Liao","Lechao Cheng","Yanbin Hao","Pengyuan Zhou"],"raw_abstract":"The current GAN inversion methods typically can only edit the appearance and\nshape of a single object and background while overlooking spatial information.\nIn this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted\nediting of affine information (scale, translation, and rotation) on multiple\nobjects. 3D-GOI realizes the complex editing function by inverting the\nabundance of attribute codes (object\nshape/appearance/scale/rotation/translation, background shape/appearance, and\ncamera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all\nthe codes is challenging, 3D-GOI solves this challenge following three main\nsteps. First, we segment the objects and the background in a multi-object\nimage. Second, we use a custom Neural Inversion Encoder to obtain coarse codes\nof each object. Finally, we use a round-robin optimization algorithm to get\nprecise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is\nthe first framework to enable multifaceted editing on multiple objects. Both\nqualitative and quantitative experiments demonstrate that 3D-GOI holds immense\npotential for flexible, multifaceted editing in complex multi-object scenes.","publication_date":1700301356,"paper_link":"http://arxiv.org/pdf/2311.12050v1","categories":["Quantitative Biology"],"abstract":"The current GAN inversion methods typically can only edit the appearance and shape of a single object and background while overlooking spatial information. In this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted editing of affine information (scale, translation, and rotation) on multiple objects. 3D-GOI realizes the complex editing function by inverting the abundance of attribute codes (object shape/appearance/scale/rotation/translation, background shape/appearance, and camera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all the codes is challenging, 3D-GOI solves this challenge following three main steps. First, we segment the objects and the background in a multi-object image. Second, we use a custom Neural Inversion Encoder to obtain coarse codes of each object. Finally, we use a round-robin optimization algorithm to get precise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is the first framework to enable multifaceted editing on multiple objects. Both qualitative and quantitative experiments demonstrate that 3D-GOI holds immense potential for flexible, multifaceted editing in complex multi-object scenes."}
{"title":"A Foundation Model for Cell Segmentation","authors":["Uriah Israel","Markus Marks","Rohit Dilip","Qilin Li","Morgan Schwartz","Elora Pradhan","Edward Pao","Shenyi Li","Alexander Pearson-Goulart","Pietro Perona","Georgia Gkioxari","Ross Barnowski","Yisong Yue","David Van Valen"],"raw_abstract":"Cells are the fundamental unit of biological organization, and identifying\nthem in imaging data - cell segmentation - is a critical task for various\ncellular imaging experiments. While deep learning methods have led to\nsubstantial progress on this problem, models that have seen wide use are\nspecialist models that work well for specific domains. Methods that have\nlearned the general notion of \"what is a cell\" and can identify them across\ndifferent domains of cellular imaging data have proven elusive. In this work,\nwe present CellSAM, a foundation model for cell segmentation that generalizes\nacross diverse cellular imaging data. CellSAM builds on top of the Segment\nAnything Model (SAM) by developing a prompt engineering approach to mask\ngeneration. We train an object detector, CellFinder, to automatically detect\ncells and prompt SAM to generate segmentations. We show that this approach\nallows a single model to achieve state-of-the-art performance for segmenting\nimages of mammalian cells (in tissues and cell culture), yeast, and bacteria\ncollected with various imaging modalities. To enable accessibility, we\nintegrate CellSAM into DeepCell Label to further accelerate human-in-the-loop\nlabeling strategies for cellular imaging data. A deployed version of CellSAM is\navailable at https://label-dev.deepcell.org/.","publication_date":1700294109,"paper_link":"http://arxiv.org/pdf/2311.11004v1","categories":["Quantitative Biology"],"abstract":"Cells are the fundamental unit of biological organization, and identifying them in imaging data - cell segmentation - is a critical task for various cellular imaging experiments. While deep learning methods have led to substantial progress on this problem, models that have seen wide use are specialist models that work well for specific domains. Methods that have learned the general notion of \"what is a cell\" and can identify them across different domains of cellular imaging data have proven elusive. In this work, we present CellSAM, a foundation model for cell segmentation that generalizes across diverse cellular imaging data. CellSAM builds on top of the Segment Anything Model (SAM) by developing a prompt engineering approach to mask generation. We train an object detector, CellFinder, to automatically detect cells and prompt SAM to generate segmentations. We show that this approach allows a single model to achieve state-of-the-art performance for segmenting images of mammalian cells (in tissues and cell culture), yeast, and bacteria collected with various imaging modalities. To enable accessibility, we integrate CellSAM into DeepCell Label to further accelerate human-in-the-loop labeling strategies for cellular imaging data. A deployed version of CellSAM is available at https://label-dev.deepcell.org/."}
{"title":"\"Centralized or Decentralized?\": Concerns and Value Judgments of Stakeholders in the Non-Fungible Tokens (NFTs) Market","authors":["Yunpeng Xiao","Bufan Deng","Siqi Chen","Kyrie Zhixuan Zhou","Ray LC","Luyao Zhang","Xin Tong"],"raw_abstract":"Non-fungible tokens (NFTs) are decentralized digital tokens to represent the\nunique ownership of items. Recently, NFTs have been gaining popularity and at\nthe same time bringing up issues, such as scams, racism, and sexism.\nDecentralization, a key attribute of NFT, contributes to some of the issues\nthat are easier to regulate under centralized schemes, which are intentionally\nleft out of the NFT marketplace. In this work, we delved into this\ncentralization-decentralization dilemma in the NFT space through mixed\nquantitative and qualitative methods. Centralization-decentralization dilemma\nis the dilemma caused by the conflict between the slogan of decentralization\nand the interests of stakeholders. We first analyzed over 30,000 NFT-related\ntweets to obtain a high-level understanding of stakeholders' concerns in the\nNFT space. We then interviewed 15 NFT stakeholders (both creators and\ncollectors) to obtain their in-depth insights into these concerns and potential\nsolutions. Our findings identify concerning issues among users: financial\nscams, counterfeit NFTs, hacking, and unethical NFTs. We further reflected on\nthe centralization-decentralization dilemma drawing upon the perspectives of\nthe stakeholders in the interviews. Finally, we gave some inferences to solve\nthe centralization-decentralization dilemma in the NFT market and thought about\nthe future of NFT and decentralization.","publication_date":1700290222,"paper_link":"http://arxiv.org/pdf/2311.10990v2","categories":["Quantitative Biology"],"abstract":"Non-fungible tokens (NFTs) are decentralized digital tokens to represent the unique ownership of items. Recently, NFTs have been gaining popularity and at the same time bringing up issues, such as scams, racism, and sexism. Decentralization, a key attribute of NFT, contributes to some of the issues that are easier to regulate under centralized schemes, which are intentionally left out of the NFT marketplace. In this work, we delved into this centralization-decentralization dilemma in the NFT space through mixed quantitative and qualitative methods. Centralization-decentralization dilemma is the dilemma caused by the conflict between the slogan of decentralization and the interests of stakeholders. We first analyzed over 30,000 NFT-related tweets to obtain a high-level understanding of stakeholders' concerns in the NFT space. We then interviewed 15 NFT stakeholders (both creators and collectors) to obtain their in-depth insights into these concerns and potential solutions. Our findings identify concerning issues among users: financial scams, counterfeit NFTs, hacking, and unethical NFTs. We further reflected on the centralization-decentralization dilemma drawing upon the perspectives of the stakeholders in the interviews. Finally, we gave some inferences to solve the centralization-decentralization dilemma in the NFT market and thought about the future of NFT and decentralization."}
{"title":"OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy","authors":["Xin Tian","Nantheera Anantrasirichai","Lindsay Nicholson","Alin Achim"],"raw_abstract":"Optical coherence tomography (OCT) and confocal microscopy are pivotal in\nretinal imaging, each presenting unique benefits and limitations. In vivo OCT\noffers rapid, non-invasive imaging but can be hampered by clarity issues and\nmotion artifacts. Ex vivo confocal microscopy provides high-resolution,\ncellular detailed color images but is invasive and poses ethical concerns and\npotential tissue damage. To bridge these modalities, we developed a 3D CycleGAN\nframework for unsupervised translation of in vivo OCT to ex vivo confocal\nmicroscopy images. Applied to our OCT2Confocal dataset, this framework\neffectively translates between 3D medical data domains, capturing vascular,\ntextural, and cellular details with precision. This marks the first attempt to\nexploit the inherent 3D information of OCT and translate it into the rich,\ndetailed color domain of confocal microscopy. Assessed through quantitative and\nqualitative metrics, the 3D CycleGAN framework demonstrates commendable image\nfidelity and quality, outperforming existing methods despite the constraints of\nlimited data. This non-invasive generation of retinal confocal images has the\npotential to further enhance diagnostic and monitoring capabilities in\nophthalmology.","publication_date":1700261330,"paper_link":"http://arxiv.org/pdf/2311.10902v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Optical coherence tomography (OCT) and confocal microscopy are pivotal in retinal imaging, each presenting unique benefits and limitations. In vivo OCT offers rapid, non-invasive imaging but can be hampered by clarity issues and motion artifacts. Ex vivo confocal microscopy provides high-resolution, cellular detailed color images but is invasive and poses ethical concerns and potential tissue damage. To bridge these modalities, we developed a 3D CycleGAN framework for unsupervised translation of in vivo OCT to ex vivo confocal microscopy images. Applied to our OCT2Confocal dataset, this framework effectively translates between 3D medical data domains, capturing vascular, textural, and cellular details with precision. This marks the first attempt to exploit the inherent 3D information of OCT and translate it into the rich, detailed color domain of confocal microscopy. Assessed through quantitative and qualitative metrics, the 3D CycleGAN framework demonstrates commendable image fidelity and quality, outperforming existing methods despite the constraints of limited data. This non-invasive generation of retinal confocal images has the potential to further enhance diagnostic and monitoring capabilities in ophthalmology."}
{"title":"Uniform Approximation by Polynomials with Integer Coefficients via the Bernstein Lattice","authors":["C. Sinan G\u00fcnt\u00fcrk","Weilin Li"],"raw_abstract":"Let $\\mathscr{C}_\\mathbb{Z}([0,1])$ be the metric space of real-valued\ncontinuous functions on $[0,1]$ with integer values at $0$ and $1$, equipped\nwith the uniform (supremum) metric $d_\\infty$. It is a classical theorem in\napproximation theory that the ring $\\mathbb{Z}[X]$ of polynomials with integer\ncoefficients, when considered as a set of functions on $[0,1]$, is dense in\n$\\mathscr{C}_\\mathbb{Z}([0,1])$. In this paper, we offer a strengthening of\nthis result by identifying a substantially small subset $\\bigcup_n\n\\mathscr{B}_n$ of $\\mathbb{Z}[X]$ which is still dense in\n$\\mathscr{C}_\\mathbb{Z}([0,1])$. Here $\\mathscr{B}_n$, which we call the\n``Bernstein lattice,'' is the lattice generated by the polynomials $$p_{n,k}(x)\n:= \\binom{n}{k} x^k(1-x)^{n-k}, ~~k=0,\\dots,n.$$ Quantitatively, we show that\nfor any $f \\in \\mathscr{C}_\\mathbb{Z}([0,1])$, $$d_\\infty(f, \\mathscr{B}_n)\n\\leq \\frac{9}{4} \\omega_f(n^{-1/3}) + 2 n^{-1/3}, ~~n \\geq 1,$$ where\n$\\omega_f$ stands for the modulus of continuity of $f$. We also offer a more\ngeneral bound which can be optimized to yield better decay of approximation\nerror for specific classes of continuous functions.","publication_date":1700261067,"paper_link":"http://arxiv.org/pdf/2311.10901v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be the metric space of real-valued continuous functions on __FORMULA__ with integer values at __FORMULA__ and __FORMULA__, equipped with the uniform (supremum) metric __FORMULA__. It is a classical theorem in approximation theory that the ring __FORMULA__ of polynomials with integer coefficients, when considered as a set of functions on __FORMULA__, is dense in __FORMULA__. In this paper, we offer a strengthening of this result by identifying a substantially small subset __FORMULA__ of __FORMULA__ which is still dense in __FORMULA__. Here __FORMULA__, which we call the ``Bernstein lattice,'' is the lattice generated by the polynomials $__FORMULA____FORMULA__f \\in C_Z([0,1])__FORMULA____FORMULA____FORMULA__\\omega_f__FORMULA__f$. We also offer a more general bound which can be optimized to yield better decay of approximation error for specific classes of continuous functions."}
{"title":"A Video-Based Activity Classification of Human Pickers in Agriculture","authors":["Abhishesh Pal","Antonio C. Leite","Jon G. O. Gjevestad","P\u00e5l J. From"],"raw_abstract":"In farming systems, harvesting operations are tedious, time- and\nresource-consuming tasks. Based on this, deploying a fleet of autonomous robots\nto work alongside farmworkers may provide vast productivity and logistics\nbenefits. Then, an intelligent robotic system should monitor human behavior,\nidentify the ongoing activities and anticipate the worker's needs. In this\nwork, the main contribution consists of creating a benchmark model for\nvideo-based human pickers detection, classifying their activities to serve in\nharvesting operations for different agricultural scenarios. Our solution uses\nthe combination of a Mask Region-based Convolutional Neural Network (Mask\nR-CNN) for object detection and optical flow for motion estimation with newly\nadded statistical attributes of flow motion descriptors, named as Correlation\nSensitivity (CS). A classification criterion is defined based on the Kernel\nDensity Estimation (KDE) analysis and K-means clustering algorithm, which are\nimplemented upon in-house collected dataset from different crop fields like\nstrawberry polytunnels and apple tree orchards. The proposed framework is\nquantitatively analyzed using sensitivity, specificity, and accuracy measures\nand shows satisfactory results amidst various dataset challenges such as\nlighting variation, blur, and occlusions.","publication_date":1700258562,"paper_link":"http://arxiv.org/pdf/2311.10885v1","categories":["Quantitative Biology"],"abstract":"In farming systems, harvesting operations are tedious, time- and resource-consuming tasks. Based on this, deploying a fleet of autonomous robots to work alongside farmworkers may provide vast productivity and logistics benefits. Then, an intelligent robotic system should monitor human behavior, identify the ongoing activities and anticipate the worker's needs. In this work, the main contribution consists of creating a benchmark model for video-based human pickers detection, classifying their activities to serve in harvesting operations for different agricultural scenarios. Our solution uses the combination of a Mask Region-based Convolutional Neural Network (Mask R-CNN) for object detection and optical flow for motion estimation with newly added statistical attributes of flow motion descriptors, named as Correlation Sensitivity (CS). A classification criterion is defined based on the Kernel Density Estimation (KDE) analysis and K-means clustering algorithm, which are implemented upon in-house collected dataset from different crop fields like strawberry polytunnels and apple tree orchards. The proposed framework is quantitatively analyzed using sensitivity, specificity, and accuracy measures and shows satisfactory results amidst various dataset challenges such as lighting variation, blur, and occlusions."}
{"title":"Pre- to Post-Contrast Breast MRI Synthesis for Enhanced Tumour Segmentation","authors":["Richard Osuala","Smriti Joshi","Apostolia Tsirikoglou","Lidia Garrucho","Walter H. L. Pinaya","Oliver Diaz","Karim Lekadir"],"raw_abstract":"Despite its benefits for tumour detection and treatment, the administration\nof contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated\nwith a range of issues, including their invasiveness, bioaccumulation, and a\nrisk of nephrogenic systemic fibrosis. This study explores the feasibility of\nproducing synthetic contrast enhancements by translating pre-contrast\nT1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI\nsequence leveraging the capabilities of a generative adversarial network (GAN).\nAdditionally, we introduce a Scaled Aggregate Measure (SAMe) designed for\nquantitatively evaluating the quality of synthetic data in a principled manner\nand serving as a basis for selecting the optimal generative model. We assess\nthe generated DCE-MRI data using quantitative image quality metrics and apply\nthem to the downstream task of 3D breast tumour segmentation. Our results\nhighlight the potential of post-contrast DCE-MRI synthesis in enhancing the\nrobustness of breast tumour segmentation models via data augmentation. Our code\nis available at https://github.com/RichardObi/pre_post_synthesis.","publication_date":1700257721,"paper_link":"http://arxiv.org/pdf/2311.10879v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Despite its benefits for tumour detection and treatment, the administration of contrast agents in dynamic contrast-enhanced MRI (DCE-MRI) is associated with a range of issues, including their invasiveness, bioaccumulation, and a risk of nephrogenic systemic fibrosis. This study explores the feasibility of producing synthetic contrast enhancements by translating pre-contrast T1-weighted fat-saturated breast MRI to their corresponding first DCE-MRI sequence leveraging the capabilities of a generative adversarial network (GAN). Additionally, we introduce a Scaled Aggregate Measure (SAMe) designed for quantitatively evaluating the quality of synthetic data in a principled manner and serving as a basis for selecting the optimal generative model. We assess the generated DCE-MRI data using quantitative image quality metrics and apply them to the downstream task of 3D breast tumour segmentation. Our results highlight the potential of post-contrast DCE-MRI synthesis in enhancing the robustness of breast tumour segmentation models via data augmentation. Our code is available at https://github.com/RichardObi/pre_post_synthesis."}
{"title":"MSPB: a longitudinal multi-sensor dataset with phenotypic trait measurements from honey bees","authors":["Yi Zhu","Mahsa Abdollahi","S\u00e9gol\u00e8ne Maucourt","Nico Coallier","Heitor R. Guimar\u00e3es","Pierre Giovenazzo","Tiago H. Falk"],"raw_abstract":"We present a longitudinal multi-sensor dataset collected from honey bee\ncolonies (Apis mellifera) with rich phenotypic measurements. Data were\ncontinuously collected between May-2020 and April-2021 from 53 hives located at\ntwo apiaries in Qu\\'ebec, Canada. The sensor data included audio features,\ntemperature, and relative humidity. The phenotypic measurements contained\nbeehive population, number of brood cells (eggs, larva and pupa), Varroa\ndestructor infestation levels, defensive and hygienic behaviors, honey yield,\nand winter mortality. Our study is amongst the first to provide a wide variety\nof phenotypic trait measurements annotated by apicultural science experts,\nwhich facilitate a broader scope of analysis. We first summarize the data\ncollection procedure, sensor data pre-processing steps, and data composition.\nWe then provide an overview of the phenotypic data distribution as well as a\nvisualization of the sensor data patterns. Lastly, we showcase several hive\nmonitoring applications based on sensor data analysis and machine learning,\nsuch as winter mortality prediction, hive population estimation, and the\npresence of an active and laying queen.","publication_date":1700256909,"paper_link":"http://arxiv.org/pdf/2311.10876v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"We present a longitudinal multi-sensor dataset collected from honey bee colonies (Apis mellifera) with rich phenotypic measurements. Data were continuously collected between May-2020 and April-2021 from 53 hives located at two apiaries in Qu\\'ebec, Canada. The sensor data included audio features, temperature, and relative humidity. The phenotypic measurements contained beehive population, number of brood cells (eggs, larva and pupa), Varroa destructor infestation levels, defensive and hygienic behaviors, honey yield, and winter mortality. Our study is amongst the first to provide a wide variety of phenotypic trait measurements annotated by apicultural science experts, which facilitate a broader scope of analysis. We first summarize the data collection procedure, sensor data pre-processing steps, and data composition. We then provide an overview of the phenotypic data distribution as well as a visualization of the sensor data patterns. Lastly, we showcase several hive monitoring applications based on sensor data analysis and machine learning, such as winter mortality prediction, hive population estimation, and the presence of an active and laying queen."}
{"title":"Imaging and simulation-based analysis of evaporation flows over wetting edges","authors":["Suraj Raju","Felix Braig","Mathis Fricke","Dirk Gr\u00fcnding","Edgar D\u00f6rsam","Hans Martin Sauer","Dieter Bothe"],"raw_abstract":"We monitor the evaporation of a volatile liquid (ethanol) from an\ninkjet-printed liquid film, consisting of a mixture of ethanol and ethylene\nglycol. Interferometric video imaging technology is used for recording 2D vapor\nconcentration profiles over the evaporating film. The vapor flow is\nreconstructed using numerical simulations. In this way, we reconstruct the\ncomplete flow velocity profile, and distinguish diffusive and convective gas\ntransport flows, with quantitative tracking of the transport balances. The\nconvective flows are driven by the buoyancy of the solvent vapor in the ambient\nair. In particular, we reconstruct the evaporation process from the interface\nof the two-component liquid. We monitor the evaporation flows, implement\nRaoult's and Henry's laws of vapor pressure reduction, as well as evaporation\nresistivity. We observe the edge-enhancement of evaporation flows at the\nwetting rims of the liquid film, and decompose the vapor flows in the diffusive\nand the convective contribution. We demonstrate how Langmuir's evaporation\nresistivity can be identified using vapor pressure profiles in the gas phase\ndata and mass transfer balances.","publication_date":1700254687,"paper_link":"http://arxiv.org/pdf/2311.10867v1","categories":["Physics"],"abstract":"We monitor the evaporation of a volatile liquid (ethanol) from an inkjet-printed liquid film, consisting of a mixture of ethanol and ethylene glycol. Interferometric video imaging technology is used for recording 2D vapor concentration profiles over the evaporating film. The vapor flow is reconstructed using numerical simulations. In this way, we reconstruct the complete flow velocity profile, and distinguish diffusive and convective gas transport flows, with quantitative tracking of the transport balances. The convective flows are driven by the buoyancy of the solvent vapor in the ambient air. In particular, we reconstruct the evaporation process from the interface of the two-component liquid. We monitor the evaporation flows, implement Raoult's and Henry's laws of vapor pressure reduction, as well as evaporation resistivity. We observe the edge-enhancement of evaporation flows at the wetting rims of the liquid film, and decompose the vapor flows in the diffusive and the convective contribution. We demonstrate how Langmuir's evaporation resistivity can be identified using vapor pressure profiles in the gas phase data and mass transfer balances."}
{"title":"Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation","authors":["Zhuoran Zhou","Zhongyu Jiang","Wenhao Chai","Cheng-Yen Yang","Lei Li","Jenq-Neng Hwang"],"raw_abstract":"Although 3D human pose estimation has gained impressive development in recent\nyears, only a few works focus on infants, that have different bone lengths and\nalso have limited data. Directly applying adult pose estimation models\ntypically achieves low performance in the infant domain and suffers from\nout-of-distribution issues. Moreover, the limitation of infant pose data\ncollection also heavily constrains the efficiency of learning-based models to\nlift 2D poses to 3D. To deal with the issues of small datasets, domain\nadaptation and data augmentation are commonly used techniques. Following this\nparadigm, we take advantage of an optimization-based method that utilizes\ngenerative priors to predict 3D infant keypoints from 2D keypoints without the\nneed of large training data. We further apply a guided diffusion model to\ndomain adapt 3D adult pose to infant pose to supplement small datasets.\nBesides, we also prove that our method, ZeDO-i, could attain efficient domain\nadaptation, even if only a small number of data is given. Quantitatively, we\nclaim that our model attains state-of-the-art MPJPE performance of 43.6 mm on\nthe SyRIP dataset and 21.2 mm on the MINI-RGBD dataset.","publication_date":1700254177,"paper_link":"http://arxiv.org/pdf/2311.12043v1","categories":["Quantitative Biology"],"abstract":"Although 3D human pose estimation has gained impressive development in recent years, only a few works focus on infants, that have different bone lengths and also have limited data. Directly applying adult pose estimation models typically achieves low performance in the infant domain and suffers from out-of-distribution issues. Moreover, the limitation of infant pose data collection also heavily constrains the efficiency of learning-based models to lift 2D poses to 3D. To deal with the issues of small datasets, domain adaptation and data augmentation are commonly used techniques. Following this paradigm, we take advantage of an optimization-based method that utilizes generative priors to predict 3D infant keypoints from 2D keypoints without the need of large training data. We further apply a guided diffusion model to domain adapt 3D adult pose to infant pose to supplement small datasets. Besides, we also prove that our method, ZeDO-i, could attain efficient domain adaptation, even if only a small number of data is given. Quantitatively, we claim that our model attains state-of-the-art MPJPE performance of 43.6 mm on the SyRIP dataset and 21.2 mm on the MINI-RGBD dataset."}
{"title":"GhostVec: A New Threat to Speaker Privacy of End-to-End Speech Recognition System","authors":["Xiaojiao Chen","Sheng Li","Jiyi Li","Hao Huang","Yang Cao","Liang He"],"raw_abstract":"Speaker adaptation systems face privacy concerns, for such systems are\ntrained on private datasets and often overfitting. This paper demonstrates that\nan attacker can extract speaker information by querying speaker-adapted speech\nrecognition (ASR) systems. We focus on the speaker information of a\ntransformer-based ASR and propose GhostVec, a simple and efficient attack\nmethod to extract the speaker information from an encoder-decoder-based ASR\nsystem without any external speaker verification system or natural human voice\nas a reference. To make our results quantitative, we pre-process GhostVec using\nsingular value decomposition (SVD) and synthesize it into waveform. Experiment\nresults show that the synthesized audio of GhostVec reaches 10.83\\% EER and\n0.47 minDCF with target speakers, which suggests the effectiveness of the\nproposed method. We hope the preliminary discovery in this study to catalyze\nfuture speech recognition research on privacy-preserving topics.","publication_date":1700245219,"paper_link":"http://arxiv.org/pdf/2311.10689v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Speaker adaptation systems face privacy concerns, for such systems are trained on private datasets and often overfitting. This paper demonstrates that an attacker can extract speaker information by querying speaker-adapted speech recognition (ASR) systems. We focus on the speaker information of a transformer-based ASR and propose GhostVec, a simple and efficient attack method to extract the speaker information from an encoder-decoder-based ASR system without any external speaker verification system or natural human voice as a reference. To make our results quantitative, we pre-process GhostVec using singular value decomposition (SVD) and synthesize it into waveform. Experiment results show that the synthesized audio of GhostVec reaches 10.83\\% EER and 0.47 minDCF with target speakers, which suggests the effectiveness of the proposed method. We hope the preliminary discovery in this study to catalyze future speech recognition research on privacy-preserving topics."}
{"title":"Learning Realistic Joint Space Boundaries for Range of Motion Analysis of Healthy and Impaired Human Arms","authors":["Shafagh Keyvanian","Michelle J. Johnson","Nadia Figueroa"],"raw_abstract":"A realistic human kinematic model that satisfies anatomical constraints is\nessential for human-robot interaction, biomechanics and robot-assisted\nrehabilitation. Modeling realistic joint constraints, however, is challenging\nas human arm motion is constrained by joint limits, inter- and intra-joint\ndependencies, self-collisions, individual capabilities and muscular or\nneurological constraints which are difficult to represent. Hence, physicians\nand researchers have relied on simple box-constraints, ignoring important\nanatomical factors. In this paper, we propose a data-driven method to learn\nrealistic anatomically constrained upper-limb range of motion (RoM) boundaries\nfrom motion capture data. This is achieved by fitting a one-class support\nvector machine to a dataset of upper-limb joint space exploration motions with\nan efficient hyper-parameter tuning scheme. Our approach outperforms similar\nworks focused on valid RoM learning. Further, we propose an impairment index\n(II) metric that offers a quantitative assessment of capability/impairment when\ncomparing healthy and impaired arms. We validate the metric on healthy subjects\nphysically constrained to emulate hemiplegia and different disability levels as\nstroke patients.","publication_date":1700241282,"paper_link":"http://arxiv.org/pdf/2311.10653v1","categories":["Quantitative Biology"],"abstract":"A realistic human kinematic model that satisfies anatomical constraints is essential for human-robot interaction, biomechanics and robot-assisted rehabilitation. Modeling realistic joint constraints, however, is challenging as human arm motion is constrained by joint limits, inter- and intra-joint dependencies, self-collisions, individual capabilities and muscular or neurological constraints which are difficult to represent. Hence, physicians and researchers have relied on simple box-constraints, ignoring important anatomical factors. In this paper, we propose a data-driven method to learn realistic anatomically constrained upper-limb range of motion (RoM) boundaries from motion capture data. This is achieved by fitting a one-class support vector machine to a dataset of upper-limb joint space exploration motions with an efficient hyper-parameter tuning scheme. Our approach outperforms similar works focused on valid RoM learning. Further, we propose an impairment index (II) metric that offers a quantitative assessment of capability/impairment when comparing healthy and impaired arms. We validate the metric on healthy subjects physically constrained to emulate hemiplegia and different disability levels as stroke patients."}
{"title":"Image-Domain Material Decomposition for Dual-energy CT using Unsupervised Learning with Data-fidelity Loss","authors":["Junbo Peng","Chih-Wei Chang","Huiqiao Xie","Richard L. J. Qiu","Justin Roper","Tonghe Wang","Beth Bradshaw","Xiangyang Tang","Xiaofeng Yang"],"raw_abstract":"Background: Dual-energy CT (DECT) and material decomposition play vital roles\nin quantitative medical imaging. However, the decomposition process may suffer\nfrom significant noise amplification, leading to severely degraded image\nsignal-to-noise ratios (SNRs). While existing iterative algorithms perform\nnoise suppression using different image priors, these heuristic image priors\ncannot accurately represent the features of the target image manifold. Although\ndeep learning-based decomposition methods have been reported, these methods are\nin the supervised-learning framework requiring paired data for training, which\nis not readily available in clinical settings.\n  Purpose: This work aims to develop an unsupervised-learning framework with\ndata-measurement consistency for image-domain material decomposition in DECT.","publication_date":1700240296,"paper_link":"http://arxiv.org/pdf/2311.10641v1","categories":["Electrical Engineering and Systems Science","Physics"],"abstract":"Background: Dual-energy CT (DECT) and material decomposition play vital roles in quantitative medical imaging. However, the decomposition process may suffer from significant noise amplification, leading to severely degraded image signal-to-noise ratios (SNRs). While existing iterative algorithms perform noise suppression using different image priors, these heuristic image priors cannot accurately represent the features of the target image manifold. Although deep learning-based decomposition methods have been reported, these methods are in the supervised-learning framework requiring paired data for training, which is not readily available in clinical settings.   Purpose: This work aims to develop an unsupervised-learning framework with data-measurement consistency for image-domain material decomposition in DECT."}
{"title":"Multi-delay arterial spin-labeled perfusion estimation with biophysics simulation and deep learning","authors":["Renjiu Hu","Qihao Zhang","Pascal Spincemaille","Thanh D. Nguyen","Yi Wang"],"raw_abstract":"Purpose: To develop biophysics-based method for estimating perfusion Q from\narterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net\n(QTMnet) was trained to estimate perfusion from 4D tracer propagation images.\nThe network was trained and tested on simulated 4D tracer concentration data\nbased on artificial vasculature structure generated by constrained constructive\noptimization (CCO) method. The trained network was further tested in a\nsynthetic brain ASL image based on vasculature network extracted from magnetic\nresonance (MR) angiography. The estimations from both trained network and a\nconventional kinetic model were compared in ASL images acquired from eight\nhealthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from\nconcentration data. Relative error of the synthetic brain ASL image was 7.04%\nfor perfusion Q, lower than the error using single-delay ASL model: 25.15% for\nQ, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet\nprovides accurate estimation on perfusion parameters and is a promising\napproach as a clinical ASL MRI image processing pipeline.","publication_date":1700240114,"paper_link":"http://arxiv.org/pdf/2311.10640v1","categories":["Quantitative Biology"],"abstract":"Purpose: To develop biophysics-based method for estimating perfusion Q from arterial spin labeling (ASL) images using deep learning. Methods: A 3D U-Net (QTMnet) was trained to estimate perfusion from 4D tracer propagation images. The network was trained and tested on simulated 4D tracer concentration data based on artificial vasculature structure generated by constrained constructive optimization (CCO) method. The trained network was further tested in a synthetic brain ASL image based on vasculature network extracted from magnetic resonance (MR) angiography. The estimations from both trained network and a conventional kinetic model were compared in ASL images acquired from eight healthy volunteers. Results: QTMnet accurately reconstructed perfusion Q from concentration data. Relative error of the synthetic brain ASL image was 7.04% for perfusion Q, lower than the error using single-delay ASL model: 25.15% for Q, and multi-delay ASL model: 12.62% for perfusion Q. Conclusion: QTMnet provides accurate estimation on perfusion parameters and is a promising approach as a clinical ASL MRI image processing pipeline."}
{"title":"Growth-rate distributions of gut microbiota time series: neutral models and temporal dependence","authors":["E. Brigatti","S. Azaele"],"raw_abstract":"Logarithmic growth-rates are fundamental observables for describing\necological systems and the characterization of their distributions with\nanalytical techniques can greatly improve their comprehension. Here a neutral\nmodel based on a stochastic differential equation with demographic noise, which\npresents a closed form for these distributions, is used to describe the\npopulation dynamics of microbiota. Results show that this model can\nsuccessfully reproduce the log-growth rate distribution of the considered\nabundance time-series. More significantly, it predicts its temporal dependence,\nby reproducing its kurtosis evolution when the time lag $\\tau$ is increased.\nFurthermore, its typical shape for large $\\tau$ is assessed, verifying that the\ndistribution variance does not diverge with $\\tau$. The simulated processes\ngenerated by the calibrated stochastic equation and the analysis of each\ntime-series, taken one by one, provided additional support for our approach.\nAlternatively, we tried to describe our dataset by using a logistic model with\nan environmental stochastic term. Analytical and numerical results show that\nthis model is not suited for describing the leptokurtic log-growth rates\ndistribution found in our data. These results effectively support a neutral\nmodel with demographic stochasticity for describing the growth-rate dynamics\nand the stationary abundance distribution of the considered microbiota. This\nsuggests that there are no significant parametric demographic differences among\nthe species, which can be statistically characterized by the same vital rates.","publication_date":1700238223,"paper_link":"http://arxiv.org/pdf/2311.10624v1","categories":["Quantitative Biology"],"abstract":"Logarithmic growth-rates are fundamental observables for describing ecological systems and the characterization of their distributions with analytical techniques can greatly improve their comprehension. Here a neutral model based on a stochastic differential equation with demographic noise, which presents a closed form for these distributions, is used to describe the population dynamics of microbiota. Results show that this model can successfully reproduce the log-growth rate distribution of the considered abundance time-series. More significantly, it predicts its temporal dependence, by reproducing its kurtosis evolution when the time lag __FORMULA__ is increased. Furthermore, its typical shape for large __FORMULA__ is assessed, verifying that the distribution variance does not diverge with __FORMULA__. The simulated processes generated by the calibrated stochastic equation and the analysis of each time-series, taken one by one, provided additional support for our approach. Alternatively, we tried to describe our dataset by using a logistic model with an environmental stochastic term. Analytical and numerical results show that this model is not suited for describing the leptokurtic log-growth rates distribution found in our data. These results effectively support a neutral model with demographic stochasticity for describing the growth-rate dynamics and the stationary abundance distribution of the considered microbiota. This suggests that there are no significant parametric demographic differences among the species, which can be statistically characterized by the same vital rates."}
{"title":"Towards quantitative Low Energy Ion Scattering on CaSiO$_3$ from Comparison to Multiple-Scattering-Resolved Dynamical Binary Collision Approximation Simulations","authors":["Johannes Br\u00f6tzner","Lukas Kalchgruber","Paul S. Szabo","Andreas Nenning","Andreas Mutzke","Hans Hofs\u00e4ss","Markus Valtiner","Richard A. Wilhelm"],"raw_abstract":"We perform Low Energy Ion Scattering with 1\\,keV He ions on CaSiO$_3$ using a\ncommercial electrostatic detector system and determine the charge fraction of\nscattered ions from comparison with Binary Collision Approximation simulations.\nThe simulations take dynamical surface changes due to surface cleaning Ar\nsputtering into account and scattered He particles are separated into single,\ndual, and multiple scattering trajectories. We find that the charge fraction of\nsingle and dual scattered He is about 10 times higher than the one for multiple\ncollisions. Our results show that quantitative concentration profiles can be\ninferred from this method, if the charge fraction components are determined\nfirst.","publication_date":1700236814,"paper_link":"http://arxiv.org/pdf/2311.10604v1","categories":["Physics"],"abstract":"We perform Low Energy Ion Scattering with 1\\,keV He ions on CaSiO__FORMULA__ using a commercial electrostatic detector system and determine the charge fraction of scattered ions from comparison with Binary Collision Approximation simulations. The simulations take dynamical surface changes due to surface cleaning Ar sputtering into account and scattered He particles are separated into single, dual, and multiple scattering trajectories. We find that the charge fraction of single and dual scattered He is about 10 times higher than the one for multiple collisions. Our results show that quantitative concentration profiles can be inferred from this method, if the charge fraction components are determined first."}
{"title":"D\u00e9tection d'objets c\u00e9lestes dans des images astronomiques par IA explicable","authors":["Olivier Parisot","Mahmoud Jaziri"],"raw_abstract":"Amateur and professional astronomers can easily capture a large number of\ndeep sky images with recent smart telescopes. However, afterwards verification\nis still required to check whether the celestial objects targeted are actually\nvisible in the images produced. Depending on the magnitude of the targets, the\nobservation conditions and the time during which the data is captured, it is\npossible that only stars are present in the images. In this study, we propose\nan approach based on explainable Artificial Intelligence to automatically\ndetect the presence and position of captured objects. -- --\n  Gr\\^ace \\`a l'apport des t\\'elescopes automatis\\'es grand public, les\nastronomes amateurs et professionnels peuvent capturer facilement une grande\nquantit\\'e d'images du ciel profond (comme par exemple les galaxies,\nn\\'ebuleuses, ou amas globulaires). N\\'eanmoins, une v\\'erification reste\nn\\'ecessaire \\`a post\\'eriori pour v\\'erifier si les objets c\\'elestes vis\\'es\nsont effectivement visibles dans les images produites: cela d\\'epend notamment\nde la magnitude des cibles, des conditions d'observation mais aussi de la\ndur\\'ee pendant laquelle les donn\\'ees sont captur\\'ees. Dans cette \\'etude,\nnous proposons une approche bas\\'ee sur l'IA explicable pour d\\'etecter\nautomatiquement la pr\\'esence et la position des objets captur\\'es.","publication_date":1700236010,"paper_link":"http://arxiv.org/pdf/2311.10592v1","categories":["Quantitative Biology"],"abstract":"Amateur and professional astronomers can easily capture a large number of deep sky images with recent smart telescopes. However, afterwards verification is still required to check whether the celestial objects targeted are actually visible in the images produced. Depending on the magnitude of the targets, the observation conditions and the time during which the data is captured, it is possible that only stars are present in the images. In this study, we propose an approach based on explainable Artificial Intelligence to automatically detect the presence and position of captured objects. -- --   Gr\\^ace \\`a l'apport des t\\'elescopes automatis\\'es grand public, les astronomes amateurs et professionnels peuvent capturer facilement une grande quantit\\'e d'images du ciel profond (comme par exemple les galaxies, n\\'ebuleuses, ou amas globulaires). N\\'eanmoins, une v\\'erification reste n\\'ecessaire \\`a post\\'eriori pour v\\'erifier si les objets c\\'elestes vis\\'es sont effectivement visibles dans les images produites: cela d\\'epend notamment de la magnitude des cibles, des conditions d'observation mais aussi de la dur\\'ee pendant laquelle les donn\\'ees sont captur\\'ees. Dans cette \\'etude, nous proposons une approche bas\\'ee sur l'IA explicable pour d\\'etecter automatiquement la pr\\'esence et la position des objets captur\\'es."}
{"title":"TransCDR: a deep learning model for enhancing the generalizability of cancer drug response prediction through transfer learning and multimodal data fusion for drug representation","authors":["Xiaoqiong Xia","Chaoyu Zhu","Yuqi Shan","Fan Zhong","Lei Liu"],"raw_abstract":"Accurate and robust drug response prediction is of utmost importance in\nprecision medicine. Although many models have been developed to utilize the\nrepresentations of drugs and cancer cell lines for predicting cancer drug\nresponses (CDR), their performances can be improved by addressing issues such\nas insufficient data modality, suboptimal fusion algorithms, and poor\ngeneralizability for novel drugs or cell lines. We introduce TransCDR, which\nuses transfer learning to learn drug representations and fuses multi-modality\nfeatures of drugs and cell lines by a self-attention mechanism, to predict the\nIC50 values or sensitive states of drugs on cell lines. We are the first to\nsystematically evaluate the generalization of the CDR prediction model to novel\n(i.e., never-before-seen) compound scaffolds and cell line clusters. TransCDR\nshows better generalizability than 8 state-of-the-art models. TransCDR\noutperforms its 5 variants that train drug encoders (i.e., RNN and AttentiveFP)\nfrom scratch under various scenarios. The most critical contributors among\nmultiple drug notations and omics profiles are Extended Connectivity\nFingerprint and genetic mutation. Additionally, the attention-based fusion\nmodule further enhances the predictive performance of TransCDR. TransCDR,\ntrained on the GDSC dataset, demonstrates strong predictive performance on the\nexternal testing set CCLE. It is also utilized to predict missing CDRs on GDSC.\nMoreover, we investigate the biological mechanisms underlying drug response by\nclassifying 7,675 patients from TCGA into drug-sensitive or drug-resistant\ngroups, followed by a Gene Set Enrichment Analysis. TransCDR emerges as a\npotent tool with significant potential in drug response prediction. The source\ncode and data can be accessed at https://github.com/XiaoqiongXia/TransCDR.","publication_date":1700232912,"paper_link":"http://arxiv.org/pdf/2311.12040v1","categories":["Quantitative Biology"],"abstract":"Accurate and robust drug response prediction is of utmost importance in precision medicine. Although many models have been developed to utilize the representations of drugs and cancer cell lines for predicting cancer drug responses (CDR), their performances can be improved by addressing issues such as insufficient data modality, suboptimal fusion algorithms, and poor generalizability for novel drugs or cell lines. We introduce TransCDR, which uses transfer learning to learn drug representations and fuses multi-modality features of drugs and cell lines by a self-attention mechanism, to predict the IC50 values or sensitive states of drugs on cell lines. We are the first to systematically evaluate the generalization of the CDR prediction model to novel (i.e., never-before-seen) compound scaffolds and cell line clusters. TransCDR shows better generalizability than 8 state-of-the-art models. TransCDR outperforms its 5 variants that train drug encoders (i.e., RNN and AttentiveFP) from scratch under various scenarios. The most critical contributors among multiple drug notations and omics profiles are Extended Connectivity Fingerprint and genetic mutation. Additionally, the attention-based fusion module further enhances the predictive performance of TransCDR. TransCDR, trained on the GDSC dataset, demonstrates strong predictive performance on the external testing set CCLE. It is also utilized to predict missing CDRs on GDSC. Moreover, we investigate the biological mechanisms underlying drug response by classifying 7,675 patients from TCGA into drug-sensitive or drug-resistant groups, followed by a Gene Set Enrichment Analysis. TransCDR emerges as a potent tool with significant potential in drug response prediction. The source code and data can be accessed at https://github.com/XiaoqiongXia/TransCDR."}
{"title":"ToSkA: Topological Skeleton Analysis for Network-Based Shape Representation and Evaluation of Objects from Cells to Death Stars","authors":["Allyson Quinn Ryan","Carl D. Modese"],"raw_abstract":"Shape analysis and classification are popular methods for biologists,\nbiophysicists and mathematicians investigating relationships between object\nfunction and form. Classic shape descriptors, such as sphericity, can be very\npowerful; however, when evaluating complex shapes, these descriptors can be\ninsufficient for rigorous assessment. Here, we present \"ToSkA: Topological\nSkeleton Analysis\" a method to analyse complex objects by representing their\nshape asymmetries as networks. Using global neighbourhood principles, classic\nnetwork science metrics and spatial feature embedding we are able to create\nunique object profiles for classification. It is also possible to track objects\nover time and extract significantly different shape features between\nexperiments. Importantly, we have incorporated the capacity to measure absolute\nspatial features of objects (e.g., branch lengths). This adds additional layers\nof sensitivity to object classification. Furthermore, because topology is an\ninherent property of system identity, ToSkA is able to identify segmentation\nerrors that alter object topology by observing the emergence or loss of cycles\nin network representations. Combined, the analytics of ToSkA presented here\nallow for the flexibility and in-depth shape profiling necessary for complex\nobjects often observed in biological and physical settings where robust, yet\nprecise, system configuration is essential to downstream processes.","publication_date":1700229970,"paper_link":"http://arxiv.org/pdf/2311.10539v1","categories":["Quantitative Biology","Physics"],"abstract":"Shape analysis and classification are popular methods for biologists, biophysicists and mathematicians investigating relationships between object function and form. Classic shape descriptors, such as sphericity, can be very powerful; however, when evaluating complex shapes, these descriptors can be insufficient for rigorous assessment. Here, we present \"ToSkA: Topological Skeleton Analysis\" a method to analyse complex objects by representing their shape asymmetries as networks. Using global neighbourhood principles, classic network science metrics and spatial feature embedding we are able to create unique object profiles for classification. It is also possible to track objects over time and extract significantly different shape features between experiments. Importantly, we have incorporated the capacity to measure absolute spatial features of objects (e.g., branch lengths). This adds additional layers of sensitivity to object classification. Furthermore, because topology is an inherent property of system identity, ToSkA is able to identify segmentation errors that alter object topology by observing the emergence or loss of cycles in network representations. Combined, the analytics of ToSkA presented here allow for the flexibility and in-depth shape profiling necessary for complex objects often observed in biological and physical settings where robust, yet precise, system configuration is essential to downstream processes."}
{"title":"Anomalous spin precession systematic effects in the search for a muon EDM using the frozen-spin technique","authors":["G. Cavoto","R. Chakraborty","A. Doinaki","C. Dutsov","M. Giovannozzi","T. Hume","K. Kirch","K. Michielsen","L. Morvaj","A. Papa","F. Renga","M. Sakurai","P. Schmidt-Wellenburg"],"raw_abstract":"At the Paul Scherrer Institut (PSI), we are currently working on the\ndevelopment of a high-precision apparatus with the aim of searching for the\nmuon electric dipole moment (EDM) with unprecedented sensitivity. The\nunderpinning principle of this experiment is the frozen-spin technique, a\nmethod that suppresses the spin precession due to the anomalous magnetic\nmoment, thereby enhancing the signal-to-noise ratio for EDM signals. This\nincreased sensitivity facilitates measurements that would be difficult to\nachieve with conventional $g - 2$ muon storage rings. Given the availability of\nthe $p = 125$ MeV/$c$ muon beam at PSI, the anticipated statistical sensitivity\nfor the EDM after a year of data collection is $6\\times 10^{-23}e\\cdot$cm. To\nachieve this goal, it is imperative to meticulously analyse and mitigate any\npotential spurious effects that could mimic EDM signals. In this study, we\npresent a quantitative methodology to evaluate the systematic effects that\nmight arise in the context of employing the frozen-spin technique within a\ncompact storage ring. Our approach entails the analytical derivation of\nequations governing the motion of the muon spin in the electromagnetic (EM)\nfields intrinsic to the experimental setup, validated through subsequent\nnumerical simulations. We also illustrate a method to calculate the cumulative\ngeometric (Berry's) phase. This work complements ongoing experimental efforts\nto detect a muon EDM at PSI and contributes to a broader understanding of\nspin-precession systematic effects.","publication_date":1700227177,"paper_link":"http://arxiv.org/pdf/2311.10508v1","categories":["Physics"],"abstract":"At the Paul Scherrer Institut (PSI), we are currently working on the development of a high-precision apparatus with the aim of searching for the muon electric dipole moment (EDM) with unprecedented sensitivity. The underpinning principle of this experiment is the frozen-spin technique, a method that suppresses the spin precession due to the anomalous magnetic moment, thereby enhancing the signal-to-noise ratio for EDM signals. This increased sensitivity facilitates measurements that would be difficult to achieve with conventional __FORMULA__ muon storage rings. Given the availability of the __FORMULA__ MeV/__FORMULA__ muon beam at PSI, the anticipated statistical sensitivity for the EDM after a year of data collection is __FORMULA__cm. To achieve this goal, it is imperative to meticulously analyse and mitigate any potential spurious effects that could mimic EDM signals. In this study, we present a quantitative methodology to evaluate the systematic effects that might arise in the context of employing the frozen-spin technique within a compact storage ring. Our approach entails the analytical derivation of equations governing the motion of the muon spin in the electromagnetic (EM) fields intrinsic to the experimental setup, validated through subsequent numerical simulations. We also illustrate a method to calculate the cumulative geometric (Berry's) phase. This work complements ongoing experimental efforts to detect a muon EDM at PSI and contributes to a broader understanding of spin-precession systematic effects."}
{"title":"MIFA: Metadata, Incentives, Formats, and Accessibility guidelines to improve the reuse of AI datasets for bioimage analysis","authors":["Teresa Zulueta-Coarasa","Florian Jug","Aastha Mathur","Josh Moore","Arrate Mu\u00f1oz-Barrutia","Liviu Anita","Kola Babalola","Pete Bankhead","Perrine Gilloteaux","Nodar Gogoberidze","Martin Jones","Gerard J. Kleywegt","Paul Korir","Anna Kreshuk","Ayb\u00fcke K\u00fcpc\u00fc Yolda\u015f","Luca Marconato","Kedar Narayan","Nils Norlin","Bugra Oezdemir","Jessica Riesterer","Norman Rzepka","Ugis Sarkans","Beatriz Serrano","Christian Tischer","Virginie Uhlmann","Vladim\u00edr Ulman","Matthew Hartley"],"raw_abstract":"Artificial Intelligence methods are powerful tools for biological image\nanalysis and processing. High-quality annotated images are key to training and\ndeveloping new methods, but access to such data is often hindered by the lack\nof standards for sharing datasets. We brought together community experts in a\nworkshop to develop guidelines to improve the reuse of bioimages and\nannotations for AI applications. These include standards on data formats,\nmetadata, data presentation and sharing, and incentives to generate new\ndatasets. We are positive that the MIFA (Metadata, Incentives, Formats, and\nAccessibility) recommendations will accelerate the development of AI tools for\nbioimage analysis by facilitating access to high quality training data.","publication_date":1700218198,"paper_link":"http://arxiv.org/pdf/2311.10443v2","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Artificial Intelligence methods are powerful tools for biological image analysis and processing. High-quality annotated images are key to training and developing new methods, but access to such data is often hindered by the lack of standards for sharing datasets. We brought together community experts in a workshop to develop guidelines to improve the reuse of bioimages and annotations for AI applications. These include standards on data formats, metadata, data presentation and sharing, and incentives to generate new datasets. We are positive that the MIFA (Metadata, Incentives, Formats, and Accessibility) recommendations will accelerate the development of AI tools for bioimage analysis by facilitating access to high quality training data."}
{"title":"Convex body domination for a class of multi-scale operators","authors":["Aapo Laukkarinen"],"raw_abstract":"The technique of sparse domination, i.e., dominating operators with sums of\naverages taken over sparsely distributed cubes, has seen rapid development\nrecently within the realms of harmonic analysis. A useful extension of sparse\ndomination called convex body domination allows one to estimate operators in\nmatrix-weighted spaces. In this paper, we extend recent sparse domination\nresults for a class of multi-scale operators due to Beltran, Roos and Seeger to\nthe convex body setting and prove that this implies quantitative\nmatrix-weighted norm bounds for these operators and their commutators.","publication_date":1700218175,"paper_link":"http://arxiv.org/pdf/2311.10442v1","categories":["Mathematics"],"abstract":"The technique of sparse domination, i.e., dominating operators with sums of averages taken over sparsely distributed cubes, has seen rapid development recently within the realms of harmonic analysis. A useful extension of sparse domination called convex body domination allows one to estimate operators in matrix-weighted spaces. In this paper, we extend recent sparse domination results for a class of multi-scale operators due to Beltran, Roos and Seeger to the convex body setting and prove that this implies quantitative matrix-weighted norm bounds for these operators and their commutators."}
{"title":"Velde: constructing cell potential landscapes by RNA velocity vector field decomposition","authors":["Junbo Jia","Luonan Chen"],"raw_abstract":"The Waddington landscape serves as a metaphor illustrating the developmental\nprocess of cells, likening it to a small ball rolling down various trajectories\ninto valleys. Constructing an epigenetic landscape of this nature aids in\nvisualizing and gaining insights into cell differentiation. Development\nencompasses intricate processes involving both cell differentiation and cell\ncycles. However, current landscape methods solely focus on constructing a\npotential landscape for cell differentiation, neglecting the accompanying cell\ncycle. This paper introduces a novel method that simultaneously constructs two\ntypes of potential landscapes using single-cell RNA sequencing data.\nSpecifically, it presents the natural Helmholtz-Hodge decomposition (nHHD) of a\ncontinuous vector field within a bounded domain in n-dimensional Euclidean\nspace. This decomposition uniquely breaks down the vector field into a gradient\nfield, a rotation field, and a harmonic field. Utilizing this approach, the RNA\nvelocity vector field is separated into a curl-free component representing cell\ndifferentiation and a curl component representing the cell cycle. By\ncalculating the corresponding potential functions, potential landscapes for\nboth cell differentiation and the cell cycle are obtained. Finally, the\nefficacy of this method is demonstrated through its application to synthetic\nand real datasets.","publication_date":1700212134,"paper_link":"http://arxiv.org/pdf/2311.10403v1","categories":["Quantitative Biology"],"abstract":"The Waddington landscape serves as a metaphor illustrating the developmental process of cells, likening it to a small ball rolling down various trajectories into valleys. Constructing an epigenetic landscape of this nature aids in visualizing and gaining insights into cell differentiation. Development encompasses intricate processes involving both cell differentiation and cell cycles. However, current landscape methods solely focus on constructing a potential landscape for cell differentiation, neglecting the accompanying cell cycle. This paper introduces a novel method that simultaneously constructs two types of potential landscapes using single-cell RNA sequencing data. Specifically, it presents the natural Helmholtz-Hodge decomposition (nHHD) of a continuous vector field within a bounded domain in n-dimensional Euclidean space. This decomposition uniquely breaks down the vector field into a gradient field, a rotation field, and a harmonic field. Utilizing this approach, the RNA velocity vector field is separated into a curl-free component representing cell differentiation and a curl component representing the cell cycle. By calculating the corresponding potential functions, potential landscapes for both cell differentiation and the cell cycle are obtained. Finally, the efficacy of this method is demonstrated through its application to synthetic and real datasets."}
{"title":"Mean Field Analysis of Two-Party Governance: Competition versus Cooperation among Leaders","authors":["Dantong Chu","Kenneth Tsz Hin Ng","Sheung Chi Phillip Yam","Harry Zheng"],"raw_abstract":"This article studies linear-quadratic Stackelberg games between two\ndominating players (or equivalently, leaders) and a large group of followers,\neach of them interacting under a mean field game (MFG) framework. Unlike the\nconventional major-minor player game, the mean field term herein is\nendogenously determined by the two leaders simultaneously. These homogeneous\nfollowers are not cooperative, whereas the two leaders can either compete or\ncooperate with each other, which are respectively formulated as a Nash and a\nPareto game. The complete solutions of the leader-follower game can be\nexpressed in terms of the solutions of some non-symmetric Riccati equations.\nConceivably, the two modes of interactions between leaders each has their own\nmerits and neither is always more favourable to the community, i.e., to the\nfollowers. While a comparative statics of the effect of different modes of\ngovernance on the society is relatively rare in the literature, we attempt to\nprovide some preliminary quantitative analysis on this topic. In particular,\nunder a broad class of practically relevant models, we provide sufficient\nconditions to decide whether cooperation or competition between leaders is more\nfavourable to the followers via a comprehensive study. Intutively, the relative\nmerits of the two games depend on whether the interests between the two leaders\nand the followers align with each other. Numerical examples supporting the\nfindings are also provided.","publication_date":1700204333,"paper_link":"http://arxiv.org/pdf/2311.10354v2","categories":["Mathematics"],"abstract":"This article studies linear-quadratic Stackelberg games between two dominating players (or equivalently, leaders) and a large group of followers, each of them interacting under a mean field game (MFG) framework. Unlike the conventional major-minor player game, the mean field term herein is endogenously determined by the two leaders simultaneously. These homogeneous followers are not cooperative, whereas the two leaders can either compete or cooperate with each other, which are respectively formulated as a Nash and a Pareto game. The complete solutions of the leader-follower game can be expressed in terms of the solutions of some non-symmetric Riccati equations. Conceivably, the two modes of interactions between leaders each has their own merits and neither is always more favourable to the community, i.e., to the followers. While a comparative statics of the effect of different modes of governance on the society is relatively rare in the literature, we attempt to provide some preliminary quantitative analysis on this topic. In particular, under a broad class of practically relevant models, we provide sufficient conditions to decide whether cooperation or competition between leaders is more favourable to the followers via a comprehensive study. Intutively, the relative merits of the two games depend on whether the interests between the two leaders and the followers align with each other. Numerical examples supporting the findings are also provided."}
{"title":"Towards Machine Learning-based Quantitative Hyperspectral Image Guidance for Brain Tumor Resection","authors":["David Black","Declan Byrne","Anna Walke","Sidong Liu","Antonio Di leva","Sadahiro Kaneko","Walter Stummer","Septimiu Salcudean","Eric Suero Molina"],"raw_abstract":"Complete resection of malignant gliomas is hampered by the difficulty in\ndistinguishing tumor cells at the infiltration zone. Fluorescence guidance with\n5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work\ncharacterized five fluorophores' emission spectra in most human brain tumors.\nIn this paper, the effectiveness of these five spectra was explored for\ndifferent tumor and tissue classification tasks in 184 patients (891\nhyperspectral measurements) harboring low- (n=30) and high-grade gliomas\n(n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2),\nmiscellaneous (n=10) and metastases (n=8). Four machine learning models were\ntrained to classify tumor type, grade, glioma margins and IDH mutation. Using\nrandom forests and multi-layer perceptrons, the classifiers achieved average\ntest accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore\nabundances varied between tumor margin types and tumor grades (p < 0.01). For\ntissue type, at least four of the five fluorophore abundances were found to be\nsignificantly different (p < 0.01) between all classes. These results\ndemonstrate the fluorophores' differing abundances in different tissue classes,\nas well as the value of the five fluorophores as potential optical biomarkers,\nopening new opportunities for intraoperative classification systems in\nfluorescence-guided neurosurgery.","publication_date":1700194527,"paper_link":"http://arxiv.org/pdf/2311.10321v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Complete resection of malignant gliomas is hampered by the difficulty in distinguishing tumor cells at the infiltration zone. Fluorescence guidance with 5-ALA assists in reaching this goal. Using hyperspectral imaging, previous work characterized five fluorophores' emission spectra in most human brain tumors. In this paper, the effectiveness of these five spectra was explored for different tumor and tissue classification tasks in 184 patients (891 hyperspectral measurements) harboring low- (n=30) and high-grade gliomas (n=115), non-glial primary brain tumors (n=19), radiation necrosis (n=2), miscellaneous (n=10) and metastases (n=8). Four machine learning models were trained to classify tumor type, grade, glioma margins and IDH mutation. Using random forests and multi-layer perceptrons, the classifiers achieved average test accuracies of 74-82%, 79%, 81%, and 93% respectively. All five fluorophore abundances varied between tumor margin types and tumor grades (p < 0.01). For tissue type, at least four of the five fluorophore abundances were found to be significantly different (p < 0.01) between all classes. These results demonstrate the fluorophores' differing abundances in different tissue classes, as well as the value of the five fluorophores as potential optical biomarkers, opening new opportunities for intraoperative classification systems in fluorescence-guided neurosurgery."}
{"title":"Interpretable Modeling of Single-cell perturbation Responses to Novel Drugs Using Cycle Consistence Learning","authors":["Wei Huang","Aichun Zhu","Hui Liu"],"raw_abstract":"Phenotype-based screening has attracted much attention for identifying\ncell-active compounds. Transcriptional and proteomic profiles of cell\npopulation or single cells are informative phenotypic measures of cellular\nresponses to perturbations. In this paper, we proposed a deep learning\nframework based on encoder-decoder architecture that maps the initial cellular\nstates to a latent space, in which we assume the effects of drug perturbation\non cellular states follow linear additivity. Next, we introduced the cycle\nconsistency constraints to enforce that initial cellular state subjected to\ndrug perturbations would produce the perturbed cellular responses, and,\nconversely, removal of drug perturbation from the perturbed cellular states\nwould restore the initial cellular states. The cycle consistency constraints\nand linear modeling in latent space enable to learn interpretable and\ntransferable drug perturbation representations, so that our model can predict\ncellular response to unseen drugs. We validated our model on three different\ntypes of datasets, including bulk transcriptional responses, bulk proteomic\nresponses, and single-cell transcriptional responses to drug perturbations. The\nexperimental results show that our model achieves better performance than\nexisting state-of-the-art methods.","publication_date":1700193539,"paper_link":"http://arxiv.org/pdf/2311.10315v1","categories":["Quantitative Biology"],"abstract":"Phenotype-based screening has attracted much attention for identifying cell-active compounds. Transcriptional and proteomic profiles of cell population or single cells are informative phenotypic measures of cellular responses to perturbations. In this paper, we proposed a deep learning framework based on encoder-decoder architecture that maps the initial cellular states to a latent space, in which we assume the effects of drug perturbation on cellular states follow linear additivity. Next, we introduced the cycle consistency constraints to enforce that initial cellular state subjected to drug perturbations would produce the perturbed cellular responses, and, conversely, removal of drug perturbation from the perturbed cellular states would restore the initial cellular states. The cycle consistency constraints and linear modeling in latent space enable to learn interpretable and transferable drug perturbation representations, so that our model can predict cellular response to unseen drugs. We validated our model on three different types of datasets, including bulk transcriptional responses, bulk proteomic responses, and single-cell transcriptional responses to drug perturbations. The experimental results show that our model achieves better performance than existing state-of-the-art methods."}
{"title":"Uniform regularity for degenerate elliptic equations in perforated domains","authors":["Zhongwei Shen","Jinping Zhuge"],"raw_abstract":"This paper is concerned with a class of degenerate elliptic equations with\nrapidly oscillating coefficients in periodically perforated domains, which\narises in the study of spectrum problems for uniformly elliptic equations in\nperforated domains. We establish a quantitative convergence rate and obtain the\nuniform weighted Lipschitz and $W^{1,p}$ estimates.","publication_date":1700182995,"paper_link":"http://arxiv.org/pdf/2311.10258v1","categories":["Mathematics"],"abstract":"This paper is concerned with a class of degenerate elliptic equations with rapidly oscillating coefficients in periodically perforated domains, which arises in the study of spectrum problems for uniformly elliptic equations in perforated domains. We establish a quantitative convergence rate and obtain the uniform weighted Lipschitz and __FORMULA__ estimates."}
{"title":"Mimicking Classical Noise in Ion Channels by Quantum Decoherence","authors":["Mina Seifi","Ali Soltanmanesh","Afshin Shafiee"],"raw_abstract":"The mechanism of selectivity in ion channels is still an open question in\nbiology. According to recent proposals, it seems that the selectivity filter of\nthe ion channel, which plays a key role in the channel's function, may show\nquantum coherence, which can play a role in explaining the selection mechanism\nand conduction of ions. However, due to decoherence theory, the presence of\nenvironmental noise causes decoherence and loss of quantum effects. Sometimes\nwe hope that the effect of calssical noise of the environment in ion channels\ncan be modeled through a picture whose the quantum decoherence theory presents.\nIn this paper, we simulated the behavior of the ion channel system in the\nSpin-Boson model using the unitary evolution of a stochastic Hamiltonian\noperator under the classical noise model. Also, in a different approach, we\nmodeled the system evolution as a two-level Spin-Boson model with tunneling\ninteracting with a bath of harmonic oscillators, using decoherence theory. The\nresults of this system were discussed in different classical and quantum\nregimes. By examining the results it was found that the Spin-Boson model at a\nhigh hopping rate of Potassium ions can simulate the behavior of the system in\nthe classical noise approach. This result is another proof for the fact that\nion channels need high speed for high selectivity.","publication_date":1700173310,"paper_link":"http://arxiv.org/pdf/2311.10222v1","categories":["Physics"],"abstract":"The mechanism of selectivity in ion channels is still an open question in biology. According to recent proposals, it seems that the selectivity filter of the ion channel, which plays a key role in the channel's function, may show quantum coherence, which can play a role in explaining the selection mechanism and conduction of ions. However, due to decoherence theory, the presence of environmental noise causes decoherence and loss of quantum effects. Sometimes we hope that the effect of calssical noise of the environment in ion channels can be modeled through a picture whose the quantum decoherence theory presents. In this paper, we simulated the behavior of the ion channel system in the Spin-Boson model using the unitary evolution of a stochastic Hamiltonian operator under the classical noise model. Also, in a different approach, we modeled the system evolution as a two-level Spin-Boson model with tunneling interacting with a bath of harmonic oscillators, using decoherence theory. The results of this system were discussed in different classical and quantum regimes. By examining the results it was found that the Spin-Boson model at a high hopping rate of Potassium ions can simulate the behavior of the system in the classical noise approach. This result is another proof for the fact that ion channels need high speed for high selectivity."}
{"title":"Investigating the Use of Traveltime and Reflection Tomography for Deep Learning-Based Sound-Speed Estimation in Ultrasound Computed Tomography","authors":["Gangwon Jeong","Fu Li","Umberto Villa","Mark A. Anastasio"],"raw_abstract":"Ultrasound computed tomography (USCT) is actively being developed to quantify\nacoustic tissue properties such as the speed-of-sound (SOS). Although\nfull-waveform inversion (FWI) is an effective method for accurate SOS\nreconstruction, it can be computationally challenging for large-scale problems.\nDeep learning-based image-to-image learned reconstruction (IILR) methods are\nbeing investigated as scalable and computationally efficient alternatives. This\nstudy investigates the impact of the chosen input modalities on IILR methods\nfor high-resolution SOS reconstruction in USCT. The selected modalities are\ntraveltime tomography (TT) and reflection tomography (RT), which produce a\nlow-resolution SOS map and a reflectivity map, respectively. These modalities\nhave been chosen for their lower computational cost relative to FWI and their\ncapacity to provide complementary information: TT offers a direct -- while low\nresolution -- SOS measure, while RT reveals tissue boundary information.\nSystematic analyses were facilitated by employing a stylized USCT imaging\nsystem with anatomically realistic numerical breast phantoms. Within this\ntestbed, a supervised convolutional neural network (CNN) was trained to map\ndual-channel (TT and RT images) to a high-resolution SOS map. Moreover, the CNN\nwas fine-tuned using a weighted reconstruction loss that prioritized tumor\nregions to address tumor underrepresentation in the training dataset. To\nunderstand the benefits of employing dual-channel inputs, single-input CNNs\nwere trained separately using inputs from each modality alone (TT or RT). The\nmethods were assessed quantitatively using normalized root mean squared error\nand structural similarity index measure for reconstruction accuracy and\nreceiver operating characteristic analysis to assess signal detection-based\nperformance measures.","publication_date":1700168100,"paper_link":"http://arxiv.org/pdf/2311.10193v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Ultrasound computed tomography (USCT) is actively being developed to quantify acoustic tissue properties such as the speed-of-sound (SOS). Although full-waveform inversion (FWI) is an effective method for accurate SOS reconstruction, it can be computationally challenging for large-scale problems. Deep learning-based image-to-image learned reconstruction (IILR) methods are being investigated as scalable and computationally efficient alternatives. This study investigates the impact of the chosen input modalities on IILR methods for high-resolution SOS reconstruction in USCT. The selected modalities are traveltime tomography (TT) and reflection tomography (RT), which produce a low-resolution SOS map and a reflectivity map, respectively. These modalities have been chosen for their lower computational cost relative to FWI and their capacity to provide complementary information: TT offers a direct -- while low resolution -- SOS measure, while RT reveals tissue boundary information. Systematic analyses were facilitated by employing a stylized USCT imaging system with anatomically realistic numerical breast phantoms. Within this testbed, a supervised convolutional neural network (CNN) was trained to map dual-channel (TT and RT images) to a high-resolution SOS map. Moreover, the CNN was fine-tuned using a weighted reconstruction loss that prioritized tumor regions to address tumor underrepresentation in the training dataset. To understand the benefits of employing dual-channel inputs, single-input CNNs were trained separately using inputs from each modality alone (TT or RT). The methods were assessed quantitatively using normalized root mean squared error and structural similarity index measure for reconstruction accuracy and receiver operating characteristic analysis to assess signal detection-based performance measures."}
{"title":"The Chosen One: Consistent Characters in Text-to-Image Diffusion Models","authors":["Omri Avrahami","Amir Hertz","Yael Vinker","Moab Arar","Shlomi Fruchter","Ohad Fried","Daniel Cohen-Or","Dani Lischinski"],"raw_abstract":"Recent advances in text-to-image generation models have unlocked vast\npotential for visual creativity. However, these models struggle with generation\nof consistent characters, a crucial aspect for numerous real-world applications\nsuch as story visualization, game development asset design, advertising, and\nmore. Current methods typically rely on multiple pre-existing images of the\ntarget character or involve labor-intensive manual processes. In this work, we\npropose a fully automated solution for consistent character generation, with\nthe sole input being a text prompt. We introduce an iterative procedure that,\nat each stage, identifies a coherent set of images sharing a similar identity\nand extracts a more consistent identity from this set. Our quantitative\nanalysis demonstrates that our method strikes a better balance between prompt\nalignment and identity consistency compared to the baseline methods, and these\nfindings are reinforced by a user study. To conclude, we showcase several\npractical applications of our approach. Project page is available at\nhttps://omriavrahami.com/the-chosen-one","publication_date":1700161191,"paper_link":"http://arxiv.org/pdf/2311.10093v1","categories":["Quantitative Biology"],"abstract":"Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, these models struggle with generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for consistent character generation, with the sole input being a text prompt. We introduce an iterative procedure that, at each stage, identifies a coherent set of images sharing a similar identity and extracts a more consistent identity from this set. Our quantitative analysis demonstrates that our method strikes a better balance between prompt alignment and identity consistency compared to the baseline methods, and these findings are reinforced by a user study. To conclude, we showcase several practical applications of our approach. Project page is available at https://omriavrahami.com/the-chosen-one"}
{"title":"Xputer: Bridging Data Gaps with NMF, XGBoost, and a Streamlined GUI Experience","authors":["Saleena Younus","Lars R\u00f6nnstrand","Julhash U. Kazi"],"raw_abstract":"The rapid proliferation of data across diverse fields has accentuated the\nimportance of accurate imputation for missing values. This task is crucial for\nensuring data integrity and deriving meaningful insights. In response to this\nchallenge, we present Xputer, a novel imputation tool that adeptly integrates\nNon-negative Matrix Factorization (NMF) with the predictive strengths of\nXGBoost. One of Xputer's standout features is its versatility: it supports zero\nimputation, enables hyperparameter optimization through Optuna, and allows\nusers to define the number of iterations. For enhanced user experience and\naccessibility, we have equipped Xputer with an intuitive Graphical User\nInterface (GUI) ensuring ease of handling, even for those less familiar with\ncomputational tools. In performance benchmarks, Xputer not only rivals the\ncomputational speed of established tools such as IterativeImputer but also\noften outperforms them in terms of imputation accuracy. Furthermore, Xputer\nautonomously handles a diverse spectrum of data types, including categorical,\ncontinuous, and Boolean, eliminating the need for prior preprocessing. Given\nits blend of performance, flexibility, and user-friendly design, Xputer emerges\nas a state-of-the-art solution in the realm of data imputation.","publication_date":1700150839,"paper_link":"http://arxiv.org/pdf/2311.09989v1","categories":["Quantitative Biology","Statistics"],"abstract":"The rapid proliferation of data across diverse fields has accentuated the importance of accurate imputation for missing values. This task is crucial for ensuring data integrity and deriving meaningful insights. In response to this challenge, we present Xputer, a novel imputation tool that adeptly integrates Non-negative Matrix Factorization (NMF) with the predictive strengths of XGBoost. One of Xputer's standout features is its versatility: it supports zero imputation, enables hyperparameter optimization through Optuna, and allows users to define the number of iterations. For enhanced user experience and accessibility, we have equipped Xputer with an intuitive Graphical User Interface (GUI) ensuring ease of handling, even for those less familiar with computational tools. In performance benchmarks, Xputer not only rivals the computational speed of established tools such as IterativeImputer but also often outperforms them in terms of imputation accuracy. Furthermore, Xputer autonomously handles a diverse spectrum of data types, including categorical, continuous, and Boolean, eliminating the need for prior preprocessing. Given its blend of performance, flexibility, and user-friendly design, Xputer emerges as a state-of-the-art solution in the realm of data imputation."}
{"title":"Self-supervised learning of multi-omics embeddings in the low-label, high-data regime","authors":["Christian John Hurry","Emma Slade"],"raw_abstract":"Contrastive, self-supervised learning (SSL) is used to train a model that\npredicts cancer type from miRNA, mRNA or RPPA expression data. This model, a\npretrained FT-Transformer, is shown to outperform XGBoost and CatBoost,\nstandard benchmarks for tabular data, when labelled samples are scarce but the\nnumber of unlabelled samples is high. This is despite the fact that the\ndatasets we use have $\\mathcal{O}(10^{1})$ classes and\n$\\mathcal{O}(10^{2})-\\mathcal{O}(10^{4})$ features. After demonstrating the\nefficacy of our chosen method of self-supervised pretraining, we investigate\nSSL for multi-modal models. A late-fusion model is proposed, where each omics\nis passed through its own sub-network, the outputs of which are averaged and\npassed to the pretraining or downstream objective function. Multi-modal\npretraining is shown to improve predictions from a single omics, and we argue\nthat this is useful for datasets with many unlabelled multi-modal samples, but\nfew labelled unimodal samples. Additionally, we show that pretraining each\nomics-specific module individually is highly effective. This enables the\napplication of the proposed model in a variety of contexts where a large amount\nof unlabelled data is available from each omics, but only a few labelled\nsamples.","publication_date":1700148742,"paper_link":"http://arxiv.org/pdf/2311.09962v1","categories":["Quantitative Biology"],"abstract":"Contrastive, self-supervised learning (SSL) is used to train a model that predicts cancer type from miRNA, mRNA or RPPA expression data. This model, a pretrained FT-Transformer, is shown to outperform XGBoost and CatBoost, standard benchmarks for tabular data, when labelled samples are scarce but the number of unlabelled samples is high. This is despite the fact that the datasets we use have __FORMULA__ classes and __FORMULA__ features. After demonstrating the efficacy of our chosen method of self-supervised pretraining, we investigate SSL for multi-modal models. A late-fusion model is proposed, where each omics is passed through its own sub-network, the outputs of which are averaged and passed to the pretraining or downstream objective function. Multi-modal pretraining is shown to improve predictions from a single omics, and we argue that this is useful for datasets with many unlabelled multi-modal samples, but few labelled unimodal samples. Additionally, we show that pretraining each omics-specific module individually is highly effective. This enables the application of the proposed model in a variety of contexts where a large amount of unlabelled data is available from each omics, but only a few labelled samples."}
{"title":"Ricci curvature bounded below and uniform rectifiability","authors":["Matthew Hyde","Michele Villa","Ivan Yuri Violo"],"raw_abstract":"We prove that Ahlfors-regular RCD spaces are uniformly rectifiable. The same\nis shown for Ahlfors regular boundaries of non-collapsed RCD spaces. As an\napplication we deduce a type of quantitative differentiation for Lipschitz\nfunctions on these spaces.","publication_date":1700143087,"paper_link":"http://arxiv.org/pdf/2311.09907v1","categories":["Mathematics"],"abstract":"We prove that Ahlfors-regular RCD spaces are uniformly rectifiable. The same is shown for Ahlfors regular boundaries of non-collapsed RCD spaces. As an application we deduce a type of quantitative differentiation for Lipschitz functions on these spaces."}
{"title":"MAM-E: Mammographic synthetic image generation with diffusion models","authors":["Ricardo Montoya-del-Angel","Karla Sam-Millan","Joan C Vilanova","Robert Mart\u00ed"],"raw_abstract":"Generative models are used as an alternative data augmentation technique to\nalleviate the data scarcity problem faced in the medical imaging field.\nDiffusion models have gathered special attention due to their innovative\ngeneration approach, the high quality of the generated images and their\nrelatively less complex training process compared with Generative Adversarial\nNetworks. Still, the implementation of such models in the medical domain\nremains at early stages. In this work, we propose exploring the use of\ndiffusion models for the generation of high quality full-field digital\nmammograms using state-of-the-art conditional diffusion pipelines.\nAdditionally, we propose using stable diffusion models for the inpainting of\nsynthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of\ngenerative models for high quality mammography synthesis controlled by a text\nprompt and capable of generating synthetic lesions on specific regions of the\nbreast. Finally, we provide quantitative and qualitative assessment of the\ngenerated images and easy-to-use graphical user interfaces for mammography\nsynthesis.","publication_date":1700135389,"paper_link":"http://arxiv.org/pdf/2311.09822v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Generative models are used as an alternative data augmentation technique to alleviate the data scarcity problem faced in the medical imaging field. Diffusion models have gathered special attention due to their innovative generation approach, the high quality of the generated images and their relatively less complex training process compared with Generative Adversarial Networks. Still, the implementation of such models in the medical domain remains at early stages. In this work, we propose exploring the use of diffusion models for the generation of high quality full-field digital mammograms using state-of-the-art conditional diffusion pipelines. Additionally, we propose using stable diffusion models for the inpainting of synthetic lesions on healthy mammograms. We introduce MAM-E, a pipeline of generative models for high quality mammography synthesis controlled by a text prompt and capable of generating synthetic lesions on specific regions of the breast. Finally, we provide quantitative and qualitative assessment of the generated images and easy-to-use graphical user interfaces for mammography synthesis."}
{"title":"Efficacy of Wolbachia-mediated sterility to suppress dengue: a synthetic control study","authors":["Jue Tao Lim","Somya Bansal","Chee Seng Chong","Borame Dickens","Youming Ng","Lu Deng","Caleb Lee","Li Yun Tan","Grace Chain","Pei Ma","Shuzhen Sim","Cheong Huat Tan","Alex R Cook","Lee Ching Ng"],"raw_abstract":"In a study conducted in Singapore, a country prone to dengue outbreaks due to\nits climate and urban population, researchers examined the effectiveness of\nreleasing male Aedes aegypti mosquitoes infected with Wolbachia (wAlbB strain)\nto reduce dengue transmission. These infected males, when mating with wild-type\nfemales, produced non-viable eggs, leading to vector suppression. Extensive\nfield trials involving over 600,000 residents in four townships were conducted\nfrom 2018 to 2022. The results showed a 57% decline in total dengue incidence\nand a 64% decline in clustered dengue incidence. This approach offers promise\nfor large-scale dengue control in regions facing rising dengue cases, providing\na critical solution in combating the disease.","publication_date":1700130565,"paper_link":"http://arxiv.org/pdf/2311.09754v1","categories":["Quantitative Biology"],"abstract":"In a study conducted in Singapore, a country prone to dengue outbreaks due to its climate and urban population, researchers examined the effectiveness of releasing male Aedes aegypti mosquitoes infected with Wolbachia (wAlbB strain) to reduce dengue transmission. These infected males, when mating with wild-type females, produced non-viable eggs, leading to vector suppression. Extensive field trials involving over 600,000 residents in four townships were conducted from 2018 to 2022. The results showed a 57% decline in total dengue incidence and a 64% decline in clustered dengue incidence. This approach offers promise for large-scale dengue control in regions facing rising dengue cases, providing a critical solution in combating the disease."}
{"title":"Stick-slip-to-stick transition of liquid oscillation in a U-tube","authors":["Alessandro Bongarzone","Fran\u00e7ois Gallaire"],"raw_abstract":"The nonlinear decay of oscillations of a liquid column in a U-shaped tube is\ninvestigated within the theoretical framework of the projection method\nformalized by Bongarzone et al. (2021) [1]. Starting from the full hydrodynamic\nsystem supplemented by a phenomenological contact line model, this\nphysics-inspired method uses successive linear eigenmode projections to\nsimulate the relaxation dynamics of liquid oscillations in the presence of\nsliding triple lines. Each projection is shown to eventually induce a rapid\nloss of total energy in the liquid motion, thus contributing to its nonlinear\ndamping. A thorough quantitative comparison with experiments by Dollet et al.\n(2020) [2] demonstrates that, in contradistinction with their simplistic\none-degree-of-freedom model, the present approach not only describes well the\ntransient stick-slip dynamics, but it also correctly captures the global\nstick-slip to stick transition, as well as the secondary bulk motion following\nthe arrest of the contact line, which has been so far overlooked by existing\ntheoretical analyses. This study offers a further contribution to rationalizing\nthe impact of contact angle hysteresis and its associated solidlike friction on\nthe decay of liquid oscillations in the presence of sliding triple lines.","publication_date":1700130253,"paper_link":"http://arxiv.org/pdf/2311.09747v1","categories":["Physics"],"abstract":"The nonlinear decay of oscillations of a liquid column in a U-shaped tube is investigated within the theoretical framework of the projection method formalized by Bongarzone et al. (2021) [1]. Starting from the full hydrodynamic system supplemented by a phenomenological contact line model, this physics-inspired method uses successive linear eigenmode projections to simulate the relaxation dynamics of liquid oscillations in the presence of sliding triple lines. Each projection is shown to eventually induce a rapid loss of total energy in the liquid motion, thus contributing to its nonlinear damping. A thorough quantitative comparison with experiments by Dollet et al. (2020) [2] demonstrates that, in contradistinction with their simplistic one-degree-of-freedom model, the present approach not only describes well the transient stick-slip dynamics, but it also correctly captures the global stick-slip to stick transition, as well as the secondary bulk motion following the arrest of the contact line, which has been so far overlooked by existing theoretical analyses. This study offers a further contribution to rationalizing the impact of contact angle hysteresis and its associated solidlike friction on the decay of liquid oscillations in the presence of sliding triple lines."}
{"title":"Dynamics of Particle-laden Turbulent Suspensions: Effect of particle roughness","authors":["S. Ghosh","P S Goswami","V. Kumaran"],"raw_abstract":"The Fluctuating Force Fluctuating Torque (F3T) model is developed and\nevaluated for the dynamics of a turbulent particle-gas suspension of rough\nspherical particles in a turbulent Couette flow in the limit where the viscous\nrelaxation time of the particles and the time between collisions are much\nlarger than the integral time for the fluid turbulence. The fluid force/torque\nexerted on the particles comprise a steady part due to the difference in the\nparticle velocity/angular velocity and the fluid mean velocity/rotation rate,\nand a fluctuating part due to the turbulent velocity/vorticity fluctuations.\nThe fluctuations are modeled as Gaussian white noise whose variance is\ndetermined from the fluid velocity and vorticity fluctuations. The smooth and\nrough inelastic collision models are considered for particle-particle and\nparticle-wall collisions. The results show that inclusion of roughness is\nimportant for accurately predicting the particle dynamics; the second moments\nof the velocity fluctuations for rough particles are higher than those for\nsmooth particles by a factor of 2-10, while the second moments of the angular\nvelocity fluctuations are higher by 1-2 orders of magnitude. The F3T model\nquantitatively predicts the number density, mean and root mean square velocity\nand angular velocity profiles, and the distribution functions for the particle\nvelocity and angular velocity, even though a Gaussian model is used for the\nhighly non-Gaussian distributions for the force and torque fluctuations.","publication_date":1700128815,"paper_link":"http://arxiv.org/pdf/2311.09729v1","categories":["Physics"],"abstract":"The Fluctuating Force Fluctuating Torque (F3T) model is developed and evaluated for the dynamics of a turbulent particle-gas suspension of rough spherical particles in a turbulent Couette flow in the limit where the viscous relaxation time of the particles and the time between collisions are much larger than the integral time for the fluid turbulence. The fluid force/torque exerted on the particles comprise a steady part due to the difference in the particle velocity/angular velocity and the fluid mean velocity/rotation rate, and a fluctuating part due to the turbulent velocity/vorticity fluctuations. The fluctuations are modeled as Gaussian white noise whose variance is determined from the fluid velocity and vorticity fluctuations. The smooth and rough inelastic collision models are considered for particle-particle and particle-wall collisions. The results show that inclusion of roughness is important for accurately predicting the particle dynamics; the second moments of the velocity fluctuations for rough particles are higher than those for smooth particles by a factor of 2-10, while the second moments of the angular velocity fluctuations are higher by 1-2 orders of magnitude. The F3T model quantitatively predicts the number density, mean and root mean square velocity and angular velocity profiles, and the distribution functions for the particle velocity and angular velocity, even though a Gaussian model is used for the highly non-Gaussian distributions for the force and torque fluctuations."}
{"title":"Now and Future of Artificial Intelligence-based Signet Ring Cell Diagnosis: A Survey","authors":["Zhu Meng","Junhao Dong","Limei Guo","Fei Su","Guangxi Wang","Zhicheng Zhao"],"raw_abstract":"Since signet ring cells (SRCs) are associated with high peripheral metastasis\nrate and dismal survival, they play an important role in determining surgical\napproaches and prognosis, while they are easily missed by even experienced\npathologists. Although automatic diagnosis SRCs based on deep learning has\nreceived increasing attention to assist pathologists in improving the\ndiagnostic efficiency and accuracy, the existing works have not been\nsystematically overviewed, which hindered the evaluation of the gap between\nalgorithms and clinical applications. In this paper, we provide a survey on SRC\nanalysis driven by deep learning from 2008 to August 2023. Specifically, the\nbiological characteristics of SRCs and the challenges of automatic\nidentification are systemically summarized. Then, the representative algorithms\nare analyzed and compared via dividing them into classification, detection, and\nsegmentation. Finally, for comprehensive consideration to the performance of\nexisting methods and the requirements for clinical assistance, we discuss the\nopen issues and future trends of SRC analysis. The retrospect research will\nhelp researchers in the related fields, particularly for who without medical\nscience background not only to clearly find the outline of SRC analysis, but\nalso gain the prospect of intelligent diagnosis, resulting in accelerating the\npractice and application of intelligent algorithms.","publication_date":1700126443,"paper_link":"http://arxiv.org/pdf/2311.10118v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Since signet ring cells (SRCs) are associated with high peripheral metastasis rate and dismal survival, they play an important role in determining surgical approaches and prognosis, while they are easily missed by even experienced pathologists. Although automatic diagnosis SRCs based on deep learning has received increasing attention to assist pathologists in improving the diagnostic efficiency and accuracy, the existing works have not been systematically overviewed, which hindered the evaluation of the gap between algorithms and clinical applications. In this paper, we provide a survey on SRC analysis driven by deep learning from 2008 to August 2023. Specifically, the biological characteristics of SRCs and the challenges of automatic identification are systemically summarized. Then, the representative algorithms are analyzed and compared via dividing them into classification, detection, and segmentation. Finally, for comprehensive consideration to the performance of existing methods and the requirements for clinical assistance, we discuss the open issues and future trends of SRC analysis. The retrospect research will help researchers in the related fields, particularly for who without medical science background not only to clearly find the outline of SRC analysis, but also gain the prospect of intelligent diagnosis, resulting in accelerating the practice and application of intelligent algorithms."}
{"title":"DECDM: Document Enhancement using Cycle-Consistent Diffusion Models","authors":["Jiaxin Zhang","Joy Rimchala","Lalla Mouatadid","Kamalika Das","Sricharan Kumar"],"raw_abstract":"The performance of optical character recognition (OCR) heavily relies on\ndocument image quality, which is crucial for automatic document processing and\ndocument intelligence. However, most existing document enhancement methods\nrequire supervised data pairs, which raises concerns about data separation and\nprivacy protection, and makes it challenging to adapt these methods to new\ndomain pairs. To address these issues, we propose DECDM, an end-to-end\ndocument-level image translation method inspired by recent advances in\ndiffusion models. Our method overcomes the limitations of paired training by\nindependently training the source (noisy input) and target (clean output)\nmodels, making it possible to apply domain-specific diffusion models to other\npairs. DECDM trains on one dataset at a time, eliminating the need to scan both\ndatasets concurrently, and effectively preserving data privacy from the source\nor target domain. We also introduce simple data augmentation strategies to\nimprove character-glyph conservation during translation. We compare DECDM with\nstate-of-the-art methods on multiple synthetic data and benchmark datasets,\nsuch as document denoising and {\\color{black}shadow} removal, and demonstrate\nthe superiority of performance quantitatively and qualitatively.","publication_date":1700118962,"paper_link":"http://arxiv.org/pdf/2311.09625v1","categories":["Quantitative Biology"],"abstract":"The performance of optical character recognition (OCR) heavily relies on document image quality, which is crucial for automatic document processing and document intelligence. However, most existing document enhancement methods require supervised data pairs, which raises concerns about data separation and privacy protection, and makes it challenging to adapt these methods to new domain pairs. To address these issues, we propose DECDM, an end-to-end document-level image translation method inspired by recent advances in diffusion models. Our method overcomes the limitations of paired training by independently training the source (noisy input) and target (clean output) models, making it possible to apply domain-specific diffusion models to other pairs. DECDM trains on one dataset at a time, eliminating the need to scan both datasets concurrently, and effectively preserving data privacy from the source or target domain. We also introduce simple data augmentation strategies to improve character-glyph conservation during translation. We compare DECDM with state-of-the-art methods on multiple synthetic data and benchmark datasets, such as document denoising and {blackshadow} removal, and demonstrate the superiority of performance quantitatively and qualitatively."}
{"title":"Apoptosis classification using attention based spatio temporal graph convolution neural network","authors":["Akash Awasthi"],"raw_abstract":"Accurate classification of apoptosis plays an important role in cell biology\nresearch. There are many state-of-the-art approaches which use deep CNNs to\nperform the apoptosis classification but these approaches do not account for\nthe cell interaction. Our paper proposes the Attention Graph spatio-temporal\ngraph convolutional network to classify the cell death based on the target\ncells in the video. This method considers the interaction of multiple target\ncells at each time stamp. We model the whole video sequence as a set of graphs\nand classify the target cell in the video as dead or alive. Our method\nencounters both spatial and temporal relationships.","publication_date":1700118890,"paper_link":"http://arxiv.org/pdf/2311.09623v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Accurate classification of apoptosis plays an important role in cell biology research. There are many state-of-the-art approaches which use deep CNNs to perform the apoptosis classification but these approaches do not account for the cell interaction. Our paper proposes the Attention Graph spatio-temporal graph convolutional network to classify the cell death based on the target cells in the video. This method considers the interaction of multiple target cells at each time stamp. We model the whole video sequence as a set of graphs and classify the target cell in the video as dead or alive. Our method encounters both spatial and temporal relationships."}
{"title":"Digital Socrates: Evaluating LLMs through explanation critiques","authors":["Yuling Gu","Oyvind Tafjord","Peter Clark"],"raw_abstract":"While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritiquing model (called Digital Socrates) using this data. Through\nquantitative and qualitative analysis, we demonstrate how Digital Socrates is\nuseful for revealing insights about student models by examining their reasoning\nchains, and how it can provide high-quality, nuanced, automatic evaluation of\nthose model explanations for the first time. Digital Socrates thus fills an\nimportant gap in evaluation tools for understanding and improving the\nexplanation behavior of models.","publication_date":1700117506,"paper_link":"http://arxiv.org/pdf/2311.09613v1","categories":["Quantitative Biology"],"abstract":"While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critiquing model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models."}
{"title":"Universal Hyperuniform Organization of Cellular Structures in Leaf Vein Networks","authors":["Yuan Liu","Duyu Chen","Yang Jiao","Jianxiang Tian"],"raw_abstract":"Leaf vein network is a hierarchical vascular system that transports water and\nnutrients to the leaf cells. The thick primary veins form a branched network,\nwhile the secondary veins develop closed circuits forming a well-defined\ncellular structure. Through extensive analysis of a variety of distinct leaf\nspecies, we discover that the apparently disordered cellular structures of the\nsecondary vein networks exhibit a universal hyperuniform organization and\npossess a hidden order on large scales. Disorder hyperuniform (DHU) systems\nlack conventional long-range order, yet they completely suppress normalized\nlarge-scale density fluctuations like crystals. Specifically, we find that the\ndistributions of the geometric centers associated with the vein network loops\npossess a vanishing static structure factor in the zero-wavenumber limit, i.e.,\n$S(k) \\sim k^{\\alpha}$, where $\\alpha \\approx 0.64$, providing an example of\nclass III hyperuniformity in biology. This hyperuniform organization leads to\nsuperior efficiency of diffusive transport, as evidenced by the much faster\nconvergence of the time-dependent spreadability $\\mathcal{S}(t)$ to its\nlong-time asymptotic limit, compared to that of other uncorrelated or\ncorrelated disordered but non-hyperuniform organizations. Our results also have\nimplications for the discovery and design of novel disordered network materials\nwith optimal transport properties.","publication_date":1700107891,"paper_link":"http://arxiv.org/pdf/2311.09551v1","categories":["Physics"],"abstract":"Leaf vein network is a hierarchical vascular system that transports water and nutrients to the leaf cells. The thick primary veins form a branched network, while the secondary veins develop closed circuits forming a well-defined cellular structure. Through extensive analysis of a variety of distinct leaf species, we discover that the apparently disordered cellular structures of the secondary vein networks exhibit a universal hyperuniform organization and possess a hidden order on large scales. Disorder hyperuniform (DHU) systems lack conventional long-range order, yet they completely suppress normalized large-scale density fluctuations like crystals. Specifically, we find that the distributions of the geometric centers associated with the vein network loops possess a vanishing static structure factor in the zero-wavenumber limit, i.e., __FORMULA__, where __FORMULA__, providing an example of class III hyperuniformity in biology. This hyperuniform organization leads to superior efficiency of diffusive transport, as evidenced by the much faster convergence of the time-dependent spreadability __FORMULA__ to its long-time asymptotic limit, compared to that of other uncorrelated or correlated disordered but non-hyperuniform organizations. Our results also have implications for the discovery and design of novel disordered network materials with optimal transport properties."}
{"title":"Entanglement constraint on wave-particle duality for tripartite systems","authors":["Zanjia Li","Yingqiu He","Dong Ding","Ting Gao","Fengli Yan"],"raw_abstract":"A global multi-partite entanglement may place a constraint on the\nwave-particle duality. We investigate this constraint relation of the global\nentanglement and the quantitative wave-particle duality in tripartite systems.\nWe perform quantum state tomography to reconstruct the reduced density matrix\nby using the OriginQ quantum computing cloud platform. As a result, we show\nthat, theoretically and experimentally, the quantitative wave-particle duality\nis indeed constrained by the global tripartite entanglement.","publication_date":1700105328,"paper_link":"http://arxiv.org/pdf/2311.09539v1","categories":["Physics"],"abstract":"A global multi-partite entanglement may place a constraint on the wave-particle duality. We investigate this constraint relation of the global entanglement and the quantitative wave-particle duality in tripartite systems. We perform quantum state tomography to reconstruct the reduced density matrix by using the OriginQ quantum computing cloud platform. As a result, we show that, theoretically and experimentally, the quantitative wave-particle duality is indeed constrained by the global tripartite entanglement."}
{"title":"Heteroskedasticity as a Signature of Association for Age-Related Genes","authors":["Salman Mohamadi","Donald A. Adjeroh"],"raw_abstract":"Human aging is a process controlled by both genetics and environment. Many\nstudies have been conducted to identify a subset of genes related to aging from\nthe human genome. Biologists implicitly categorize age-related genes into genes\nthat cause aging and genes that are influenced by aging, which resulted in both\ncausal inference and inference of associations studies. While inference of\nassociation is better explored, causal inference and computational causal\ninference, remains less explored. In this work, we are primarily motivated to\ntackle the problem of identifying genes associated with aging, while having a\nbrief look into genes with probable causal relations, both from a computational\nperspective. Specifically, we form a set of hypotheses and accordingly,\nintroduce a data-tailored framework for inference. First we perform linear\nmodeling on the expression values of age-related genes, and then examine the\npresence of heteroskedastic properties in the residual of the model. We\nevaluate this framework and our results suggest that, 1) presence of\nheteroskedasticity in these residuals is a potential signature of association\nfor age-related genes, and 2) consistent heteroskedasticity along the human\nlife span could imply some sort of causality. To our knowledge, along with\nidentifying age-associated genes, this is the first work to propose a framework\nfor computational causal inference on age-related genes, using a dataset of\nhuman dermal fibroblast gene expression data. Hence the results of our simple,\nyet effective approach can be used not only to assess future age-related genes,\nbut also as a possible criterion to select new associative or potential causal\ngenes with respect to aging.","publication_date":1700087192,"paper_link":"http://arxiv.org/pdf/2311.09411v1","categories":["Quantitative Biology"],"abstract":"Human aging is a process controlled by both genetics and environment. Many studies have been conducted to identify a subset of genes related to aging from the human genome. Biologists implicitly categorize age-related genes into genes that cause aging and genes that are influenced by aging, which resulted in both causal inference and inference of associations studies. While inference of association is better explored, causal inference and computational causal inference, remains less explored. In this work, we are primarily motivated to tackle the problem of identifying genes associated with aging, while having a brief look into genes with probable causal relations, both from a computational perspective. Specifically, we form a set of hypotheses and accordingly, introduce a data-tailored framework for inference. First we perform linear modeling on the expression values of age-related genes, and then examine the presence of heteroskedastic properties in the residual of the model. We evaluate this framework and our results suggest that, 1) presence of heteroskedasticity in these residuals is a potential signature of association for age-related genes, and 2) consistent heteroskedasticity along the human life span could imply some sort of causality. To our knowledge, along with identifying age-associated genes, this is the first work to propose a framework for computational causal inference on age-related genes, using a dataset of human dermal fibroblast gene expression data. Hence the results of our simple, yet effective approach can be used not only to assess future age-related genes, but also as a possible criterion to select new associative or potential causal genes with respect to aging."}
{"title":"Nondestructive, quantitative viability analysis of 3D tissue cultures using machine learning image segmentation","authors":["Kylie J. Trettner","Jeremy Hsieh","Weikun Xiao","Jerry S. H. Lee","Andrea M. Armani"],"raw_abstract":"Ascertaining the collective viability of cells in different cell culture\nconditions has typically relied on averaging colorimetric indicators and is\noften reported out in simple binary readouts. Recent research has combined\nviability assessment techniques with image-based deep-learning models to\nautomate the characterization of cellular properties. However, further\ndevelopment of viability measurements to assess the continuity of possible\ncellular states and responses to perturbation across cell culture conditions is\nneeded. In this work, we demonstrate an image processing algorithm for\nquantifying cellular viability in 3D cultures without the need for assay-based\nindicators. We show that our algorithm performs similarly to a pair of human\nexperts in whole-well images over a range of days and culture matrix\ncompositions. To demonstrate potential utility, we perform a longitudinal study\ninvestigating the impact of a known therapeutic on pancreatic cancer spheroids.\nUsing images taken with a high content imaging system, the algorithm\nsuccessfully tracks viability at the individual spheroid and whole-well level.\nThe method we propose reduces analysis time by 97% in comparison to the\nexperts. Because the method is independent of the microscope or imaging system\nused, this approach lays the foundation for accelerating progress in and for\nimproving the robustness and reproducibility of 3D culture analysis across\nbiological and clinical research.","publication_date":1700080111,"paper_link":"http://arxiv.org/pdf/2311.09354v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Ascertaining the collective viability of cells in different cell culture conditions has typically relied on averaging colorimetric indicators and is often reported out in simple binary readouts. Recent research has combined viability assessment techniques with image-based deep-learning models to automate the characterization of cellular properties. However, further development of viability measurements to assess the continuity of possible cellular states and responses to perturbation across cell culture conditions is needed. In this work, we demonstrate an image processing algorithm for quantifying cellular viability in 3D cultures without the need for assay-based indicators. We show that our algorithm performs similarly to a pair of human experts in whole-well images over a range of days and culture matrix compositions. To demonstrate potential utility, we perform a longitudinal study investigating the impact of a known therapeutic on pancreatic cancer spheroids. Using images taken with a high content imaging system, the algorithm successfully tracks viability at the individual spheroid and whole-well level. The method we propose reduces analysis time by 97% in comparison to the experts. Because the method is independent of the microscope or imaging system used, this approach lays the foundation for accelerating progress in and for improving the robustness and reproducibility of 3D culture analysis across biological and clinical research."}
{"title":"Proximity-induced gapless superconductivity in two-dimensional Rashba semiconductor in magnetic field","authors":["Serafim S. Babkin","Andrew P. Higginbotham","Maksym Serbyn"],"raw_abstract":"Two-dimensional semiconductor-superconductor heterostructures form the\nfoundation of numerous nanoscale physical systems. However, measuring the\nproperties of such heterostructures, and characterizing the semiconductor\nin-situ is challenging. A recent experimental study [arXiv:2107.03695] was able\nto probe the semiconductor within the heterostructure using microwave\nmeasurements of the superfluid density. This work revealed a rapid depletion of\nsuperfluid density in semiconductor, caused by the in-plane magnetic field\nwhich in presence of spin-orbit coupling creates so-called Bogoliubov Fermi\nsurfaces. The experimental work used a simplified theoretical model that\nneglected the presence of non-magnetic disorder in the semiconductor, hence\ndescribing the data only qualitatively. Motivated by experiments, we introduce\na theoretical model describing a disordered semiconductor with strong\nspin-orbit coupling that is proximitized by a superconductor. Our model\nprovides specific predictions for the density of states and superfluid density.\nPresence of disorder leads to the emergence of a gapless superconducting phase,\nthat may be viewed as a manifestation of Bogoliubov Fermi surface. When applied\nto real experimental data, our model showcases excellent quantitative\nagreement, enabling the extraction of material parameters such as mean free\npath and mobility, and estimating $g$-tensor after taking into account the\norbital contribution of magnetic field. Our model can be used to probe in-situ\nparameters of other superconductor-semiconductor heterostructures and can be\nfurther extended to give access to transport properties.","publication_date":1700079017,"paper_link":"http://arxiv.org/pdf/2311.09347v1","categories":["Physics"],"abstract":"Two-dimensional semiconductor-superconductor heterostructures form the foundation of numerous nanoscale physical systems. However, measuring the properties of such heterostructures, and characterizing the semiconductor in-situ is challenging. A recent experimental study [arXiv:2107.03695] was able to probe the semiconductor within the heterostructure using microwave measurements of the superfluid density. This work revealed a rapid depletion of superfluid density in semiconductor, caused by the in-plane magnetic field which in presence of spin-orbit coupling creates so-called Bogoliubov Fermi surfaces. The experimental work used a simplified theoretical model that neglected the presence of non-magnetic disorder in the semiconductor, hence describing the data only qualitatively. Motivated by experiments, we introduce a theoretical model describing a disordered semiconductor with strong spin-orbit coupling that is proximitized by a superconductor. Our model provides specific predictions for the density of states and superfluid density. Presence of disorder leads to the emergence of a gapless superconducting phase, that may be viewed as a manifestation of Bogoliubov Fermi surface. When applied to real experimental data, our model showcases excellent quantitative agreement, enabling the extraction of material parameters such as mean free path and mobility, and estimating __FORMULA__-tensor after taking into account the orbital contribution of magnetic field. Our model can be used to probe in-situ parameters of other superconductor-semiconductor heterostructures and can be further extended to give access to transport properties."}
{"title":"Phylogenetic trees defined by at most three characters","authors":["Katharina T. Huber","Simone Linz","Vincent Moulton","Charles Semple"],"raw_abstract":"In evolutionary biology, phylogenetic trees are commonly inferred from a set\nof characters (partitions) of a collection of biological entities (e.g.,\nspecies or individuals in a population). Such characters naturally arise from\nmolecular sequences or morphological data. Interestingly, it has been known for\nsome time that any binary phylogenetic tree can be (convexly) defined by a set\nof at most four characters, and that there are binary phylogenetic trees for\nwhich three characters are not enough. Thus, it is of interest to characterise\nthose phylogenetic trees that are defined by a set of at most three characters.\nIn this paper, we provide such a characterisation, in particular proving that a\nbinary phylogenetic tree $T$ is defined by a set of at most three characters\nprecisely if $T$ has no internal subtree isomorphic to a certain tree.","publication_date":1700078322,"paper_link":"http://arxiv.org/pdf/2311.09340v1","categories":["Mathematics","Quantitative Biology"],"abstract":"In evolutionary biology, phylogenetic trees are commonly inferred from a set of characters (partitions) of a collection of biological entities (e.g., species or individuals in a population). Such characters naturally arise from molecular sequences or morphological data. Interestingly, it has been known for some time that any binary phylogenetic tree can be (convexly) defined by a set of at most four characters, and that there are binary phylogenetic trees for which three characters are not enough. Thus, it is of interest to characterise those phylogenetic trees that are defined by a set of at most three characters. In this paper, we provide such a characterisation, in particular proving that a binary phylogenetic tree __FORMULA__ is defined by a set of at most three characters precisely if __FORMULA__ has no internal subtree isomorphic to a certain tree."}
{"title":"Physical models of traffic safety at crossings","authors":["Andreas Leich","Ronald Nippold","Andreas Schadschneider","Peter Wagner"],"raw_abstract":"Traffic safety at intersections is studied quantitatively using methods from\nStatistical Mechanics on the basis of simple microscopic traffic flow models.\nIn order to determine a relationship between traffic flow and the number of\ncrashes, the modelling focus is on the building block of any road network,\nnamely the crossing of two streams. In this paper, it is shown that the number\nof crossing conflicts is proportional to the product of the two traffic flows\nfrom which a simple model is developed. This model substantiates known\nempirical findings. Since real crash data are obtained by an involved process\nfrom such building blocks, there is a difference between the theoretical and\nempirical results. This process is modelled here as well and narrows the gap\nbetween theory and observation.","publication_date":1700076271,"paper_link":"http://arxiv.org/pdf/2311.09318v1","categories":["Physics"],"abstract":"Traffic safety at intersections is studied quantitatively using methods from Statistical Mechanics on the basis of simple microscopic traffic flow models. In order to determine a relationship between traffic flow and the number of crashes, the modelling focus is on the building block of any road network, namely the crossing of two streams. In this paper, it is shown that the number of crossing conflicts is proportional to the product of the two traffic flows from which a simple model is developed. This model substantiates known empirical findings. Since real crash data are obtained by an involved process from such building blocks, there is a difference between the theoretical and empirical results. This process is modelled here as well and narrows the gap between theory and observation."}
{"title":"Project Dinos I: A joint lensing-dynamics constraint on the deviation from the power law in the mass profile of massive ellipticals","authors":["Chin Yi Tan","Anowar J. Shajib","Simon Birrer","Alessandro Sonnenfeld","Tommaso Treu","Patrick Wells","Devon Williams","Elizabeth J. Buckley-Geer","Alex Drlica-Wagner","Joshua Frieman"],"raw_abstract":"The mass distribution in massive elliptical galaxies encodes their\nevolutionary history, thus providing an avenue to constrain the baryonic\nastrophysics in their evolution. The power-law assumption for the radial mass\nprofile in ellipticals has been sufficient to describe several observables to\nthe noise level, including strong lensing and stellar dynamics. In this paper,\nwe quantitatively constrained any deviation, or the lack thereof, from the\npower-law mass profile in massive ellipticals through joint lensing-dynamics\nanalysis of a large statistical sample with 77 galaxy-galaxy lens systems. We\nperformed an improved and uniform lens modelling of these systems from archival\nHubble Space Telescope imaging using the automated lens modelling pipeline\ndolphin. We combined the lens model posteriors with the stellar dynamics to\nconstrain the deviation from the power law after accounting for the\nline-of-sight lensing effects, a first for analyses on galaxy-galaxy lenses. We\nfind that the Sloan Lens ACS Survey (SLACS) lens galaxies with a mean redshift\nof 0.2 are consistent with the power-law profile within 1.1$\\sigma$\n(2.8$\\sigma$) and the Strong Lensing Legacy Survey (SL2S) lens galaxies with a\nmean redshift of 0.6 are consistent within 0.8$\\sigma$ (2.1$\\sigma$), for a\nspatially constant (Osipkov-Merritt) stellar anisotropy profile. We adopted the\nspatially constant anisotropy profile as our baseline choice based on previous\ndynamical observables of local ellipticals. However, spatially resolved stellar\nkinematics of lens galaxies are necessary to differentiate between the two\nanisotropy models. Future studies will use our lens models to constrain the\nmass distribution individually in the dark matter and baryonic components.","publication_date":1700074862,"paper_link":"http://arxiv.org/pdf/2311.09307v1","categories":["Physics"],"abstract":"The mass distribution in massive elliptical galaxies encodes their evolutionary history, thus providing an avenue to constrain the baryonic astrophysics in their evolution. The power-law assumption for the radial mass profile in ellipticals has been sufficient to describe several observables to the noise level, including strong lensing and stellar dynamics. In this paper, we quantitatively constrained any deviation, or the lack thereof, from the power-law mass profile in massive ellipticals through joint lensing-dynamics analysis of a large statistical sample with 77 galaxy-galaxy lens systems. We performed an improved and uniform lens modelling of these systems from archival Hubble Space Telescope imaging using the automated lens modelling pipeline dolphin. We combined the lens model posteriors with the stellar dynamics to constrain the deviation from the power law after accounting for the line-of-sight lensing effects, a first for analyses on galaxy-galaxy lenses. We find that the Sloan Lens ACS Survey (SLACS) lens galaxies with a mean redshift of 0.2 are consistent with the power-law profile within 1.1__FORMULA__ (2.8__FORMULA__) and the Strong Lensing Legacy Survey (SL2S) lens galaxies with a mean redshift of 0.6 are consistent within 0.8__FORMULA__ (2.1__FORMULA__), for a spatially constant (Osipkov-Merritt) stellar anisotropy profile. We adopted the spatially constant anisotropy profile as our baseline choice based on previous dynamical observables of local ellipticals. However, spatially resolved stellar kinematics of lens galaxies are necessary to differentiate between the two anisotropy models. Future studies will use our lens models to constrain the mass distribution individually in the dark matter and baryonic components."}
{"title":"Kagome Materials II: SG 191: FeGe as a LEGO Building Block for the Entire 1:6:6 series: hidden d-orbital decoupling of flat band sectors, effective models and interaction Hamiltonians","authors":["Yi Jiang","Haoyu Hu","Dumitru C\u0103lug\u0103ru","Claudia Felser","Santiago Blanco-Canosa","Hongming Weng","Yuanfeng Xu","B. Andrei Bernevig"],"raw_abstract":"The electronic structure and interactions of kagome materials such as 1:1\n(FeGe class) and 1:6:6 (MgFe$_6$Ge$_6$ class) are complicated and involve many\norbitals and bands at the Fermi level. Current theoretical models treat the\nsystems in an $s$-orbital kagome representation, unsuited and incorrect both\nquantitatively and qualitatively to the material realities. In this work, we\nlay the basis of a faithful framework of the electronic model for this large\nclass of materials. We show that the complicated ``spaghetti\" of electronic\nbands near the Fermi level can be decomposed into three groups of $d$-Fe\norbitals coupled to specific Ge orbitals. Such decomposition allows for a clear\nanalytical understanding (leading to different results than the simple\n$s$-orbital kagome models) of the flat bands in the system based on the\n$S$-matrix formalism of generalized bipartite lattices. Our three minimal\nHamiltonians can reproduce the quasi-flat bands, van Hove singularities,\ntopology, and Dirac points close to the Fermi level, which we prove by\nextensive ab initio studies. We also obtain the interacting Hamiltonian of $d$\norbitals in FeGe using the constraint random phase approximation (cRPA) method.\nWe then use this as a fundamental ``LEGO\"-like building block for a large\nfamily of 1:6:6 kagome materials, which can be obtained by doubling and\nperturbing the FeGe Hamiltonian. We applied the model to its kagome siblings\nFeSn and CoSn, and also MgFe$_6$Ge$_6$. Our work serves as the first complete\nframework for the study of the interacting phase diagram of kagome compounds.","publication_date":1700074801,"paper_link":"http://arxiv.org/pdf/2311.09290v1","categories":["Physics"],"abstract":"The electronic structure and interactions of kagome materials such as 1:1 (FeGe class) and 1:6:6 (MgFe__FORMULA__Ge__FORMULA__ class) are complicated and involve many orbitals and bands at the Fermi level. Current theoretical models treat the systems in an __FORMULA__-orbital kagome representation, unsuited and incorrect both quantitatively and qualitatively to the material realities. In this work, we lay the basis of a faithful framework of the electronic model for this large class of materials. We show that the complicated ``spaghetti\" of electronic bands near the Fermi level can be decomposed into three groups of __FORMULA__-Fe orbitals coupled to specific Ge orbitals. Such decomposition allows for a clear analytical understanding (leading to different results than the simple __FORMULA__-orbital kagome models) of the flat bands in the system based on the __FORMULA__-matrix formalism of generalized bipartite lattices. Our three minimal Hamiltonians can reproduce the quasi-flat bands, van Hove singularities, topology, and Dirac points close to the Fermi level, which we prove by extensive ab initio studies. We also obtain the interacting Hamiltonian of __FORMULA__ orbitals in FeGe using the constraint random phase approximation (cRPA) method. We then use this as a fundamental ``LEGO\"-like building block for a large family of 1:6:6 kagome materials, which can be obtained by doubling and perturbing the FeGe Hamiltonian. We applied the model to its kagome siblings FeSn and CoSn, and also MgFe__FORMULA__Ge__FORMULA__. Our work serves as the first complete framework for the study of the interacting phase diagram of kagome compounds."}
{"title":"Full range spectral correlations and their spectral form factors in chaotic and integrable models","authors":["Ruth Shir","Pablo Martinez-Azcona","Aur\u00e9lia Chenu"],"raw_abstract":"Quantum chaotic systems are characterized by energy correlations in their\nspectral statistics, usually probed by the distribution of nearest-neighbor\nlevel spacings. Some signatures of chaos, like the spectral form factor (SFF),\ntake all the correlations into account, while others sample only short-range or\nlong-range correlations. Here, we characterize correlations between\neigenenergies at all possible spectral distances. Specifically, we study the\ndistribution of $k$-th neighbor level spacings ($k$nLS) and compute its\nassociated $k$-th neighbor spectral form factor ($k$nSFF). This leads to two\nnew full-range signatures of quantum chaos, the variance of the $k$nLS\ndistribution and the minimum value of the $k$nSFF, which quantitatively\ncharacterize correlations between pairs of eigenenergies with any number of\nlevels $k$ between them. We find exact and approximate expressions for these\nsignatures in the three Gaussian ensembles of random matrix theory (GOE, GUE\nand GSE) and in integrable systems with completely uncorrelated spectra (the\nPoisson ensemble). We illustrate our findings in a XXZ spin chain with\ndisorder, which interpolates between chaotic and integrable behavior. Our\nrefined measures of chaos allow us to probe deviations from Poissonian and\nRandom Matrix behavior in realistic systems. This illustrates how the measures\nwe introduce bring a new light into studying many-body quantum systems, which\nlie in-between the fully chaotic or fully integrable models.","publication_date":1700074801,"paper_link":"http://arxiv.org/pdf/2311.09292v1","categories":["Mathematics","Physics"],"abstract":"Quantum chaotic systems are characterized by energy correlations in their spectral statistics, usually probed by the distribution of nearest-neighbor level spacings. Some signatures of chaos, like the spectral form factor (SFF), take all the correlations into account, while others sample only short-range or long-range correlations. Here, we characterize correlations between eigenenergies at all possible spectral distances. Specifically, we study the distribution of __FORMULA__-th neighbor level spacings (__FORMULA__nLS) and compute its associated __FORMULA__-th neighbor spectral form factor (__FORMULA__nSFF). This leads to two new full-range signatures of quantum chaos, the variance of the __FORMULA__nLS distribution and the minimum value of the __FORMULA__nSFF, which quantitatively characterize correlations between pairs of eigenenergies with any number of levels __FORMULA__ between them. We find exact and approximate expressions for these signatures in the three Gaussian ensembles of random matrix theory (GOE, GUE and GSE) and in integrable systems with completely uncorrelated spectra (the Poisson ensemble). We illustrate our findings in a XXZ spin chain with disorder, which interpolates between chaotic and integrable behavior. Our refined measures of chaos allow us to probe deviations from Poissonian and Random Matrix behavior in realistic systems. This illustrates how the measures we introduce bring a new light into studying many-body quantum systems, which lie in-between the fully chaotic or fully integrable models."}
{"title":"Hyperlinear approximations to amenable groups come from sofic approximations","authors":["Peter Burton","Maksym Chaudkhari","Kate Juschenko","Kyrylo Muliarchyk"],"raw_abstract":"We provide a quantitative formulation of the equivalence between\nhyperlinearity and soficity for amenable groups, showing that every hyperlinear\napproximation to such a group is essentially produced from a sofic\napproximation. The proof is probabilistic, using the concentration of measure\nin high dimensional spheres to control the deviation of an operator's matrix\ncoefficients from its trace. As a corollary, we obtain a result connecting\nstability of sofic approximations with stability of hyperlinear approximations.","publication_date":1700073876,"paper_link":"http://arxiv.org/pdf/2311.09202v1","categories":["Mathematics"],"abstract":"We provide a quantitative formulation of the equivalence between hyperlinearity and soficity for amenable groups, showing that every hyperlinear approximation to such a group is essentially produced from a sofic approximation. The proof is probabilistic, using the concentration of measure in high dimensional spheres to control the deviation of an operator's matrix coefficients from its trace. As a corollary, we obtain a result connecting stability of sofic approximations with stability of hyperlinear approximations."}
{"title":"RBPGAN: Recurrent Back-Projection GAN for Video Super Resolution","authors":["Israa Fahmy","Marwah Sulaiman","Zahraa Shehabeldin","Mohammed Barakat","Dareen Hussein","Mohammed El-Naggar","Hesham Eraqi","Moustafa Youssef"],"raw_abstract":"Recently, video super resolution (VSR) has become a very impactful task in\nthe area of Computer Vision due to its various applications. In this paper, we\npropose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for\nVSR in an attempt to generate temporally coherent solutions while preserving\nspatial details. RBPGAN integrates two state-of-the-art models to get the best\nin both worlds without compromising the accuracy of produced video. The\ngenerator of the model is inspired by RBPN system, while the discriminator is\ninspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal\nconsistency over time. Our contribution together results in a model that\noutperforms earlier work in terms of temporally consistent details, as we will\ndemonstrate qualitatively and quantitatively using different datasets.","publication_date":1700072130,"paper_link":"http://arxiv.org/pdf/2311.09178v2","categories":["Quantitative Biology"],"abstract":"Recently, video super resolution (VSR) has become a very impactful task in the area of Computer Vision due to its various applications. In this paper, we propose Recurrent Back-Projection Generative Adversarial Network (RBPGAN) for VSR in an attempt to generate temporally coherent solutions while preserving spatial details. RBPGAN integrates two state-of-the-art models to get the best in both worlds without compromising the accuracy of produced video. The generator of the model is inspired by RBPN system, while the discriminator is inspired by TecoGAN. We also utilize Ping-Pong loss to increase temporal consistency over time. Our contribution together results in a model that outperforms earlier work in terms of temporally consistent details, as we will demonstrate qualitatively and quantitatively using different datasets."}
{"title":"Planetary nebulae as tracers of stellar population properties: unlocking their potential with integral-field spectroscopy","authors":["Ana Ennis","Johanna Hartke","Magda Arnaboldi","Claudia Pulsoni","Fuyan Bian","Chiara Spiniello"],"raw_abstract":"Planetary nebulae (PNe) are essential tracers of the kinematics of the\ndiffuse halo and intracluster light where stellar spectroscopy is unfeasible,\ndue to their strong emission lines. However, that is not all they can reveal\nabout the underlying stellar population. In recent years, it has also been\nfound that PNe in the metal-poor halos of galaxies have different properties\n(specific frequency, luminosity function), than PNe in the more metal-rich\ngalaxy centers. A more quantitative understanding of the role of age and\nmetallicity in these relations would turn PNe into valuable stellar-population\ntracers. In order to do that, a full characterization of PNe in regions where\nthe stellar light can also be analysed in detail is necessary. In this work, we\nmake use of integral-field spectroscopic data covering the central regions of\ngalaxies, which allow us to measure both stellar ages and metallicities as well\nas to detect PNe. This analysis is fundamental to calibrate PNe as stellar\npopulation tracers and to push our understanding of galaxy properties at\nunprecedented galactocentric distances.","publication_date":1700072112,"paper_link":"http://arxiv.org/pdf/2311.09176v1","categories":["Physics"],"abstract":"Planetary nebulae (PNe) are essential tracers of the kinematics of the diffuse halo and intracluster light where stellar spectroscopy is unfeasible, due to their strong emission lines. However, that is not all they can reveal about the underlying stellar population. In recent years, it has also been found that PNe in the metal-poor halos of galaxies have different properties (specific frequency, luminosity function), than PNe in the more metal-rich galaxy centers. A more quantitative understanding of the role of age and metallicity in these relations would turn PNe into valuable stellar-population tracers. In order to do that, a full characterization of PNe in regions where the stellar light can also be analysed in detail is necessary. In this work, we make use of integral-field spectroscopic data covering the central regions of galaxies, which allow us to measure both stellar ages and metallicities as well as to detect PNe. This analysis is fundamental to calibrate PNe as stellar population tracers and to push our understanding of galaxy properties at unprecedented galactocentric distances."}
{"title":"A short note on effective Pauli noise models","authors":["Michael A. Perlin"],"raw_abstract":"We provide a simple prescription to extract an effective Pauli noise model\nfrom classical simulations of a noisy experimental protocol for a unitary gate.\nThis prescription yields the closest Pauli channel approximation to the error\nchannel associated with the gate implementation, as measured by the Frobenius\ndistance between quantum channels. Informed by these results, we highlight some\npuzzles regarding the quantitative treatment of coherent errors.","publication_date":1700068727,"paper_link":"http://arxiv.org/pdf/2311.09129v1","categories":["Physics"],"abstract":"We provide a simple prescription to extract an effective Pauli noise model from classical simulations of a noisy experimental protocol for a unitary gate. This prescription yields the closest Pauli channel approximation to the error channel associated with the gate implementation, as measured by the Frobenius distance between quantum channels. Informed by these results, we highlight some puzzles regarding the quantitative treatment of coherent errors."}
{"title":"Constructing interpretable principal curve using Neural ODEs","authors":["Guangzheng Zhang","Bingxian Xu"],"raw_abstract":"The study of high dimensional data sets often rely on their low dimensional\nprojections that preserve the local geometry of the original space. While\nnumerous methods have been developed to summarize this space as variations of\ntree-like structures, they are usually non-parametric and \"static\" in nature.\nAs data may come from systems that are dynamical such as a differentiating\ncell, a static, non-parametric characterization of the space may not be the\nmost appropriate. Here, we developed a framework, the principal flow, that is\ncapable of characterizing the space in a dynamical manner. The principal flow,\ndefined using neural ODEs, directs motion of a particle through the space,\nwhere the trajectory of the particle resembles the principal curve of the\ndataset. We illustrate that our framework can be used to characterize shapes of\nvarious complexities, and is flexible to incorporate summaries of relaxation\ndynamics.","publication_date":1700066813,"paper_link":"http://arxiv.org/pdf/2311.09274v1","categories":["Quantitative Biology"],"abstract":"The study of high dimensional data sets often rely on their low dimensional projections that preserve the local geometry of the original space. While numerous methods have been developed to summarize this space as variations of tree-like structures, they are usually non-parametric and \"static\" in nature. As data may come from systems that are dynamical such as a differentiating cell, a static, non-parametric characterization of the space may not be the most appropriate. Here, we developed a framework, the principal flow, that is capable of characterizing the space in a dynamical manner. The principal flow, defined using neural ODEs, directs motion of a particle through the space, where the trajectory of the particle resembles the principal curve of the dataset. We illustrate that our framework can be used to characterize shapes of various complexities, and is flexible to incorporate summaries of relaxation dynamics."}
{"title":"Posterior accuracy and calibration under misspecification in Bayesian generalized linear models","authors":["Maximilian Scholz","Paul-Christian B\u00fcrkner"],"raw_abstract":"Generalized linear models (GLMs) are popular for data-analysis in almost all\nquantitative sciences, but the choice of likelihood family and link function is\noften difficult. This motivates the search for likelihoods and links that\nminimize the impact of potential misspecification. We perform a large-scale\nsimulation study on double-bounded and lower-bounded response data where we\nsystematically vary both true and assumed likelihoods and links. In contrast to\nprevious studies, we also study posterior calibration and uncertainty metrics\nin addition to point-estimate accuracy. Our results indicate that certain\nlikelihoods and links can be remarkably robust to misspecification, performing\nalmost on par with their respective true counterparts. Additionally, normal\nlikelihood models with identity link (i.e., linear regression) often achieve\ncalibration comparable to the more structurally faithful alternatives, at least\nin the studied scenarios. On the basis of our findings, we provide practical\nsuggestions for robust likelihood and link choices in GLMs.","publication_date":1700065420,"paper_link":"http://arxiv.org/pdf/2311.09081v1","categories":["Statistics"],"abstract":"Generalized linear models (GLMs) are popular for data-analysis in almost all quantitative sciences, but the choice of likelihood family and link function is often difficult. This motivates the search for likelihoods and links that minimize the impact of potential misspecification. We perform a large-scale simulation study on double-bounded and lower-bounded response data where we systematically vary both true and assumed likelihoods and links. In contrast to previous studies, we also study posterior calibration and uncertainty metrics in addition to point-estimate accuracy. Our results indicate that certain likelihoods and links can be remarkably robust to misspecification, performing almost on par with their respective true counterparts. Additionally, normal likelihood models with identity link (i.e., linear regression) often achieve calibration comparable to the more structurally faithful alternatives, at least in the studied scenarios. On the basis of our findings, we provide practical suggestions for robust likelihood and link choices in GLMs."}
{"title":"Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness","authors":["Ahmed Emam","Mohamed Farag","Ribana Roscher"],"raw_abstract":"Protected natural areas are regions that have been minimally affected by\nhuman activities such as urbanization, agriculture, and other human\ninterventions. To better understand and map the naturalness of these areas,\nmachine learning models can be used to analyze satellite imagery. Specifically,\nexplainable machine learning methods show promise in uncovering patterns that\ncontribute to the concept of naturalness within these protected environments.\nAdditionally, addressing the uncertainty inherent in machine learning models is\ncrucial for a comprehensive understanding of this concept. However, existing\napproaches have limitations. They either fail to provide explanations that are\nboth valid and objective or struggle to offer a quantitative metric that\naccurately measures the contribution of specific patterns to naturalness, along\nwith the associated confidence. In this paper, we propose a novel framework\ncalled the Confident Naturalness Explanation (CNE) framework. This framework\ncombines explainable machine learning and uncertainty quantification to assess\nand explain naturalness. We introduce a new quantitative metric that describes\nthe confident contribution of patterns to the concept of naturalness.\nFurthermore, we generate an uncertainty-aware segmentation mask for each input\nsample, highlighting areas where the model lacks knowledge. To demonstrate the\neffectiveness of our framework, we apply it to a study site in Fennoscandia\nusing two open-source satellite datasets.","publication_date":1700054342,"paper_link":"http://arxiv.org/pdf/2311.08936v2","categories":["Quantitative Biology"],"abstract":"Protected natural areas are regions that have been minimally affected by human activities such as urbanization, agriculture, and other human interventions. To better understand and map the naturalness of these areas, machine learning models can be used to analyze satellite imagery. Specifically, explainable machine learning methods show promise in uncovering patterns that contribute to the concept of naturalness within these protected environments. Additionally, addressing the uncertainty inherent in machine learning models is crucial for a comprehensive understanding of this concept. However, existing approaches have limitations. They either fail to provide explanations that are both valid and objective or struggle to offer a quantitative metric that accurately measures the contribution of specific patterns to naturalness, along with the associated confidence. In this paper, we propose a novel framework called the Confident Naturalness Explanation (CNE) framework. This framework combines explainable machine learning and uncertainty quantification to assess and explain naturalness. We introduce a new quantitative metric that describes the confident contribution of patterns to the concept of naturalness. Furthermore, we generate an uncertainty-aware segmentation mask for each input sample, highlighting areas where the model lacks knowledge. To demonstrate the effectiveness of our framework, we apply it to a study site in Fennoscandia using two open-source satellite datasets."}
{"title":"The Anatomy of a Topological Phase Transition in a 2D Chern Insulator","authors":["Arjo Dasgupta","Indra Dasgupta"],"raw_abstract":"The onset of the topological phase transition in a two-dimensional model for\na Chern Insulator, namely the Qi-Wu-Zhang(QWZ) model, is illustrated, with\nparticular emphasis on the appearance of chiral edge-modes. The edge-modes are\nstudied by analysing the dynamics of the edge-states in an equivalent model for\na one-dimensional charge pump, using a technique known as dimensional\nextension. A further real-space analysis allows us to explain the onset of the\ntopological phase transition in terms of time-reversal symmetry breaking and to\nquantitatively study the localisation of the edge-modes.","publication_date":1700053698,"paper_link":"http://arxiv.org/pdf/2311.08932v1","categories":["Physics"],"abstract":"The onset of the topological phase transition in a two-dimensional model for a Chern Insulator, namely the Qi-Wu-Zhang(QWZ) model, is illustrated, with particular emphasis on the appearance of chiral edge-modes. The edge-modes are studied by analysing the dynamics of the edge-states in an equivalent model for a one-dimensional charge pump, using a technique known as dimensional extension. A further real-space analysis allows us to explain the onset of the topological phase transition in terms of time-reversal symmetry breaking and to quantitatively study the localisation of the edge-modes."}
{"title":"Engineering Transport via Collisional Noise: a Toolbox for Biology Systems","authors":["Alessandro Civolani","Vittoria Stanzione","Maria Luisa Chiofalo","Jorge Yago Malo"],"raw_abstract":"The study of noise assisted transport in quantum systems is essential in a\nwide range of applications from near-term NISQ devices to models for quantum\nbiology. Here, we study a generalised XXZ model in the presence of stochastic\ncollision noise, which allows to describe environments beyond the standard\nMarkovian formulation. Our analysis through the study of the local\nmagnetization, the inverse participation ratio (IPR) or its generalisation, the\nInverse Ergodicity Ratio (IER), showed clear regimes where the transport rate\nand coherence time can be controlled by the dissipation in a consistent manner.\nIn addition, when considering several excitations, we characterize the\ninterplay between collisions and system interactions identifying regimes in\nwhich transport is counterintuitively enhanced when increasing the collision\nrate, even in the case of initially separated excitations. These results\nconstitute an example of the essential building blocks for the understanding of\nquantum transport in structured noisy and warm disordered environments.","publication_date":1700052928,"paper_link":"http://arxiv.org/pdf/2311.08924v1","categories":["Quantitative Biology","Physics"],"abstract":"The study of noise assisted transport in quantum systems is essential in a wide range of applications from near-term NISQ devices to models for quantum biology. Here, we study a generalised XXZ model in the presence of stochastic collision noise, which allows to describe environments beyond the standard Markovian formulation. Our analysis through the study of the local magnetization, the inverse participation ratio (IPR) or its generalisation, the Inverse Ergodicity Ratio (IER), showed clear regimes where the transport rate and coherence time can be controlled by the dissipation in a consistent manner. In addition, when considering several excitations, we characterize the interplay between collisions and system interactions identifying regimes in which transport is counterintuitively enhanced when increasing the collision rate, even in the case of initially separated excitations. These results constitute an example of the essential building blocks for the understanding of quantum transport in structured noisy and warm disordered environments."}
{"title":"Personalized Video Relighting Using Casual Light Stage","authors":["Jun Myeong Choi","Max Christman","Roni Sengupta"],"raw_abstract":"In this paper, we develop a personalized video relighting algorithm that\nproduces high-quality and temporally consistent relit video under any pose,\nexpression, and lighting conditions in real-time. Existing relighting\nalgorithms typically rely either on publicly available synthetic data, which\nyields poor relighting results, or instead on Light Stage data which is\ninaccessible and is not publicly available. We show that by casually capturing\nvideo of a user watching YouTube videos on a monitor we can train a\npersonalized algorithm capable of producing high-quality relighting under any\ncondition. Our key contribution is a novel neural relighting architecture that\neffectively separates the intrinsic appearance features, geometry and\nreflectance, from the source lighting and then combines it with the target\nlighting to generate a relit image. This neural architecture enables smoothing\nof intrinsic appearance features leading to temporally stable video relighting.\nBoth qualitative and quantitative evaluations show that our relighting\narchitecture improves portrait image relighting quality and temporal\nconsistency over state-of-the-art approaches on both casually captured Light\nStage at Your Desk (LSYD) data and Light Stage captured One Light At a Time\n(OLAT) datasets.","publication_date":1700044400,"paper_link":"http://arxiv.org/pdf/2311.08843v1","categories":["Quantitative Biology"],"abstract":"In this paper, we develop a personalized video relighting algorithm that produces high-quality and temporally consistent relit video under any pose, expression, and lighting conditions in real-time. Existing relighting algorithms typically rely either on publicly available synthetic data, which yields poor relighting results, or instead on Light Stage data which is inaccessible and is not publicly available. We show that by casually capturing video of a user watching YouTube videos on a monitor we can train a personalized algorithm capable of producing high-quality relighting under any condition. Our key contribution is a novel neural relighting architecture that effectively separates the intrinsic appearance features, geometry and reflectance, from the source lighting and then combines it with the target lighting to generate a relit image. This neural architecture enables smoothing of intrinsic appearance features leading to temporally stable video relighting. Both qualitative and quantitative evaluations show that our relighting architecture improves portrait image relighting quality and temporal consistency over state-of-the-art approaches on both casually captured Light Stage at Your Desk (LSYD) data and Light Stage captured One Light At a Time (OLAT) datasets."}
{"title":"Quantity versus quality in publication activity: knowledge production at the regional level","authors":["Timur Gareev","Irina Peker"],"raw_abstract":"This study contributes to the ongoing debate regarding the balance between\nquality and quantity in research productivity and publication activity. Using\nempirical regional knowledge production functions, we establish a significant\ncorrelation between R&D spending and research output, specifically publication\nproductivity, while controlling for patenting activity and socioeconomic\nfactors. Our focus is on the dilemma of research quantity versus quality, which\nis analysed in the context of regional thematic specialization using spatial\nlags. When designing policies and making forecasts, it is important to consider\nthe quality of research measured by established indicators. In this study, we\nexamine the dual effect of research quality on publication activity. We\nidentify two groups of quality factors: those related to the quality of\njournals and those related to the impact of publications. On average, these\nfactors have different influences on quantitative measures. The quality of\njournals shows a negative relationship with quantity, indicating that as\njournal quality increases, the number of publications decreases. On the other\nhand, the impact of publications can be approximated by an inverse parabolic\nshape, with a positive decreasing slope within a common range of values. This\nduality in the relationship between quality factors and quantitative measures\nmay explain some of the significant variations in conclusions found in the\nliterature. We compare several models that explore factors influencing\npublication activity using a balanced panel dataset of Russian regions from\n2009 to 2021. Additionally, we propose a novel approach using thematic\nscientometric parameters as a special type of proximity measure between regions\nin thematic space. Incorporating spatial spillovers in thematic space allows us\nto account for potential cross-sectional dependence in regional data.","publication_date":1700043536,"paper_link":"http://arxiv.org/pdf/2311.08830v1","categories":["Economics","Quantitative Finance"],"abstract":"This study contributes to the ongoing debate regarding the balance between quality and quantity in research productivity and publication activity. Using empirical regional knowledge production functions, we establish a significant correlation between R&D spending and research output, specifically publication productivity, while controlling for patenting activity and socioeconomic factors. Our focus is on the dilemma of research quantity versus quality, which is analysed in the context of regional thematic specialization using spatial lags. When designing policies and making forecasts, it is important to consider the quality of research measured by established indicators. In this study, we examine the dual effect of research quality on publication activity. We identify two groups of quality factors: those related to the quality of journals and those related to the impact of publications. On average, these factors have different influences on quantitative measures. The quality of journals shows a negative relationship with quantity, indicating that as journal quality increases, the number of publications decreases. On the other hand, the impact of publications can be approximated by an inverse parabolic shape, with a positive decreasing slope within a common range of values. This duality in the relationship between quality factors and quantitative measures may explain some of the significant variations in conclusions found in the literature. We compare several models that explore factors influencing publication activity using a balanced panel dataset of Russian regions from 2009 to 2021. Additionally, we propose a novel approach using thematic scientometric parameters as a special type of proximity measure between regions in thematic space. Incorporating spatial spillovers in thematic space allows us to account for potential cross-sectional dependence in regional data."}
{"title":"Quantification of cell contractile behavior based on non-destructive macroscopic measurement of tension forces on bioprinted hydrogel","authors":["Sarah Pragnere","Naima El Kholti","Leslie Gudimard","Lucie Essayan","Christophe Marquette","Emma Petiot","Cyril Pailler-Mattei"],"raw_abstract":"Contraction assay based on surface measurement have been widely used to\nevaluate cell contractility in 3D models. This method is straightforward and\nrequires no specific equipment, but it does not provide quantitative data about\ncontraction forces generated by cells. We expanded this method with a new\nbiomechanical model, based on the work-energy theorem, to provide\nnon-destructive longitudinal monitoring of contraction forces generated by\ncells in 3D.We applied this method on hydrogels seeded with either fibroblasts\nor osteoblasts. Hydrogel mechanical characteristics were modulated to enhance\n(condition HCA$_{High}$: hydrogel contraction assay high contraction) or limit\n(condition HCA$_{Low}$: hydrogel contraction assay low contraction) cell\ncontractile behaviors. Macroscopic measures were further correlated with cell\ncontractile behavior and descriptive analysis of their physiology in response\nto different mechanical environments. Fibroblasts and osteoblasts contracted\ntheir matrix up to 47% and 77% respectively. Contraction stress peaked at day 5\nwith 1.1 10$^{-14}$Pa for fibroblasts and 3.5 10$^{-14}$Pa for osteoblasts,\nwhich correlated with cell attachment and spreading. Negligible contraction was\nseen in HCA$_{Low}$. Both fibroblasts and osteoblasts expressed $\\alpha$-SMA\ncontractile fibers in HCA$_{High}$ and HCA$_{Low}$. Failure to contract\nHCA$_{Low}$ was attributed to increased cross-linking and resistance to\nproteolytic degradation of the hydrogel.","publication_date":1700037418,"paper_link":"http://arxiv.org/pdf/2311.08773v1","categories":["Quantitative Biology"],"abstract":"Contraction assay based on surface measurement have been widely used to evaluate cell contractility in 3D models. This method is straightforward and requires no specific equipment, but it does not provide quantitative data about contraction forces generated by cells. We expanded this method with a new biomechanical model, based on the work-energy theorem, to provide non-destructive longitudinal monitoring of contraction forces generated by cells in 3D.We applied this method on hydrogels seeded with either fibroblasts or osteoblasts. Hydrogel mechanical characteristics were modulated to enhance (condition HCA__FORMULA__: hydrogel contraction assay high contraction) or limit (condition HCA__FORMULA__: hydrogel contraction assay low contraction) cell contractile behaviors. Macroscopic measures were further correlated with cell contractile behavior and descriptive analysis of their physiology in response to different mechanical environments. Fibroblasts and osteoblasts contracted their matrix up to 47% and 77% respectively. Contraction stress peaked at day 5 with 1.1 10__FORMULA__Pa for fibroblasts and 3.5 10__FORMULA__Pa for osteoblasts, which correlated with cell attachment and spreading. Negligible contraction was seen in HCA__FORMULA__. Both fibroblasts and osteoblasts expressed __FORMULA__-SMA contractile fibers in HCA__FORMULA__ and HCA__FORMULA__. Failure to contract HCA__FORMULA__ was attributed to increased cross-linking and resistance to proteolytic degradation of the hydrogel."}
{"title":"Cross-domain feature disentanglement for interpretable modeling of tumor microenvironment impact on drug response","authors":["Jia Zhai","Hui Liu"],"raw_abstract":"High-throughput screening technology has facilitated the generation of\nlarge-scale drug responses across hundreds of cancer cell lines. However, there\nexists significant discrepancy between in vitro cell lines and actual tumors in\nvivo in terms of their response to drug treatments, because of tumors comprise\nof complex cellular compositions and histopathology structure, known as tumor\nmicroenvironment (TME), which greatly influences the drug cytotoxicity against\ntumor cells. To date, no study has focused on modeling the impact of the TME on\nclinical drug response. This paper proposed a domain adaptation network for\nfeature disentanglement to separate representations of cancer cells and TME of\na tumor in patients. Two denoising autoencoders were separately used to extract\nfeatures from cell lines (source domain) and tumors (target domain) for partial\ndomain alignment and feature decoupling. The specific encoder was enforced to\nextract information only about TME. Moreover, to ensure generalizability to\nnovel drugs, we applied a graph attention network to learn the latent\nrepresentation of drugs, allowing us to linearly model the drug perturbation on\ncellular state in latent space. We calibrated our model on a benchmark dataset\nand demonstrated its superior performance in predicting clinical drug response\nand dissecting the influence of the TME on drug efficacy.","publication_date":1700034654,"paper_link":"http://arxiv.org/pdf/2311.09264v1","categories":["Quantitative Biology"],"abstract":"High-throughput screening technology has facilitated the generation of large-scale drug responses across hundreds of cancer cell lines. However, there exists significant discrepancy between in vitro cell lines and actual tumors in vivo in terms of their response to drug treatments, because of tumors comprise of complex cellular compositions and histopathology structure, known as tumor microenvironment (TME), which greatly influences the drug cytotoxicity against tumor cells. To date, no study has focused on modeling the impact of the TME on clinical drug response. This paper proposed a domain adaptation network for feature disentanglement to separate representations of cancer cells and TME of a tumor in patients. Two denoising autoencoders were separately used to extract features from cell lines (source domain) and tumors (target domain) for partial domain alignment and feature decoupling. The specific encoder was enforced to extract information only about TME. Moreover, to ensure generalizability to novel drugs, we applied a graph attention network to learn the latent representation of drugs, allowing us to linearly model the drug perturbation on cellular state in latent space. We calibrated our model on a benchmark dataset and demonstrated its superior performance in predicting clinical drug response and dissecting the influence of the TME on drug efficacy."}
{"title":"High-speed photoelastic tomography for axisymmetric stress fields in a soft material: temporal evolution of all stress components","authors":["Yuto Yokoyama","Sayaka Ichihara","Yoshiyuki Tagawa"],"raw_abstract":"This study presents a novel approach for reconstructing all stress components\nof the dynamic axisymmetric fields of a soft material using photoelastic\ntomography (PT) and a high-speed polarization camera. This study focuses on\nstatic and dynamic Hertzian contact as an example of transient stress field\nreconstructions. For the static Hertzian contact (a solid sphere pressed\nagainst a gel block), all stress components in the urethane gel, which has an\nelastic modulus of 47.4 kPa, were reconstructed by PT using the measured\nphotoelastic parameters. The results were compared with theoretical solutions\nand showed good agreement. For the dynamic Hertzian contact (a sphere impacting\ngel), a high-speed polarization camera was used to reconstruct the transient\nstress field within the gel. PT was used to quantitatively measure the shear\nand axial stress waves and showed different propagation speeds on the\nsubstrate. The technique allowed the simultaneous measurement of stress fields\nranging from $O(10^{-1})$ to $O(10^1)$ kPa during large deformations,\ndemonstrating its accuracy in capturing rapidly changing stress tensor\ncomponents in dynamic scenarios. The scaling laws of the calculated impact\nforce agreed with theoretical predictions, validating the accuracy of PT for\nmeasuring dynamic axisymmetric stress fields in soft materials.","publication_date":1700030777,"paper_link":"http://arxiv.org/pdf/2311.10107v1","categories":["Physics"],"abstract":"This study presents a novel approach for reconstructing all stress components of the dynamic axisymmetric fields of a soft material using photoelastic tomography (PT) and a high-speed polarization camera. This study focuses on static and dynamic Hertzian contact as an example of transient stress field reconstructions. For the static Hertzian contact (a solid sphere pressed against a gel block), all stress components in the urethane gel, which has an elastic modulus of 47.4 kPa, were reconstructed by PT using the measured photoelastic parameters. The results were compared with theoretical solutions and showed good agreement. For the dynamic Hertzian contact (a sphere impacting gel), a high-speed polarization camera was used to reconstruct the transient stress field within the gel. PT was used to quantitatively measure the shear and axial stress waves and showed different propagation speeds on the substrate. The technique allowed the simultaneous measurement of stress fields ranging from __FORMULA__ to __FORMULA__ kPa during large deformations, demonstrating its accuracy in capturing rapidly changing stress tensor components in dynamic scenarios. The scaling laws of the calculated impact force agreed with theoretical predictions, validating the accuracy of PT for measuring dynamic axisymmetric stress fields in soft materials."}
{"title":"Emerging Drug Interaction Prediction Enabled by Flow-based Graph Neural Network with Biomedical Network","authors":["Yongqi Zhang","Quanming Yao","Ling Yue","Xian Wu","Ziheng Zhang","Zhenxi Lin","Yefeng Zheng"],"raw_abstract":"Accurately predicting drug-drug interactions (DDI) for emerging drugs, which\noffer possibilities for treating and alleviating diseases, with computational\nmethods can improve patient care and contribute to efficient drug development.\nHowever, many existing computational methods require large amounts of known DDI\ninformation, which is scarce for emerging drugs. In this paper, we propose\nEmerGNN, a graph neural network (GNN) that can effectively predict interactions\nfor emerging drugs by leveraging the rich information in biomedical networks.\nEmerGNN learns pairwise representations of drugs by extracting the paths\nbetween drug pairs, propagating information from one drug to the other, and\nincorporating the relevant biomedical concepts on the paths. The different\nedges on the biomedical network are weighted to indicate the relevance for the\ntarget DDI prediction. Overall, EmerGNN has higher accuracy than existing\napproaches in predicting interactions for emerging drugs and can identify the\nmost relevant information on the biomedical network.","publication_date":1700030040,"paper_link":"http://arxiv.org/pdf/2311.09261v1","categories":["Quantitative Biology"],"abstract":"Accurately predicting drug-drug interactions (DDI) for emerging drugs, which offer possibilities for treating and alleviating diseases, with computational methods can improve patient care and contribute to efficient drug development. However, many existing computational methods require large amounts of known DDI information, which is scarce for emerging drugs. In this paper, we propose EmerGNN, a graph neural network (GNN) that can effectively predict interactions for emerging drugs by leveraging the rich information in biomedical networks. EmerGNN learns pairwise representations of drugs by extracting the paths between drug pairs, propagating information from one drug to the other, and incorporating the relevant biomedical concepts on the paths. The different edges on the biomedical network are weighted to indicate the relevance for the target DDI prediction. Overall, EmerGNN has higher accuracy than existing approaches in predicting interactions for emerging drugs and can identify the most relevant information on the biomedical network."}
{"title":"Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory","authors":["Lei Liu","Xiaoyan Yang","Yue Shen","Binbin Hu","Zhiqiang Zhang","Jinjie Gu","Guannan Zhang"],"raw_abstract":"Memory-augmented Large Language Models (LLMs) have demonstrated remarkable\nperformance in long-term human-machine interactions, which basically relies on\niterative recalling and reasoning of history to generate high-quality\nresponses. However, such repeated recall-reason steps easily produce biased\nthoughts, \\textit{i.e.}, inconsistent reasoning results when recalling the same\nhistory for different questions. On the contrary, humans can keep thoughts in\nthe memory and recall them without repeated reasoning. Motivated by this human\ncapability, we propose a novel memory mechanism called TiM (Think-in-Memory)\nthat enables LLMs to maintain an evolved memory for storing historical thoughts\nalong the conversation stream. The TiM framework consists of two crucial\nstages: (1) before generating a response, a LLM agent recalls relevant thoughts\nfrom memory, and (2) after generating a response, the LLM agent post-thinks and\nincorporates both historical and new thoughts to update the memory. Thus, TiM\ncan eliminate the issue of repeated reasoning by saving the post-thinking\nthoughts as the history. Besides, we formulate the basic principles to organize\nthe thoughts in memory based on the well-established operations,\n(\\textit{i.e.}, insert, forget, and merge operations), allowing for dynamic\nupdates and evolution of the thoughts. Furthermore, we introduce\nLocality-Sensitive Hashing into TiM to achieve efficient retrieval for the\nlong-term conversations. We conduct qualitative and quantitative experiments on\nreal-world and simulated dialogues covering a wide range of topics,\ndemonstrating that equipping existing LLMs with TiM significantly enhances\ntheir performance in generating responses for long-term interactions.","publication_date":1700028515,"paper_link":"http://arxiv.org/pdf/2311.08719v1","categories":["Quantitative Biology"],"abstract":"Memory-augmented Large Language Models (LLMs) have demonstrated remarkable performance in long-term human-machine interactions, which basically relies on iterative recalling and reasoning of history to generate high-quality responses. However, such repeated recall-reason steps easily produce biased thoughts, i.e., inconsistent reasoning results when recalling the same history for different questions. On the contrary, humans can keep thoughts in the memory and recall them without repeated reasoning. Motivated by this human capability, we propose a novel memory mechanism called TiM (Think-in-Memory) that enables LLMs to maintain an evolved memory for storing historical thoughts along the conversation stream. The TiM framework consists of two crucial stages: (1) before generating a response, a LLM agent recalls relevant thoughts from memory, and (2) after generating a response, the LLM agent post-thinks and incorporates both historical and new thoughts to update the memory. Thus, TiM can eliminate the issue of repeated reasoning by saving the post-thinking thoughts as the history. Besides, we formulate the basic principles to organize the thoughts in memory based on the well-established operations, (i.e., insert, forget, and merge operations), allowing for dynamic updates and evolution of the thoughts. Furthermore, we introduce Locality-Sensitive Hashing into TiM to achieve efficient retrieval for the long-term conversations. We conduct qualitative and quantitative experiments on real-world and simulated dialogues covering a wide range of topics, demonstrating that equipping existing LLMs with TiM significantly enhances their performance in generating responses for long-term interactions."}
{"title":"A collage of results on the divisibility and indivisibility of class numbers of quadratic fields","authors":["Srilakshmi Krishnamoorthy","Sunil Kumar Pasupulati","Muneeswaran R"],"raw_abstract":"The investigation of the ideal class group $Cl_K$ of an algebraic number\nfield $K$ is one of the key subjects of inquiry in algebraic number theory\nsince it encodes a lot of arithmetic information about K. There is a\nconsiderable amount of research on many topics linked to quadratic field class\ngroups notably intriguing aspect is the divisibility of the class numbers. This\narticle discusses a few recent results on the divisibility of class numbers and\nthe Izuka conjecture. We also discuss the quantitative aspect of the Izuka\nconjecture.","publication_date":1700026028,"paper_link":"http://arxiv.org/pdf/2311.08710v1","categories":["Mathematics"],"abstract":"The investigation of the ideal class group __FORMULA__ of an algebraic number field __FORMULA__ is one of the key subjects of inquiry in algebraic number theory since it encodes a lot of arithmetic information about K. There is a considerable amount of research on many topics linked to quadratic field class groups notably intriguing aspect is the divisibility of the class numbers. This article discusses a few recent results on the divisibility of class numbers and the Izuka conjecture. We also discuss the quantitative aspect of the Izuka conjecture."}
{"title":"Deep Neural Network Identification of Limnonectes Species and New Class Detection Using Image Data","authors":["Li Xu","Yili Hong","Eric P. Smith","David S. McLeod","Xinwei Deng","Laura J. Freeman"],"raw_abstract":"As is true of many complex tasks, the work of discovering, describing, and\nunderstanding the diversity of life on Earth (viz., biological systematics and\ntaxonomy) requires many tools. Some of this work can be accomplished as it has\nbeen done in the past, but some aspects present us with challenges which\ntraditional knowledge and tools cannot adequately resolve. One such challenge\nis presented by species complexes in which the morphological similarities among\nthe group members make it difficult to reliably identify known species and\ndetect new ones. We address this challenge by developing new tools using the\nprinciples of machine learning to resolve two specific questions related to\nspecies complexes. The first question is formulated as a classification problem\nin statistics and machine learning and the second question is an\nout-of-distribution (OOD) detection problem. We apply these tools to a species\ncomplex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex)\nand employ a morphological character (hind limb skin texture) traditionally\ntreated qualitatively in a quantitative and objective manner. We demonstrate\nthat deep neural networks can successfully automate the classification of an\nimage into a known species group for which it has been trained. We further\ndemonstrate that the algorithm can successfully classify an image into a new\nclass if the image does not belong to the existing classes. Additionally, we\nuse the larger MNIST dataset to test the performance of our OOD detection\nalgorithm. We finish our paper with some concluding remarks regarding the\napplication of these methods to species complexes and our efforts to document\ntrue biodiversity. This paper has online supplementary materials.","publication_date":1700017079,"paper_link":"http://arxiv.org/pdf/2311.08661v1","categories":["Electrical Engineering and Systems Science","Statistics"],"abstract":"As is true of many complex tasks, the work of discovering, describing, and understanding the diversity of life on Earth (viz., biological systematics and taxonomy) requires many tools. Some of this work can be accomplished as it has been done in the past, but some aspects present us with challenges which traditional knowledge and tools cannot adequately resolve. One such challenge is presented by species complexes in which the morphological similarities among the group members make it difficult to reliably identify known species and detect new ones. We address this challenge by developing new tools using the principles of machine learning to resolve two specific questions related to species complexes. The first question is formulated as a classification problem in statistics and machine learning and the second question is an out-of-distribution (OOD) detection problem. We apply these tools to a species complex comprising Southeast Asian stream frogs (Limnonectes kuhlii complex) and employ a morphological character (hind limb skin texture) traditionally treated qualitatively in a quantitative and objective manner. We demonstrate that deep neural networks can successfully automate the classification of an image into a known species group for which it has been trained. We further demonstrate that the algorithm can successfully classify an image into a new class if the image does not belong to the existing classes. Additionally, we use the larger MNIST dataset to test the performance of our OOD detection algorithm. We finish our paper with some concluding remarks regarding the application of these methods to species complexes and our efforts to document true biodiversity. This paper has online supplementary materials."}
{"title":"Structured Estimation of Heterogeneous Time Series","authors":["Zachary F. Fisher","Younghoon Kim","Vladas Pipiras","Christopher Crawford","Daniel J. Petrie","Michael D. Hunter","Charles F. Geier"],"raw_abstract":"How best to model structurally heterogeneous processes is a foundational\nquestion in the social, health and behavioral sciences. Recently, Fisher et\nal., (2022) introduced the multi-VAR approach for simultaneously estimating\nmultiple-subject multivariate time series characterized by common and\nindividualizing features using penalized estimation. This approach differs from\nmany popular modeling approaches for multiple-subject time series in that\nqualitative and quantitative differences in a large number of individual\ndynamics are well-accommodated. The current work extends the multi-VAR\nframework to include new adaptive weighting schemes that greatly improve\nestimation performance. In a small set of simulation studies we compare\nadaptive multi-VAR with these new penalty weights to common alternative\nestimators in terms of path recovery and bias. Furthermore, we provide toy\nexamples and code demonstrating the utility of multi-VAR under different\nheterogeneity regimes using the multivar package for R (Fisher, 2022).","publication_date":1700015953,"paper_link":"http://arxiv.org/pdf/2311.08658v1","categories":["Statistics"],"abstract":"How best to model structurally heterogeneous processes is a foundational question in the social, health and behavioral sciences. Recently, Fisher et al., (2022) introduced the multi-VAR approach for simultaneously estimating multiple-subject multivariate time series characterized by common and individualizing features using penalized estimation. This approach differs from many popular modeling approaches for multiple-subject time series in that qualitative and quantitative differences in a large number of individual dynamics are well-accommodated. The current work extends the multi-VAR framework to include new adaptive weighting schemes that greatly improve estimation performance. In a small set of simulation studies we compare adaptive multi-VAR with these new penalty weights to common alternative estimators in terms of path recovery and bias. Furthermore, we provide toy examples and code demonstrating the utility of multi-VAR under different heterogeneity regimes using the multivar package for R (Fisher, 2022)."}
{"title":"XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making","authors":["Zichen Chen","Jianda Chen","Mitali Gaidhani","Ambuj Singh","Misha Sra"],"raw_abstract":"Large Language Models (LLMs) have recently made impressive strides in natural\nlanguage understanding tasks. Despite their remarkable performance,\nunderstanding their decision-making process remains a big challenge. In this\npaper, we look into bringing some transparency to this process by introducing a\nnew explanation dataset for question answering (QA) tasks that integrates\nknowledge graphs (KGs) in a novel way. Our dataset includes 12,102\nquestion-answer-explanation (QAE) triples. Each explanation in the dataset\nlinks the LLM's reasoning to entities and relations in the KGs. The explanation\ncomponent includes a why-choose explanation, a why-not-choose explanation, and\na set of reason-elements that underlie the LLM's decision. We leverage KGs and\ngraph attention networks (GAT) to find the reason-elements and transform them\ninto why-choose and why-not-choose explanations that are comprehensible to\nhumans. Through quantitative and qualitative evaluations, we demonstrate the\npotential of our dataset to improve the in-context learning of LLMs, and\nenhance their interpretability and explainability. Our work contributes to the\nfield of explainable AI by enabling a deeper understanding of the LLMs\ndecision-making process to make them more transparent and thereby, potentially\nmore reliable, to researchers and practitioners alike. Our dataset is available\nat: https://github.com/chen-zichen/XplainLLM_dataset.git","publication_date":1700008468,"paper_link":"http://arxiv.org/pdf/2311.08614v1","categories":["Quantitative Biology"],"abstract":"Large Language Models (LLMs) have recently made impressive strides in natural language understanding tasks. Despite their remarkable performance, understanding their decision-making process remains a big challenge. In this paper, we look into bringing some transparency to this process by introducing a new explanation dataset for question answering (QA) tasks that integrates knowledge graphs (KGs) in a novel way. Our dataset includes 12,102 question-answer-explanation (QAE) triples. Each explanation in the dataset links the LLM's reasoning to entities and relations in the KGs. The explanation component includes a why-choose explanation, a why-not-choose explanation, and a set of reason-elements that underlie the LLM's decision. We leverage KGs and graph attention networks (GAT) to find the reason-elements and transform them into why-choose and why-not-choose explanations that are comprehensible to humans. Through quantitative and qualitative evaluations, we demonstrate the potential of our dataset to improve the in-context learning of LLMs, and enhance their interpretability and explainability. Our work contributes to the field of explainable AI by enabling a deeper understanding of the LLMs decision-making process to make them more transparent and thereby, potentially more reliable, to researchers and practitioners alike. Our dataset is available at: https://github.com/chen-zichen/XplainLLM_dataset.git"}
{"title":"Theory of Infectious Diseases with Testing and Testing-less Covid-19 Endemic","authors":["Bo Deng","Chayu Yang"],"raw_abstract":"What is the long term dynamics of the Covid-19 pandemic? How will it end?\nHere we constructed an infectious disease model with testing and analyzed the\nexistence and stability of its endemic states. For a large parameter set,\nincluding those relevant to the SARS-CoV-2 virus, we demonstrated the existence\nof one endemic equilibrium without testing and one endemic equilibrium with\ntesting and proved their local and global stabilities for some cases. Our\nresults suggest that the pandemic is to end with a testing-less endemic state\nthrough a novel and surprising mechanism called stochastic trapping.","publication_date":1700007900,"paper_link":"http://arxiv.org/pdf/2311.08611v1","categories":["Mathematics","Quantitative Biology","Physics"],"abstract":"What is the long term dynamics of the Covid-19 pandemic? How will it end? Here we constructed an infectious disease model with testing and analyzed the existence and stability of its endemic states. For a large parameter set, including those relevant to the SARS-CoV-2 virus, we demonstrated the existence of one endemic equilibrium without testing and one endemic equilibrium with testing and proved their local and global stabilities for some cases. Our results suggest that the pandemic is to end with a testing-less endemic state through a novel and surprising mechanism called stochastic trapping."}
{"title":"MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration","authors":["Lin Xu","Zhiyuan Hu","Daquan Zhou","Hongyu Ren","Zhen Dong","Kurt Keutzer","See Kiong Ng","Jiashi Feng"],"raw_abstract":"Large Language Models (LLMs) have marked a significant advancement in the\nfield of natural language processing, demonstrating exceptional capabilities in\nreasoning, tool usage, and memory. As their applications extend into\nmulti-agent environments, a need has arisen for a comprehensive evaluation\nframework that captures their abilities in reasoning, planning, collaboration,\nand more. This work introduces a novel benchmarking framework specifically\ntailored to assess LLMs within multi-agent settings, providing quantitative\nmetrics to evaluate their judgment, reasoning, deception, self-awareness,\ncooperation, coordination, and rationality. We utilize games such as Chameleon\nand Undercover, alongside game theory scenarios like Cost Sharing, Multi-player\nPrisoner's Dilemma, and Public Good, to create diverse testing environments.\nOur framework is fortified with the Probabilistic Graphical Modeling (PGM)\nmethod, enhancing the LLMs' capabilities in navigating complex social and\ncognitive dimensions. The benchmark evaluates seven multi-agent systems powered\nby different LLMs, quantitatively highlighting a significant capability gap\nover threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It\nalso confirms that our PGM enhancement boosts the inherent abilities of all\nselected models by 50% on average. Our codes are released here\nhttps://github.com/cathyxl/MAgIC.","publication_date":1699998387,"paper_link":"http://arxiv.org/pdf/2311.08562v2","categories":["Quantitative Biology"],"abstract":"Large Language Models (LLMs) have marked a significant advancement in the field of natural language processing, demonstrating exceptional capabilities in reasoning, tool usage, and memory. As their applications extend into multi-agent environments, a need has arisen for a comprehensive evaluation framework that captures their abilities in reasoning, planning, collaboration, and more. This work introduces a novel benchmarking framework specifically tailored to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality. We utilize games such as Chameleon and Undercover, alongside game theory scenarios like Cost Sharing, Multi-player Prisoner's Dilemma, and Public Good, to create diverse testing environments. Our framework is fortified with the Probabilistic Graphical Modeling (PGM) method, enhancing the LLMs' capabilities in navigating complex social and cognitive dimensions. The benchmark evaluates seven multi-agent systems powered by different LLMs, quantitatively highlighting a significant capability gap over threefold between the strongest, GPT-4, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the inherent abilities of all selected models by 50% on average. Our codes are released here https://github.com/cathyxl/MAgIC."}
{"title":"Alternatives to the ROC Curve AUC and C-statistic for Risk Prediction Models","authors":["Ralph H. Stern"],"raw_abstract":"Assessment of risk prediction models has primarily utilized measures of\ndiscrimination, the ROC curve AUC and C-statistic. These derive from the risk\ndistributions of patients and nonpatients, which in turn are derived from a\npopulation risk distribution. As greater dispersion of the population risk\ndistribution produces greater separation of patient and nonpatient risks\n(discrimination), its parameters can be used as alternatives to the ROC curve\nAUC and C-statistic. Here continuous probability distributions are employed to\ndevelop insight into the relationship between their parameters and the ROC\ncurve AUC and C-statistic derived from them.\n  The ROC curve AUC and C-statistic are shown to have a straight-line\nrelationship with the SD for uniform, half-sine, and symmetric triangular\nprobability distributions, with slight differences in the slope: AUC approx\n1/2+0.28 SD/(mean(1-mean)). This also characterizes the beta distribution over\nthe same range of SD's. But at larger beta distribution SD's the plot of AUC\nversus SD deviates downward from this straight-line relationship, approaching\nthe ROC curve AUC and SD of a perfect model (AUC=1, SD= $\\sqrt{\\rm\nmean(1-mean)}$).\n  A simpler and more intuitive discrimination metric is the coefficient of\ndiscrimination, the difference between the mean risk in patients and\nnonpatients. This is SD2/(mean(1-mean)), which is also the same for any\ndistribution. Since estimating parameters or metrics discards information, the\npopulation risk distribution should always be presented. As the ROC curve AUC\nand C-statistic are functions of this distribution's parameters, the parameters\nrepresent simpler, intuitive alternatives to these discrimination metrics.\nAmong discrimination metrics, the coefficient of discrimination provides a\nsimple, intuitive alternative to the ROC curve AUC and C-statistic.","publication_date":1699998118,"paper_link":"http://arxiv.org/pdf/2311.08559v1","categories":["Quantitative Biology"],"abstract":"Assessment of risk prediction models has primarily utilized measures of discrimination, the ROC curve AUC and C-statistic. These derive from the risk distributions of patients and nonpatients, which in turn are derived from a population risk distribution. As greater dispersion of the population risk distribution produces greater separation of patient and nonpatient risks (discrimination), its parameters can be used as alternatives to the ROC curve AUC and C-statistic. Here continuous probability distributions are employed to develop insight into the relationship between their parameters and the ROC curve AUC and C-statistic derived from them.   The ROC curve AUC and C-statistic are shown to have a straight-line relationship with the SD for uniform, half-sine, and symmetric triangular probability distributions, with slight differences in the slope: AUC approx 1/2+0.28 SD/(mean(1-mean)). This also characterizes the beta distribution over the same range of SD's. But at larger beta distribution SD's the plot of AUC versus SD deviates downward from this straight-line relationship, approaching the ROC curve AUC and SD of a perfect model (AUC=1, SD= __FORMULA__).   A simpler and more intuitive discrimination metric is the coefficient of discrimination, the difference between the mean risk in patients and nonpatients. This is SD2/(mean(1-mean)), which is also the same for any distribution. Since estimating parameters or metrics discards information, the population risk distribution should always be presented. As the ROC curve AUC and C-statistic are functions of this distribution's parameters, the parameters represent simpler, intuitive alternatives to these discrimination metrics. Among discrimination metrics, the coefficient of discrimination provides a simple, intuitive alternative to the ROC curve AUC and C-statistic."}
{"title":"Topology of Surface Electromyogram Signals: Hand Gesture Decoding on Riemannian Manifolds","authors":["Harshavardhana T. Gowda","Lee M. Miller"],"raw_abstract":"Decoding gestures from the upper limb using noninvasive surface\nelectromyogram (sEMG) signals is of keen interest for the rehabilitation of\namputees, artificial supernumerary limb augmentation, gestural control of\ncomputers, and virtual/augmented realities. We show that sEMG signals recorded\nacross an array of sensor electrodes in multiple spatial locations around the\nforearm evince a rich geometric pattern of global motor unit (MU) activity that\ncan be leveraged to distinguish different hand gestures. We demonstrate a\nsimple technique to analyze spatial patterns of muscle MU activity within a\ntemporal window and show that distinct gestures can be classified in both\nsupervised and unsupervised manners. Specifically, we construct symmetric\npositive definite (SPD) covariance matrices to represent the spatial\ndistribution of MU activity in a time window of interest, calculated as\npairwise covariance of electrical signals measured across different electrodes.\nThis allows us to understand and manipulate multivariate sEMG timeseries on a\nmore natural subspace -the Riemannian manifold. Furthermore, it directly\naddresses signal variability across individuals and sessions, which remains a\nmajor challenge in the field. sEMG signals measured at a single electrode lack\ncontextual information such as how various anatomical and physiological factors\ninfluence the signals and how their combined effect alters the evident\ninteraction among neighboring muscles. As we show here, analyzing spatial\npatterns using covariance matrices on Riemannian manifolds allows us to\nrobustly model complex interactions across spatially distributed MUs and\nprovides a flexible and transparent framework to quantify differences in sEMG\nsignals across individuals. The proposed method is novel in the study of sEMG\nsignals and its performance exceeds the current benchmarks while maintaining\nexceptional computational efficiency.","publication_date":1699996854,"paper_link":"http://arxiv.org/pdf/2311.08548v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Decoding gestures from the upper limb using noninvasive surface electromyogram (sEMG) signals is of keen interest for the rehabilitation of amputees, artificial supernumerary limb augmentation, gestural control of computers, and virtual/augmented realities. We show that sEMG signals recorded across an array of sensor electrodes in multiple spatial locations around the forearm evince a rich geometric pattern of global motor unit (MU) activity that can be leveraged to distinguish different hand gestures. We demonstrate a simple technique to analyze spatial patterns of muscle MU activity within a temporal window and show that distinct gestures can be classified in both supervised and unsupervised manners. Specifically, we construct symmetric positive definite (SPD) covariance matrices to represent the spatial distribution of MU activity in a time window of interest, calculated as pairwise covariance of electrical signals measured across different electrodes. This allows us to understand and manipulate multivariate sEMG timeseries on a more natural subspace -the Riemannian manifold. Furthermore, it directly addresses signal variability across individuals and sessions, which remains a major challenge in the field. sEMG signals measured at a single electrode lack contextual information such as how various anatomical and physiological factors influence the signals and how their combined effect alters the evident interaction among neighboring muscles. As we show here, analyzing spatial patterns using covariance matrices on Riemannian manifolds allows us to robustly model complex interactions across spatially distributed MUs and provides a flexible and transparent framework to quantify differences in sEMG signals across individuals. The proposed method is novel in the study of sEMG signals and its performance exceeds the current benchmarks while maintaining exceptional computational efficiency."}
{"title":"A Category of Genes","authors":["Yanying Wu"],"raw_abstract":"Understanding how genes interact and relate to each other is a fundamental\nquestion in biology. However, current practices for describing these\nrelationships, such as drawing diagrams or graphs in a somewhat arbitrary\nmanner, limit our ability to integrate various aspects of the gene functions\nand view the genome holistically. To overcome these limitations, we need a more\nappropriate way to describe the intricate relationships between genes.\nInterestingly, category theory, an abstract field of mathematics seemingly\nunrelated to biology, has emerged as a powerful language for describing\nrelations in general. We propose that category theory could provide a framework\nfor unifying our knowledge of genes and their relationships.\n  As a starting point, we construct a category of genes, with its morphisms\nabstracting various aspects of the relationships betweens genes. These\nrelationships include, but not limited to, the order of genes on the\nchromosomes, the physical or genetic interactions, the signalling pathways, the\ngene ontology causal activity models (GO-CAM) and gene groups. Previously, they\nwere encoded by miscellaneous networks or graphs, while our work unifies them\nin a consistent manner as a category. By doing so, we hope to view the\nrelationships between genes systematically. In the long run, this paves a\npromising way for us to understand the fundamental principles that govern gene\nregulation and function.","publication_date":1699996754,"paper_link":"http://arxiv.org/pdf/2311.08546v1","categories":["Mathematics","Quantitative Biology"],"abstract":"Understanding how genes interact and relate to each other is a fundamental question in biology. However, current practices for describing these relationships, such as drawing diagrams or graphs in a somewhat arbitrary manner, limit our ability to integrate various aspects of the gene functions and view the genome holistically. To overcome these limitations, we need a more appropriate way to describe the intricate relationships between genes. Interestingly, category theory, an abstract field of mathematics seemingly unrelated to biology, has emerged as a powerful language for describing relations in general. We propose that category theory could provide a framework for unifying our knowledge of genes and their relationships.   As a starting point, we construct a category of genes, with its morphisms abstracting various aspects of the relationships betweens genes. These relationships include, but not limited to, the order of genes on the chromosomes, the physical or genetic interactions, the signalling pathways, the gene ontology causal activity models (GO-CAM) and gene groups. Previously, they were encoded by miscellaneous networks or graphs, while our work unifies them in a consistent manner as a category. By doing so, we hope to view the relationships between genes systematically. In the long run, this paves a promising way for us to understand the fundamental principles that govern gene regulation and function."}
{"title":"Taxonomy, Semantic Data Schema, and Schema Alignment for Open Data in Urban Building Energy Modeling","authors":["Liang Zhang","Jianli Chen","Jia Zou"],"raw_abstract":"Urban Building Energy Modeling (UBEM) is a critical tool to provide\nquantitative analysis on building decarbonization, sustainability,\nbuilding-to-grid integration, and renewable energy applications on city,\nregional, and national scales. Researchers usually use open data as inputs to\nbuild and calibrate UBEM. However, open data are from thousands of sources\ncovering various perspectives of weather, building characteristics, etc.\nBesides, a lack of semantic features of open data further increases the\nengineering effort to process information to be directly used for UBEM as\ninputs. In this paper, we first reviewed open data types used for UBEM and\ndeveloped a taxonomy to categorize open data. Based on that, we further\ndeveloped a semantic data schema for each open data category to maintain data\nconsistency and improve model automation for UBEM. In a case study, we use\nthree popular open data to show how they can be automatically processed based\non the proposed schematic data structure using large language models. The\naccurate results generated by large language models indicate the\nmachine-readability and human-interpretability of the developed semantic data\nschema.","publication_date":1699995626,"paper_link":"http://arxiv.org/pdf/2311.08535v1","categories":["Quantitative Biology"],"abstract":"Urban Building Energy Modeling (UBEM) is a critical tool to provide quantitative analysis on building decarbonization, sustainability, building-to-grid integration, and renewable energy applications on city, regional, and national scales. Researchers usually use open data as inputs to build and calibrate UBEM. However, open data are from thousands of sources covering various perspectives of weather, building characteristics, etc. Besides, a lack of semantic features of open data further increases the engineering effort to process information to be directly used for UBEM as inputs. In this paper, we first reviewed open data types used for UBEM and developed a taxonomy to categorize open data. Based on that, we further developed a semantic data schema for each open data category to maintain data consistency and improve model automation for UBEM. In a case study, we use three popular open data to show how they can be automatically processed based on the proposed schematic data structure using large language models. The accurate results generated by large language models indicate the machine-readability and human-interpretability of the developed semantic data schema."}
{"title":"Instant3D: Instant Text-to-3D Generation","authors":["Ming Li","Pan Zhou","Jia-Wei Liu","Jussi Keppo","Min Lin","Shuicheng Yan","Xiangyu Xu"],"raw_abstract":"Text-to-3D generation, which aims to synthesize vivid 3D objects from text\nprompts, has attracted much attention from the computer vision community. While\nseveral existing works have achieved impressive results for this task, they\nmainly rely on a time-consuming optimization paradigm. Specifically, these\nmethods optimize a neural field from scratch for each text prompt, taking\napproximately one hour or more to generate one object. This heavy and\nrepetitive training cost impedes their practical deployment. In this paper, we\npropose a novel framework for fast text-to-3D generation, dubbed Instant3D.\nOnce trained, Instant3D is able to create a 3D object for an unseen text prompt\nin less than one second with a single run of a feedforward network. We achieve\nthis remarkable speed by devising a new network that directly constructs a 3D\ntriplane from a text prompt. The core innovation of our Instant3D lies in our\nexploration of strategies to effectively inject text conditions into the\nnetwork. Furthermore, we propose a simple yet effective activation function,\nthe scaled-sigmoid, to replace the original sigmoid function, which speeds up\nthe training convergence by more than ten times. Finally, to address the Janus\n(multi-head) problem in 3D generation, we propose an adaptive Perp-Neg\nalgorithm that can dynamically adjust its concept negation scales according to\nthe severity of the Janus problem during training, effectively reducing the\nmulti-head effect. Extensive experiments on a wide variety of benchmark\ndatasets demonstrate that the proposed algorithm performs favorably against the\nstate-of-the-art methods both qualitatively and quantitatively, while achieving\nsignificantly better efficiency. The project page is at\nhttps://ming1993li.github.io/Instant3DProj.","publication_date":1699988399,"paper_link":"http://arxiv.org/pdf/2311.08403v1","categories":["Quantitative Biology"],"abstract":"Text-to-3D generation, which aims to synthesize vivid 3D objects from text prompts, has attracted much attention from the computer vision community. While several existing works have achieved impressive results for this task, they mainly rely on a time-consuming optimization paradigm. Specifically, these methods optimize a neural field from scratch for each text prompt, taking approximately one hour or more to generate one object. This heavy and repetitive training cost impedes their practical deployment. In this paper, we propose a novel framework for fast text-to-3D generation, dubbed Instant3D. Once trained, Instant3D is able to create a 3D object for an unseen text prompt in less than one second with a single run of a feedforward network. We achieve this remarkable speed by devising a new network that directly constructs a 3D triplane from a text prompt. The core innovation of our Instant3D lies in our exploration of strategies to effectively inject text conditions into the network. Furthermore, we propose a simple yet effective activation function, the scaled-sigmoid, to replace the original sigmoid function, which speeds up the training convergence by more than ten times. Finally, to address the Janus (multi-head) problem in 3D generation, we propose an adaptive Perp-Neg algorithm that can dynamically adjust its concept negation scales according to the severity of the Janus problem during training, effectively reducing the multi-head effect. Extensive experiments on a wide variety of benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods both qualitatively and quantitatively, while achieving significantly better efficiency. The project page is at https://ming1993li.github.io/Instant3DProj."}
{"title":"A Material Lens on Coloniality in NLP","authors":["William Held","Camille Harris","Michael Best","Diyi Yang"],"raw_abstract":"Coloniality, the continuation of colonial harms beyond \"official\"\ncolonization, has pervasive effects across society and scientific fields.\nNatural Language Processing (NLP) is no exception to this broad phenomenon. In\nthis work, we argue that coloniality is implicitly embedded in and amplified by\nNLP data, algorithms, and software. We formalize this analysis using\nActor-Network Theory (ANT): an approach to understanding social phenomena\nthrough the network of relationships between human stakeholders and technology.\nWe use our Actor-Network to guide a quantitative survey of the geography of\ndifferent phases of NLP research, providing evidence that inequality along\ncolonial boundaries increases as NLP builds on itself. Based on this, we argue\nthat combating coloniality in NLP requires not only changing current values but\nalso active work to remove the accumulation of colonial ideals in our\nfoundational data and algorithms.","publication_date":1699987929,"paper_link":"http://arxiv.org/pdf/2311.08391v1","categories":["Quantitative Biology"],"abstract":"Coloniality, the continuation of colonial harms beyond \"official\" colonization, has pervasive effects across society and scientific fields. Natural Language Processing (NLP) is no exception to this broad phenomenon. In this work, we argue that coloniality is implicitly embedded in and amplified by NLP data, algorithms, and software. We formalize this analysis using Actor-Network Theory (ANT): an approach to understanding social phenomena through the network of relationships between human stakeholders and technology. We use our Actor-Network to guide a quantitative survey of the geography of different phases of NLP research, providing evidence that inequality along colonial boundaries increases as NLP builds on itself. Based on this, we argue that combating coloniality in NLP requires not only changing current values but also active work to remove the accumulation of colonial ideals in our foundational data and algorithms."}
{"title":"Hierarchical Experience-informed Navigation for Multi-modal Quadrupedal Rebar Grid Traversal","authors":["Max Asselmeier","Jane Ivanova","Ziyi Zhou","Patricio A. Vela","Ye Zhao"],"raw_abstract":"This study focuses on a layered, experience-based, multi-modal contact\nplanning framework for agile quadrupedal locomotion over a constrained rebar\nenvironment. To this end, our hierarchical planner incorporates\nlocomotion-specific modules into the high-level contact sequence planner and\nsolves kinodynamically-aware trajectory optimization as the low-level motion\nplanner. Through quantitative analysis of the experience accumulation process\nand experimental validation of the kinodynamic feasibility of the generated\nlocomotion trajectories, we demonstrate that the experience planning heuristic\noffers an effective way of providing candidate footholds for a legged contact\nplanner. Additionally, we introduce a guiding torso path heuristic at the\nglobal planning level to enhance the navigation success rate in the presence of\nenvironmental obstacles. Our results indicate that the torso-path guided\nexperience accumulation requires significantly fewer offline trials to\nsuccessfully reach the goal compared to regular experience accumulation.\nFinally, our planning framework is validated in both dynamics simulations and\nreal hardware implementations on a quadrupedal robot provided by Skymul Inc.","publication_date":1699984434,"paper_link":"http://arxiv.org/pdf/2311.08354v1","categories":["Quantitative Biology"],"abstract":"This study focuses on a layered, experience-based, multi-modal contact planning framework for agile quadrupedal locomotion over a constrained rebar environment. To this end, our hierarchical planner incorporates locomotion-specific modules into the high-level contact sequence planner and solves kinodynamically-aware trajectory optimization as the low-level motion planner. Through quantitative analysis of the experience accumulation process and experimental validation of the kinodynamic feasibility of the generated locomotion trajectories, we demonstrate that the experience planning heuristic offers an effective way of providing candidate footholds for a legged contact planner. Additionally, we introduce a guiding torso path heuristic at the global planning level to enhance the navigation success rate in the presence of environmental obstacles. Our results indicate that the torso-path guided experience accumulation requires significantly fewer offline trials to successfully reach the goal compared to regular experience accumulation. Finally, our planning framework is validated in both dynamics simulations and real hardware implementations on a quadrupedal robot provided by Skymul Inc."}
{"title":"A Gaussian Convexity for Logarithmic Moment Generating Functions","authors":["Wei-Kuo Chen"],"raw_abstract":"For any convex function $F$ of $n$-dimensional Gaussian vector $g$ with\n$\\mathbb{E} e^{\\lambda F(g)}<\\infty$ for any $\\lambda>0$, we show that\n$\\lambda^{-1}\\ln \\mathbb{E} e^{\\lambda F(g)}$ is convex in\n$\\lambda\\in\\mathbb{R}$. Based on this convexity, we draw two applications. The\nfirst deduces an improvement of the Paouris-Valettas small deviation inequality\nin convex geometry. The second establishes a quantitative bound for the\nDotsenko-Franz-M\\'ezard conjecture arising from the study of the\nSherrington-Kirkpatrick mean-field spin glass model, which states that the\nlogarithmic anneal partition function of negative replica is asymptotically\nequal to the free energy.","publication_date":1699984143,"paper_link":"http://arxiv.org/pdf/2311.08351v1","categories":["Mathematics","Physics"],"abstract":"For any convex function __FORMULA__ of __FORMULA__-dimensional Gaussian vector __FORMULA__ with __FORMULA__ for any __FORMULA__, we show that __FORMULA__ is convex in __FORMULA__. Based on this convexity, we draw two applications. The first deduces an improvement of the Paouris-Valettas small deviation inequality in convex geometry. The second establishes a quantitative bound for the Dotsenko-Franz-M\\'ezard conjecture arising from the study of the Sherrington-Kirkpatrick mean-field spin glass model, which states that the logarithmic anneal partition function of negative replica is asymptotically equal to the free energy."}
{"title":"Comparison of model selection techniques for seafloor scattering statistics","authors":["Derek R Olson","Marc Geilhufe"],"raw_abstract":"In quantitative analysis of seafloor imagery, it is common to model the\ncollection of individual pixel intensities scattered by the seafloor as a\nrandom variable with a given statistical distribution. There is a considerable\nliterature on statistical models for seafloor scattering, mostly focused on\nareas with statistically homogeneous properties (i.e. exhibiting spatial\nstationarity). For more complex seafloors, the pixel intensity distribution is\nmore appropriately modeled using a mixture of simple distributions. For very\ncomplex seafloors, fitting 3 or more mixture components makes physical sense,\nbut the statistical model becomes much more complex in these cases. Therefore,\npicking the number of components of the mixture model is a decision that must\nbe made, using a priori information, or using a data driven approach. However,\nthis information is time consuming to collect, and depends on the skill and\nexperience of the human. Therefore, a data-driven approach is advantageous to\nuse, and is explored in this work. Criteria for choosing a model always need to\nbalance the trade-off for the best fit for the data on the one hand and the\nmodel complexity on the other hand. In this work, we compare several\nstatistical model selection criteria, e.g., the Bayesian information criterion.\nExamples are given for SAS data collected by an autonomous underwater vehicle\nin a rocky environment off the coast of Bergen, Norway using data from the\nHISAS-1032 synthetic aperture sonar system.","publication_date":1699982943,"paper_link":"http://arxiv.org/pdf/2311.08337v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"In quantitative analysis of seafloor imagery, it is common to model the collection of individual pixel intensities scattered by the seafloor as a random variable with a given statistical distribution. There is a considerable literature on statistical models for seafloor scattering, mostly focused on areas with statistically homogeneous properties (i.e. exhibiting spatial stationarity). For more complex seafloors, the pixel intensity distribution is more appropriately modeled using a mixture of simple distributions. For very complex seafloors, fitting 3 or more mixture components makes physical sense, but the statistical model becomes much more complex in these cases. Therefore, picking the number of components of the mixture model is a decision that must be made, using a priori information, or using a data driven approach. However, this information is time consuming to collect, and depends on the skill and experience of the human. Therefore, a data-driven approach is advantageous to use, and is explored in this work. Criteria for choosing a model always need to balance the trade-off for the best fit for the data on the one hand and the model complexity on the other hand. In this work, we compare several statistical model selection criteria, e.g., the Bayesian information criterion. Examples are given for SAS data collected by an autonomous underwater vehicle in a rocky environment off the coast of Bergen, Norway using data from the HISAS-1032 synthetic aperture sonar system."}
{"title":"Optimally Managing the Impacts of Convergence Tolerance for Distributed Optimal Power Flow","authors":["Rachel Harris","Mohannad Alkhraijah","Daniel K. Molzahn"],"raw_abstract":"The future power grid may rely on distributed optimization to determine the\nset-points for huge numbers of distributed energy resources. There has been\nsignificant work on applying distributed algorithms to optimal power flow (OPF)\nproblems, which require separate computing agents to agree on shared boundary\nvariable values. Looser tolerances for the mismatches in these shared variables\ngenerally yield faster convergence at the expense of exacerbating constraint\nviolations, but there is little quantitative understanding of how the\nconvergence tolerance affects solution quality. To address this gap, we first\nquantify how convergence tolerance impacts constraint violations when the\ndistributed OPF generator dispatch is applied to the power system. Using\ninsights from this analysis, we then develop a bound tightening algorithm which\nguarantees that operating points from distributed OPF algorithms will not\nresult in violations despite the possibility of shared variable mismatches\nwithin the convergence tolerance. We also explore how bounding the cumulative\nshared variable mismatches can prevent unnecessary conservativeness in the\nbound tightening. The proposed approach enables control of the trade-off\nbetween computational speed, which improves as the convergence tolerance\nincreases, and distributed OPF solution cost, which increases with convergence\ntolerance due to tightened constraints, while ensuring feasibility.","publication_date":1699980549,"paper_link":"http://arxiv.org/pdf/2311.08305v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The future power grid may rely on distributed optimization to determine the set-points for huge numbers of distributed energy resources. There has been significant work on applying distributed algorithms to optimal power flow (OPF) problems, which require separate computing agents to agree on shared boundary variable values. Looser tolerances for the mismatches in these shared variables generally yield faster convergence at the expense of exacerbating constraint violations, but there is little quantitative understanding of how the convergence tolerance affects solution quality. To address this gap, we first quantify how convergence tolerance impacts constraint violations when the distributed OPF generator dispatch is applied to the power system. Using insights from this analysis, we then develop a bound tightening algorithm which guarantees that operating points from distributed OPF algorithms will not result in violations despite the possibility of shared variable mismatches within the convergence tolerance. We also explore how bounding the cumulative shared variable mismatches can prevent unnecessary conservativeness in the bound tightening. The proposed approach enables control of the trade-off between computational speed, which improves as the convergence tolerance increases, and distributed OPF solution cost, which increases with convergence tolerance due to tightened constraints, while ensuring feasibility."}
{"title":"Defining the boundaries: challenges and advances in identifying cells in microscopy images","authors":["Nodar Gogoberidze","Beth A. Cimini"],"raw_abstract":"Segmentation, or the outlining of objects within images, is a critical step\nin the measurement and analysis of cells within microscopy images. While\nimprovements continue to be made in tools that rely on classical methods for\nsegmentation, deep learning-based tools increasingly dominate advances in the\ntechnology. Specialist models such as Cellpose continue to improve in accuracy\nand user-friendliness, and segmentation challenges such as the Multi-Modality\nCell Segmentation Challenge continue to push innovation in accuracy across\nwidely-varying test data as well as efficiency and usability. Increased\nattention on documentation, sharing, and evaluation standards are leading to\nincreased user-friendliness and acceleration towards the goal of a truly\nuniversal method.","publication_date":1699977738,"paper_link":"http://arxiv.org/pdf/2311.08269v1","categories":["Quantitative Biology"],"abstract":"Segmentation, or the outlining of objects within images, is a critical step in the measurement and analysis of cells within microscopy images. While improvements continue to be made in tools that rely on classical methods for segmentation, deep learning-based tools increasingly dominate advances in the technology. Specialist models such as Cellpose continue to improve in accuracy and user-friendliness, and segmentation challenges such as the Multi-Modality Cell Segmentation Challenge continue to push innovation in accuracy across widely-varying test data as well as efficiency and usability. Increased attention on documentation, sharing, and evaluation standards are leading to increased user-friendliness and acceleration towards the goal of a truly universal method."}
{"title":"Diffusion-based generation of Histopathological Whole Slide Images at a Gigapixel scale","authors":["Robert Harb","Thomas Pock","Heimo M\u00fcller"],"raw_abstract":"We present a novel diffusion-based approach to generate synthetic\nhistopathological Whole Slide Images (WSIs) at an unprecedented gigapixel\nscale. Synthetic WSIs have many potential applications: They can augment\ntraining datasets to enhance the performance of many computational pathology\napplications. They allow the creation of synthesized copies of datasets that\ncan be shared without violating privacy regulations. Or they can facilitate\nlearning representations of WSIs without requiring data annotations. Despite\nthis variety of applications, no existing deep-learning-based method generates\nWSIs at their typically high resolutions. Mainly due to the high computational\ncomplexity. Therefore, we propose a novel coarse-to-fine sampling scheme to\ntackle image generation of high-resolution WSIs. In this scheme, we increase\nthe resolution of an initial low-resolution image to a high-resolution WSI.\nParticularly, a diffusion model sequentially adds fine details to images and\nincreases their resolution. In our experiments, we train our method with WSIs\nfrom the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also\nperformed a user study with pathologists. The study results suggest that our\ngenerated WSIs resemble the structure of real WSIs.","publication_date":1699972419,"paper_link":"http://arxiv.org/pdf/2311.08199v1","categories":["Electrical Engineering and Systems Science"],"abstract":"We present a novel diffusion-based approach to generate synthetic histopathological Whole Slide Images (WSIs) at an unprecedented gigapixel scale. Synthetic WSIs have many potential applications: They can augment training datasets to enhance the performance of many computational pathology applications. They allow the creation of synthesized copies of datasets that can be shared without violating privacy regulations. Or they can facilitate learning representations of WSIs without requiring data annotations. Despite this variety of applications, no existing deep-learning-based method generates WSIs at their typically high resolutions. Mainly due to the high computational complexity. Therefore, we propose a novel coarse-to-fine sampling scheme to tackle image generation of high-resolution WSIs. In this scheme, we increase the resolution of an initial low-resolution image to a high-resolution WSI. Particularly, a diffusion model sequentially adds fine details to images and increases their resolution. In our experiments, we train our method with WSIs from the TCGA-BRCA dataset. Additionally to quantitative evaluations, we also performed a user study with pathologists. The study results suggest that our generated WSIs resemble the structure of real WSIs."}
{"title":"Understanding learning from EEG data: Combining machine learning and feature engineering based on hidden Markov models and mixed models","authors":["Gabriel Rodrigues Palma","Conor Thornberry","Se\u00e1n Commins","Rafael de Andrade Moral"],"raw_abstract":"Theta oscillations, ranging from 4-8 Hz, play a significant role in spatial\nlearning and memory functions during navigation tasks. Frontal theta\noscillations are thought to play an important role in spatial navigation and\nmemory. Electroencephalography (EEG) datasets are very complex, making any\nchanges in the neural signal related to behaviour difficult to interpret.\nHowever, multiple analytical methods are available to examine complex data\nstructure, especially machine learning based techniques. These methods have\nshown high classification performance and the combination with feature\nengineering enhances the capability of these methods. This paper proposes using\nhidden Markov and linear mixed effects models to extract features from EEG\ndata. Based on the engineered features obtained from frontal theta EEG data\nduring a spatial navigation task in two key trials (first, last) and between\ntwo conditions (learner and non-learner), we analysed the performance of six\nmachine learning methods (Polynomial Support Vector Machines, Non-linear\nSupport Vector Machines, Random Forests, K-Nearest Neighbours, Ridge, and Deep\nNeural Networks) on classifying learner and non-learner participants. We also\nanalysed how different standardisation methods used to pre-process the EEG data\ncontribute to classification performance. We compared the classification\nperformance of each trial with data gathered from the same subjects, including\nsolely coordinate-based features, such as idle time and average speed. We found\nthat more machine learning methods perform better classification using\ncoordinate-based data. However, only deep neural networks achieved an area\nunder the ROC curve higher than 80% using the theta EEG data alone. Our\nfindings suggest that standardising the theta EEG data and using deep neural\nnetworks enhances the classification of learner and non-learner subjects in a\nspatial learning task.","publication_date":1699964652,"paper_link":"http://arxiv.org/pdf/2311.08113v1","categories":["Quantitative Biology","Electrical Engineering and Systems Science"],"abstract":"Theta oscillations, ranging from 4-8 Hz, play a significant role in spatial learning and memory functions during navigation tasks. Frontal theta oscillations are thought to play an important role in spatial navigation and memory. Electroencephalography (EEG) datasets are very complex, making any changes in the neural signal related to behaviour difficult to interpret. However, multiple analytical methods are available to examine complex data structure, especially machine learning based techniques. These methods have shown high classification performance and the combination with feature engineering enhances the capability of these methods. This paper proposes using hidden Markov and linear mixed effects models to extract features from EEG data. Based on the engineered features obtained from frontal theta EEG data during a spatial navigation task in two key trials (first, last) and between two conditions (learner and non-learner), we analysed the performance of six machine learning methods (Polynomial Support Vector Machines, Non-linear Support Vector Machines, Random Forests, K-Nearest Neighbours, Ridge, and Deep Neural Networks) on classifying learner and non-learner participants. We also analysed how different standardisation methods used to pre-process the EEG data contribute to classification performance. We compared the classification performance of each trial with data gathered from the same subjects, including solely coordinate-based features, such as idle time and average speed. We found that more machine learning methods perform better classification using coordinate-based data. However, only deep neural networks achieved an area under the ROC curve higher than 80% using the theta EEG data alone. Our findings suggest that standardising the theta EEG data and using deep neural networks enhances the classification of learner and non-learner subjects in a spatial learning task."}
{"title":"GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy","authors":["Hongyang Jiang","Mengdi Gao","Zirong Liu","Chen Tang","Xiaoqing Zhang","Shuai Jiang","Wu Yuan","Jiang Liu"],"raw_abstract":"Early-stage diabetic retinopathy (DR) presents challenges in clinical\ndiagnosis due to inconspicuous and minute microangioma lesions, resulting in\nlimited research in this area. Additionally, the potential of emerging\nfoundation models, such as the segment anything model (SAM), in medical\nscenarios remains rarely explored. In this work, we propose a\nhuman-in-the-loop, label-free early DR diagnosis framework called GlanceSeg,\nbased on SAM. GlanceSeg enables real-time segmentation of microangioma lesions\nas ophthalmologists review fundus images. Our human-in-the-loop framework\nintegrates the ophthalmologist's gaze map, allowing for rough localization of\nminute lesions in fundus images. Subsequently, a saliency map is generated\nbased on the located region of interest, which provides prompt points to assist\nthe foundation model in efficiently segmenting microangioma lesions. Finally, a\ndomain knowledge filter refines the segmentation of minute lesions. We\nconducted experiments on two newly-built public datasets, i.e., IDRiD and\nRetinal-Lesions, and validated the feasibility and superiority of GlanceSeg\nthrough visualized illustrations and quantitative measures. Additionally, we\ndemonstrated that GlanceSeg improves annotation efficiency for clinicians and\nenhances segmentation performance through fine-tuning using annotations. This\nstudy highlights the potential of GlanceSeg-based annotations for self-model\noptimization, leading to enduring performance advancements through continual\nlearning.","publication_date":1699959585,"paper_link":"http://arxiv.org/pdf/2311.08075v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Early-stage diabetic retinopathy (DR) presents challenges in clinical diagnosis due to inconspicuous and minute microangioma lesions, resulting in limited research in this area. Additionally, the potential of emerging foundation models, such as the segment anything model (SAM), in medical scenarios remains rarely explored. In this work, we propose a human-in-the-loop, label-free early DR diagnosis framework called GlanceSeg, based on SAM. GlanceSeg enables real-time segmentation of microangioma lesions as ophthalmologists review fundus images. Our human-in-the-loop framework integrates the ophthalmologist's gaze map, allowing for rough localization of minute lesions in fundus images. Subsequently, a saliency map is generated based on the located region of interest, which provides prompt points to assist the foundation model in efficiently segmenting microangioma lesions. Finally, a domain knowledge filter refines the segmentation of minute lesions. We conducted experiments on two newly-built public datasets, i.e., IDRiD and Retinal-Lesions, and validated the feasibility and superiority of GlanceSeg through visualized illustrations and quantitative measures. Additionally, we demonstrated that GlanceSeg improves annotation efficiency for clinicians and enhances segmentation performance through fine-tuning using annotations. This study highlights the potential of GlanceSeg-based annotations for self-model optimization, leading to enduring performance advancements through continual learning."}
{"title":"The statistical signal for Milgrom's critical acceleration boundary being an objective characteristic of the optical disk","authors":["David Roscoe"],"raw_abstract":"The various successes of Milgrom's MOND have led to suggestions that its\ncritical acceleration parameter $a_0 \\approx 1.2\\times 10^{-10}\\,mtrs/sec^2$ is\na fundamental physical constant in the same category as the gravitational\nconstant (for example), and therefore requiring no further explanation. There\nis no independent evidence supporting this conjecture.\n  Motivated by empirical indications of self-similarities on the exterior part\nof the optical disk (the optical annulus), we describe a statistical analysis\nof four large samples of optical rotation curves and find that quantitative\nindicators of self-similar dynamics on the optical annulus are irreducibly\npresent in each of the samples. These symmetries lead to the unambiguous\nidentification of a characteristic point, $(R_c,V_c)$, on each annular rotation\ncurve where $R_c \\approx f(M,S)$ and $V_c \\approx g(M)$ for absolute magnitude\n$M$ and surface brightness $S$.\n  This opens the door to an investigation of the behaviour of the associated\ncharacteristic acceleration $a_c \\equiv V_c^2/R_c$ across each sample. The\nfirst observation is that since $a_c \\approx g^2(M)/f(M,S)$, then $a_c$ is a\nconstant within any given disk, but varies between disks.\n  Calculation then shows that $a_c$ varies in the approximate range\n$(1.2\\pm0.5)\\times 10^{-10}\\,mtrs/sec^2$ for each sample. It follows that\nMilgrom's $a_0$ is effectively identical to $a_c$, and his critical\nacceleration boundary is actually the characteristic boundary, $R=R_c$, on any\ngiven disk. Since $a_c$ varies between galaxies, then so must $a_0$ also. In\nsummary,Milgrom's critical acceleration boundary is an objective characteristic\nof the optical disk and $a_0$ cannot be a fundamental physical constant.","publication_date":1699956028,"paper_link":"http://arxiv.org/pdf/2311.08039v1","categories":["Physics"],"abstract":"The various successes of Milgrom's MOND have led to suggestions that its critical acceleration parameter __FORMULA__ is a fundamental physical constant in the same category as the gravitational constant (for example), and therefore requiring no further explanation. There is no independent evidence supporting this conjecture.   Motivated by empirical indications of self-similarities on the exterior part of the optical disk (the optical annulus), we describe a statistical analysis of four large samples of optical rotation curves and find that quantitative indicators of self-similar dynamics on the optical annulus are irreducibly present in each of the samples. These symmetries lead to the unambiguous identification of a characteristic point, __FORMULA__, on each annular rotation curve where __FORMULA__ and __FORMULA__ for absolute magnitude __FORMULA__ and surface brightness __FORMULA__.   This opens the door to an investigation of the behaviour of the associated characteristic acceleration __FORMULA__ across each sample. The first observation is that since __FORMULA__, then __FORMULA__ is a constant within any given disk, but varies between disks.   Calculation then shows that __FORMULA__ varies in the approximate range __FORMULA__ for each sample. It follows that Milgrom's __FORMULA__ is effectively identical to __FORMULA__, and his critical acceleration boundary is actually the characteristic boundary, __FORMULA__, on any given disk. Since __FORMULA__ varies between galaxies, then so must __FORMULA__ also. In summary,Milgrom's critical acceleration boundary is an objective characteristic of the optical disk and __FORMULA__ cannot be a fundamental physical constant."}
{"title":"Time-efficient combined morphologic and quantitative joint MRI based on clinical image contrasts -- An exploratory in-situ study of standardized cartilage defects","authors":["Teresa Lemainque","Nicola Prid\u00f6hl","Shuo Zhang","Marc Huppertz","Manuel Post","Can Y\u00fcksel","Masami Yoneyama","Andreas Prescher","Christiane Kuhl","Daniel Truhn","Sven Nebelung"],"raw_abstract":"OBJECTIVES: Quantitative MRI techniques such as T2 and T1$\\rho$ mapping are\nbeneficial in evaluating cartilage and meniscus. We aimed to evaluate the\nMIXTURE (Multi-Interleaved X-prepared Turbo-Spin Echo with IntUitive\nRElaxometry) sequences that provide morphologic images with clinical turbo\nspin-echo (TSE) contrasts and additional parameter maps versus reference TSE\nsequences in an in-situ model of human cartilage defects.\n  MATERIALS AND METHODS: Prospectively, standardized cartilage defects of 8mm,\n5mm, and 3mm diameter were created in the lateral femora of 10 human cadaveric\nknee specimens (81$\\pm$10 years, nine male/one female). Using a clinical 3T MRI\nscanner and knee coil, MIXTURE sequences combining (i) proton-density weighted\nfat-saturated (PD-w FS) images and T2 maps and (ii) T1-weighted images and\nT1$\\rho$ maps were acquired before and after defect creation, alongside the\ncorresponding 2D TSE and 3D TSE reference sequences. Defect delineability, bone\ntexture, and cartilage relaxation times were quantified. Inter-sequence\ncomparisons were made using appropriate parametric and non-parametric tests.\n  RESULTS: Overall, defect delineability and texture features were not\nsignificantly different between the MIXTURE and reference sequences. After\ndefect creation, relaxation times increased significantly in the central femur\n(for T2) and all regions combined (for T1$\\rho$).\n  CONCLUSION: MIXTURE sequences permit time-efficient simultaneous morphologic\nand quantitative joint assessment based on clinical image contrasts. While\nproviding T2 or T1$\\rho$ maps in clinically feasible scan time, morphologic\nimage features, i.e., cartilage defect delineability and bone texture, were\ncomparable between MIXTURE and corresponding reference sequences.","publication_date":1699955737,"paper_link":"http://arxiv.org/pdf/2311.08036v1","categories":["Electrical Engineering and Systems Science","Physics"],"abstract":"OBJECTIVES: Quantitative MRI techniques such as T2 and T1__FORMULA__ mapping are beneficial in evaluating cartilage and meniscus. We aimed to evaluate the MIXTURE (Multi-Interleaved X-prepared Turbo-Spin Echo with IntUitive RElaxometry) sequences that provide morphologic images with clinical turbo spin-echo (TSE) contrasts and additional parameter maps versus reference TSE sequences in an in-situ model of human cartilage defects.   MATERIALS AND METHODS: Prospectively, standardized cartilage defects of 8mm, 5mm, and 3mm diameter were created in the lateral femora of 10 human cadaveric knee specimens (81__FORMULA__10 years, nine male/one female). Using a clinical 3T MRI scanner and knee coil, MIXTURE sequences combining (i) proton-density weighted fat-saturated (PD-w FS) images and T2 maps and (ii) T1-weighted images and T1__FORMULA__ maps were acquired before and after defect creation, alongside the corresponding 2D TSE and 3D TSE reference sequences. Defect delineability, bone texture, and cartilage relaxation times were quantified. Inter-sequence comparisons were made using appropriate parametric and non-parametric tests.   RESULTS: Overall, defect delineability and texture features were not significantly different between the MIXTURE and reference sequences. After defect creation, relaxation times increased significantly in the central femur (for T2) and all regions combined (for T1__FORMULA__).   CONCLUSION: MIXTURE sequences permit time-efficient simultaneous morphologic and quantitative joint assessment based on clinical image contrasts. While providing T2 or T1__FORMULA__ maps in clinically feasible scan time, morphologic image features, i.e., cartilage defect delineability and bone texture, were comparable between MIXTURE and corresponding reference sequences."}
{"title":"Unified Quantification of Quantum Defects in Small-Diameter Single-Walled Carbon Nanotubes by Raman Spectroscopy","authors":["Finn L. Sebastian","Felicitas Becker","Yohei Yomogida","Yuuya Hosokawa","Simon Settele","Sebastian Lindenthal","Kazuhiro Yanagi","Jana Zaumseil"],"raw_abstract":"The covalent functionalization of single-walled carbon nanotubes (SWCNTs)\nwith luminescent quantum defects enables their application as near-infrared\nsingle-photon sources, as optical sensors, and for in-vivo tissue imaging.\nTuning the emission wavelength and defect density are crucial for these\napplications. While the former can be controlled by different synthetic\nprotocols and is easily measured, defect densities are still determined as\nrelative rather than absolute values, limiting the comparability between\ndifferent nanotube batches and chiralities. Here, we present an absolute and\nunified quantification metric for the defect density in SWCNT samples based on\nRaman spectroscopy. It is applicable to a range of small-diameter nanotubes and\nfor arbitrary laser wavelengths. We observe a clear inverse correlation of the\nD/G$^{+}$ ratio increase with nanotube diameter, indicating that curvature\neffects contribute significantly to the defect-activation of Raman modes.\nCorrelation of intermediate frequency modes with defect densities further\ncorroborates their activation by defects and provides additional quantitative\nmetrics for the characterization of functionalized SWCNTs.","publication_date":1699954933,"paper_link":"http://arxiv.org/pdf/2311.08029v1","categories":["Physics"],"abstract":"The covalent functionalization of single-walled carbon nanotubes (SWCNTs) with luminescent quantum defects enables their application as near-infrared single-photon sources, as optical sensors, and for in-vivo tissue imaging. Tuning the emission wavelength and defect density are crucial for these applications. While the former can be controlled by different synthetic protocols and is easily measured, defect densities are still determined as relative rather than absolute values, limiting the comparability between different nanotube batches and chiralities. Here, we present an absolute and unified quantification metric for the defect density in SWCNT samples based on Raman spectroscopy. It is applicable to a range of small-diameter nanotubes and for arbitrary laser wavelengths. We observe a clear inverse correlation of the D/G__FORMULA__ ratio increase with nanotube diameter, indicating that curvature effects contribute significantly to the defect-activation of Raman modes. Correlation of intermediate frequency modes with defect densities further corroborates their activation by defects and provides additional quantitative metrics for the characterization of functionalized SWCNTs."}
{"title":"Clinical Characteristics and Laboratory Biomarkers in ICU-admitted Septic Patients with and without Bacteremia","authors":["Sangwon Baek","Seung Jun Lee"],"raw_abstract":"Few studies have investigated the diagnostic utilities of biomarkers for\npredicting bacteremia among septic patients admitted to intensive care units\n(ICU). Therefore, this study evaluated the prediction power of laboratory\nbiomarkers to utilize those markers with high performance to optimize the\npredictive model for bacteremia. This retrospective cross-sectional study was\nconducted at the ICU department of Gyeongsang National University Changwon\nHospital in 2019. Adult patients qualifying SEPSIS-3 (increase in sequential\norgan failure score greater than or equal to 2) criteria with at least two sets\nof blood culture were selected. Collected data was initially analyzed\nindependently to identify the significant predictors, which was then used to\nbuild the multivariable logistic regression (MLR) model. A total of 218\npatients with 48 cases of true bacteremia were analyzed in this research. Both\nCRP and PCT showed a substantial area under the curve (AUC) value for\ndiscriminating bacteremia among septic patients (0.757 and 0.845,\nrespectively). To further enhance the predictive accuracy, we combined PCT,\nbilirubin, neutrophil lymphocyte ratio (NLR), platelets, lactic acid,\nerythrocyte sedimentation rate (ESR), and Glasgow Coma Scale (GCS) score to\nbuild the predictive model with an AUC of 0.907 (95% CI, 0.843 to 0.956). In\naddition, a high association between bacteremia and mortality rate was\ndiscovered through the survival analysis (0.004). While PCT is certainly a\nuseful index for distinguishing patients with and without bacteremia by itself,\nour MLR model indicates that the accuracy of bacteremia prediction\nsubstantially improves by the combined use of PCT, bilirubin, NLR, platelets,\nlactic acid, ESR, and GCS score.","publication_date":1699944266,"paper_link":"http://arxiv.org/pdf/2311.08433v2","categories":["Quantitative Biology","Statistics"],"abstract":"Few studies have investigated the diagnostic utilities of biomarkers for predicting bacteremia among septic patients admitted to intensive care units (ICU). Therefore, this study evaluated the prediction power of laboratory biomarkers to utilize those markers with high performance to optimize the predictive model for bacteremia. This retrospective cross-sectional study was conducted at the ICU department of Gyeongsang National University Changwon Hospital in 2019. Adult patients qualifying SEPSIS-3 (increase in sequential organ failure score greater than or equal to 2) criteria with at least two sets of blood culture were selected. Collected data was initially analyzed independently to identify the significant predictors, which was then used to build the multivariable logistic regression (MLR) model. A total of 218 patients with 48 cases of true bacteremia were analyzed in this research. Both CRP and PCT showed a substantial area under the curve (AUC) value for discriminating bacteremia among septic patients (0.757 and 0.845, respectively). To further enhance the predictive accuracy, we combined PCT, bilirubin, neutrophil lymphocyte ratio (NLR), platelets, lactic acid, erythrocyte sedimentation rate (ESR), and Glasgow Coma Scale (GCS) score to build the predictive model with an AUC of 0.907 (95% CI, 0.843 to 0.956). In addition, a high association between bacteremia and mortality rate was discovered through the survival analysis (0.004). While PCT is certainly a useful index for distinguishing patients with and without bacteremia by itself, our MLR model indicates that the accuracy of bacteremia prediction substantially improves by the combined use of PCT, bilirubin, NLR, platelets, lactic acid, ESR, and GCS score."}
{"title":"Plug-and-Play Latent Feature Editing for Orientation-Adaptive Quantitative Susceptibility Mapping Neural Networks","authors":["Yang Gao","Zhuang Xiong","Shanshan Shan","Yin Liu","Pengfei Rong","Min Li","Alan H Wilman","G. Bruce Pike","Feng Liu","Hongfu Sun"],"raw_abstract":"Quantitative susceptibility mapping (QSM) is a post-processing technique for\nderiving tissue magnetic susceptibility distribution from MRI phase\nmeasurements. Deep learning (DL) algorithms hold great potential for solving\nthe ill-posed QSM reconstruction problem. However, a significant challenge\nfacing current DL-QSM approaches is their limited adaptability to magnetic\ndipole field orientation variations during training and testing. In this work,\nwe propose a novel Orientation-Adaptive Latent Feature Editing (OA-LFE) module\nto learn the encoding of acquisition orientation vectors and seamlessly\nintegrate them into the latent features of deep networks. Importantly, it can\nbe directly Plug-and-Play (PnP) into various existing DL-QSM architectures,\nenabling reconstructions of QSM from arbitrary magnetic dipole orientations.\nIts effectiveness is demonstrated by combining the OA-LFE module into our\npreviously proposed phase-to-susceptibility single-step instant QSM (iQSM)\nnetwork, which was initially tailored for pure-axial acquisitions. The proposed\nOA-LFE-empowered iQSM, which we refer to as iQSM+, is trained in a\nself-supervised manner on a specially-designed simulation brain dataset.\nComprehensive experiments are conducted on simulated and in vivo human brain\ndatasets, encompassing subjects ranging from healthy individuals to those with\npathological conditions. These experiments involve various MRI platforms (3T\nand 7T) and aim to compare our proposed iQSM+ against several established QSM\nreconstruction frameworks, including the original iQSM. The iQSM+ yields QSM\nimages with significantly improved accuracies and mitigates artifacts,\nsurpassing other state-of-the-art DL-QSM algorithms.","publication_date":1699922996,"paper_link":"http://arxiv.org/pdf/2311.07823v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Quantitative susceptibility mapping (QSM) is a post-processing technique for deriving tissue magnetic susceptibility distribution from MRI phase measurements. Deep learning (DL) algorithms hold great potential for solving the ill-posed QSM reconstruction problem. However, a significant challenge facing current DL-QSM approaches is their limited adaptability to magnetic dipole field orientation variations during training and testing. In this work, we propose a novel Orientation-Adaptive Latent Feature Editing (OA-LFE) module to learn the encoding of acquisition orientation vectors and seamlessly integrate them into the latent features of deep networks. Importantly, it can be directly Plug-and-Play (PnP) into various existing DL-QSM architectures, enabling reconstructions of QSM from arbitrary magnetic dipole orientations. Its effectiveness is demonstrated by combining the OA-LFE module into our previously proposed phase-to-susceptibility single-step instant QSM (iQSM) network, which was initially tailored for pure-axial acquisitions. The proposed OA-LFE-empowered iQSM, which we refer to as iQSM+, is trained in a self-supervised manner on a specially-designed simulation brain dataset. Comprehensive experiments are conducted on simulated and in vivo human brain datasets, encompassing subjects ranging from healthy individuals to those with pathological conditions. These experiments involve various MRI platforms (3T and 7T) and aim to compare our proposed iQSM+ against several established QSM reconstruction frameworks, including the original iQSM. The iQSM+ yields QSM images with significantly improved accuracies and mitigates artifacts, surpassing other state-of-the-art DL-QSM algorithms."}
{"title":"Comprehensive Overview of Bottom-up Proteomics using Mass Spectrometry","authors":["Yuming Jiang","Devasahayam Arokia Balaya Rex","Dina Schuster","Benjamin A. Neely","Germ\u00e1n L. Rosano","Norbert Volkmar","Amanda Momenzadeh","Trenton M. Peters-Clarke","Susan B. Egbert","Simion Kreimer","Emma H. Doud","Oliver M. Crook","Amit Kumar Yadav","Muralidharan Vanuopadath","Mart\u00edn L. Mayta","Anna G. Duboff","Nicholas M. Riley","Robert L. Moritz","Jesse G. Meyer"],"raw_abstract":"Proteomics is the large scale study of protein structure and function from\nbiological systems through protein identification and quantification. \"Shotgun\nproteomics\" or \"bottom-up proteomics\" is the prevailing strategy, in which\nproteins are hydrolyzed into peptides that are analyzed by mass spectrometry.\nProteomics studies can be applied to diverse studies ranging from simple\nprotein identification to studies of proteoforms, protein-protein interactions,\nprotein structural alterations, absolute and relative protein quantification,\npost-translational modifications, and protein stability. To enable this range\nof different experiments, there are diverse strategies for proteome analysis.\nThe nuances of how proteomic workflows differ may be challenging to understand\nfor new practitioners. Here, we provide a comprehensive overview of different\nproteomics methods to aid the novice and experienced researcher. We cover from\nbiochemistry basics and protein extraction to biological interpretation and\northogonal validation. We expect this work to serve as a basic resource for new\npractitioners in the field of shotgun or bottom-up proteomics.","publication_date":1699916310,"paper_link":"http://arxiv.org/pdf/2311.07791v1","categories":["Quantitative Biology"],"abstract":"Proteomics is the large scale study of protein structure and function from biological systems through protein identification and quantification. \"Shotgun proteomics\" or \"bottom-up proteomics\" is the prevailing strategy, in which proteins are hydrolyzed into peptides that are analyzed by mass spectrometry. Proteomics studies can be applied to diverse studies ranging from simple protein identification to studies of proteoforms, protein-protein interactions, protein structural alterations, absolute and relative protein quantification, post-translational modifications, and protein stability. To enable this range of different experiments, there are diverse strategies for proteome analysis. The nuances of how proteomic workflows differ may be challenging to understand for new practitioners. Here, we provide a comprehensive overview of different proteomics methods to aid the novice and experienced researcher. We cover from biochemistry basics and protein extraction to biological interpretation and orthogonal validation. We expect this work to serve as a basic resource for new practitioners in the field of shotgun or bottom-up proteomics."}
{"title":"Histopathologic Cancer Detection","authors":["Varan Singh Rohila","Neeraj Lalwani","Lochan Basyal"],"raw_abstract":"Early diagnosis of the cancer cells is necessary for making an effective\ntreatment plan and for the health and safety of a patient. Nowadays, doctors\nusually use a histological grade that pathologists determine by performing a\nsemi-quantitative analysis of the histopathological and cytological features of\nhematoxylin-eosin (HE) stained histopathological images. This research\ncontributes a potential classification model for cancer prognosis to\nefficiently utilize the valuable information underlying the HE-stained\nhistopathological images. This work uses the PatchCamelyon benchmark datasets\nand trains them in a multi-layer perceptron and convolution model to observe\nthe model's performance in terms of precision, Recall, F1 Score, Accuracy, and\nAUC Score. The evaluation result shows that the baseline convolution model\noutperforms the baseline MLP model. Also, this paper introduced ResNet50 and\nInceptionNet models with data augmentation, where ResNet50 is able to beat the\nstate-of-the-art model. Furthermore, the majority vote and concatenation\nensemble were evaluated and provided the future direction of using transfer\nlearning and segmentation to understand the specific features.","publication_date":1699905106,"paper_link":"http://arxiv.org/pdf/2311.07711v1","categories":["Quantitative Biology"],"abstract":"Early diagnosis of the cancer cells is necessary for making an effective treatment plan and for the health and safety of a patient. Nowadays, doctors usually use a histological grade that pathologists determine by performing a semi-quantitative analysis of the histopathological and cytological features of hematoxylin-eosin (HE) stained histopathological images. This research contributes a potential classification model for cancer prognosis to efficiently utilize the valuable information underlying the HE-stained histopathological images. This work uses the PatchCamelyon benchmark datasets and trains them in a multi-layer perceptron and convolution model to observe the model's performance in terms of precision, Recall, F1 Score, Accuracy, and AUC Score. The evaluation result shows that the baseline convolution model outperforms the baseline MLP model. Also, this paper introduced ResNet50 and InceptionNet models with data augmentation, where ResNet50 is able to beat the state-of-the-art model. Furthermore, the majority vote and concatenation ensemble were evaluated and provided the future direction of using transfer learning and segmentation to understand the specific features."}
{"title":"Deep Phenotyping of Non-Alcoholic Fatty Liver Disease Patients with Genetic Factors for Insights into the Complex Disease","authors":["Tahmina Sultana Priya","Fan Leng","Anthony C. Luehrs","Eric W. Klee","Alina M. Allen","Konstantinos N. Lazaridis","Danfeng","Yao","Shulan Tian"],"raw_abstract":"Non-alcoholic fatty liver disease (NAFLD) is a prevalent chronic liver\ndisorder characterized by the excessive accumulation of fat in the liver in\nindividuals who do not consume significant amounts of alcohol, including risk\nfactors like obesity, insulin resistance, type 2 diabetes, etc. We aim to\nidentify subgroups of NAFLD patients based on demographic, clinical, and\ngenetic characteristics for precision medicine. The genomic and phenotypic data\n(3,408 cases and 4,739 controls) for this study were gathered from participants\nin Mayo Clinic Tapestry Study (IRB#19-000001) and their electric health\nrecords, including their demographic, clinical, and comorbidity data, and the\ngenotype information through whole exome sequencing performed at Helix using\nthe Exome+$^\\circledR$ Assay according to standard procedure\n(www$.$helix$.$com). Factors highly relevant to NAFLD were determined by the\nchi-square test and stepwise backward-forward regression model. Latent class\nanalysis (LCA) was performed on NAFLD cases using significant indicator\nvariables to identify subgroups. The optimal clustering revealed 5 latent\nsubgroups from 2,013 NAFLD patients (mean age 60.6 years and 62.1% women),\nwhile a polygenic risk score based on 6 single-nucleotide polymorphism (SNP)\nvariants and disease outcomes were used to analyze the subgroups. The groups\nare characterized by metabolic syndrome, obesity, different comorbidities,\npsychoneurological factors, and genetic factors. Odds ratios were utilized to\ncompare the risk of complex diseases, such as fibrosis, cirrhosis, and\nhepatocellular carcinoma (HCC), as well as liver failure between the clusters.\nCluster 2 has a significantly higher complex disease outcome compared to other\nclusters. Keywords: Fatty liver disease; Polygenic risk score; Precision\nmedicine; Deep phenotyping; NAFLD comorbidities; Latent class analysis.","publication_date":1699903872,"paper_link":"http://arxiv.org/pdf/2311.08428v1","categories":["Quantitative Biology"],"abstract":"Non-alcoholic fatty liver disease (NAFLD) is a prevalent chronic liver disorder characterized by the excessive accumulation of fat in the liver in individuals who do not consume significant amounts of alcohol, including risk factors like obesity, insulin resistance, type 2 diabetes, etc. We aim to identify subgroups of NAFLD patients based on demographic, clinical, and genetic characteristics for precision medicine. The genomic and phenotypic data (3,408 cases and 4,739 controls) for this study were gathered from participants in Mayo Clinic Tapestry Study (IRB#19-000001) and their electric health records, including their demographic, clinical, and comorbidity data, and the genotype information through whole exome sequencing performed at Helix using the Exome+__FORMULA__ Assay according to standard procedure (www__FORMULA__helix__FORMULA__com). Factors highly relevant to NAFLD were determined by the chi-square test and stepwise backward-forward regression model. Latent class analysis (LCA) was performed on NAFLD cases using significant indicator variables to identify subgroups. The optimal clustering revealed 5 latent subgroups from 2,013 NAFLD patients (mean age 60.6 years and 62.1% women), while a polygenic risk score based on 6 single-nucleotide polymorphism (SNP) variants and disease outcomes were used to analyze the subgroups. The groups are characterized by metabolic syndrome, obesity, different comorbidities, psychoneurological factors, and genetic factors. Odds ratios were utilized to compare the risk of complex diseases, such as fibrosis, cirrhosis, and hepatocellular carcinoma (HCC), as well as liver failure between the clusters. Cluster 2 has a significantly higher complex disease outcome compared to other clusters. Keywords: Fatty liver disease; Polygenic risk score; Precision medicine; Deep phenotyping; NAFLD comorbidities; Latent class analysis."}
{"title":"GPT-4V(ision) as A Social Media Analysis Engine","authors":["Hanjia Lyu","Jinfa Huang","Daoan Zhang","Yongsheng Yu","Xinyi Mou","Jinsheng Pan","Zhengyuan Yang","Zhongyu Wei","Jiebo Luo"],"raw_abstract":"Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.","publication_date":1699900610,"paper_link":"http://arxiv.org/pdf/2311.07547v1","categories":["Quantitative Biology"],"abstract":"Recent research has offered insights into the extraordinary capabilities of Large Multimodal Models (LMMs) in various general vision and language tasks. There is growing interest in how LMMs perform in more specialized domains. Social media content, inherently multimodal, blends text, images, videos, and sometimes audio. Understanding social multimedia content remains a challenging problem for contemporary machine learning frameworks. In this paper, we explore GPT-4V(ision)'s capabilities for social multimedia analysis. We select five representative tasks, including sentiment analysis, hate speech detection, fake news identification, demographic inference, and political ideology detection, to evaluate GPT-4V. Our investigation begins with a preliminary quantitative analysis for each task using existing benchmark datasets, followed by a careful review of the results and a selection of qualitative samples that illustrate GPT-4V's potential in understanding multimodal social media content. GPT-4V demonstrates remarkable efficacy in these tasks, showcasing strengths such as joint understanding of image-text pairs, contextual and cultural awareness, and extensive commonsense knowledge. Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context of evolving celebrity and politician knowledge, reflecting the known hallucination problem. The insights gleaned from our findings underscore a promising future for LMMs in enhancing our comprehension of social media content and its users through the analysis of multimodal information."}
{"title":"Non-affinity of liquid networks and bicontinuous mesophases","authors":["Michael S. Dimitriyev","Xueyan Feng","Edwin L. Thomas","Gregory M. Grason"],"raw_abstract":"Amphiphiles self-assemble into a variety of bicontinuous mesophases whose\nequilibrium structures take the form of high-symmetry cubic networks. Here, we\nshow that the symmetry-breaking distortions in these systems give rise to\nanomalously large, non-affine collective deformations, which we argue to be a\ngeneric consequence of mass equilibration within deformed networks. We propose\nand study a minimal liquid network model of bicontinuous networks, in which\nacubic distortions are modeled by the relaxation of residually-stressed\nmechanical networks with constant-tension bonds. We show that non-affinity is\nstrongly dependent on the valency of the network as well as the degree of\nstrain-softening/stiffening force in the bonds. Taking diblock copolymer melts\nas a model system, liquid network theory captures quantitative features of two\nbicontinuous phases based on comparison with self-consistent field theory\npredictions and direct experimental characterization of acubic distortions,\nwhich are likely to be pronounced in soft amphiphilic systems more generally.","publication_date":1699900514,"paper_link":"http://arxiv.org/pdf/2311.07544v1","categories":["Physics"],"abstract":"Amphiphiles self-assemble into a variety of bicontinuous mesophases whose equilibrium structures take the form of high-symmetry cubic networks. Here, we show that the symmetry-breaking distortions in these systems give rise to anomalously large, non-affine collective deformations, which we argue to be a generic consequence of mass equilibration within deformed networks. We propose and study a minimal liquid network model of bicontinuous networks, in which acubic distortions are modeled by the relaxation of residually-stressed mechanical networks with constant-tension bonds. We show that non-affinity is strongly dependent on the valency of the network as well as the degree of strain-softening/stiffening force in the bonds. Taking diblock copolymer melts as a model system, liquid network theory captures quantitative features of two bicontinuous phases based on comparison with self-consistent field theory predictions and direct experimental characterization of acubic distortions, which are likely to be pronounced in soft amphiphilic systems more generally."}
{"title":"How Contentious Terms About People and Cultures are Used in Linked Open Data","authors":["Andrei Nesterov","Laura Hollink","Jacco van Ossenbruggen"],"raw_abstract":"Web resources in linked open data (LOD) are comprehensible to humans through\nliteral textual values attached to them, such as labels, notes, or comments.\nWord choices in literals may not always be neutral. When outdated and\nculturally stereotyping terminology is used in literals, they may appear as\noffensive to users in interfaces and propagate stereotypes to algorithms\ntrained on them. We study how frequently and in which literals contentious\nterms about people and cultures occur in LOD and whether there are attempts to\nmark the usage of such terms. For our analysis, we reuse English and Dutch\nterms from a knowledge graph that provides opinions of experts from the\ncultural heritage domain about terms' contentiousness. We inspect occurrences\nof these terms in four widely used datasets: Wikidata, The Getty Art &\nArchitecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms\nare ambiguous and contentious only in particular senses. Applying word sense\ndisambiguation, we generate a set of literals relevant to our analysis. We\nfound that outdated, derogatory, stereotyping terms frequently appear in\ndescriptive and labelling literals, such as preferred labels that are usually\ndisplayed in interfaces and used for indexing. In some cases, LOD contributors\nmark contentious terms with words and phrases in literals (implicit markers) or\nproperties linked to resources (explicit markers). However, such marking is\nrare and non-consistent in all datasets. Our quantitative and qualitative\ninsights could be helpful in developing more systematic approaches to address\nthe propagation of stereotypes via LOD.","publication_date":1699899920,"paper_link":"http://arxiv.org/pdf/2311.10757v1","categories":["Quantitative Biology"],"abstract":"Web resources in linked open data (LOD) are comprehensible to humans through literal textual values attached to them, such as labels, notes, or comments. Word choices in literals may not always be neutral. When outdated and culturally stereotyping terminology is used in literals, they may appear as offensive to users in interfaces and propagate stereotypes to algorithms trained on them. We study how frequently and in which literals contentious terms about people and cultures occur in LOD and whether there are attempts to mark the usage of such terms. For our analysis, we reuse English and Dutch terms from a knowledge graph that provides opinions of experts from the cultural heritage domain about terms' contentiousness. We inspect occurrences of these terms in four widely used datasets: Wikidata, The Getty Art & Architecture Thesaurus, Princeton WordNet, and Open Dutch WordNet. Some terms are ambiguous and contentious only in particular senses. Applying word sense disambiguation, we generate a set of literals relevant to our analysis. We found that outdated, derogatory, stereotyping terms frequently appear in descriptive and labelling literals, such as preferred labels that are usually displayed in interfaces and used for indexing. In some cases, LOD contributors mark contentious terms with words and phrases in literals (implicit markers) or properties linked to resources (explicit markers). However, such marking is rare and non-consistent in all datasets. Our quantitative and qualitative insights could be helpful in developing more systematic approaches to address the propagation of stereotypes via LOD."}
{"title":"Biaxial nematic order in fundamental measure theory","authors":["Anouar El Moumane","Michael te Vrugt","Hartmut L\u00f6wen","Ren\u00e9 Wittmann"],"raw_abstract":"Liquid crystals consisting of biaxial particles can exhibit a much richer\nphase behavior than their uniaxial counterparts. Usually, one has to rely on\nsimulation results to understand the phase diagram of these systems, since very\nfew analytical results exist. In this work, we apply fundamental measure\ntheory, which allows to derive free energy functionals for hard particles from\nfirst principles and with quantitative accuracy, to systems of hard cylinders,\ncones and spherotriangles. We provide a general recipe for incorporating\nbiaxial liquid crystal order parameters into fundamental measure theory and use\nthis framework to obtain the phase boundaries for the emergence of\norientational order in the considered systems. Our results provide insights\ninto the phase behavior of biaxial nematic liquid crystals, and in particular\ninto methods for their analytical investigation.","publication_date":1699897328,"paper_link":"http://arxiv.org/pdf/2311.07500v1","categories":["Physics"],"abstract":"Liquid crystals consisting of biaxial particles can exhibit a much richer phase behavior than their uniaxial counterparts. Usually, one has to rely on simulation results to understand the phase diagram of these systems, since very few analytical results exist. In this work, we apply fundamental measure theory, which allows to derive free energy functionals for hard particles from first principles and with quantitative accuracy, to systems of hard cylinders, cones and spherotriangles. We provide a general recipe for incorporating biaxial liquid crystal order parameters into fundamental measure theory and use this framework to obtain the phase boundaries for the emergence of orientational order in the considered systems. Our results provide insights into the phase behavior of biaxial nematic liquid crystals, and in particular into methods for their analytical investigation."}
{"title":"A Guide to Evaluating the Experience of Media and Arts Technology","authors":["Nick Bryan-Kinns","Courtney N. Reed"],"raw_abstract":"Evaluation is essential to understanding the value that digital creativity\nbrings to people's experience, for example in terms of their enjoyment,\ncreativity, and engagement. There is a substantial body of research on how to\ndesign and evaluate interactive arts and digital creativity applications. There\nis also extensive Human-Computer Interaction (HCI) literature on how to\nevaluate user interfaces and user experiences. However, it can be difficult for\nartists, practitioners, and researchers to navigate such a broad and disparate\ncollection of materials when considering how to evaluate technology they create\nthat is at the intersection of art and interaction. This chapter provides a\nguide to designing robust user studies of creative applications at the\nintersection of art, technology and interaction, which we refer to as Media and\nArts Technology (MAT). We break MAT studies down into two main kinds:\nproof-of-concept and comparative studies. As MAT studies are exploratory in\nnature, their evaluation requires the collection and analysis of both\nqualitative data such as free text questionnaire responses, interviews, and\nobservations, and also quantitative data such as questionnaires, number of\ninteractions, and length of time spent interacting. This chapter draws on over\n15 years of experience of designing and evaluating novel interactive systems to\nprovide a concrete template on how to structure a study to evaluate MATs that\nis both rigorous and repeatable, and how to report study results that are\npublishable and accessible to a wide readership in art and science communities\nalike.","publication_date":1699896481,"paper_link":"http://arxiv.org/pdf/2311.07490v1","categories":["Quantitative Biology"],"abstract":"Evaluation is essential to understanding the value that digital creativity brings to people's experience, for example in terms of their enjoyment, creativity, and engagement. There is a substantial body of research on how to design and evaluate interactive arts and digital creativity applications. There is also extensive Human-Computer Interaction (HCI) literature on how to evaluate user interfaces and user experiences. However, it can be difficult for artists, practitioners, and researchers to navigate such a broad and disparate collection of materials when considering how to evaluate technology they create that is at the intersection of art and interaction. This chapter provides a guide to designing robust user studies of creative applications at the intersection of art, technology and interaction, which we refer to as Media and Arts Technology (MAT). We break MAT studies down into two main kinds: proof-of-concept and comparative studies. As MAT studies are exploratory in nature, their evaluation requires the collection and analysis of both qualitative data such as free text questionnaire responses, interviews, and observations, and also quantitative data such as questionnaires, number of interactions, and length of time spent interacting. This chapter draws on over 15 years of experience of designing and evaluating novel interactive systems to provide a concrete template on how to structure a study to evaluate MATs that is both rigorous and repeatable, and how to report study results that are publishable and accessible to a wide readership in art and science communities alike."}
{"title":"Finding and Editing Multi-Modal Neurons in Pre-Trained Transformer","authors":["Haowen Pan","Yixin Cao","Xiaozhi Wang","Xun Yang"],"raw_abstract":"Multi-modal large language models (LLM) have achieved powerful capabilities\nfor visual semantic understanding in recent years. However, little is known\nabout how LLMs comprehend visual information and interpret different modalities\nof features. In this paper, we propose a new method for identifying multi-modal\nneurons in transformer-based multi-modal LLMs. Through a series of experiments,\nWe highlight three critical properties of multi-modal neurons by four\nwell-designed quantitative evaluation metrics. Furthermore, we introduce a\nknowledge editing method based on the identified multi-modal neurons, for\nmodifying a specific token to another designative token. We hope our findings\ncan inspire further explanatory researches on understanding mechanisms of\nmulti-modal LLMs.","publication_date":1699894982,"paper_link":"http://arxiv.org/pdf/2311.07470v1","categories":["Quantitative Biology"],"abstract":"Multi-modal large language models (LLM) have achieved powerful capabilities for visual semantic understanding in recent years. However, little is known about how LLMs comprehend visual information and interpret different modalities of features. In this paper, we propose a new method for identifying multi-modal neurons in transformer-based multi-modal LLMs. Through a series of experiments, We highlight three critical properties of multi-modal neurons by four well-designed quantitative evaluation metrics. Furthermore, we introduce a knowledge editing method based on the identified multi-modal neurons, for modifying a specific token to another designative token. We hope our findings can inspire further explanatory researches on understanding mechanisms of multi-modal LLMs."}
{"title":"A Bayesian Approach to Strong Lens Finding in the Era of Wide-area Surveys","authors":["Philip Holloway","Philip J. Marshall","Aprajita Verma","Anupreeta More","Raoul Ca\u00f1ameras","Anton T. Jaelani","Yuichiro Ishida","Kenneth C. Wong"],"raw_abstract":"The arrival of the Vera C. Rubin Observatory's Legacy Survey of Space and\nTime (LSST), Euclid-Wide and Roman wide area sensitive surveys will herald a\nnew era in strong lens science in which the number of strong lenses known is\nexpected to rise from $\\mathcal{O}(10^3)$ to $\\mathcal{O}(10^5)$. However,\ncurrent lens-finding methods still require time-consuming follow-up visual\ninspection by strong-lens experts to remove false positives which is only set\nto increase with these surveys. In this work we demonstrate a range of methods\nto produce calibrated probabilities to help determine the veracity of any given\nlens candidate. To do this we use the classifications from citizen science and\nmultiple neural networks for galaxies selected from the Hyper Suprime-Cam (HSC)\nsurvey. Our methodology is not restricted to particular classifier types and\ncould be applied to any strong lens classifier which produces quantitative\nscores. Using these calibrated probabilities, we generate an ensemble\nclassifier, combining citizen science and neural network lens finders. We find\nsuch an ensemble can provide improved classification over the individual\nclassifiers. We find a false positive rate of $10^{-3}$ can be achieved with a\ncompleteness of $46\\%$, compared to $34\\%$ for the best individual classifier.\nGiven the large number of galaxy-galaxy strong lenses anticipated in LSST, such\nimprovement would still produce significant numbers of false positives, in\nwhich case using calibrated probabilities will be essential for population\nanalysis of large populations of lenses.","publication_date":1699893599,"paper_link":"http://arxiv.org/pdf/2311.07455v1","categories":["Physics"],"abstract":"The arrival of the Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST), Euclid-Wide and Roman wide area sensitive surveys will herald a new era in strong lens science in which the number of strong lenses known is expected to rise from __FORMULA__ to __FORMULA__. However, current lens-finding methods still require time-consuming follow-up visual inspection by strong-lens experts to remove false positives which is only set to increase with these surveys. In this work we demonstrate a range of methods to produce calibrated probabilities to help determine the veracity of any given lens candidate. To do this we use the classifications from citizen science and multiple neural networks for galaxies selected from the Hyper Suprime-Cam (HSC) survey. Our methodology is not restricted to particular classifier types and could be applied to any strong lens classifier which produces quantitative scores. Using these calibrated probabilities, we generate an ensemble classifier, combining citizen science and neural network lens finders. We find such an ensemble can provide improved classification over the individual classifiers. We find a false positive rate of __FORMULA__ can be achieved with a completeness of __FORMULA__, compared to __FORMULA__ for the best individual classifier. Given the large number of galaxy-galaxy strong lenses anticipated in LSST, such improvement would still produce significant numbers of false positives, in which case using calibrated probabilities will be essential for population analysis of large populations of lenses."}
{"title":"Supersampling of Data from Structured-light Scanner with Deep Learning","authors":["Martin Melicher\u010d\u00edk","Luk\u00e1\u0161 Gajdo\u0161ech","Viktor Kocur","Martin Madaras"],"raw_abstract":"This paper focuses on increasing the resolution of depth maps obtained from\n3D cameras using structured light technology. Two deep learning models FDSR and\nDKN are modified to work with high-resolution data, and data pre-processing\ntechniques are implemented for stable training. The models are trained on our\ncustom dataset of 1200 3D scans. The resulting high-resolution depth maps are\nevaluated using qualitative and quantitative metrics. The approach for depth\nmap upsampling offers benefits such as reducing the processing time of a\npipeline by first downsampling a high-resolution depth map, performing various\nprocessing steps at the lower resolution and upsampling the resulting depth map\nor increasing the resolution of a point cloud captured in lower resolution by a\ncheaper device. The experiments demonstrate that the FDSR model excels in terms\nof faster processing time, making it a suitable choice for applications where\nspeed is crucial. On the other hand, the DKN model provides results with higher\nprecision, making it more suitable for applications that prioritize accuracy.","publication_date":1699891481,"paper_link":"http://arxiv.org/pdf/2311.07432v1","categories":["Quantitative Biology"],"abstract":"This paper focuses on increasing the resolution of depth maps obtained from 3D cameras using structured light technology. Two deep learning models FDSR and DKN are modified to work with high-resolution data, and data pre-processing techniques are implemented for stable training. The models are trained on our custom dataset of 1200 3D scans. The resulting high-resolution depth maps are evaluated using qualitative and quantitative metrics. The approach for depth map upsampling offers benefits such as reducing the processing time of a pipeline by first downsampling a high-resolution depth map, performing various processing steps at the lower resolution and upsampling the resulting depth map or increasing the resolution of a point cloud captured in lower resolution by a cheaper device. The experiments demonstrate that the FDSR model excels in terms of faster processing time, making it a suitable choice for applications where speed is crucial. On the other hand, the DKN model provides results with higher precision, making it more suitable for applications that prioritize accuracy."}
{"title":"Recurrence of Nonlinear Control Systems: Entropy and Bit Rates","authors":["Hussein Sibai","Enrique Mallada"],"raw_abstract":"In this paper, we introduce the notion of recurrence entropy in the context\nof nonlinear control systems. A set is said to be ($\\tau$-)recurrent if every\ntrajectory that starts in the set returns to it (within at most $\\tau$ units of\ntime). Recurrence entropy quantifies the complexity of making a set\n$\\tau$-recurrent measured by the average rate of growth, as time increases, of\nthe number of control signals required to achieve this goal. Our analysis\nreveals that, compared to invariance, recurrence is quantitatively less\ncomplex, meaning that the recurrence entropy of a set is no larger than, and\noften strictly smaller than, the invariance entropy. Our results further offer\ninsights into the minimum data rate required for achieving recurrence. We also\npresent an algorithm for achieving recurrence asymptotically.","publication_date":1699891140,"paper_link":"http://arxiv.org/pdf/2311.07425v1","categories":["Electrical Engineering and Systems Science"],"abstract":"In this paper, we introduce the notion of recurrence entropy in the context of nonlinear control systems. A set is said to be (__FORMULA__-)recurrent if every trajectory that starts in the set returns to it (within at most __FORMULA__ units of time). Recurrence entropy quantifies the complexity of making a set __FORMULA__-recurrent measured by the average rate of growth, as time increases, of the number of control signals required to achieve this goal. Our analysis reveals that, compared to invariance, recurrence is quantitatively less complex, meaning that the recurrence entropy of a set is no larger than, and often strictly smaller than, the invariance entropy. Our results further offer insights into the minimum data rate required for achieving recurrence. We also present an algorithm for achieving recurrence asymptotically."}
{"title":"The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4","authors":["Microsoft Research AI4Science","Microsoft Azure Quantum"],"raw_abstract":"In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.","publication_date":1699885572,"paper_link":"http://arxiv.org/pdf/2311.07361v1","categories":["Quantitative Biology"],"abstract":"In recent years, groundbreaking advancements in natural language processing have culminated in the emergence of powerful large language models (LLMs), which have showcased remarkable capabilities across a vast array of domains, including the understanding, generation, and translation of natural language, and even tasks that extend beyond language processing. In this report, we delve into the performance of LLMs within the context of scientific discovery, focusing on GPT-4, the state-of-the-art language model. Our investigation spans a diverse range of scientific areas encompassing drug discovery, biology, computational chemistry (density functional theory (DFT) and molecular dynamics (MD)), materials design, and partial differential equations (PDE). Evaluating GPT-4 on scientific tasks is crucial for uncovering its potential across various research domains, validating its domain-specific expertise, accelerating scientific progress, optimizing resource allocation, guiding future model development, and fostering interdisciplinary research. Our exploration methodology primarily consists of expert-driven case assessments, which offer qualitative insights into the model's comprehension of intricate scientific concepts and relationships, and occasionally benchmark testing, which quantitatively evaluates the model's capacity to solve well-defined domain-specific problems. Our preliminary exploration indicates that GPT-4 exhibits promising potential for a variety of scientific applications, demonstrating its aptitude for handling complex problem-solving and knowledge integration tasks. Broadly speaking, we evaluate GPT-4's knowledge base, scientific understanding, scientific numerical calculation abilities, and various scientific prediction capabilities."}
{"title":"Deformable Groupwise Registration Using a Locally Low-Rank Dissimilarity Metric for Myocardial Strain Estimation from Cardiac Cine MRI Images","authors":["Haiyang Chen","Juan Gao","Chenxi Hu"],"raw_abstract":"Objective: Cardiovascular magnetic resonance-feature tracking (CMR-FT)\nrepresents a group of methods for myocardial strain estimation from cardiac\ncine MRI images. Established CMR-FT methods are mainly based on optical flow or\npairwise registration. However, these methods suffer from either inaccurate\nestimation of large motion or drift effect caused by accumulative tracking\nerrors. In this work, we propose a deformable groupwise registration method\nusing a locally low-rank (LLR) dissimilarity metric for CMR-FT. Methods: The\nproposed method (Groupwise-LLR) tracks the feature points by a groupwise\nregistration-based two-step strategy. Unlike the globally low-rank (GLR)\ndissimilarity metric, the proposed LLR metric imposes low-rankness on local\nimage patches rather than the whole image. We quantitatively compared\nGroupwise-LLR with the Farneback optical flow, a pairwise registration method,\nand a GLR-based groupwise registration method on simulated and in vivo\ndatasets. Results: Results from the simulated dataset showed that Groupwise-LLR\nachieved more accurate tracking and strain estimation compared with the other\nmethods. Results from the in vivo dataset showed that Groupwise-LLR achieved\nmore accurate tracking and elimination of the drift effect in late-diastole.\nInter-observer reproducibility of strain estimates was similar between all\nstudied methods. Conclusion: The proposed method estimates myocardial strains\nmore accurately due to the application of a groupwise registration-based\ntracking strategy and an LLR-based dissimilarity metric. Significance: The\nproposed CMR-FT method may facilitate more accurate estimation of myocardial\nstrains, especially in diastole, for clinical assessments of cardiac\ndysfunction.","publication_date":1699884404,"paper_link":"http://arxiv.org/pdf/2311.07348v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Objective: Cardiovascular magnetic resonance-feature tracking (CMR-FT) represents a group of methods for myocardial strain estimation from cardiac cine MRI images. Established CMR-FT methods are mainly based on optical flow or pairwise registration. However, these methods suffer from either inaccurate estimation of large motion or drift effect caused by accumulative tracking errors. In this work, we propose a deformable groupwise registration method using a locally low-rank (LLR) dissimilarity metric for CMR-FT. Methods: The proposed method (Groupwise-LLR) tracks the feature points by a groupwise registration-based two-step strategy. Unlike the globally low-rank (GLR) dissimilarity metric, the proposed LLR metric imposes low-rankness on local image patches rather than the whole image. We quantitatively compared Groupwise-LLR with the Farneback optical flow, a pairwise registration method, and a GLR-based groupwise registration method on simulated and in vivo datasets. Results: Results from the simulated dataset showed that Groupwise-LLR achieved more accurate tracking and strain estimation compared with the other methods. Results from the in vivo dataset showed that Groupwise-LLR achieved more accurate tracking and elimination of the drift effect in late-diastole. Inter-observer reproducibility of strain estimates was similar between all studied methods. Conclusion: The proposed method estimates myocardial strains more accurately due to the application of a groupwise registration-based tracking strategy and an LLR-based dissimilarity metric. Significance: The proposed CMR-FT method may facilitate more accurate estimation of myocardial strains, especially in diastole, for clinical assessments of cardiac dysfunction."}
{"title":"On the mathematical replication of the MacKay effect from redundant stimulation","authors":["Cyprien Tamekue","Dario Prandi","Yacine Chitour"],"raw_abstract":"In this study, we investigate the intricate connection between visual\nperception and the mathematical modelling of neural activity in the primary\nvisual cortex (V1), focusing on replicating the MacKay effect [Mackay, Nature\n1957]. While bifurcation theory has been a prominent mathematical approach for\naddressing issues in neuroscience, especially in describing spontaneous pattern\nformations in V1 due to parameter changes, it faces challenges in scenarios\nwith localised sensory inputs. This is evident, for instance, in Mackay's\npsychophysical experiments, where the redundancy of visual stimuli information\nresults in irregular shapes, making bifurcation theory and multi-scale analysis\nless effective. To address this, we follow a mathematical viewpoint based on\nthe input-output controllability of an Amari-type neural fields model. This\nframework views the sensory input as a control function, cortical\nrepresentation via the retino-cortical map of the visual stimulus that captures\nthe distinct features of the stimulus, specifically the central redundancy in\nMacKay's funnel pattern ``MacKay rays''. From a control theory point of view,\nthe exact controllability property of the Amari-type equation is discussed both\nfor linear and nonlinear response functions. Then, applied to the MacKay effect\nreplication, we adjust the parameter representing intra-neuron connectivity to\nensure that, in the absence of sensory input, cortical activity exponentially\nstabilises to the stationary state that we perform quantitative and qualitative\nstudies to show that it captures all the essential features of the induced\nafter-image reported by MacKay","publication_date":1699883300,"paper_link":"http://arxiv.org/pdf/2311.07338v1","categories":["Mathematics","Quantitative Biology"],"abstract":"In this study, we investigate the intricate connection between visual perception and the mathematical modelling of neural activity in the primary visual cortex (V1), focusing on replicating the MacKay effect [Mackay, Nature 1957]. While bifurcation theory has been a prominent mathematical approach for addressing issues in neuroscience, especially in describing spontaneous pattern formations in V1 due to parameter changes, it faces challenges in scenarios with localised sensory inputs. This is evident, for instance, in Mackay's psychophysical experiments, where the redundancy of visual stimuli information results in irregular shapes, making bifurcation theory and multi-scale analysis less effective. To address this, we follow a mathematical viewpoint based on the input-output controllability of an Amari-type neural fields model. This framework views the sensory input as a control function, cortical representation via the retino-cortical map of the visual stimulus that captures the distinct features of the stimulus, specifically the central redundancy in MacKay's funnel pattern ``MacKay rays''. From a control theory point of view, the exact controllability property of the Amari-type equation is discussed both for linear and nonlinear response functions. Then, applied to the MacKay effect replication, we adjust the parameter representing intra-neuron connectivity to ensure that, in the absence of sensory input, cortical activity exponentially stabilises to the stationary state that we perform quantitative and qualitative studies to show that it captures all the essential features of the induced after-image reported by MacKay"}
{"title":"Multi Sentence Description of Complex Manipulation Action Videos","authors":["Fatemeh Ziaeetabar","Reza Safabakhsh","Saeedeh Momtazi","Minija Tamosiunaite","Florentin W\u00f6rg\u00f6tter"],"raw_abstract":"Automatic video description requires the generation of natural language\nstatements about the actions, events, and objects in the video. An important\nhuman trait, when we describe a video, is that we are able to do this with\nvariable levels of detail. Different from this, existing approaches for\nautomatic video descriptions are mostly focused on single sentence generation\nat a fixed level of detail. Instead, here we address video description of\nmanipulation actions where different levels of detail are required for being\nable to convey information about the hierarchical structure of these actions\nrelevant also for modern approaches of robot learning. We propose one hybrid\nstatistical and one end-to-end framework to address this problem. The hybrid\nmethod needs much less data for training, because it models statistically\nuncertainties within the video clips, while in the end-to-end method, which is\nmore data-heavy, we are directly connecting the visual encoder to the language\ndecoder without any intermediate (statistical) processing step. Both frameworks\nuse LSTM stacks to allow for different levels of description granularity and\nvideos can be described by simple single-sentences or complex multiple-sentence\ndescriptions. In addition, quantitative results demonstrate that these methods\nproduce more realistic descriptions than other competing approaches.","publication_date":1699878426,"paper_link":"http://arxiv.org/pdf/2311.07285v1","categories":["Quantitative Biology"],"abstract":"Automatic video description requires the generation of natural language statements about the actions, events, and objects in the video. An important human trait, when we describe a video, is that we are able to do this with variable levels of detail. Different from this, existing approaches for automatic video descriptions are mostly focused on single sentence generation at a fixed level of detail. Instead, here we address video description of manipulation actions where different levels of detail are required for being able to convey information about the hierarchical structure of these actions relevant also for modern approaches of robot learning. We propose one hybrid statistical and one end-to-end framework to address this problem. The hybrid method needs much less data for training, because it models statistically uncertainties within the video clips, while in the end-to-end method, which is more data-heavy, we are directly connecting the visual encoder to the language decoder without any intermediate (statistical) processing step. Both frameworks use LSTM stacks to allow for different levels of description granularity and videos can be described by simple single-sentences or complex multiple-sentence descriptions. In addition, quantitative results demonstrate that these methods produce more realistic descriptions than other competing approaches."}
{"title":"A micro-mechanics based extension of the GTN continuum model accounting for random void distributions","authors":["I. Holte","K. L. Nielsen","E. Mart\u00ednez-Pa\u00f1eda","C. F. Niordson"],"raw_abstract":"Randomness in the void distribution within a ductile metal complicates\nquantitative modeling of damage following the void growth to coalescence\nfailure process. Though the sequence of micro-mechanisms leading to ductile\nfailure is known from unit cell models, often based on assumptions of a regular\ndistribution of voids, the effect of randomness remains a challenge. In the\npresent work, mesoscale unit cell models, each containing an ensemble of four\nvoids of equal size that are randomly distributed, are used to find statistical\neffects on the yield surface of the homogenized material. A yield locus is\nfound based on a mean yield surface and a standard deviation of yield points\nobtained from 15 realizations of the four-void unit cells. It is found that the\nclassical GTN model very closely agrees with the mean of the yield points\nextracted from the unit cell calculations with random void distributions, while\nthe standard deviation $\\textbf{S}$ varies with the imposed stress state. It is\nshown that the standard deviation is nearly zero for stress triaxialities\n$T\\leq1/3$, while it rapidly increases for triaxialities above $T\\approx 1$,\nreaching maximum values of about $\\textbf{S}/\\sigma_0\\approx0.1$ at $T \\approx\n4$. At even higher triaxialities it decreases slightly. The results indicate\nthat the dependence of the standard deviation on the stress state follows from\nvariations in the deformation mechanism since a well-correlated variation is\nfound for the volume fraction of the unit cell that deforms plastically at\nyield. Thus, the random void distribution activates different complex\nlocalization mechanisms at high stress triaxialities that differ from the\nligament thinning mechanism forming the basis for the classical GTN model. A\nmethod for introducing the effect of randomness into the GTN continuum model is\npresented, and an excellent comparison to the unit cell yield locus is\nachieved.","publication_date":1699872970,"paper_link":"http://arxiv.org/pdf/2311.07236v1","categories":["Physics"],"abstract":"Randomness in the void distribution within a ductile metal complicates quantitative modeling of damage following the void growth to coalescence failure process. Though the sequence of micro-mechanisms leading to ductile failure is known from unit cell models, often based on assumptions of a regular distribution of voids, the effect of randomness remains a challenge. In the present work, mesoscale unit cell models, each containing an ensemble of four voids of equal size that are randomly distributed, are used to find statistical effects on the yield surface of the homogenized material. A yield locus is found based on a mean yield surface and a standard deviation of yield points obtained from 15 realizations of the four-void unit cells. It is found that the classical GTN model very closely agrees with the mean of the yield points extracted from the unit cell calculations with random void distributions, while the standard deviation __FORMULA__ varies with the imposed stress state. It is shown that the standard deviation is nearly zero for stress triaxialities __FORMULA__, while it rapidly increases for triaxialities above __FORMULA__, reaching maximum values of about __FORMULA__ at __FORMULA__. At even higher triaxialities it decreases slightly. The results indicate that the dependence of the standard deviation on the stress state follows from variations in the deformation mechanism since a well-correlated variation is found for the volume fraction of the unit cell that deforms plastically at yield. Thus, the random void distribution activates different complex localization mechanisms at high stress triaxialities that differ from the ligament thinning mechanism forming the basis for the classical GTN model. A method for introducing the effect of randomness into the GTN continuum model is presented, and an excellent comparison to the unit cell yield locus is achieved."}
{"title":"Observational Detection of Higher Order Secular Perturbations in Tight Hierarchical Triple Stars","authors":["Tam\u00e1s Borkovits","Tibor Mitnyan"],"raw_abstract":"In this work, we search for observational evidence of higher-order secular\nperturbations in three eclipsing binaries. These are slightly eccentric\nbinaries, and they form the inner pairs of tight, compact, hierarchical triple\nstar systems. We analyze simultaneously the high precision satellite ($Kepler$\nand $TESS$) light curves, eclipse timing variations, combined spectral energy\ndistributions (through catalog passband magnitudes) and, where available,\nradial velocities of KICs 9714358, 5771589 and TIC 219885468. Besides the\ndetermination of robust astrophysical and dynamical properties of the three\nsystems, we find evidence that the observed unusual eclipse timing variations\nof KIC 9714358 are a direct consequence of the octupole-order secular\neccentricity perturbations forced by an unusual, resonant behaviour between the\nlines of the apsides of the inner and outer orbital ellipses. We also show\nthat, despite its evident cyclic eclipse depth variations, KIC~5771589 is an\nalmost perfectly coplanar system (to within $0.3^\\circ$), and we explain the\nrapid eclipse depth variations with the grazing nature of the eclipses.\nFinally, we find that the inner pair of TIC~219885468 consists of two twin\nstars and, hence, in this triple there are no octupole order three-body\nperturbations. Moreover, we show that this triple is also coplanar on the same\nlevel as the former one, but due to its deep eclipses, it does not exhibit\neclipse depth variations. We intend to follow this work up with further\nanalyses and a quantitative comparison of the theoretical and the observed\nperturbations.","publication_date":1699872767,"paper_link":"http://arxiv.org/pdf/2311.07232v1","categories":["Physics"],"abstract":"In this work, we search for observational evidence of higher-order secular perturbations in three eclipsing binaries. These are slightly eccentric binaries, and they form the inner pairs of tight, compact, hierarchical triple star systems. We analyze simultaneously the high precision satellite (__FORMULA__ and __FORMULA__) light curves, eclipse timing variations, combined spectral energy distributions (through catalog passband magnitudes) and, where available, radial velocities of KICs 9714358, 5771589 and TIC 219885468. Besides the determination of robust astrophysical and dynamical properties of the three systems, we find evidence that the observed unusual eclipse timing variations of KIC 9714358 are a direct consequence of the octupole-order secular eccentricity perturbations forced by an unusual, resonant behaviour between the lines of the apsides of the inner and outer orbital ellipses. We also show that, despite its evident cyclic eclipse depth variations, KIC~5771589 is an almost perfectly coplanar system (to within __FORMULA__), and we explain the rapid eclipse depth variations with the grazing nature of the eclipses. Finally, we find that the inner pair of TIC~219885468 consists of two twin stars and, hence, in this triple there are no octupole order three-body perturbations. Moreover, we show that this triple is also coplanar on the same level as the former one, but due to its deep eclipses, it does not exhibit eclipse depth variations. We intend to follow this work up with further analyses and a quantitative comparison of the theoretical and the observed perturbations."}
{"title":"Error Analysis of Option Pricing via Deep PDE Solvers: Empirical Study","authors":["Rawin Assabumrungrat","Kentaro Minami","Masanori Hirano"],"raw_abstract":"Option pricing, a fundamental problem in finance, often requires solving\nnon-linear partial differential equations (PDEs). When dealing with multi-asset\noptions, such as rainbow options, these PDEs become high-dimensional, leading\nto challenges posed by the curse of dimensionality. While deep learning-based\nPDE solvers have recently emerged as scalable solutions to this\nhigh-dimensional problem, their empirical and quantitative accuracy remains not\nwell-understood, hindering their real-world applicability. In this study, we\naimed to offer actionable insights into the utility of Deep PDE solvers for\npractical option pricing implementation. Through comparative experiments, we\nassessed the empirical performance of these solvers in high-dimensional\ncontexts. Our investigation identified three primary sources of errors in Deep\nPDE solvers: (i) errors inherent in the specifications of the target option and\nunderlying assets, (ii) errors originating from the asset model simulation\nmethods, and (iii) errors stemming from the neural network training. Through\nablation studies, we evaluated the individual impact of each error source. Our\nresults indicate that the Deep BSDE method (DBSDE) is superior in performance\nand exhibits robustness against variations in option specifications. In\ncontrast, some other methods are overly sensitive to option specifications,\nsuch as time to expiration. We also find that the performance of these methods\nimproves inversely proportional to the square root of batch size and the number\nof time steps. This observation can aid in estimating computational resources\nfor achieving desired accuracies with Deep PDE solvers.","publication_date":1699872764,"paper_link":"http://arxiv.org/pdf/2311.07231v1","categories":["Mathematics","Quantitative Finance"],"abstract":"Option pricing, a fundamental problem in finance, often requires solving non-linear partial differential equations (PDEs). When dealing with multi-asset options, such as rainbow options, these PDEs become high-dimensional, leading to challenges posed by the curse of dimensionality. While deep learning-based PDE solvers have recently emerged as scalable solutions to this high-dimensional problem, their empirical and quantitative accuracy remains not well-understood, hindering their real-world applicability. In this study, we aimed to offer actionable insights into the utility of Deep PDE solvers for practical option pricing implementation. Through comparative experiments, we assessed the empirical performance of these solvers in high-dimensional contexts. Our investigation identified three primary sources of errors in Deep PDE solvers: (i) errors inherent in the specifications of the target option and underlying assets, (ii) errors originating from the asset model simulation methods, and (iii) errors stemming from the neural network training. Through ablation studies, we evaluated the individual impact of each error source. Our results indicate that the Deep BSDE method (DBSDE) is superior in performance and exhibits robustness against variations in option specifications. In contrast, some other methods are overly sensitive to option specifications, such as time to expiration. We also find that the performance of these methods improves inversely proportional to the square root of batch size and the number of time steps. This observation can aid in estimating computational resources for achieving desired accuracies with Deep PDE solvers."}
{"title":"Effect of Wearing a New Prophylactic Orthosis on Postural Balance","authors":["Julien Romain","Ahlem Arfaoui","William Bertucci"],"raw_abstract":"Purpose: The purpose of this study is to evaluate the effect of an innovative\nprophylactic knee orthosis on postural balance. This prophylactic knee orthosis\nis designed with a compression that is oriented in a chosen direction. The\npurpose of this compression is to improve stability in dynamic situations.\nOrthoses are used to provide functional improvements to knee problems. However,\nmore scientific validation is needed for this type of product. Methods: 20\nsportsmen in team sports performed a functional test: the Y-Balance Test. This\nreliable and reproducible test allows to evaluate the postural balance of the\nlower limb. The subjects were tested in 3 conditions: prophylactic orthosis\nwith innovative compression, control orthosis (with no compression) and without\northosis. The average of the three trials were collected in each direction and\ncondition. Results: The prophylactic orthosis had a better standardized score\nin the anterior direction (p<0.05) and a better composite score (p<0.05) than\nthe control orthosis (no compression). However, there were no differences in\nthe normalized score in the other directions. There were no significant\ndifferences between the prophylactic orthosis and without orthosis.\nConclusion:Wearing the prophylactic orthosis improves postural balance compared\nto a orthosis with no compression. But there is no difference between the\nprophylactic orthosis and without orthosis on postural balance.","publication_date":1699865306,"paper_link":"http://arxiv.org/pdf/2311.07628v1","categories":["Quantitative Biology"],"abstract":"Purpose: The purpose of this study is to evaluate the effect of an innovative prophylactic knee orthosis on postural balance. This prophylactic knee orthosis is designed with a compression that is oriented in a chosen direction. The purpose of this compression is to improve stability in dynamic situations. Orthoses are used to provide functional improvements to knee problems. However, more scientific validation is needed for this type of product. Methods: 20 sportsmen in team sports performed a functional test: the Y-Balance Test. This reliable and reproducible test allows to evaluate the postural balance of the lower limb. The subjects were tested in 3 conditions: prophylactic orthosis with innovative compression, control orthosis (with no compression) and without orthosis. The average of the three trials were collected in each direction and condition. Results: The prophylactic orthosis had a better standardized score in the anterior direction (p<0.05) and a better composite score (p<0.05) than the control orthosis (no compression). However, there were no differences in the normalized score in the other directions. There were no significant differences between the prophylactic orthosis and without orthosis. Conclusion:Wearing the prophylactic orthosis improves postural balance compared to a orthosis with no compression. But there is no difference between the prophylactic orthosis and without orthosis on postural balance."}
{"title":"Disordered hyperuniformity signals functioning and resilience of self-organized vegetation patterns","authors":["Wensi Hu","Quan-Xing Liu","Bo Wang","Nuo Xu","Lijuan Cui","Chi Xu"],"raw_abstract":"In harsh environments, organisms may self-organize into spatially patterned\nsystems in various ways. So far, studies of ecosystem spatial self-organization\nhave primarily focused on apparent orders reflected by regular patterns.\nHowever, self-organized ecosystems may also have cryptic orders that can be\nunveiled only through certain quantitative analyses. Here we show that\ndisordered hyperuniformity as a striking class of hidden orders can exist in\nspatially self-organized vegetation landscapes. By analyzing the\nhigh-resolution remotely sensed images across the American drylands, we\ndemonstrate that it is not uncommon to find disordered hyperuniform vegetation\nstates characterized by suppressed density fluctuations at long range. Such\nlong-range hyperuniformity has been documented in a wide range of microscopic\nsystems. Our finding contributes to expanding this domain to accommodate\nnatural landscape ecological systems. We use theoretical modeling to propose\nthat disordered hyperuniform vegetation patterning can arise from three\ngeneralized mechanisms prevalent in dryland ecosystems, including (1) critical\nabsorbing states driven by an ecological legacy effect, (2) scale-dependent\nfeedbacks driven by plant-plant facilitation and competition, and (3)\ndensity-dependent aggregation driven by plant-sediment feedbacks. Our modeling\nresults also show that disordered hyperuniform patterns can help ecosystems\ncope with arid conditions with enhanced functioning of soil moisture\nacquisition. However, this advantage may come at the cost of slower recovery of\necosystem structure upon perturbations. Our work highlights that disordered\nhyperuniformity as a distinguishable but underexplored ecosystem\nself-organization state merits systematic studies to better understand its\nunderlying mechanisms, functioning, and resilience.","publication_date":1699862589,"paper_link":"http://arxiv.org/pdf/2311.07624v1","categories":["Quantitative Biology","Statistics"],"abstract":"In harsh environments, organisms may self-organize into spatially patterned systems in various ways. So far, studies of ecosystem spatial self-organization have primarily focused on apparent orders reflected by regular patterns. However, self-organized ecosystems may also have cryptic orders that can be unveiled only through certain quantitative analyses. Here we show that disordered hyperuniformity as a striking class of hidden orders can exist in spatially self-organized vegetation landscapes. By analyzing the high-resolution remotely sensed images across the American drylands, we demonstrate that it is not uncommon to find disordered hyperuniform vegetation states characterized by suppressed density fluctuations at long range. Such long-range hyperuniformity has been documented in a wide range of microscopic systems. Our finding contributes to expanding this domain to accommodate natural landscape ecological systems. We use theoretical modeling to propose that disordered hyperuniform vegetation patterning can arise from three generalized mechanisms prevalent in dryland ecosystems, including (1) critical absorbing states driven by an ecological legacy effect, (2) scale-dependent feedbacks driven by plant-plant facilitation and competition, and (3) density-dependent aggregation driven by plant-sediment feedbacks. Our modeling results also show that disordered hyperuniform patterns can help ecosystems cope with arid conditions with enhanced functioning of soil moisture acquisition. However, this advantage may come at the cost of slower recovery of ecosystem structure upon perturbations. Our work highlights that disordered hyperuniformity as a distinguishable but underexplored ecosystem self-organization state merits systematic studies to better understand its underlying mechanisms, functioning, and resilience."}
{"title":"Revisit to the yield ratio of triton and $^3$He as an indicator of neutron-rich neck emission","authors":["Yijie Wang","Mengting Wan","Xinyue Diao","Sheng Xiao","Yuhao Qin","Zhi Qin","Dong Guo","Dawei Si","Boyuan Zhang","Baiting Tian","Fenhai Guan","Qianghua Wu","Xianglun Wei","Herun Yang","Peng Ma","Rongjiang Hu","Limin Duan","Fangfang Duan","Junbing Ma","Shiwei Xu","Qiang Hu","Zhen Bai","Yanyun Yang","Jiansong Wang","Wenbo Liu","Wanqing Su","Xiaobao Wei","Chunwang Ma","Xinxiang Li","Hongwei Wang","Yingxun Zhang","Micha\u0142 Warda","Arthur Dobrowolski","Bo\u017cena Nerlo-Pomorska","Krzysztof Pomorski","Li Ou","Zhigang Xiao"],"raw_abstract":"The neutron rich neck zone created in heavy ion reaction is experimentally\nprobed by the production of the $A=3$ isobars. The energy spectra and angular\ndistributions of triton and $^3$He are measured with the CSHINE detector in\n$^{86}$Kr +$^{208}$Pb reactions at 25 MeV/u. While the energy spectrum of\n$^{3}$He is harder than that of triton, known as \"$^{3}$He-puzzle\", the yield\nratio $R({\\rm t/^3He})$ presents a robust rising trend with the polar angle in\nlaboratory. Using the fission fragments to reconstruct the fission plane, the\nenhancement of out-plane $R({\\rm t/^3He})$ is confirmed in comparison to the\nin-plane ratios. Transport model simulations reproduce qualitatively the\nexperimental trends, but the quantitative agreement is not achieved. The\nresults demonstrate that a neutron rich neck zone is formed in the reactions.\nFurther studies are called for to understand the clustering and the isospin\ndynamics related to neck formation.","publication_date":1699854486,"paper_link":"http://arxiv.org/pdf/2311.07095v1","categories":["Physics"],"abstract":"The neutron rich neck zone created in heavy ion reaction is experimentally probed by the production of the __FORMULA__ isobars. The energy spectra and angular distributions of triton and __FORMULA__He are measured with the CSHINE detector in __FORMULA__Kr +__FORMULA__Pb reactions at 25 MeV/u. While the energy spectrum of __FORMULA__He is harder than that of triton, known as \"__FORMULA__He-puzzle\", the yield ratio __FORMULA__ presents a robust rising trend with the polar angle in laboratory. Using the fission fragments to reconstruct the fission plane, the enhancement of out-plane __FORMULA__ is confirmed in comparison to the in-plane ratios. Transport model simulations reproduce qualitatively the experimental trends, but the quantitative agreement is not achieved. The results demonstrate that a neutron rich neck zone is formed in the reactions. Further studies are called for to understand the clustering and the isospin dynamics related to neck formation."}
{"title":"Technostress and Job Performance: Understanding the Negative Impacts and Strategic Responses in the Workplace","authors":["Armita Atrian","Saleh Ghobbeh"],"raw_abstract":"This study delves into the increasingly pertinent issue of technostress in\nthe workplace and its multifaceted impact on job performance. Technostress,\nemerging from the rapid integration of technology in professional settings, is\nidentified as a significant stressor affecting employees across various\nindustries. The research primarily focuses on the ways in which technostress\ninfluences job performance, both negatively and positively, depending on the\ncontext and individual coping mechanisms. Through a blend of qualitative and\nquantitative methodologies, including surveys and in-depth interviews, the\nstudy examines the experiences of employees from diverse sectors. It highlights\nhow technostress manifests in different forms: from anxiety and frustration due\nto constant connectivity to the pressure of adapting to new technologies. The\npaper also explores the dual role of technology as both a facilitator and a\nhindrance in the workplace.\n  Significant findings indicate that technostress adversely impacts job\nperformance, leading to decreased productivity, diminished job satisfaction,\nand increased turnover intentions. However, the study also uncovers that\nstrategic interventions, such as training programs, supportive leadership, and\nfostering a positive technological culture, can mitigate these negative\neffects. These interventions not only help in managing technostress but also in\nharnessing the potential of technology for enhanced job performance.\n  Furthermore, the research proposes a model outlining the relationship between\ntechnostress, coping mechanisms, and job performance. This model serves as a\nframework for organizations to understand and address the challenges posed by\ntechnostress. The study concludes with recommendations for future research,\nparticularly in exploring the long-term effects of technostress and the\nefficacy of various coping strategies.","publication_date":1699850296,"paper_link":"http://arxiv.org/pdf/2311.07072v1","categories":["Economics","Quantitative Finance"],"abstract":"This study delves into the increasingly pertinent issue of technostress in the workplace and its multifaceted impact on job performance. Technostress, emerging from the rapid integration of technology in professional settings, is identified as a significant stressor affecting employees across various industries. The research primarily focuses on the ways in which technostress influences job performance, both negatively and positively, depending on the context and individual coping mechanisms. Through a blend of qualitative and quantitative methodologies, including surveys and in-depth interviews, the study examines the experiences of employees from diverse sectors. It highlights how technostress manifests in different forms: from anxiety and frustration due to constant connectivity to the pressure of adapting to new technologies. The paper also explores the dual role of technology as both a facilitator and a hindrance in the workplace.   Significant findings indicate that technostress adversely impacts job performance, leading to decreased productivity, diminished job satisfaction, and increased turnover intentions. However, the study also uncovers that strategic interventions, such as training programs, supportive leadership, and fostering a positive technological culture, can mitigate these negative effects. These interventions not only help in managing technostress but also in harnessing the potential of technology for enhanced job performance.   Furthermore, the research proposes a model outlining the relationship between technostress, coping mechanisms, and job performance. This model serves as a framework for organizations to understand and address the challenges posed by technostress. The study concludes with recommendations for future research, particularly in exploring the long-term effects of technostress and the efficacy of various coping strategies."}
{"title":"Advancements in Enhancing Resilience of Electrical Distribution Systems: A Review on Frameworks, Metrics, and Technological Innovations","authors":["Divyanshi Dwivedi","Sagar Babu Mitikiri","K. Victor Sam Moses Babu","Pradeep Kumar Yemula","Vedantham Lakshmi Srininvas","Pratyush Chakraborty","Mayukha Pal"],"raw_abstract":"This comprehensive review paper explores power system resilience, emphasizing\nits evolution, comparison with reliability, and conducting a thorough analysis\nof the definition and characteristics of resilience. The paper presents the\nresilience frameworks and the application of quantitative power system\nresilience metrics to assess and quantify resilience. Additionally, it\ninvestigates the relevance of complex network theory in the context of power\nsystem resilience. An integral part of this review involves examining the\nincorporation of data-driven techniques in enhancing power system resilience.\nThis includes the role of data-driven methods in enhancing power system\nresilience and predictive analytics. Further, the paper explores the recent\ntechniques employed for resilience enhancement, which includes planning and\noperational techniques. Also, a detailed explanation of microgrid (MG)\ndeployment, renewable energy integration, and peer-to-peer (P2P) energy trading\nin fortifying power systems against disruptions is provided. An analysis of\nexisting research gaps and challenges is discussed for future directions toward\nimprovements in power system resilience. Thus, a comprehensive understanding of\npower system resilience is provided, which helps in improving the ability of\ndistribution systems to withstand and recover from extreme events and\ndisruptions.","publication_date":1699846421,"paper_link":"http://arxiv.org/pdf/2311.07050v1","categories":["Quantitative Biology"],"abstract":"This comprehensive review paper explores power system resilience, emphasizing its evolution, comparison with reliability, and conducting a thorough analysis of the definition and characteristics of resilience. The paper presents the resilience frameworks and the application of quantitative power system resilience metrics to assess and quantify resilience. Additionally, it investigates the relevance of complex network theory in the context of power system resilience. An integral part of this review involves examining the incorporation of data-driven techniques in enhancing power system resilience. This includes the role of data-driven methods in enhancing power system resilience and predictive analytics. Further, the paper explores the recent techniques employed for resilience enhancement, which includes planning and operational techniques. Also, a detailed explanation of microgrid (MG) deployment, renewable energy integration, and peer-to-peer (P2P) energy trading in fortifying power systems against disruptions is provided. An analysis of existing research gaps and challenges is discussed for future directions toward improvements in power system resilience. Thus, a comprehensive understanding of power system resilience is provided, which helps in improving the ability of distribution systems to withstand and recover from extreme events and disruptions."}
{"title":"Context-dependent Instruction Tuning for Dialogue Response Generation","authors":["Jin Myung Kwak","Minseon Kim","Sung Ju Hwang"],"raw_abstract":"Recent language models have achieved impressive performance in natural\nlanguage tasks by incorporating instructions with task input during\nfine-tuning. Since all samples in the same natural language task can be\nexplained with the same task instructions, many instruction datasets only\nprovide a few instructions for the entire task, without considering the input\nof each example in the task. However, this approach becomes ineffective in\ncomplex multi-turn dialogue generation tasks, where the input varies highly\nwith each turn as the dialogue context changes, so that simple task\ninstructions cannot improve the generation performance. To address this\nlimitation, we introduce a context-based instruction fine-tuning framework for\neach multi-turn dialogue which generates both responses and instructions based\non the previous context as input. During the evaluation, the model generates\ninstructions based on the previous context to self-guide the response. The\nproposed framework produces comparable or even outstanding results compared to\nthe baselines by aligning instructions to the input during fine-tuning with the\ninstructions in quantitative evaluations on dialogue benchmark datasets with\nreduced computation budget.","publication_date":1699838730,"paper_link":"http://arxiv.org/pdf/2311.07006v1","categories":["Quantitative Biology"],"abstract":"Recent language models have achieved impressive performance in natural language tasks by incorporating instructions with task input during fine-tuning. Since all samples in the same natural language task can be explained with the same task instructions, many instruction datasets only provide a few instructions for the entire task, without considering the input of each example in the task. However, this approach becomes ineffective in complex multi-turn dialogue generation tasks, where the input varies highly with each turn as the dialogue context changes, so that simple task instructions cannot improve the generation performance. To address this limitation, we introduce a context-based instruction fine-tuning framework for each multi-turn dialogue which generates both responses and instructions based on the previous context as input. During the evaluation, the model generates instructions based on the previous context to self-guide the response. The proposed framework produces comparable or even outstanding results compared to the baselines by aligning instructions to the input during fine-tuning with the instructions in quantitative evaluations on dialogue benchmark datasets with reduced computation budget."}
{"title":"Shot noise-mitigated secondary electron imaging with ion count-aided microscopy","authors":["Akshay Agarwal","Leila Kasaei","Xinglin He","Ruangrawee Kitichotkul","Oguz Kagan Hitit","Minxu Peng","J. Albert Schultz","Leonard C. Feldman","Vivek K Goyal"],"raw_abstract":"Modern science is dependent on imaging on the nanoscale, often achieved\nthrough processes that detect secondary electrons created by a highly focused\nincident charged particle beam. Scanning electron microscopy is employed in\napplications such as critical-dimension metrology and inspection for\nsemiconductor devices, materials characterization in geology, and examination\nof biological samples. With its applicability to non-conducting materials (not\nrequiring sample coating before imaging), helium ion microscopy (HIM) is\nespecially useful in the high-resolution imaging of biological samples such as\nanimal organs, tumor cells, and viruses. However, multiple types of measurement\nnoise limit the ultimate trade-off between image quality and the incident\nparticle dose, which can preclude useful imaging of dose-sensitive samples.\nExisting methods to improve image quality do not fundamentally mitigate the\nnoise sources. Furthermore, barriers to assigning a physically meaningful scale\nmake these modalities qualitative. Here we introduce ion count-aided microscopy\n(ICAM), which is a quantitative imaging technique that uses statistically\nprincipled estimation of the secondary electron yield. With a readily\nimplemented change in data collection, ICAM nearly eliminates the influence of\nsource shot noise -- the random variation in the number of incident ions in a\nfixed time duration. In HIM, we demonstrate 3x dose reduction; based on a good\nmatch between these empirical results and theoretical performance predictions,\nthe dose reduction factor is larger when the secondary electron yield is\nhigher. ICAM thus facilitates imaging of fragile samples and may make imaging\nwith heavier particles more attractive.","publication_date":1699837667,"paper_link":"http://arxiv.org/pdf/2311.07003v1","categories":["Physics"],"abstract":"Modern science is dependent on imaging on the nanoscale, often achieved through processes that detect secondary electrons created by a highly focused incident charged particle beam. Scanning electron microscopy is employed in applications such as critical-dimension metrology and inspection for semiconductor devices, materials characterization in geology, and examination of biological samples. With its applicability to non-conducting materials (not requiring sample coating before imaging), helium ion microscopy (HIM) is especially useful in the high-resolution imaging of biological samples such as animal organs, tumor cells, and viruses. However, multiple types of measurement noise limit the ultimate trade-off between image quality and the incident particle dose, which can preclude useful imaging of dose-sensitive samples. Existing methods to improve image quality do not fundamentally mitigate the noise sources. Furthermore, barriers to assigning a physically meaningful scale make these modalities qualitative. Here we introduce ion count-aided microscopy (ICAM), which is a quantitative imaging technique that uses statistically principled estimation of the secondary electron yield. With a readily implemented change in data collection, ICAM nearly eliminates the influence of source shot noise -- the random variation in the number of incident ions in a fixed time duration. In HIM, we demonstrate 3x dose reduction; based on a good match between these empirical results and theoretical performance predictions, the dose reduction factor is larger when the secondary electron yield is higher. ICAM thus facilitates imaging of fragile samples and may make imaging with heavier particles more attractive."}
{"title":"Virtual Photons Shed Light on the Early Temperature of Dense QCD Matter","authors":["Jessica Churchill","Lipei Du","Charles Gale","Greg Jackson","Sangyong Jeon"],"raw_abstract":"Dileptons produced during heavy-ion collisions represent a unique probe of\nthe QCD phase diagram, and convey information about the state of the strongly\ninteracting system at the moment their preceding off-shell photon is created.\nIn this study, we compute thermal dilepton yields from Au+Au collisions\nperformed at different beam energies, employing a (3+1)-dimensional dynamic\nframework combined with emission rates accurate at next-to-leading order in\nperturbation theory and which include baryon chemical potential dependencies.\nBy comparing the effective temperature extracted from the thermal dilepton\ninvariant mass spectrum with the average temperature of the fluid, we offer a\nrobust quantitative validation of dileptons as effective probe of the early\nquark-gluon plasma stage.","publication_date":1699820998,"paper_link":"http://arxiv.org/pdf/2311.06951v1","categories":["Physics"],"abstract":"Dileptons produced during heavy-ion collisions represent a unique probe of the QCD phase diagram, and convey information about the state of the strongly interacting system at the moment their preceding off-shell photon is created. In this study, we compute thermal dilepton yields from Au+Au collisions performed at different beam energies, employing a (3+1)-dimensional dynamic framework combined with emission rates accurate at next-to-leading order in perturbation theory and which include baryon chemical potential dependencies. By comparing the effective temperature extracted from the thermal dilepton invariant mass spectrum with the average temperature of the fluid, we offer a robust quantitative validation of dileptons as effective probe of the early quark-gluon plasma stage."}
{"title":"Programmatic Strategy Synthesis: Resolving Nondeterminism in Probabilistic Programs","authors":["Kevin Batz","Tom Jannik Biskup","Joost-Pieter Katoen","Tobias Winkler"],"raw_abstract":"We consider imperative programs that involve both randomization and pure\nnondeterminism. The central question is how to find a strategy resolving the\npure nondeterminism such that the so-obtained determinized program satisfies a\ngiven quantitative specification, i.e., bounds on expected outcomes such as the\nexpected final value of a program variable or the probability to terminate in a\ngiven set of states. We show how memoryless and deterministic (MD) strategies\ncan be obtained in a semi-automatic fashion using deductive verification\ntechniques. For loop-free programs, the MD strategies resulting from our\nweakest precondition-style framework are correct by construction. This extends\nto loopy programs, provided the loops are equipped with suitable loop\ninvariants - just like in program verification. We show how our technique\nrelates to the well-studied problem of obtaining strategies in countably\ninfinite Markov decision processes with reachability-reward objectives.\nFinally, we apply our technique to several case studies.","publication_date":1699806394,"paper_link":"http://arxiv.org/pdf/2311.06889v2","categories":["Quantitative Biology"],"abstract":"We consider imperative programs that involve both randomization and pure nondeterminism. The central question is how to find a strategy resolving the pure nondeterminism such that the so-obtained determinized program satisfies a given quantitative specification, i.e., bounds on expected outcomes such as the expected final value of a program variable or the probability to terminate in a given set of states. We show how memoryless and deterministic (MD) strategies can be obtained in a semi-automatic fashion using deductive verification techniques. For loop-free programs, the MD strategies resulting from our weakest precondition-style framework are correct by construction. This extends to loopy programs, provided the loops are equipped with suitable loop invariants - just like in program verification. We show how our technique relates to the well-studied problem of obtaining strategies in countably infinite Markov decision processes with reachability-reward objectives. Finally, we apply our technique to several case studies."}
{"title":"Coupled dynamics of steady jet flow control for flexible membrane wings","authors":["Guojun Li","Rajeev Kumar Jaiman","Hongzhong Liu"],"raw_abstract":"We present a steady jet flow-based flow control of flexible membrane wings\nfor an adaptive and efficient motion of bat-inspired drones in complex flight\nenvironments. A body-fitted variational computational aeroelastic framework is\nadopted for the modeling of fluid-structure interactions. High-momentum jet\nflows are injected from the leading edge and transported to the wake flows to\nalter the aerodynamic performance and the membrane vibration. The phase\ndiagrams of the coupled fluid-membrane dynamics are constructed in the\nparameter space of the angle of attack and the jet momentum coefficient. The\ncoupled dynamical effect of active jet flow control on the membrane performance\nis systematically explored. While the results indicate that the current active\nflow control strategy performs well at low angles of attack, the effectiveness\ndegrades at high angles of attack with large flow separation. To understand the\ncoupling mechanism, the variations of the vortex patterns at different jet\nmomentum coefficients are examined by the proper orthogonal decomposition modes\nin the Eulerian view and the fluid transport process is studied by the coherent\nflow structures in the Lagrange description. Two scaling relations that\nquantitatively connect the membrane deformation with the aerodynamic loads\npresented in our previous work are verified even when active jet flow control\nis applied. A unifying feedback loop that reveals the fluid-membrane coupling\nmechanism is proposed. This feedback loop provides useful guidance for\ndesigning optimal active flow control strategies and enhancing flight\ncapabilities. These findings can facilitate the development of next-generation\nbio-inspired drones that incorporate smart sensing and intelligent control.","publication_date":1699770992,"paper_link":"http://arxiv.org/pdf/2311.06751v1","categories":["Physics"],"abstract":"We present a steady jet flow-based flow control of flexible membrane wings for an adaptive and efficient motion of bat-inspired drones in complex flight environments. A body-fitted variational computational aeroelastic framework is adopted for the modeling of fluid-structure interactions. High-momentum jet flows are injected from the leading edge and transported to the wake flows to alter the aerodynamic performance and the membrane vibration. The phase diagrams of the coupled fluid-membrane dynamics are constructed in the parameter space of the angle of attack and the jet momentum coefficient. The coupled dynamical effect of active jet flow control on the membrane performance is systematically explored. While the results indicate that the current active flow control strategy performs well at low angles of attack, the effectiveness degrades at high angles of attack with large flow separation. To understand the coupling mechanism, the variations of the vortex patterns at different jet momentum coefficients are examined by the proper orthogonal decomposition modes in the Eulerian view and the fluid transport process is studied by the coherent flow structures in the Lagrange description. Two scaling relations that quantitatively connect the membrane deformation with the aerodynamic loads presented in our previous work are verified even when active jet flow control is applied. A unifying feedback loop that reveals the fluid-membrane coupling mechanism is proposed. This feedback loop provides useful guidance for designing optimal active flow control strategies and enhancing flight capabilities. These findings can facilitate the development of next-generation bio-inspired drones that incorporate smart sensing and intelligent control."}
{"title":"In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering","authors":["Sheng Liu","Lei Xing","James Zou"],"raw_abstract":"Large language models (LLMs) demonstrate emergent in-context learning\ncapabilities, where they adapt to new tasks based on example demonstrations.\nHowever, in-context learning has seen limited effectiveness in many settings,\nis difficult to quantitatively control and takes up context window space. To\novercome these limitations, we propose an alternative approach that recasts\nin-context learning as in-context vectors (ICV). Using ICV has two steps. We\nfirst use a forward pass on demonstration examples to create the in-context\nvector from the latent embedding of the LLM. This vector captures essential\ninformation about the intended task. On a new query, instead of adding\ndemonstrations to the prompt, we shift the latent states of the LLM using the\nICV. The ICV approach has several benefits: 1) it enables the LLM to more\neffectively follow the demonstration examples; 2) it's easy to control by\nadjusting the magnitude of the ICV; 3) it reduces the length of the prompt by\nremoving the in-context demonstrations; 4) ICV is computationally much more\nefficient than fine-tuning. We demonstrate that ICV achieves better performance\ncompared to standard in-context learning and fine-tuning on diverse tasks\nincluding safety, style transfer, role-playing and formatting. Moreover, we\nshow that we can flexibly teach LLM to simultaneously follow different types of\ninstructions by simple vector arithmetics on the corresponding ICVs.","publication_date":1699737584,"paper_link":"http://arxiv.org/pdf/2311.06668v2","categories":["Quantitative Biology"],"abstract":"Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3) it reduces the length of the prompt by removing the in-context demonstrations; 4) ICV is computationally much more efficient than fine-tuning. We demonstrate that ICV achieves better performance compared to standard in-context learning and fine-tuning on diverse tasks including safety, style transfer, role-playing and formatting. Moreover, we show that we can flexibly teach LLM to simultaneously follow different types of instructions by simple vector arithmetics on the corresponding ICVs."}
{"title":"A 3D Conditional Diffusion Model for Image Quality Transfer -- An Application to Low-Field MRI","authors":["Seunghoi Kim","Henry F. J. Tregidgo","Ahmed K. Eldaly","Matteo Figini","Daniel C. Alexander"],"raw_abstract":"Low-field (LF) MRI scanners (<1T) are still prevalent in settings with\nlimited resources or unreliable power supply. However, they often yield images\nwith lower spatial resolution and contrast than high-field (HF) scanners. This\nquality disparity can result in inaccurate clinician interpretations. Image\nQuality Transfer (IQT) has been developed to enhance the quality of images by\nlearning a mapping function between low and high-quality images. Existing IQT\nmodels often fail to restore high-frequency features, leading to blurry output.\nIn this paper, we propose a 3D conditional diffusion model to improve 3D\nvolumetric data, specifically LF MR images. Additionally, we incorporate a\ncross-batch mechanism into the self-attention and padding of our network,\nensuring broader contextual awareness even under small 3D patches. Experiments\non the publicly available Human Connectome Project (HCP) dataset for IQT and\nbrain parcellation demonstrate that our model outperforms existing methods both\nquantitatively and qualitatively. The code is publicly available at\n\\url{https://github.com/edshkim98/DiffusionIQT}.","publication_date":1699727456,"paper_link":"http://arxiv.org/pdf/2311.06631v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Low-field (LF) MRI scanners (<1T) are still prevalent in settings with limited resources or unreliable power supply. However, they often yield images with lower spatial resolution and contrast than high-field (HF) scanners. This quality disparity can result in inaccurate clinician interpretations. Image Quality Transfer (IQT) has been developed to enhance the quality of images by learning a mapping function between low and high-quality images. Existing IQT models often fail to restore high-frequency features, leading to blurry output. In this paper, we propose a 3D conditional diffusion model to improve 3D volumetric data, specifically LF MR images. Additionally, we incorporate a cross-batch mechanism into the self-attention and padding of our network, ensuring broader contextual awareness even under small 3D patches. Experiments on the publicly available Human Connectome Project (HCP) dataset for IQT and brain parcellation demonstrate that our model outperforms existing methods both quantitatively and qualitatively. The code is publicly available at https://github.com/edshkim98/DiffusionIQT."}
{"title":"Streamlining Energy Transition Scenarios to Key Policy Decisions","authors":["Florian Joseph Baader","Stefano Moret","Wolfram Wiesemann","Iain Staffell","Andr\u00e9 Bardow"],"raw_abstract":"Uncertainties surrounding the energy transition often lead modelers to\npresent large sets of scenarios that are challenging for policymakers to\ninterpret and act upon. An alternative approach is to define a few qualitative\nstorylines from stakeholder discussions, which can be affected by biases and\ninfeasibilities. Leveraging decision trees, a popular machine-learning\ntechnique, we derive interpretable storylines from many quantitative scenarios\nand show how the key decisions in the energy transition are interlinked.\nSpecifically, our results demonstrate that choosing a high deployment of\nrenewables and sector coupling makes global decarbonization scenarios robust\nagainst uncertainties in climate sensitivity and demand. Also, the energy\ntransition to a fossil-free Europe is primarily determined by choices on the\nroles of bioenergy, storage, and heat electrification. Our transferrable\napproach translates vast energy model results into a small set of critical\ndecisions, guiding decision-makers in prioritizing the key factors that will\nshape the energy transition.","publication_date":1699726232,"paper_link":"http://arxiv.org/pdf/2311.06625v1","categories":["Quantitative Biology"],"abstract":"Uncertainties surrounding the energy transition often lead modelers to present large sets of scenarios that are challenging for policymakers to interpret and act upon. An alternative approach is to define a few qualitative storylines from stakeholder discussions, which can be affected by biases and infeasibilities. Leveraging decision trees, a popular machine-learning technique, we derive interpretable storylines from many quantitative scenarios and show how the key decisions in the energy transition are interlinked. Specifically, our results demonstrate that choosing a high deployment of renewables and sector coupling makes global decarbonization scenarios robust against uncertainties in climate sensitivity and demand. Also, the energy transition to a fossil-free Europe is primarily determined by choices on the roles of bioenergy, storage, and heat electrification. Our transferrable approach translates vast energy model results into a small set of critical decisions, guiding decision-makers in prioritizing the key factors that will shape the energy transition."}
{"title":"Quantum Computing for Financial Mathematics","authors":["Antoine Jacquier","Oleksiy Kondratyev","Gordon Lee","Mugad Oumgari"],"raw_abstract":"Quantum computing has recently appeared in the headlines of many scientific\nand popular publications. In the context of quantitative finance, we provide\nhere an overview of its potential.","publication_date":1699724356,"paper_link":"http://arxiv.org/pdf/2311.06621v1","categories":["Quantitative Finance"],"abstract":"Quantum computing has recently appeared in the headlines of many scientific and popular publications. In the context of quantitative finance, we provide here an overview of its potential."}
{"title":"BizBench: A Quantitative Reasoning Benchmark for Business and Finance","authors":["Rik Koncel-Kedziorski","Michael Krumdick","Viet Lai","Varshini Reddy","Charles Lovering","Chris Tanner"],"raw_abstract":"As large language models (LLMs) impact a growing number of complex domains,\nit is becoming increasingly important to have fair, accurate, and rigorous\nevaluation benchmarks. Evaluating the reasoning skills required for business\nand financial NLP stands out as a particularly difficult challenge. We\nintroduce BizBench, a new benchmark for evaluating models' ability to reason\nabout realistic financial problems. BizBench comprises 8 quantitative reasoning\ntasks. Notably, BizBench targets the complex task of question-answering (QA)\nfor structured and unstructured financial data via program synthesis (i.e.,\ncode generation). We introduce three diverse financially-themed code-generation\ntasks from newly collected and augmented QA data. Additionally, we isolate\ndistinct financial reasoning capabilities required to solve these QA tasks:\nreading comprehension of financial text and tables, which is required to\nextract correct intermediate values; and understanding domain knowledge (e.g.,\nfinancial formulas) needed to calculate complex solutions. Collectively, these\ntasks evaluate a model's financial background knowledge, ability to extract\nnumeric entities from financial documents, and capacity to solve problems with\ncode. We conduct an in-depth evaluation of open-source and commercial LLMs,\nillustrating that BizBench is a challenging benchmark for quantitative\nreasoning in the finance and business domain.","publication_date":1699719371,"paper_link":"http://arxiv.org/pdf/2311.06602v1","categories":["Quantitative Biology"],"abstract":"As large language models (LLMs) impact a growing number of complex domains, it is becoming increasingly important to have fair, accurate, and rigorous evaluation benchmarks. Evaluating the reasoning skills required for business and financial NLP stands out as a particularly difficult challenge. We introduce BizBench, a new benchmark for evaluating models' ability to reason about realistic financial problems. BizBench comprises 8 quantitative reasoning tasks. Notably, BizBench targets the complex task of question-answering (QA) for structured and unstructured financial data via program synthesis (i.e., code generation). We introduce three diverse financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate distinct financial reasoning capabilities required to solve these QA tasks: reading comprehension of financial text and tables, which is required to extract correct intermediate values; and understanding domain knowledge (e.g., financial formulas) needed to calculate complex solutions. Collectively, these tasks evaluate a model's financial background knowledge, ability to extract numeric entities from financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, illustrating that BizBench is a challenging benchmark for quantitative reasoning in the finance and business domain."}
{"title":"Effects of bursty synthesis in organelle biogenesis","authors":["Binayak Banerjee","Dipjyoti Das"],"raw_abstract":"A fundamental question of cell biology is how cells control the number of\norganelles. The processes of organelle biogenesis, namely de novo synthesis,\nfission, fusion, and decay, are inherently stochastic, producing cell-to-cell\nvariability in organelle abundance. In addition, experiments suggest that the\nsynthesis of some organelles can be bursty. We thus ask how bursty synthesis\nimpacts intracellular organelle number distribution. We develop an organelle\nbiogenesis model with bursty de novo synthesis by considering geometrically\ndistributed burst sizes. We analytically solve the model in biologically\nrelevant limits and provide exact expressions for the steady-state organelle\nnumber distributions and their means and variances. We also present approximate\nsolutions for the whole model, complementing with exact stochastic simulations.\nWe show that bursts generally increase the noise in organelle numbers,\nproducing distinct signatures in noise profiles depending on different\nmechanisms of organelle biogenesis. We also find different shapes of organelle\nnumber distributions, including bimodal distributions in some parameter\nregimes. Notably, bursty synthesis broadens the parameter regime of observing\nbimodality compared to the `non-bursty' case. Together, our framework utilizes\nnumber fluctuations to elucidate the role of bursty synthesis in producing\norganelle number heterogeneity in cells.","publication_date":1699718129,"paper_link":"http://arxiv.org/pdf/2311.06598v1","categories":["Physics"],"abstract":"A fundamental question of cell biology is how cells control the number of organelles. The processes of organelle biogenesis, namely de novo synthesis, fission, fusion, and decay, are inherently stochastic, producing cell-to-cell variability in organelle abundance. In addition, experiments suggest that the synthesis of some organelles can be bursty. We thus ask how bursty synthesis impacts intracellular organelle number distribution. We develop an organelle biogenesis model with bursty de novo synthesis by considering geometrically distributed burst sizes. We analytically solve the model in biologically relevant limits and provide exact expressions for the steady-state organelle number distributions and their means and variances. We also present approximate solutions for the whole model, complementing with exact stochastic simulations. We show that bursts generally increase the noise in organelle numbers, producing distinct signatures in noise profiles depending on different mechanisms of organelle biogenesis. We also find different shapes of organelle number distributions, including bimodal distributions in some parameter regimes. Notably, bursty synthesis broadens the parameter regime of observing bimodality compared to the `non-bursty' case. Together, our framework utilizes number fluctuations to elucidate the role of bursty synthesis in producing organelle number heterogeneity in cells."}
{"title":"Swin UNETR++: Advancing Transformer-Based Dense Dose Prediction Towards Fully Automated Radiation Oncology Treatments","authors":["Kuancheng Wang","Hai Siong Tan","Rafe Mcbeth"],"raw_abstract":"The field of Radiation Oncology is uniquely positioned to benefit from the\nuse of artificial intelligence to fully automate the creation of radiation\ntreatment plans for cancer therapy. This time-consuming and specialized task\ncombines patient imaging with organ and tumor segmentation to generate a 3D\nradiation dose distribution to meet clinical treatment goals, similar to\nvoxel-level dense prediction. In this work, we propose Swin UNETR++, that\ncontains a lightweight 3D Dual Cross-Attention (DCA) module to capture the\nintra and inter-volume relationships of each patient's unique anatomy, which\nfully convolutional neural networks lack. Our model was trained, validated, and\ntested on the Open Knowledge-Based Planning dataset. In addition to metrics of\nDose Score $\\overline{S_{\\text{Dose}}}$ and DVH Score\n$\\overline{S_{\\text{DVH}}}$ that quantitatively measure the difference between\nthe predicted and ground-truth 3D radiation dose distribution, we propose the\nqualitative metrics of average volume-wise acceptance rate\n$\\overline{R_{\\text{VA}}}$ and average patient-wise clinical acceptance rate\n$\\overline{R_{\\text{PA}}}$ to assess the clinical reliability of the\npredictions. Swin UNETR++ demonstrates near-state-of-the-art performance on\nvalidation and test dataset (validation: $\\overline{S_{\\text{DVH}}}$=1.492 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.649 Gy, $\\overline{R_{\\text{VA}}}$=88.58%,\n$\\overline{R_{\\text{PA}}}$=100.0%; test: $\\overline{S_{\\text{DVH}}}$=1.634 Gy,\n$\\overline{S_{\\text{Dose}}}$=2.757 Gy, $\\overline{R_{\\text{VA}}}$=90.50%,\n$\\overline{R_{\\text{PA}}}$=98.0%), establishing a basis for future studies to\ntranslate 3D dose predictions into a deliverable treatment plan, facilitating\nfull automation.","publication_date":1699710779,"paper_link":"http://arxiv.org/pdf/2311.06572v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The field of Radiation Oncology is uniquely positioned to benefit from the use of artificial intelligence to fully automate the creation of radiation treatment plans for cancer therapy. This time-consuming and specialized task combines patient imaging with organ and tumor segmentation to generate a 3D radiation dose distribution to meet clinical treatment goals, similar to voxel-level dense prediction. In this work, we propose Swin UNETR++, that contains a lightweight 3D Dual Cross-Attention (DCA) module to capture the intra and inter-volume relationships of each patient's unique anatomy, which fully convolutional neural networks lack. Our model was trained, validated, and tested on the Open Knowledge-Based Planning dataset. In addition to metrics of Dose Score __FORMULA__ and DVH Score __FORMULA__ that quantitatively measure the difference between the predicted and ground-truth 3D radiation dose distribution, we propose the qualitative metrics of average volume-wise acceptance rate __FORMULA__ and average patient-wise clinical acceptance rate __FORMULA__ to assess the clinical reliability of the predictions. Swin UNETR++ demonstrates near-state-of-the-art performance on validation and test dataset (validation: __FORMULA__=1.492 Gy, __FORMULA__=2.649 Gy, __FORMULA__=88.58%, __FORMULA__=100.0%; test: __FORMULA__=1.634 Gy, __FORMULA__=2.757 Gy, __FORMULA__=90.50%, __FORMULA__=98.0%), establishing a basis for future studies to translate 3D dose predictions into a deliverable treatment plan, facilitating full automation."}
{"title":"Long-range $S$-wave $DD^*$ interaction in covariant chiral effective field theory","authors":["Qing-Yu Zhai","Ming-Zhu Liu","Jun-Xu Lu","Li-Sheng Geng"],"raw_abstract":"Motivated by the recent lattice QCD study of the $DD^*$ interaction at\nunphysical quark masses, we perform a theoretical study of the $DD^*$\ninteraction in covariant chiral effective field theory (ChEFT). In particular,\nwe calculate the relevant leading-order two-pion exchange contributions. The\nresults compare favorably with the lattice QCD results, supporting the\nconclusion that the intermediate-range $DD^*$ interaction is dominated by\ntwo-pion exchanges and the one-pion exchange contribution is absent. At a\nquantitative level, the covariant ChPT results agree better with the lattice\nQCD results than their non-relativistic counterparts, showing the relevance of\nrelativistic corrections in the charm sector.","publication_date":1699709787,"paper_link":"http://arxiv.org/pdf/2311.06569v1","categories":["Physics"],"abstract":"Motivated by the recent lattice QCD study of the __FORMULA__ interaction at unphysical quark masses, we perform a theoretical study of the __FORMULA__ interaction in covariant chiral effective field theory (ChEFT). In particular, we calculate the relevant leading-order two-pion exchange contributions. The results compare favorably with the lattice QCD results, supporting the conclusion that the intermediate-range __FORMULA__ interaction is dominated by two-pion exchanges and the one-pion exchange contribution is absent. At a quantitative level, the covariant ChPT results agree better with the lattice QCD results than their non-relativistic counterparts, showing the relevance of relativistic corrections in the charm sector."}
{"title":"Artificial Intelligence in Assessing Cardiovascular Diseases and Risk Factors via Retinal Fundus Images: A Review of the Last Decade","authors":["Mirsaeed Abdollahi","Ali Jafarizadeh","Amirhosein Ghafouri Asbagh","Navid Sobhi","Keysan Pourmoghtader","Siamak Pedrammehr","Houshyar Asadi","Roohallah Alizadehsani","Ru-San Tan","U. Rajendra Acharya"],"raw_abstract":"Background: Cardiovascular diseases (CVDs) continue to be the leading cause\nof mortality on a global scale. In recent years, the application of artificial\nintelligence (AI) techniques, particularly deep learning (DL), has gained\nconsiderable popularity for evaluating the various aspects of CVDs. Moreover,\nusing fundus images and optical coherence tomography angiography (OCTA) to\ndiagnose retinal diseases has been extensively studied. To better understand\nheart function and anticipate changes based on microvascular characteristics\nand function, researchers are currently exploring the integration of AI with\nnon-invasive retinal scanning. Leveraging AI-assisted early detection and\nprediction of cardiovascular diseases on a large scale holds excellent\npotential to mitigate cardiovascular events and alleviate the economic burden\non healthcare systems. Method: A comprehensive search was conducted across\nvarious databases, including PubMed, Medline, Google Scholar, Scopus, Web of\nSciences, IEEE Xplore, and ACM Digital Library, using specific keywords related\nto cardiovascular diseases and artificial intelligence. Results: A total of 87\nEnglish-language publications, selected for relevance were included in the\nstudy, and additional references were considered. This study presents an\noverview of the current advancements and challenges in employing retinal\nimaging and artificial intelligence to identify cardiovascular disorders and\nprovides insights for further exploration in this field. Conclusion:\nResearchers aim to develop precise disease prognosis patterns as the aging\npopulation and global CVD burden increase. AI and deep learning are\ntransforming healthcare, offering the potential for single retinal image-based\ndiagnosis of various CVDs, albeit with the need for accelerated adoption in\nhealthcare systems.","publication_date":1699708151,"paper_link":"http://arxiv.org/pdf/2311.07609v1","categories":["Quantitative Biology","Physics","Electrical Engineering and Systems Science"],"abstract":"Background: Cardiovascular diseases (CVDs) continue to be the leading cause of mortality on a global scale. In recent years, the application of artificial intelligence (AI) techniques, particularly deep learning (DL), has gained considerable popularity for evaluating the various aspects of CVDs. Moreover, using fundus images and optical coherence tomography angiography (OCTA) to diagnose retinal diseases has been extensively studied. To better understand heart function and anticipate changes based on microvascular characteristics and function, researchers are currently exploring the integration of AI with non-invasive retinal scanning. Leveraging AI-assisted early detection and prediction of cardiovascular diseases on a large scale holds excellent potential to mitigate cardiovascular events and alleviate the economic burden on healthcare systems. Method: A comprehensive search was conducted across various databases, including PubMed, Medline, Google Scholar, Scopus, Web of Sciences, IEEE Xplore, and ACM Digital Library, using specific keywords related to cardiovascular diseases and artificial intelligence. Results: A total of 87 English-language publications, selected for relevance were included in the study, and additional references were considered. This study presents an overview of the current advancements and challenges in employing retinal imaging and artificial intelligence to identify cardiovascular disorders and provides insights for further exploration in this field. Conclusion: Researchers aim to develop precise disease prognosis patterns as the aging population and global CVD burden increase. AI and deep learning are transforming healthcare, offering the potential for single retinal image-based diagnosis of various CVDs, albeit with the need for accelerated adoption in healthcare systems."}
{"title":"Convolve and Conquer: Data Comparison with Wiener Filters","authors":["Deborah Pelacani Cruz","George Strong","Oscar Bates","Carlos Cueto","Jiashun Yao","Lluis Guasch"],"raw_abstract":"Quantitative evaluations of differences and/or similarities between data\nsamples define and shape optimisation problems associated with learning data\ndistributions. Current methods to compare data often suffer from limitations in\ncapturing such distributions or lack desirable mathematical properties for\noptimisation (e.g. smoothness, differentiability, or convexity). In this paper,\nwe introduce a new method to measure (dis)similarities between paired samples\ninspired by Wiener-filter theory. The convolutional nature of Wiener filters\nallows us to comprehensively compare data samples in a globally correlated way.\nWe validate our approach in four machine learning applications: data\ncompression, medical imaging imputation, translated classification, and\nnon-parametric generative modelling. Our results demonstrate increased\nresolution in reconstructed images with better perceptual quality and higher\ndata fidelity, as well as robustness against translations, compared to\nconventional mean-squared-error analogue implementations.","publication_date":1699705711,"paper_link":"http://arxiv.org/pdf/2311.06558v1","categories":["Quantitative Biology"],"abstract":"Quantitative evaluations of differences and/or similarities between data samples define and shape optimisation problems associated with learning data distributions. Current methods to compare data often suffer from limitations in capturing such distributions or lack desirable mathematical properties for optimisation (e.g. smoothness, differentiability, or convexity). In this paper, we introduce a new method to measure (dis)similarities between paired samples inspired by Wiener-filter theory. The convolutional nature of Wiener filters allows us to comprehensively compare data samples in a globally correlated way. We validate our approach in four machine learning applications: data compression, medical imaging imputation, translated classification, and non-parametric generative modelling. Our results demonstrate increased resolution in reconstructed images with better perceptual quality and higher data fidelity, as well as robustness against translations, compared to conventional mean-squared-error analogue implementations."}
{"title":"Uniform-in-time boundedness in a class of local and nonlocal nonlinear attraction-repulsion chemotaxis models with logistics","authors":["Alessandro Columbu","Rafael Diaz Fuentes","Silvia Frassu"],"raw_abstract":"This article deals with a class of chemotaxis systems describing mechanisms\nfrom mathematical biology. In the specific, for a rather general class of\nattraction-repulsion models, with nonlinear productions, diffusion,\nsensitivities and logistic term, we are interested in deriving interplays on\nthe parameters involved in the problem capable to ensure globality and\nboundedness of related solutions.\n  This project is precisely contextualized in the frame of a series of results\ndealing with local and nonlocal, and linear and nonlinear, attraction-repulsion\nchemotaxis systems. For these problems, rooms of improvements are still open;\nin this sense, this research exactly enhances and extends already known\nanalyses in the literature. More specifically, a direct comparison with [Jiao,\nJadlovsk\\'a, Li, Nonlinear Anal. Real World Appl., 2023; Ren, Liu, Math. Models\nMethods Appl. Sci., 2020; Chiyo, Yokota, Z. Angew. Math. Phys., 2022; Columbu,\nFrassu, Viglialoro, Appl. Anal., 2023] is carried out.","publication_date":1699698771,"paper_link":"http://arxiv.org/pdf/2311.06526v1","categories":["Mathematics"],"abstract":"This article deals with a class of chemotaxis systems describing mechanisms from mathematical biology. In the specific, for a rather general class of attraction-repulsion models, with nonlinear productions, diffusion, sensitivities and logistic term, we are interested in deriving interplays on the parameters involved in the problem capable to ensure globality and boundedness of related solutions.   This project is precisely contextualized in the frame of a series of results dealing with local and nonlocal, and linear and nonlinear, attraction-repulsion chemotaxis systems. For these problems, rooms of improvements are still open; in this sense, this research exactly enhances and extends already known analyses in the literature. More specifically, a direct comparison with [Jiao, Jadlovsk\\'a, Li, Nonlinear Anal. Real World Appl., 2023; Ren, Liu, Math. Models Methods Appl. Sci., 2020; Chiyo, Yokota, Z. Angew. Math. Phys., 2022; Columbu, Frassu, Viglialoro, Appl. Anal., 2023] is carried out."}
{"title":"Exploring the photometric variability of ultra-cool dwarfs with TESS","authors":["Romina P. Petrucci","Yilen G\u00f3mez Maqueo Chew","Emiliano Jofr\u00e9","Ant\u00edgona Segura","Leticia V. Ferrero"],"raw_abstract":"We present a photometric characterization of 208 ultra-cool dwarfs (UCDs)\nwith spectral types between M4 and L4, from 20-second and 2-minute cadence TESS\nlight curves. We determine rotation periods for 87 objects (42 percent) and\nidentify 778 flare events in 103 UCDs (49.5 percent). For 777 flaring events\n(corresponding to 102 objects), we derive bolometric energies between 2.1e30\nand 1.1e34 erg , with 56 superflare events. No transiting planets or eclipsing\nbinaries were identified. We find that the fraction of UCDs with rotation and\nflaring activity is, at least, 20 percent higher in M4-M6 spectral types than\nin later UCDs (M7-L4). For spectral types between M4 and L0, we measure the\nslope of the flare bolometric energy-duration correlation to be gamma = 0.497\n+/- 0.058, which agrees with that found in previous studies for solar-type and\nM dwarfs. Moreover, we determine the slope of the flare frequency distribution\nto be alpha = -1.75 +/- 0.04 for M4-M5 dwarfs, alpha = -1.69 +/- 0.04 and alpha\n= -1.72 +/- 0.1 for M6-M7 and M8-L0 dwarfs, respectively, which are consistent\nwith previous works that exclusively analysed UCDs. These results support the\nidea that independently of the physical mechanisms that produce magnetic\nactivity, the characteristics of the rotational modulation and flares are\nsimilar for both fully-convective UCDs and partially-convective solar-type and\nearly-M stars. Based on the measured UCD flare distributions, we find that UV\nradiation emitted from flares does not have the potential to start prebiotic\nchemistry.","publication_date":1700679179,"paper_link":"http://arxiv.org/pdf/2311.13591v1","categories":["Physics"],"abstract":"We present a photometric characterization of 208 ultra-cool dwarfs (UCDs) with spectral types between M4 and L4, from 20-second and 2-minute cadence TESS light curves. We determine rotation periods for 87 objects (42 percent) and identify 778 flare events in 103 UCDs (49.5 percent). For 777 flaring events (corresponding to 102 objects), we derive bolometric energies between 2.1e30 and 1.1e34 erg , with 56 superflare events. No transiting planets or eclipsing binaries were identified. We find that the fraction of UCDs with rotation and flaring activity is, at least, 20 percent higher in M4-M6 spectral types than in later UCDs (M7-L4). For spectral types between M4 and L0, we measure the slope of the flare bolometric energy-duration correlation to be gamma = 0.497 +/- 0.058, which agrees with that found in previous studies for solar-type and M dwarfs. Moreover, we determine the slope of the flare frequency distribution to be alpha = -1.75 +/- 0.04 for M4-M5 dwarfs, alpha = -1.69 +/- 0.04 and alpha = -1.72 +/- 0.1 for M6-M7 and M8-L0 dwarfs, respectively, which are consistent with previous works that exclusively analysed UCDs. These results support the idea that independently of the physical mechanisms that produce magnetic activity, the characteristics of the rotational modulation and flares are similar for both fully-convective UCDs and partially-convective solar-type and early-M stars. Based on the measured UCD flare distributions, we find that UV radiation emitted from flares does not have the potential to start prebiotic chemistry."}
{"title":"Nuclear Structure Effects on Hyperfine Splittings in Ordinary and Muonic Deuterium","authors":["Chen Ji","Xiang Zhang","Lucas Platter"],"raw_abstract":"Precision spectroscopy of hyperfine splitting (HFS) is a crucial tool for\ninvestigating the structure of nuclei and testing quantum electrodynamics\n(QED). However, accurate theoretical predictions are hindered by two-photon\nexchange (TPE) effects. We propose a novel formalism that accounts for nuclear\nexcitations and recoil in TPE, providing a model-independent description of TPE\neffects on HFS in light ordinary and muonic atoms. Combining our formalism with\npionless effective field theory at next-to-next-to-leading order, the predicted\nTPE effects on HFS are 41.2(2.6) kHz and 0.116(9) meV for the 1S state in\ndeuterium and the 2S state in muonic deuterium. These results are within\n1.4-1.7 standard deviation from recent measurements and highlight the\nimportance of nuclear structure effects on HFS and indicate the value of more\nprecise measurements in future experiments.","publication_date":1700678527,"paper_link":"http://arxiv.org/pdf/2311.13585v1","categories":["Physics"],"abstract":"Precision spectroscopy of hyperfine splitting (HFS) is a crucial tool for investigating the structure of nuclei and testing quantum electrodynamics (QED). However, accurate theoretical predictions are hindered by two-photon exchange (TPE) effects. We propose a novel formalism that accounts for nuclear excitations and recoil in TPE, providing a model-independent description of TPE effects on HFS in light ordinary and muonic atoms. Combining our formalism with pionless effective field theory at next-to-next-to-leading order, the predicted TPE effects on HFS are 41.2(2.6) kHz and 0.116(9) meV for the 1S state in deuterium and the 2S state in muonic deuterium. These results are within 1.4-1.7 standard deviation from recent measurements and highlight the importance of nuclear structure effects on HFS and indicate the value of more precise measurements in future experiments."}
{"title":"Nucleation phenomena and extreme vulnerability of spatial k-core systems","authors":["Leyang Xue","Shengling Gao","Lazaros K. Gallos","Orr Levy","Bnaya Gross","Zengru Di","Shlomo Havlin"],"raw_abstract":"K-core percolation is a fundamental dynamical process in complex networks\nwith applications that span numerous real-world systems. Earlier studies focus\nprimarily on random networks without spatial constraints and reveal intriguing\nmixed-order transitions. However, real-world systems, ranging from\ntransportation and communication networks to complex brain networks, are not\nrandom but are spatially embedded. Here, we study k-core percolation on\ntwo-dimensional spatially embedded networks and show that, in contrast to\nregular percolation, the length of connections can control the transition type,\nleading to four different types of phase transitions associated with novel\nphenomena and a rich phase diagram. A key finding is the existence of a\nmetastable phase in which microscopic localized damage, independent of system\nsize, can cause a macroscopic phase transition, a result which cannot be\nachieved in traditional percolation. In this case, local failures can\nspontaneously propagate the damage radially until the system entirely\ncollapses, a phenomenon analogous to the nucleation process. These findings\nsuggest novel features and extreme vulnerabilities of spatially embedded k-core\nnetwork systems, and highlight the necessity to take into account the\ncharacteristic length of links when designing robust spatial networks.\nFurthermore, our insight about the microscopic processes and their origin\nduring the mixed order and first order abrupt transitions in k-core networks\ncould shed light on the mechanisms of many systems where such transitions\noccur.","publication_date":1700678012,"paper_link":"http://arxiv.org/pdf/2311.13579v1","categories":["Physics"],"abstract":"K-core percolation is a fundamental dynamical process in complex networks with applications that span numerous real-world systems. Earlier studies focus primarily on random networks without spatial constraints and reveal intriguing mixed-order transitions. However, real-world systems, ranging from transportation and communication networks to complex brain networks, are not random but are spatially embedded. Here, we study k-core percolation on two-dimensional spatially embedded networks and show that, in contrast to regular percolation, the length of connections can control the transition type, leading to four different types of phase transitions associated with novel phenomena and a rich phase diagram. A key finding is the existence of a metastable phase in which microscopic localized damage, independent of system size, can cause a macroscopic phase transition, a result which cannot be achieved in traditional percolation. In this case, local failures can spontaneously propagate the damage radially until the system entirely collapses, a phenomenon analogous to the nucleation process. These findings suggest novel features and extreme vulnerabilities of spatially embedded k-core network systems, and highlight the necessity to take into account the characteristic length of links when designing robust spatial networks. Furthermore, our insight about the microscopic processes and their origin during the mixed order and first order abrupt transitions in k-core networks could shed light on the mechanisms of many systems where such transitions occur."}
{"title":"Hadronic Mono-$W'$ Probes of Dark Matter at Colliders","authors":["Ryan Holder","John Reddick","Matteo Cremonesi","Doug Berry","Kun Cheng","Matthew Low","Tim M. P. Tait","Daniel Whiteson"],"raw_abstract":"Particle collisions at the energy frontier can probe the nature of invisible\ndark matter via production in association with recoiling visible objects. We\npropose a new potential production mode, in which dark matter is produced by\nthe decay of a heavy dark Higgs boson radiated from a heavy $W'$ boson. In such\na model, motivated by left-right symmetric theories, dark matter would not be\npair produced in association with other recoiling objects due to its lack of\ndirect coupling to quarks or gluons. We study the hadronic decay mode via\n$W'\\rightarrow tb$ and estimate the LHC exclusion sensitivity at 95\\%\nconfidence level to be $10^2-10^5$ fb for $W'$ boson masses between 250 and\n1750 GeV.","publication_date":1700677972,"paper_link":"http://arxiv.org/pdf/2311.13578v1","categories":["Physics"],"abstract":"Particle collisions at the energy frontier can probe the nature of invisible dark matter via production in association with recoiling visible objects. We propose a new potential production mode, in which dark matter is produced by the decay of a heavy dark Higgs boson radiated from a heavy __FORMULA__ boson. In such a model, motivated by left-right symmetric theories, dark matter would not be pair produced in association with other recoiling objects due to its lack of direct coupling to quarks or gluons. We study the hadronic decay mode via __FORMULA__ and estimate the LHC exclusion sensitivity at 95\\% confidence level to be __FORMULA__ fb for __FORMULA__ boson masses between 250 and 1750 GeV."}
{"title":"Physical Reasoning and Object Planning for Household Embodied Agents","authors":["Ayush Agrawal","Raghav Prabhakar","Anirudh Goyal","Dianbo Liu"],"raw_abstract":"In this study, we explore the sophisticated domain of task planning for\nrobust household embodied agents, with a particular emphasis on the intricate\ntask of selecting substitute objects. We introduce the CommonSense Object\nAffordance Task (COAT), a novel framework designed to analyze reasoning\ncapabilities in commonsense scenarios. This approach is centered on\nunderstanding how these agents can effectively identify and utilize alternative\nobjects when executing household tasks, thereby offering insights into the\ncomplexities of practical decision-making in real-world environments.Drawing\ninspiration from human decision-making, we explore how large language models\ntackle this challenge through three meticulously crafted commonsense\nquestion-and-answer datasets, featuring refined rules and human annotations.\nOur evaluation of state-of-the-art language models on these datasets sheds\nlight on three pivotal considerations: 1) aligning an object's inherent utility\nwith the task at hand, 2) navigating contextual dependencies (societal norms,\nsafety, appropriateness, and efficiency), and 3) accounting for the current\nphysical state of the object. To maintain accessibility, we introduce five\nabstract variables reflecting an object's physical condition, modulated by\nhuman insights to simulate diverse household scenarios. Our contributions\ninclude insightful Object-Utility mappings addressing the first consideration\nand two extensive QA datasets (15k and 130k questions) probing the intricacies\nof contextual dependencies and object states. The datasets, along with our\nfindings, are accessible at: \\url{https://github.com/com-phy-affordance/COAT}.\nThis research not only advances our understanding of physical commonsense\nreasoning in language models but also paves the way for future improvements in\nhousehold agent intelligence.","publication_date":1700677923,"paper_link":"http://arxiv.org/pdf/2311.13577v1","categories":["Physics"],"abstract":"In this study, we explore the sophisticated domain of task planning for robust household embodied agents, with a particular emphasis on the intricate task of selecting substitute objects. We introduce the CommonSense Object Affordance Task (COAT), a novel framework designed to analyze reasoning capabilities in commonsense scenarios. This approach is centered on understanding how these agents can effectively identify and utilize alternative objects when executing household tasks, thereby offering insights into the complexities of practical decision-making in real-world environments.Drawing inspiration from human decision-making, we explore how large language models tackle this challenge through three meticulously crafted commonsense question-and-answer datasets, featuring refined rules and human annotations. Our evaluation of state-of-the-art language models on these datasets sheds light on three pivotal considerations: 1) aligning an object's inherent utility with the task at hand, 2) navigating contextual dependencies (societal norms, safety, appropriateness, and efficiency), and 3) accounting for the current physical state of the object. To maintain accessibility, we introduce five abstract variables reflecting an object's physical condition, modulated by human insights to simulate diverse household scenarios. Our contributions include insightful Object-Utility mappings addressing the first consideration and two extensive QA datasets (15k and 130k questions) probing the intricacies of contextual dependencies and object states. The datasets, along with our findings, are accessible at: https://github.com/com-phy-affordance/COAT. This research not only advances our understanding of physical commonsense reasoning in language models but also paves the way for future improvements in household agent intelligence."}
{"title":"Aufbau Suppressed Coupled Cluster Theory for Electronically Excited States","authors":["Harrison Tuckman","Eric Neuscamman"],"raw_abstract":"We introduce an approach to improve single-reference coupled cluster theory\nin settings where the Aufbau determinant is absent from or plays only a small\nrole in the true wave function. Using a de-excitation operator that can be\nefficiently hidden within a similarity transform, we create a coupled cluster\nwave function in which de-excitations work to suppress the Aufbau determinant\nand produce wave functions dominated by other determinants. Thanks to an\ninvertible and fully exponential form, the approach is systematically\nimprovable, size consistent, size extensive, and, interestingly, size intensive\nin a granular way that should make the adoption of some ground state techniques\nsuch as local correlation relatively straightforward. In this initial study, we\napply the general formalism to create a state-specific method for\norbital-relaxed singly excited states. We find that this approach matches the\naccuracy of similar-cost equation-of-motion methods in valence excitations\nwhile offering improved accuracy for charge transfer states. We also find the\napproach to be more accurate than excited-state-specific perturbation theory in\nboth types of states.","publication_date":1700677869,"paper_link":"http://arxiv.org/pdf/2311.13576v1","categories":["Physics"],"abstract":"We introduce an approach to improve single-reference coupled cluster theory in settings where the Aufbau determinant is absent from or plays only a small role in the true wave function. Using a de-excitation operator that can be efficiently hidden within a similarity transform, we create a coupled cluster wave function in which de-excitations work to suppress the Aufbau determinant and produce wave functions dominated by other determinants. Thanks to an invertible and fully exponential form, the approach is systematically improvable, size consistent, size extensive, and, interestingly, size intensive in a granular way that should make the adoption of some ground state techniques such as local correlation relatively straightforward. In this initial study, we apply the general formalism to create a state-specific method for orbital-relaxed singly excited states. We find that this approach matches the accuracy of similar-cost equation-of-motion methods in valence excitations while offering improved accuracy for charge transfer states. We also find the approach to be more accurate than excited-state-specific perturbation theory in both types of states."}
{"title":"Exploring Physically-Motivated Models to Fit Gamma-Ray Burst Spectra","authors":["Suraj Poolakkil","Robert Preece","Peter Veres"],"raw_abstract":"We explore fitting gamma-ray burst spectra with three physically-motivated\nmodels, and thus revisit the viability of synchrotron radiation as the primary\nsource of GRB prompt emission. We pick a sample of 100 bright GRBs observed by\nthe Fermi Gamma-ray Burst Monitor (GBM), based on their energy flux values. In\naddition to the standard empirical spectral models used in previous GBM\nspectroscopy catalogs, we also consider three physically-motivated models; (a)\na Thermal Synchrotron model, (b) a Band model with a High-energy Cutoff, and\n(c) a Smoothly Broken Power Law (SBPL) model with a Multiplicative Broken Power\nLaw (MBPL). We then adopt the Bayesian information criterion (BIC) to compare\nthe fits obtained and choose the best model. We find that 42% of the GRBs from\nthe fluence spectra and 23% of GRBs from the peak-flux spectra have one of the\nthree physically-motivated models as their preferred one. From the peak-flux\nspectral fits, we find that the low-energy index distributions from the\nempirical model fits for long GRBs peak around the synchrotron value of -2/3,\nwhile the two low-energy indices from the SBPL+MBPL fits of long GRBs peak\nclose to the -2/3 and -3/2 values expected for a synchrotron spectrum below and\nabove the cooling frequency.","publication_date":1700677394,"paper_link":"http://arxiv.org/pdf/2311.13566v1","categories":["Physics"],"abstract":"We explore fitting gamma-ray burst spectra with three physically-motivated models, and thus revisit the viability of synchrotron radiation as the primary source of GRB prompt emission. We pick a sample of 100 bright GRBs observed by the Fermi Gamma-ray Burst Monitor (GBM), based on their energy flux values. In addition to the standard empirical spectral models used in previous GBM spectroscopy catalogs, we also consider three physically-motivated models; (a) a Thermal Synchrotron model, (b) a Band model with a High-energy Cutoff, and (c) a Smoothly Broken Power Law (SBPL) model with a Multiplicative Broken Power Law (MBPL). We then adopt the Bayesian information criterion (BIC) to compare the fits obtained and choose the best model. We find that 42% of the GRBs from the fluence spectra and 23% of GRBs from the peak-flux spectra have one of the three physically-motivated models as their preferred one. From the peak-flux spectral fits, we find that the low-energy index distributions from the empirical model fits for long GRBs peak around the synchrotron value of -2/3, while the two low-energy indices from the SBPL+MBPL fits of long GRBs peak close to the -2/3 and -3/2 values expected for a synchrotron spectrum below and above the cooling frequency."}
{"title":"Horizon brightened accelerated radiation in the background of braneworld black holes","authors":["Ashmita Das","Soham Sen","Sunandan Gangopadhyay"],"raw_abstract":"The concept of horizon brightened acceleration radiation (HBAR) has brought\nto us a distinct mechanism of particle production in curved spacetime. In this\nmanuscript we examine the HBAR phenomena for a braneworld black hole (BBH)\nwhich emerges as an effective theory in our $(3+1)$ dimensional universe due to\nthe higher dimensional gravitational effects. Despite being somewhat similar to\nthe Reissner-Nordstr$\\ddot{\\rm o}$m solution in general relativity, the BBH is\nunique with respect to its charge term which is rather the tidal charge. In\nthis background, we study the transition probability of the atom due to the\natom-field interaction and the associated HBAR entropy. Both the quantities\nacquire modifications over the standard Schwarzschild results and turn out to\nbe the function of the tidal charge. This modifications appear solely due to\nthe bulk gravitational effects as induced on the 3-brane. Studying the Wien's\ndisplacement, we observe an important feature that the wavelengths of HBAR\ncorresponding to the Schwarzschild and the BBH, deviate from each other\ndepending on their masses. This deviation is found to be more pronounced for\nthe mass values slightly greater or comparable to the Planck mass.","publication_date":1700676053,"paper_link":"http://arxiv.org/pdf/2311.13557v1","categories":["Physics"],"abstract":"The concept of horizon brightened acceleration radiation (HBAR) has brought to us a distinct mechanism of particle production in curved spacetime. In this manuscript we examine the HBAR phenomena for a braneworld black hole (BBH) which emerges as an effective theory in our __FORMULA__ dimensional universe due to the higher dimensional gravitational effects. Despite being somewhat similar to the Reissner-Nordstr__FORMULA__m solution in general relativity, the BBH is unique with respect to its charge term which is rather the tidal charge. In this background, we study the transition probability of the atom due to the atom-field interaction and the associated HBAR entropy. Both the quantities acquire modifications over the standard Schwarzschild results and turn out to be the function of the tidal charge. This modifications appear solely due to the bulk gravitational effects as induced on the 3-brane. Studying the Wien's displacement, we observe an important feature that the wavelengths of HBAR corresponding to the Schwarzschild and the BBH, deviate from each other depending on their masses. This deviation is found to be more pronounced for the mass values slightly greater or comparable to the Planck mass."}
{"title":"The transient IDEMIX model as a nonorographic gravity wave parameterization in an atmospheric circulation model","authors":["B. Quinn","C. Eden","D. Olbers","G. S. Voelker","U. Achatz"],"raw_abstract":"The Internal wave Dissipation, Energy and Mixing (IDEMIX) model presents a\nnovel way of parameterizing internal gravity waves in the atmosphere. Using a\ncontinuous full wave spectrum in the energy balance equation and integrating\nover all vertical wavenumbers and frequencies results in prognostic equations\nfor the energy density of gravity waves in multiple azimuthal compartments. It\nincludes their non-dissipative interaction with the mean flow, allowing for an\nevolving and local description of momentum flux and gravity wave drag. A\nsaturation mechanism maintains the wave field within convective stability\nlimits, and an energetically consistent closure for critical-layer effects\ncontrols how much wave flux propagates from the troposphere into the middle\natmosphere. IDEMIX can simulate zonal gravity wave drag around the mesopause,\nsimilar to a traditional gravity wave parameterization and to a\nstate-of-the-art wave ray tracing model in an atmospheric circulation model. In\naddition, IDEMIX shows a reversal of the gravity wave drag around the mesopause\nregion due to changes in the momentum flux there. When compared to empirical\nmodel data, IDEMIX captures well the summer hemisphere flow reversal, the cold\nsummer mesospheric pole and the alternate positive and negative structures in\nthe meridional mean flow.","publication_date":1700675832,"paper_link":"http://arxiv.org/pdf/2311.13555v1","categories":["Physics"],"abstract":"The Internal wave Dissipation, Energy and Mixing (IDEMIX) model presents a novel way of parameterizing internal gravity waves in the atmosphere. Using a continuous full wave spectrum in the energy balance equation and integrating over all vertical wavenumbers and frequencies results in prognostic equations for the energy density of gravity waves in multiple azimuthal compartments. It includes their non-dissipative interaction with the mean flow, allowing for an evolving and local description of momentum flux and gravity wave drag. A saturation mechanism maintains the wave field within convective stability limits, and an energetically consistent closure for critical-layer effects controls how much wave flux propagates from the troposphere into the middle atmosphere. IDEMIX can simulate zonal gravity wave drag around the mesopause, similar to a traditional gravity wave parameterization and to a state-of-the-art wave ray tracing model in an atmospheric circulation model. In addition, IDEMIX shows a reversal of the gravity wave drag around the mesopause region due to changes in the momentum flux there. When compared to empirical model data, IDEMIX captures well the summer hemisphere flow reversal, the cold summer mesospheric pole and the alternate positive and negative structures in the meridional mean flow."}
{"title":"Tachyon condensation in a chromomagnetic center-vortex background","authors":["M. Bordag"],"raw_abstract":"The chromomagnetic vacuum of SU(2) gluodynamics is considered in the\nbackground of a finite radius flux tube (center-vortex) with homogeneous field\ninside and zero field outside. In this background there are tachyonic modes.\nThese modes cause an instability. It is assumed that the selfinteraction of\nthese modes stops the creation of gluons and that a condensate will be formed.\nFor constant condensates, the minimum of the effective potential is found on\nthe tree level. In the background of these condensates, all tachyonic modes\nacquire nonzero, real masses which will result in a real effective potential of\nthis system.\n  Considering only the tachyonic modes and adding the energy of the background\nfield, the total energy is found to have a minimum at some value of the\nbackground field, which depends on the coupling of the initial SU(2) model. For\nsmall coupling, this dependence is polynomial in distinction from the Savvidy\nvacuum where it is exponentially suppressed. The minimum of this energy will\ndeepens with shrinking radius of the flux tube. It can be expected that this\nprocess can be stopped by adding quantum effects. Using the high temperature\nexpansion of the effective potential, it can be expected that the symmetry,\nwhich is broken by the condensate, will be restored at sufficiently high\ntemperature.","publication_date":1700675589,"paper_link":"http://arxiv.org/pdf/2311.13553v1","categories":["Physics"],"abstract":"The chromomagnetic vacuum of SU(2) gluodynamics is considered in the background of a finite radius flux tube (center-vortex) with homogeneous field inside and zero field outside. In this background there are tachyonic modes. These modes cause an instability. It is assumed that the selfinteraction of these modes stops the creation of gluons and that a condensate will be formed. For constant condensates, the minimum of the effective potential is found on the tree level. In the background of these condensates, all tachyonic modes acquire nonzero, real masses which will result in a real effective potential of this system.   Considering only the tachyonic modes and adding the energy of the background field, the total energy is found to have a minimum at some value of the background field, which depends on the coupling of the initial SU(2) model. For small coupling, this dependence is polynomial in distinction from the Savvidy vacuum where it is exponentially suppressed. The minimum of this energy will deepens with shrinking radius of the flux tube. It can be expected that this process can be stopped by adding quantum effects. Using the high temperature expansion of the effective potential, it can be expected that the symmetry, which is broken by the condensate, will be restored at sufficiently high temperature."}
{"title":"A Unified Framework for Trace-induced Quantum Kernels","authors":["Beng Yee Gan","Daniel Leykam","Supanut Thanasilp"],"raw_abstract":"Quantum kernel methods are promising candidates for achieving a practical\nquantum advantage for certain machine learning tasks. Similar to classical\nmachine learning, an exact form of a quantum kernel is expected to have a great\nimpact on the model performance. In this work we combine all trace-induced\nquantum kernels, including the commonly-used global fidelity and local\nprojected quantum kernels, into a common framework. We show how generalized\ntrace-induced quantum kernels can be constructed as combinations of the\nfundamental building blocks we coin \"Lego\" kernels, which impose an inductive\nbias on the resulting quantum models. We relate the expressive power and\ngeneralization ability to the number of non-zero weight Lego kernels and\npropose a systematic approach to increase the complexity of a quantum kernel\nmodel, leading to a new form of the local projected kernels that require fewer\nquantum resources in terms of the number of quantum gates and measurement\nshots. We show numerically that models based on local projected kernels can\nachieve comparable performance to the global fidelity quantum kernel. Our work\nunifies existing quantum kernels and provides a systematic framework to compare\ntheir properties.","publication_date":1700675400,"paper_link":"http://arxiv.org/pdf/2311.13552v1","categories":["Statistics","Physics"],"abstract":"Quantum kernel methods are promising candidates for achieving a practical quantum advantage for certain machine learning tasks. Similar to classical machine learning, an exact form of a quantum kernel is expected to have a great impact on the model performance. In this work we combine all trace-induced quantum kernels, including the commonly-used global fidelity and local projected quantum kernels, into a common framework. We show how generalized trace-induced quantum kernels can be constructed as combinations of the fundamental building blocks we coin \"Lego\" kernels, which impose an inductive bias on the resulting quantum models. We relate the expressive power and generalization ability to the number of non-zero weight Lego kernels and propose a systematic approach to increase the complexity of a quantum kernel model, leading to a new form of the local projected kernels that require fewer quantum resources in terms of the number of quantum gates and measurement shots. We show numerically that models based on local projected kernels can achieve comparable performance to the global fidelity quantum kernel. Our work unifies existing quantum kernels and provides a systematic framework to compare their properties."}
{"title":"Blind analysis in Physics experiments: Is this trip necessary?","authors":["Robert Golub"],"raw_abstract":"Based on the work of Klein and Roodman [\\cite{JoshK}] we present an alternate\nconclusion as to the charm of blind analysis in physics experiments.","publication_date":1700674332,"paper_link":"http://arxiv.org/pdf/2311.13542v1","categories":["Physics"],"abstract":"Based on the work of Klein and Roodman [JoshK] we present an alternate conclusion as to the charm of blind analysis in physics experiments."}
{"title":"Limiting flux in quantum thermodynamics","authors":["Domingos S. P. Salazar"],"raw_abstract":"In quantum systems, entropy production is typically defined as the quantum\nrelative entropy between two states. This definition provides an upper bound\nfor any flux (of particles, energy, entropy, etc.) of bounded observables,\nwhich proves especially useful near equilibrium. However, this bound tends to\nbe irrelevant in general nonequilibrium situations. We propose a new upper\nbound for such fluxes in terms of quantum relative entropy, applicable even far\nfrom equilibrium and in the strong coupling regime. Additionally, we compare\nthis bound with Monte Carlo simulations of random qubits with coherence, as\nwell as with a model of two interacting nuclear spins.","publication_date":1700673443,"paper_link":"http://arxiv.org/pdf/2311.13536v1","categories":["Physics"],"abstract":"In quantum systems, entropy production is typically defined as the quantum relative entropy between two states. This definition provides an upper bound for any flux (of particles, energy, entropy, etc.) of bounded observables, which proves especially useful near equilibrium. However, this bound tends to be irrelevant in general nonequilibrium situations. We propose a new upper bound for such fluxes in terms of quantum relative entropy, applicable even far from equilibrium and in the strong coupling regime. Additionally, we compare this bound with Monte Carlo simulations of random qubits with coherence, as well as with a model of two interacting nuclear spins."}
{"title":"Lee-Yang and Langer edge singularities from analytic continuation of scaling functions","authors":["Frithjof Karsch","Christian Schmidt","Simran Singh"],"raw_abstract":"We discuss the analytic continuation of scaling function in the 3-dimensional\nZ(2),O(2) andO(4) universality classes using the Schofield representation of\nthe magnetic equation of state. We show that a determination of the location of\nLee-Yang edge singularities and, in the case of Z(2), also the Langer edge\nsingularity yields stable results. Results for the former are in good agreement\nwith Functional Renormalization Group calculations. We also present results for\nthe location of the Langer edge singularity in the 3-d,Z(2) universality class.\nWe find that in terms of the complex scaling variable z the distance of the\nLanger edge singularity to the critical point agrees within errors with that of\nthe Lee-Yang edge singularity. Furthermore the magnitude of the discontinuity\nalong the Langer branch cut is an order of magnitude smaller than that along\nthe Lee-Yang branch cut.","publication_date":1700672761,"paper_link":"http://arxiv.org/pdf/2311.13530v1","categories":["Physics"],"abstract":"We discuss the analytic continuation of scaling function in the 3-dimensional Z(2),O(2) andO(4) universality classes using the Schofield representation of the magnetic equation of state. We show that a determination of the location of Lee-Yang edge singularities and, in the case of Z(2), also the Langer edge singularity yields stable results. Results for the former are in good agreement with Functional Renormalization Group calculations. We also present results for the location of the Langer edge singularity in the 3-d,Z(2) universality class. We find that in terms of the complex scaling variable z the distance of the Langer edge singularity to the critical point agrees within errors with that of the Lee-Yang edge singularity. Furthermore the magnitude of the discontinuity along the Langer branch cut is an order of magnitude smaller than that along the Lee-Yang branch cut."}
{"title":"Improvements on Device Independent and Semi-Device Independent Protocols of Randomness Expansion","authors":["Rutvij Bhavsar"],"raw_abstract":"To generate genuine random numbers, random number generators based on quantum\ntheory are essential. However, ensuring that the process used to produce\nrandomness meets desired security standards can pose challenges for traditional\nquantum random number generators. This thesis delves into Device Independent\n(DI) and Semi-Device Independent (semi-DI) protocols of randomness expansion,\nbased on a minimal set of experimentally verifiable security assumptions. The\nsecurity in DI protocols relies on the violation of Bell inequalities, which\ncertify the quantum behavior of devices. The semi-DI protocols discussed in\nthis thesis require the characterization of only one device - a power meter.\nThese protocols exploit the fact that quantum states can be prepared such that\nthey cannot be distinguished with certainty, thereby creating a randomness\nresource. In this study, we introduce enhanced DI and semi-DI protocols that\nsurpass existing ones in terms of output randomness rate, security, or in some\ninstances, both. Our analysis employs the Entropy Accumulation Theorem (EAT) to\ndetermine the extractable randomness for finite rounds. A notable contribution\nis the introduction of randomness expansion protocols that recycle input\nrandomness, significantly enhancing finite round randomness rates for DI\nprotocols based on the CHSH inequality violation. In the final section of the\nthesis, we delve into Generalized Probability Theories (GPTs), with a focus on\nBoxworld, the largest GPT capable of producing correlations consistent with\nrelativity. A tractable criterion for identifying a Boxworld channel is\npresented.","publication_date":1700672584,"paper_link":"http://arxiv.org/pdf/2311.13528v1","categories":["Physics"],"abstract":"To generate genuine random numbers, random number generators based on quantum theory are essential. However, ensuring that the process used to produce randomness meets desired security standards can pose challenges for traditional quantum random number generators. This thesis delves into Device Independent (DI) and Semi-Device Independent (semi-DI) protocols of randomness expansion, based on a minimal set of experimentally verifiable security assumptions. The security in DI protocols relies on the violation of Bell inequalities, which certify the quantum behavior of devices. The semi-DI protocols discussed in this thesis require the characterization of only one device - a power meter. These protocols exploit the fact that quantum states can be prepared such that they cannot be distinguished with certainty, thereby creating a randomness resource. In this study, we introduce enhanced DI and semi-DI protocols that surpass existing ones in terms of output randomness rate, security, or in some instances, both. Our analysis employs the Entropy Accumulation Theorem (EAT) to determine the extractable randomness for finite rounds. A notable contribution is the introduction of randomness expansion protocols that recycle input randomness, significantly enhancing finite round randomness rates for DI protocols based on the CHSH inequality violation. In the final section of the thesis, we delve into Generalized Probability Theories (GPTs), with a focus on Boxworld, the largest GPT capable of producing correlations consistent with relativity. A tractable criterion for identifying a Boxworld channel is presented."}
{"title":"Imprint of massive neutrinos on Persistent Homology of large-scale structure","authors":["M. H. Jalali Kanafi","S. Ansarifard","S. M. S. Movahed"],"raw_abstract":"Exploiting the Persistent Homology technique and an associated complementary\nrepresentation which enables us to construct a synergistic pipeline for\ndifferent topological features quantified by Betti curves in reducing the\ndegeneracy between cosmological parameters, we investigate the footprint of\nsummed massive neutrinos ($M_{\\nu}$) in different density fields simulated by\nthe publicly available Quijote suite. Evolution of topological features in the\ncontext of super-level filtration on three-dimensional density fields, reveals\nremarkable indicators for constraining the $M_{\\nu}$ and $\\sigma_8$. The\nabundance of 2-holes is more sensitive to the presence of $M_{\\nu}$, also the\npersistence of topological features plays a crucial role in cosmological\ninference and reducing the degeneracy associated with $M_{\\nu}$ simulation\nrather than their birth thresholds when either the total matter density ($m$)\nfield or those part including only cold dark matter+baryons ($cb$) is utilized.\nIncorporating the Betti-1 and Betti-2 for $cb$ part of $M^+_{\\nu}$ simulation\nmarginalized over the thresholds implies $5\\%$ variation compared to the\nmassless neutrinos simulation. The constraint on $M_{\\nu}$ from $\\beta_k$ and\nits joint analysis with birth threshold and persistency of topological features\nfor total mass density field smoothed by $R=5$ Mpc h$^{-1}$ at zero redshift\nreach to $0.0172$ eV and $0.0152$ eV, at $1\\sigma$ confidence interval,\nrespectively.","publication_date":1700671691,"paper_link":"http://arxiv.org/pdf/2311.13520v1","categories":["Physics"],"abstract":"Exploiting the Persistent Homology technique and an associated complementary representation which enables us to construct a synergistic pipeline for different topological features quantified by Betti curves in reducing the degeneracy between cosmological parameters, we investigate the footprint of summed massive neutrinos (__FORMULA__) in different density fields simulated by the publicly available Quijote suite. Evolution of topological features in the context of super-level filtration on three-dimensional density fields, reveals remarkable indicators for constraining the __FORMULA__ and __FORMULA__. The abundance of 2-holes is more sensitive to the presence of __FORMULA__, also the persistence of topological features plays a crucial role in cosmological inference and reducing the degeneracy associated with __FORMULA__ simulation rather than their birth thresholds when either the total matter density (__FORMULA__) field or those part including only cold dark matter+baryons (__FORMULA__) is utilized. Incorporating the Betti-1 and Betti-2 for __FORMULA__ part of __FORMULA__ simulation marginalized over the thresholds implies __FORMULA__ variation compared to the massless neutrinos simulation. The constraint on __FORMULA__ from __FORMULA__ and its joint analysis with birth threshold and persistency of topological features for total mass density field smoothed by __FORMULA__ Mpc h__FORMULA__ at zero redshift reach to __FORMULA__ eV and __FORMULA__ eV, at __FORMULA__ confidence interval, respectively."}
{"title":"Orbital-Free Density Functional Theory with Continuous Normalizing Flows","authors":["Alexandre de Camargo","Ricky T. Q. Chen","Rodrigo A. Vargas-Hern\u00e1ndez"],"raw_abstract":"Orbital-free density functional theory (OF-DFT) provides an alternative\napproach for calculating the molecular electronic energy, relying solely on the\nelectron density. In OF-DFT, both the ground-state density is optimized\nvariationally to minimize the total energy functional while satisfying the\nnormalization constraint. In this work, we introduce a novel approach by\nparameterizing the electronic density with a normalizing flow ansatz, which is\nalso optimized by minimizing the total energy functional. Our model\nsuccessfully replicates the electronic density for a diverse range of chemical\nsystems, including a one-dimensional diatomic molecule, specifically Lithium\nhydride with varying interatomic distances, as well as comprehensive\nsimulations of hydrogen and water molecules, all conducted in Cartesian space.","publication_date":1700671379,"paper_link":"http://arxiv.org/pdf/2311.13518v1","categories":["Physics"],"abstract":"Orbital-free density functional theory (OF-DFT) provides an alternative approach for calculating the molecular electronic energy, relying solely on the electron density. In OF-DFT, both the ground-state density is optimized variationally to minimize the total energy functional while satisfying the normalization constraint. In this work, we introduce a novel approach by parameterizing the electronic density with a normalizing flow ansatz, which is also optimized by minimizing the total energy functional. Our model successfully replicates the electronic density for a diverse range of chemical systems, including a one-dimensional diatomic molecule, specifically Lithium hydride with varying interatomic distances, as well as comprehensive simulations of hydrogen and water molecules, all conducted in Cartesian space."}
{"title":"Quantum Sensing of Magnetic Fields with Molecular Spins","authors":["Claudio Bonizzoni","Alberto Ghirri","Fabio Santanni","Marco Affronte"],"raw_abstract":"Spins are prototypical systems with the potential to probe magnetic fields\ndown to the atomic scale limit. Exploiting their quantum nature through\nappropriate sensing protocols allows to enlarge their applicability to fields\nnot always accessible by classical sensors. Here we first show that quantum\nsensing protocols for AC magnetic fields can be implemented on molecular spin\nensembles embedded into hybrid quantum circuits. We then show that, using only\necho detection at microwave frequency and no optical readout, Dynamical\nDecoupling protocols synchronized with the AC magnetic fields can enhance the\nsensitivity up to $S = 10^{-10}-10^{-9}T/\\sqrt{Hz}$ with a low (4-5) number of\napplied pulses. These results paves the way for the development of strategies\nto exploit molecular spins as quantum sensors.","publication_date":1700670109,"paper_link":"http://arxiv.org/pdf/2311.13504v1","categories":["Physics"],"abstract":"Spins are prototypical systems with the potential to probe magnetic fields down to the atomic scale limit. Exploiting their quantum nature through appropriate sensing protocols allows to enlarge their applicability to fields not always accessible by classical sensors. Here we first show that quantum sensing protocols for AC magnetic fields can be implemented on molecular spin ensembles embedded into hybrid quantum circuits. We then show that, using only echo detection at microwave frequency and no optical readout, Dynamical Decoupling protocols synchronized with the AC magnetic fields can enhance the sensitivity up to __FORMULA__ with a low (4-5) number of applied pulses. These results paves the way for the development of strategies to exploit molecular spins as quantum sensors."}
{"title":"Non-Gaussian correlations in the steady-state of driven-dissipative clouds of two-level atoms","authors":["Giovanni Ferioli","Sara Pancaldi","Antoine Glicenstein","David Clement","Antoine Browaeys","Igor Ferrier-Barbut"],"raw_abstract":"We report experimental measurements of the second-order coherence function\n$g^{(2)}(\\tau)$ of the light emitted by a laser-driven dense ensemble of\n$^{87}$Rb atoms. We observe a clear departure from the Siegert relation valid\nfor Gaussian chaotic light. Measuring intensity and first-order coherence, we\nconclude that the violation is not due to the emergence of a coherent field.\nThis indicates that the light obeys non-Gaussian statistics, stemming from\nnon-Gaussian correlations in the atomic medium. More specifically, the\nsteady-state of this driven-dissipative many-body system sustains high-order\ncorrelations in the absence of first-order coherence. These findings call for\nnew theoretical and experimental explorations to uncover their origin and they\nopen new perspectives for the realization of non-Gaussian states of light.","publication_date":1700670087,"paper_link":"http://arxiv.org/pdf/2311.13503v1","categories":["Physics"],"abstract":"We report experimental measurements of the second-order coherence function __FORMULA__ of the light emitted by a laser-driven dense ensemble of __FORMULA__Rb atoms. We observe a clear departure from the Siegert relation valid for Gaussian chaotic light. Measuring intensity and first-order coherence, we conclude that the violation is not due to the emergence of a coherent field. This indicates that the light obeys non-Gaussian statistics, stemming from non-Gaussian correlations in the atomic medium. More specifically, the steady-state of this driven-dissipative many-body system sustains high-order correlations in the absence of first-order coherence. These findings call for new theoretical and experimental explorations to uncover their origin and they open new perspectives for the realization of non-Gaussian states of light."}
{"title":"Optimal optomechanical cavity setups with highly reflecting membranes","authors":["Georg Enzian","Eugene S. Polzik","Alexander K. Tagantsev"],"raw_abstract":"Highly reflecting mechanically compliant membranes based on photonic-crystal\npatterns have recently gained increasing attention within cavity optomechanics\ndue to their prospects of reaching high coupling rates in\nmembrane-in-the-middle experiments. Here we present an analysis and comparison\nof four different setups in which highly reflecting membranes can be employed\nfor cavity optomechanics, and discuss optimal choices w.r.t. the figures of\nmerit cooperativity and efficiency-weighted cooperativity. The analysis\nencompasses three different types of membrane-in-the-middle setups\n(membrane-at-the-edge, membrane-in-the-actual-middle, and\nmembrane-at-the-back), as well as the simple Fabry-Perot cavity. Interestingly,\nwe identify and propose the membrane-at-the-back setup as an optimal choice in\nthe limit of negligible membrane parasitic loss, which can reach enormous\nenhancements of optomechanical cooperativity, and if implemented with a\nlow-loss membrane would pave the way to nonlinear optomechanics in the quantum\nregime.","publication_date":1700669877,"paper_link":"http://arxiv.org/pdf/2311.13499v1","categories":["Physics"],"abstract":"Highly reflecting mechanically compliant membranes based on photonic-crystal patterns have recently gained increasing attention within cavity optomechanics due to their prospects of reaching high coupling rates in membrane-in-the-middle experiments. Here we present an analysis and comparison of four different setups in which highly reflecting membranes can be employed for cavity optomechanics, and discuss optimal choices w.r.t. the figures of merit cooperativity and efficiency-weighted cooperativity. The analysis encompasses three different types of membrane-in-the-middle setups (membrane-at-the-edge, membrane-in-the-actual-middle, and membrane-at-the-back), as well as the simple Fabry-Perot cavity. Interestingly, we identify and propose the membrane-at-the-back setup as an optimal choice in the limit of negligible membrane parasitic loss, which can reach enormous enhancements of optomechanical cooperativity, and if implemented with a low-loss membrane would pave the way to nonlinear optomechanics in the quantum regime."}
{"title":"Quantum Cascade Lasers as Broadband Sources via Strong RF Modulation","authors":["Alessio Cargioli","Diego Piciocchi","Mathieu Bertrand","Richard Maulini","Tobias Gresch","Antonine Muller","Giacomo Scalari","Jerome Faist"],"raw_abstract":"In this work, we demonstrate that in a regime of strong modulation, by\ngenerating pulses of the length of the order of a few cavity lifetimes\n(hundreds of ps), a broadband quantum cascade laser can be driven to lase on a\nbandwidth (250cm-1) limited by the gain. In addition, the amplitude noise of\nthe radiation was shown to be limited by the detector. A laser linewidth study\nhas been performed under different operating conditions finding values spanning\nfrom 20MHz to 800MHz, indicating a trade-off between emission bandwidth,\namplitude stability and coherence.","publication_date":1700669628,"paper_link":"http://arxiv.org/pdf/2311.13496v1","categories":["Physics"],"abstract":"In this work, we demonstrate that in a regime of strong modulation, by generating pulses of the length of the order of a few cavity lifetimes (hundreds of ps), a broadband quantum cascade laser can be driven to lase on a bandwidth (250cm-1) limited by the gain. In addition, the amplitude noise of the radiation was shown to be limited by the detector. A laser linewidth study has been performed under different operating conditions finding values spanning from 20MHz to 800MHz, indicating a trade-off between emission bandwidth, amplitude stability and coherence."}
{"title":"Turbulence modulation by suspended finite-sized particles -- Towards physics-based multiphase subgrid modeling","authors":["S. Balachandar","C. Peng","L. -P. Wang"],"raw_abstract":"The presence of a dispersed phase substantially modifies small-scale\nturbulence. However, there has not been a comprehensive mechanistically-based\nunderstanding to predict turbulence modulation. Based on the energy flux\nbalance, we propose a theoretical model to predict the turbulent kinetic energy\nmodulation in isotropic turbulence due to the dispersed phase. The comparison\nbetween model predictions and results from particle-resolved simulations and\nhigh-fidelity experiments validates the performance of the model over a wide\nrange of turbulence and particle parameters.","publication_date":1700669427,"paper_link":"http://arxiv.org/pdf/2311.13493v1","categories":["Physics"],"abstract":"The presence of a dispersed phase substantially modifies small-scale turbulence. However, there has not been a comprehensive mechanistically-based understanding to predict turbulence modulation. Based on the energy flux balance, we propose a theoretical model to predict the turbulent kinetic energy modulation in isotropic turbulence due to the dispersed phase. The comparison between model predictions and results from particle-resolved simulations and high-fidelity experiments validates the performance of the model over a wide range of turbulence and particle parameters."}
{"title":"Machine Learning based Post Event Analysis for Cybersecurity of Cyber-Physical System","authors":["Kuchan Park","Junho Hong","Wencong Su","HyoJong Lee"],"raw_abstract":"As Information and Communication Technology (ICT) equipment continues to be\nintegrated into power systems, issues related to cybersecurity are increasingly\nemerging. Particularly noteworthy is the transition to digital substations,\nwhich is shifting operations from traditional hardwired-based systems to\ncommunication-based Supervisory Control and Data Acquisition (SCADA) system\noperations. These changes in the power system have increased the vulnerability\nof the system to cyber-attacks and emphasized its importance. This paper\nproposes a machine learning (ML) based post event analysis of the power system\nin order to respond to these cybersecurity issues. An artificial neural network\n(ANN) and other ML models are trained using transient fault measurements and\ncyber-attack data on substations. The trained models can successfully\ndistinguish between power system faults and cyber-attacks. Furthermore, the\nresults of the proposed ML-based methods can also identify 10 different fault\ntypes and the location where the event occurred.","publication_date":1700668955,"paper_link":"http://arxiv.org/pdf/2311.13488v1","categories":["Electrical Engineering and Systems Science"],"abstract":"As Information and Communication Technology (ICT) equipment continues to be integrated into power systems, issues related to cybersecurity are increasingly emerging. Particularly noteworthy is the transition to digital substations, which is shifting operations from traditional hardwired-based systems to communication-based Supervisory Control and Data Acquisition (SCADA) system operations. These changes in the power system have increased the vulnerability of the system to cyber-attacks and emphasized its importance. This paper proposes a machine learning (ML) based post event analysis of the power system in order to respond to these cybersecurity issues. An artificial neural network (ANN) and other ML models are trained using transient fault measurements and cyber-attack data on substations. The trained models can successfully distinguish between power system faults and cyber-attacks. Furthermore, the results of the proposed ML-based methods can also identify 10 different fault types and the location where the event occurred."}
{"title":"Explicit no-$\u03c0^2$ renormalization schemes in QCD at five loops","authors":["J. A. Gracey"],"raw_abstract":"We examine a variety of renormalization schemes in QCD based on its $3$-point\nvertices where the $\\beta$-functions, gluon, ghost, quark and quark mass\nanomalous dimensions in each scheme do not depend on $\\zeta_4$ or $\\zeta_6$ in\nan arbitrary linear covariant gauge at five loops. We comment on the\n$C$-scheme.","publication_date":1700668876,"paper_link":"http://arxiv.org/pdf/2311.13484v1","categories":["Physics"],"abstract":"We examine a variety of renormalization schemes in QCD based on its __FORMULA__-point vertices where the __FORMULA__-functions, gluon, ghost, quark and quark mass anomalous dimensions in each scheme do not depend on __FORMULA__ or __FORMULA__ in an arbitrary linear covariant gauge at five loops. We comment on the __FORMULA__-scheme."}
{"title":"Higher twist corrections to $B$-meson decays into a proton and dark antibaryon from QCD light-cone sum rules","authors":["Anastasia Boushmelev","Marcel Wald"],"raw_abstract":"The $B$-Mesogenesis framework anticipates decays of $B$ mesons into a dark\nantibaryon $\\Psi$ and various Standard Model baryons. Here, we focus on the\nexclusive decay process $B\\to p \\Psi$ observed as a proton and missing energy\nin the final state and determine the decay width by employing the QCD\nlight-cone sum rule framework. We include all contributions up to twist six to\nthe nucleon distribution amplitudes in order to parameterize the\nnon-perturbative effects in the operator product expansion. We obtain the decay\nwidth and branching fraction with respect to the mass $m_{\\Psi}$ of the dark\nantibaryon $\\Psi$, normalized to the model-dependent effective four-fermion\ncoupling.","publication_date":1700668285,"paper_link":"http://arxiv.org/pdf/2311.13482v1","categories":["Physics"],"abstract":"The __FORMULA__-Mesogenesis framework anticipates decays of __FORMULA__ mesons into a dark antibaryon __FORMULA__ and various Standard Model baryons. Here, we focus on the exclusive decay process __FORMULA__ observed as a proton and missing energy in the final state and determine the decay width by employing the QCD light-cone sum rule framework. We include all contributions up to twist six to the nucleon distribution amplitudes in order to parameterize the non-perturbative effects in the operator product expansion. We obtain the decay width and branching fraction with respect to the mass __FORMULA__ of the dark antibaryon __FORMULA__, normalized to the model-dependent effective four-fermion coupling."}
{"title":"From internal waves to turbulence in a stably stratified fluid","authors":["Costanza Rodda","Cl\u00e9ment Savaro","Vincent Bouillaut","Pierre Augier","Jo\u00ebl Sommeria","Thomas Valran","Samuel Viboud","Nicolas Mordant"],"raw_abstract":"We report on the statistical analysis of stratified turbulence forced by\nlarge-scale waves. The setup mimics some features of the tidal forcing of\nturbulence in the ocean interior at submesoscales. Our experiments are\nperformed in the large-scale Coriolis facility in Grenoble which is 13 m in\ndiameter and 1 m deep. Four wavemakers excite large scale waves of moderate\namplitude. In addition to weak internal wave turbulence at large scales, we\nobserve strongly nonlinear waves, the breaking of which triggers intermittently\nstrong turbulence at small scales. A transition to strongly nonlinear\nturbulence is observed at smaller scales. Our measurements are reminiscent of\noceanic observations. Despite similarities with the empirical Garrett & Munk\nspectrum that assumes weak wave turbulence, our observed energy spectra are\nrather be attributed to strongly nonlinear internal waves.","publication_date":1700668028,"paper_link":"http://arxiv.org/pdf/2311.13476v1","categories":["Physics"],"abstract":"We report on the statistical analysis of stratified turbulence forced by large-scale waves. The setup mimics some features of the tidal forcing of turbulence in the ocean interior at submesoscales. Our experiments are performed in the large-scale Coriolis facility in Grenoble which is 13 m in diameter and 1 m deep. Four wavemakers excite large scale waves of moderate amplitude. In addition to weak internal wave turbulence at large scales, we observe strongly nonlinear waves, the breaking of which triggers intermittently strong turbulence at small scales. A transition to strongly nonlinear turbulence is observed at smaller scales. Our measurements are reminiscent of oceanic observations. Despite similarities with the empirical Garrett & Munk spectrum that assumes weak wave turbulence, our observed energy spectra are rather be attributed to strongly nonlinear internal waves."}
{"title":"Alternative robust ways of witnessing nonclassicality in the simplest scenario","authors":["Massy Khoshbin","Lorenzo Catani","Matthew Leifer"],"raw_abstract":"In this work we relate notions of nonclassicality in the simplest nontrivial\nscenario (a prepare and measure scenario composed of four preparations and two\nbinary-outcome tomographically complete measurements). Specifically, we relate\nthe established method developed in [Pusey, PRA 98,022112(2018)] to witness a\nviolation of preparation noncontextuality, that is not suitable in experiments\nwhere the operational equivalences to be tested are specified in advance, with\na novel approach based on the notion of bounded ontological distinctness for\npreparations, defined in [Chaturvedi and Saha, Quantum 4, 345 (2020)]. In our\napproach, we test bounded ontological distinctness for two particular\npreparations that are relevant in certain information processing tasks in that\nthey are associated with the even and odd parity of the bits to communicate.\nWhen there exists an ontological model where this distance is preserved we talk\nof parity preservation. Our main result provides a noise threshold under which\nviolating parity preservation (and so bounded ontological distinctness) agrees\nwith the established method for witnessing preparation contextuality in the\nsimplest nontrivial scenario. This is achieved by first relating the violation\nof parity preservation to the quantification of contextuality in terms of\ninaccessible information as developed in [Marvian, arXiv:2003.05984(2020)],\nthat we also show, given the way we quantify noise, to be more robust in\nwitnessing contextuality than Pusey's noncontextuality inequality. As an\napplication of our findings, we treat the case of 2 bit parity-oblivious\nmultiplexing in the presence of noise. In particular, we provide a condition\nfor which the result establishing preparation contextuality as a resource for\nthe quantum advantage of the protocol in the noiseless case still holds in the\nnoisy case.","publication_date":1700667694,"paper_link":"http://arxiv.org/pdf/2311.13474v1","categories":["Physics"],"abstract":"In this work we relate notions of nonclassicality in the simplest nontrivial scenario (a prepare and measure scenario composed of four preparations and two binary-outcome tomographically complete measurements). Specifically, we relate the established method developed in [Pusey, PRA 98,022112(2018)] to witness a violation of preparation noncontextuality, that is not suitable in experiments where the operational equivalences to be tested are specified in advance, with a novel approach based on the notion of bounded ontological distinctness for preparations, defined in [Chaturvedi and Saha, Quantum 4, 345 (2020)]. In our approach, we test bounded ontological distinctness for two particular preparations that are relevant in certain information processing tasks in that they are associated with the even and odd parity of the bits to communicate. When there exists an ontological model where this distance is preserved we talk of parity preservation. Our main result provides a noise threshold under which violating parity preservation (and so bounded ontological distinctness) agrees with the established method for witnessing preparation contextuality in the simplest nontrivial scenario. This is achieved by first relating the violation of parity preservation to the quantification of contextuality in terms of inaccessible information as developed in [Marvian, arXiv:2003.05984(2020)], that we also show, given the way we quantify noise, to be more robust in witnessing contextuality than Pusey's noncontextuality inequality. As an application of our findings, we treat the case of 2 bit parity-oblivious multiplexing in the presence of noise. In particular, we provide a condition for which the result establishing preparation contextuality as a resource for the quantum advantage of the protocol in the noiseless case still holds in the noisy case."}
{"title":"$Z$ polarization at an $e^+e^-$ collider and properties of decay-lepton angular asymmetries","authors":["Kumar Rao","Saurabh D. Rindani","Priyanka Sarmah","Balbeer Singh"],"raw_abstract":"$Z$ production at an $e^+e^-$ collider, associated with production of other\nparticles, can be an accurate source of information of details of electroweak\ninteractions, including possible interactions beyond the standard model. We\ndiscuss from a general physical point of view the properties of the density\nmatrix as well as lepton angular asymmetries. While most considerations will be\napplicable to processes of $Z$ production associated with any other particle or\nparticles, for some discussions, we specialize to a $HZ$ final state. While\nmany of the results can be found in earlier literature, especially for the\nprocess $e^+e^- \\to HZ$, we give details of the reasoning, which are not always\nfound. We discuss the properties of the spin density matrix under C, P and T\ntransformations, and combinations thereof, as also the predictions of these for\nthe corresponding leptonic asymmetries. The specific transformations P, CP, T\nand CPT are of special importance and we discuss the consequences of these\nsymmetries or their absence for the leptonic asymmetries. A specific issue\nwhich has been given attention to is the role of beam polarization, and how one\ncan infer on general grounds which asymmetries get enhanced by the use of beam\npolarization. Similarly, we also discuss which asymmetries would be sensitive\nto the measurement of tau polarization in $Z$ decay in to $\\tau^+\\tau^-$.","publication_date":1700667685,"paper_link":"http://arxiv.org/pdf/2311.13473v1","categories":["Physics"],"abstract":"__FORMULA__ production at an __FORMULA__ collider, associated with production of other particles, can be an accurate source of information of details of electroweak interactions, including possible interactions beyond the standard model. We discuss from a general physical point of view the properties of the density matrix as well as lepton angular asymmetries. While most considerations will be applicable to processes of __FORMULA__ production associated with any other particle or particles, for some discussions, we specialize to a __FORMULA__ final state. While many of the results can be found in earlier literature, especially for the process __FORMULA__, we give details of the reasoning, which are not always found. We discuss the properties of the spin density matrix under C, P and T transformations, and combinations thereof, as also the predictions of these for the corresponding leptonic asymmetries. The specific transformations P, CP, T and CPT are of special importance and we discuss the consequences of these symmetries or their absence for the leptonic asymmetries. A specific issue which has been given attention to is the role of beam polarization, and how one can infer on general grounds which asymmetries get enhanced by the use of beam polarization. Similarly, we also discuss which asymmetries would be sensitive to the measurement of tau polarization in __FORMULA__ decay in to __FORMULA__."}
{"title":"Variable-temperature lightwave-driven scanning tunneling microscope with a compact, turn-key terahertz source","authors":["H\u00fcseyin Azazoglu","Philip Kapitza","Martin Mittendorff","Rolf M\u00f6ller","Manuel Gruber"],"raw_abstract":"We report on a lightwave-driven scanning tunneling microscope based on a\nhome-built microscope and a compact, commercial, and cost-effective\nterahertz-generation unit with a repetition rate of 100 MHz. The measurements\nare performed in ultrahigh vacuum at temperatures between 10 K and 300 K. The\ncross-correlation of the pump and probe pulses indicate a temporal resolution\non the order of a picosecond. In terms of spatial resolution, CO molecules,\nstep edges and atomically resolved terraces are readily observed in terahertz\nimages, with sometimes better contrast than in the topographic and (DC) current\nchannels. The utilization of a compact, turn-key terahertz-generation system\nrequires only limited experience with optics and terahertz generation, which\nmay facilitate the deployment of the technique to further research groups.","publication_date":1700666586,"paper_link":"http://arxiv.org/pdf/2311.13456v1","categories":["Physics"],"abstract":"We report on a lightwave-driven scanning tunneling microscope based on a home-built microscope and a compact, commercial, and cost-effective terahertz-generation unit with a repetition rate of 100 MHz. The measurements are performed in ultrahigh vacuum at temperatures between 10 K and 300 K. The cross-correlation of the pump and probe pulses indicate a temporal resolution on the order of a picosecond. In terms of spatial resolution, CO molecules, step edges and atomically resolved terraces are readily observed in terahertz images, with sometimes better contrast than in the topographic and (DC) current channels. The utilization of a compact, turn-key terahertz-generation system requires only limited experience with optics and terahertz generation, which may facilitate the deployment of the technique to further research groups."}
{"title":"From local to nonlocal high-Q plasmonic metasurfaces","authors":["Yao Liang","Din Ping Tsai","Yuri Kivshar"],"raw_abstract":"The physics of bound states in the continuum (BICs) allows to design and\ndemonstrate optical resonant structures with large values of the quality factor\n($Q$-factor) by employing dielectric structures with low losses. However, BIC\nis a general wave phenomenon that should be observed in many systems, including\nthe metal-dielectric structures supporting plasmons where the resonances are\nhindered by losses. Here we develop a comprehensive strategy to achieve\nhigh-$Q$ resonances in plasmonic metasurfaces by effectively tailoring the\nresonant modes from local and nonlocal regimes.","publication_date":1700666220,"paper_link":"http://arxiv.org/pdf/2311.13452v1","categories":["Physics"],"abstract":"The physics of bound states in the continuum (BICs) allows to design and demonstrate optical resonant structures with large values of the quality factor (__FORMULA__-factor) by employing dielectric structures with low losses. However, BIC is a general wave phenomenon that should be observed in many systems, including the metal-dielectric structures supporting plasmons where the resonances are hindered by losses. Here we develop a comprehensive strategy to achieve high-__FORMULA__ resonances in plasmonic metasurfaces by effectively tailoring the resonant modes from local and nonlocal regimes."}
{"title":"Luther-Emery liquid and dominant singlet superconductivity in the two-orbital Hubbard chain","authors":["Pontus Laurell","Jacek Herbrych","Gonzalo Alvarez","Elbio Dagotto"],"raw_abstract":"We investigate the pairing tendencies in the two-orbital Hubbard chain at\nintermediate repulsive interaction strengths $U$, and for degenerate orbitals.\nAt half-filling and large $U$, the ferromagnetic Hund's coupling,\n$J_\\mathrm{H}$, generates effective spin-$1$ moments, with antiferromagnetic\ncorrelations between sites. Thus the system can be viewed as an electronic\ngeneralization of Haldane's spin-$1$ chain in that limit. Using large-scale\ndensity matrix renormalization group calculations, we study the system's\nbehavior under light hole-doping. For $U=1.6$ in units of the non-interacting\nbandwidth and $J_\\mathrm{H}/U\\gtrsim 0.275$ we find that singlet pairing\ndominates the long-distance physics, establishing this system as a promising\nplatform for repulsively mediated superconductivity. We provide evidence that\nthe system approaches a Luther-Emery liquid state at large system sizes,\nsimilarly to the behavior of doped one-orbital two-leg ladders at weak\ncoupling. The numerically calculated central charge approaches one in the\nthermodynamic limit, indicating a single gapless mode as is expected for the\nLuther-Emery state. Exponents characterizing the power-law decays of singlet\npair-pair and charge density-density correlations are determined, and found to\napproximately satisfy the Luther-Emery identity. Candidate materials to realize\nthis physics are discussed.","publication_date":1700665201,"paper_link":"http://arxiv.org/pdf/2311.13440v1","categories":["Physics"],"abstract":"We investigate the pairing tendencies in the two-orbital Hubbard chain at intermediate repulsive interaction strengths __FORMULA__, and for degenerate orbitals. At half-filling and large __FORMULA__, the ferromagnetic Hund's coupling, __FORMULA__, generates effective spin-__FORMULA__ moments, with antiferromagnetic correlations between sites. Thus the system can be viewed as an electronic generalization of Haldane's spin-__FORMULA__ chain in that limit. Using large-scale density matrix renormalization group calculations, we study the system's behavior under light hole-doping. For __FORMULA__ in units of the non-interacting bandwidth and __FORMULA__ we find that singlet pairing dominates the long-distance physics, establishing this system as a promising platform for repulsively mediated superconductivity. We provide evidence that the system approaches a Luther-Emery liquid state at large system sizes, similarly to the behavior of doped one-orbital two-leg ladders at weak coupling. The numerically calculated central charge approaches one in the thermodynamic limit, indicating a single gapless mode as is expected for the Luther-Emery state. Exponents characterizing the power-law decays of singlet pair-pair and charge density-density correlations are determined, and found to approximately satisfy the Luther-Emery identity. Candidate materials to realize this physics are discussed."}
{"title":"Modelling the propagation of coronal mass ejections with COCONUT: implementation of the Regularized Biot-Savart Laws flux rope model","authors":["Jinhan Guo","L. Linan","S. Poedts","Y. Guo","A. Lani","B. Schmieder","M. Brchnelova","B. Perri","T. Baratashvili","Y. W. Ni","P. F. Chen"],"raw_abstract":"Context: Coronal mass ejections (CMEs) are rapid eruptions of magnetized\nplasma that occur on the Sun, which are known as the main drivers of adverse\nspace weather. Accurately tracking their evolution in the heliosphere in\nnumerical models is of utmost importance for space weather forecasting. Aims:\nThe main objective of this paper is to implement the Regularized Biot-Savart\nLaws (RBSL) method in a new global corona model COCONUT. This approach has the\ncapability to construct the magnetic flux rope with an axis of arbitrary shape.\nMethods: We present the implementation process of the RBSL flux rope model in\nCOCONUT, which is superposed onto a realistic solar wind reconstructed from the\nobserved magnetogram around the minimum of solar activity. Based on this, we\nsimulate the propagation of an S-shaped flux rope from the solar surface to a\ndistance of 25 solar radii. Results: Our simulation successfully reproduces the\nbirth process of a CME originating from a sigmoid in a self-consistent way. The\nmodel effectively captures various physical processes and retrieves the\nprominent features of the CMEs in observations. In addition, the simulation\nresults indicate that the magnetic topology of the CME flux rope at around 20\nsolar radii deviates from a coherent structure, and manifests as a mix of open\nand closed field lines with diverse footpoints. Conclusions: This work\ndemonstrates the potential of the RBSL flux rope model in reproducing CME\nevents that are more consistent with observations. Moreover, our findings\nstrongly suggest that magnetic reconnection during the CME propagation plays a\ncritical role in destroying the coherent characteristic of a CME flux rope.","publication_date":1700664421,"paper_link":"http://arxiv.org/pdf/2311.13432v1","categories":["Physics"],"abstract":"Context: Coronal mass ejections (CMEs) are rapid eruptions of magnetized plasma that occur on the Sun, which are known as the main drivers of adverse space weather. Accurately tracking their evolution in the heliosphere in numerical models is of utmost importance for space weather forecasting. Aims: The main objective of this paper is to implement the Regularized Biot-Savart Laws (RBSL) method in a new global corona model COCONUT. This approach has the capability to construct the magnetic flux rope with an axis of arbitrary shape. Methods: We present the implementation process of the RBSL flux rope model in COCONUT, which is superposed onto a realistic solar wind reconstructed from the observed magnetogram around the minimum of solar activity. Based on this, we simulate the propagation of an S-shaped flux rope from the solar surface to a distance of 25 solar radii. Results: Our simulation successfully reproduces the birth process of a CME originating from a sigmoid in a self-consistent way. The model effectively captures various physical processes and retrieves the prominent features of the CMEs in observations. In addition, the simulation results indicate that the magnetic topology of the CME flux rope at around 20 solar radii deviates from a coherent structure, and manifests as a mix of open and closed field lines with diverse footpoints. Conclusions: This work demonstrates the potential of the RBSL flux rope model in reproducing CME events that are more consistent with observations. Moreover, our findings strongly suggest that magnetic reconnection during the CME propagation plays a critical role in destroying the coherent characteristic of a CME flux rope."}
{"title":"Differential method for measuring periodic magnetic structure","authors":["G. Yakopov","P. Vagin"],"raw_abstract":"The improvement of accelerator parameters requires the transition to\nsuperconducting undulator technologies since SCUs allow higher magnetic fields\ncompared to classical planar undulators with the same gap. The primary method\nfor characterizing magnetic structures is to measure the magnetic field\ndistribution along the undulator axis with a Hall probe. Special test rigs\nbased on vertical cryostats with liquid helium are used to parameterize\nsuperconducting magnetic structures (undulator coils). However, during the\nprobe immersion and cooling to cryogenic temperatures, its linear dimensions\nchange, which needs to be taken into account by applying complex thermodynamic\nmodels since the position encoder of the probe is located on the cryostat\nflange at room temperature. To eliminate this effect, it is proposed to equip\nthe probe with two Hall sensors spaced apart half a period of the measured\nstructure.","publication_date":1700664330,"paper_link":"http://arxiv.org/pdf/2311.13430v1","categories":["Physics"],"abstract":"The improvement of accelerator parameters requires the transition to superconducting undulator technologies since SCUs allow higher magnetic fields compared to classical planar undulators with the same gap. The primary method for characterizing magnetic structures is to measure the magnetic field distribution along the undulator axis with a Hall probe. Special test rigs based on vertical cryostats with liquid helium are used to parameterize superconducting magnetic structures (undulator coils). However, during the probe immersion and cooling to cryogenic temperatures, its linear dimensions change, which needs to be taken into account by applying complex thermodynamic models since the position encoder of the probe is located on the cryostat flange at room temperature. To eliminate this effect, it is proposed to equip the probe with two Hall sensors spaced apart half a period of the measured structure."}
{"title":"Waves in space-dependent and time-dependent materials: a systematic comparison","authors":["Kees Wapenaar","Johannes Aichele","Dirk-Jan van Manen"],"raw_abstract":"Waves in space-dependent and time-dependent materials obey similar wave\nequations, with interchanged time- and space-coordinates. However, since the\ncausality conditions are the same in both types of material (i.e., without\ninterchangement of coordinates), the solutions are dissimilar.\n  We present a systematic treatment of wave propagation and scattering in 1D\nspace-dependent and time-dependent materials. After a review of reflection and\ntransmission coefficients, we discuss Green's functions and simple wave field\nrepresentations for both types of material. Next we discuss propagation\ninvariants, i.e., quantities that are independent of the space coordinate in a\nspace-dependent material (such as the net power-flux density) or of the time\ncoordinate in a time-dependent material (such as the net field-momentum\ndensity). A discussion of reciprocity theorems leads to the well-known\nsource-receiver reciprocity relation for the Green's function of a\nspace-dependent material and a new source-receiver reciprocity relation for the\nGreen's function of a time-dependent material. A discussion of general wave\nfield representations leads to the well-known expression for Green's function\nretrieval from the correlation of passive measurements in a space-dependent\nmaterial and a new expression for Green's function retrieval in a\ntime-dependent material.\n  After an introduction of a matrix-vector wave equation, we discuss propagator\nmatrices for both types of material. Since the initial condition for a\npropagator matrix in a time-dependent material follows from the boundary\ncondition for a propagator matrix in a space-dependent material by\ninterchanging the time- and space-coordinates, the propagator matrices for both\ntypes of material are interrelated in the same way. This also applies to\nrepresentations and reciprocity theorems involving propagator matrices.","publication_date":1700664301,"paper_link":"http://arxiv.org/pdf/2311.13428v1","categories":["Physics"],"abstract":"Waves in space-dependent and time-dependent materials obey similar wave equations, with interchanged time- and space-coordinates. However, since the causality conditions are the same in both types of material (i.e., without interchangement of coordinates), the solutions are dissimilar.   We present a systematic treatment of wave propagation and scattering in 1D space-dependent and time-dependent materials. After a review of reflection and transmission coefficients, we discuss Green's functions and simple wave field representations for both types of material. Next we discuss propagation invariants, i.e., quantities that are independent of the space coordinate in a space-dependent material (such as the net power-flux density) or of the time coordinate in a time-dependent material (such as the net field-momentum density). A discussion of reciprocity theorems leads to the well-known source-receiver reciprocity relation for the Green's function of a space-dependent material and a new source-receiver reciprocity relation for the Green's function of a time-dependent material. A discussion of general wave field representations leads to the well-known expression for Green's function retrieval from the correlation of passive measurements in a space-dependent material and a new expression for Green's function retrieval in a time-dependent material.   After an introduction of a matrix-vector wave equation, we discuss propagator matrices for both types of material. Since the initial condition for a propagator matrix in a time-dependent material follows from the boundary condition for a propagator matrix in a space-dependent material by interchanging the time- and space-coordinates, the propagator matrices for both types of material are interrelated in the same way. This also applies to representations and reciprocity theorems involving propagator matrices."}
{"title":"Coupling undetected sensing modes by quantum erasure","authors":["Nathan R. Gemmell","Yue Ma","Emma Pearce","Jefferson Florez","Olaf Czerwinski","M. S. Kim","Rupert F. Oulton","Alex S. Clark","Chris C. Phillips"],"raw_abstract":"The effect known as ``induced coherence without induced emission'' has\nspawned a field dedicated to imaging with undetected photons (IUP), where\nphotons from two distinct photon-pair sources interfere if their outputs are\nmade indistinguishable. The indistinguishability is commonly achieved in two\nsetups. Induced coherence IUP (IC-IUP) has only the idler photons from the\nfirst source passing through the second, whilst nonlinear interferometry\n(NI-IUP) has both signal and idler photons from the first source passing\nthrough the second and can be simpler to implement. In both cases, changes in\nthe idler path between sources can be detected by measuring the interference\nfringes in the signal path in a way that allows image information to be moved\nbetween different wavelengths. Here we model and implement a novel setup that\nuses a polarization state quantum eraser approach to move continuously between\nIC-IUP and NI-IUP operation. We find excellent agreement between experiment and\ntheory in the low-gain or quantum regime. The system also provides a new route\nfor optimizing IUP interference by using controllable quantum erasure to\nbalance the interferometer.","publication_date":1700663737,"paper_link":"http://arxiv.org/pdf/2311.13421v1","categories":["Physics"],"abstract":"The effect known as ``induced coherence without induced emission'' has spawned a field dedicated to imaging with undetected photons (IUP), where photons from two distinct photon-pair sources interfere if their outputs are made indistinguishable. The indistinguishability is commonly achieved in two setups. Induced coherence IUP (IC-IUP) has only the idler photons from the first source passing through the second, whilst nonlinear interferometry (NI-IUP) has both signal and idler photons from the first source passing through the second and can be simpler to implement. In both cases, changes in the idler path between sources can be detected by measuring the interference fringes in the signal path in a way that allows image information to be moved between different wavelengths. Here we model and implement a novel setup that uses a polarization state quantum eraser approach to move continuously between IC-IUP and NI-IUP operation. We find excellent agreement between experiment and theory in the low-gain or quantum regime. The system also provides a new route for optimizing IUP interference by using controllable quantum erasure to balance the interferometer."}
{"title":"Simulation of universal optical logic gates under energy sharing collisions of Manakov solitons and fulfillment of practical optical logic criteria","authors":["M. Vijayajayanthi","T. Kanna","M. Lakshmanan"],"raw_abstract":"The universal optical logic gates, namely NAND and NOR gates, have been\ntheoretically simulated by employing the energy sharing collision of bright\noptical solitons in the Manakov system, governing pulse propagation in a highly\nbirefringent fiber. Further, we also realize the two input optical logic gates\nsuch as AND, OR, XOR, XNOR for completeness of our scheme. Interestingly, our\nidea behind the simulation naturally satisfies all the criteria for practical\noptical logic which in turn displays the strength and versatility of our\ntheoretical simulation of universal optical logic gates. Hence, our approach\npaves the way for the experimentalists to create a new avenue in this direction\nif the energy sharing collisions of Manakov solitons are experimentally\nrealized in the future.","publication_date":1700663411,"paper_link":"http://arxiv.org/pdf/2311.13419v1","categories":["Physics"],"abstract":"The universal optical logic gates, namely NAND and NOR gates, have been theoretically simulated by employing the energy sharing collision of bright optical solitons in the Manakov system, governing pulse propagation in a highly birefringent fiber. Further, we also realize the two input optical logic gates such as AND, OR, XOR, XNOR for completeness of our scheme. Interestingly, our idea behind the simulation naturally satisfies all the criteria for practical optical logic which in turn displays the strength and versatility of our theoretical simulation of universal optical logic gates. Hence, our approach paves the way for the experimentalists to create a new avenue in this direction if the energy sharing collisions of Manakov solitons are experimentally realized in the future."}
{"title":"Zig-zag dynamics in a Stern-Gerlach spin measurement","authors":["Simon Krekels","Christian Maes","Kasper Meerts","Ward Struyve"],"raw_abstract":"The one-century-old Stern-Gerlach setup is paradigmatic for a quantum\nmeasurement. We visualize the electron trajectories following the Bohmian\nzig-zag dynamics. This dynamics was developed in order to deal with the\nfundamentally massless nature of particles (with mass emerging from the\nBrout-Englert-Higgs mechanism). The corresponding trajectories exhibit a\nstochastic zig-zagging, as the result of the coupling between left- and\nright-handed chiral Weyl states. This zig-zagging persists in the\nnon-relativistic limit, which will be considered here, and which is described\nthe Pauli equation for a nonuniform external magnetic field. Our results\nclarify the different meanings of \"spin\" as a property of the wave function and\nas a random variable in the Stern-Gerlach setup, and they illustrate the notion\nof effective collapse. We also examine the case of an EPR-pair. By letting one\nof the entangled particles pass through a Stern-Gerlach device, the nonlocal\ninfluence (action-at-a-distance) on the other particle is manifest in its\ntrajectory, e.g. by initiating its zig-zagging.","publication_date":1700661862,"paper_link":"http://arxiv.org/pdf/2311.13406v1","categories":["Physics"],"abstract":"The one-century-old Stern-Gerlach setup is paradigmatic for a quantum measurement. We visualize the electron trajectories following the Bohmian zig-zag dynamics. This dynamics was developed in order to deal with the fundamentally massless nature of particles (with mass emerging from the Brout-Englert-Higgs mechanism). The corresponding trajectories exhibit a stochastic zig-zagging, as the result of the coupling between left- and right-handed chiral Weyl states. This zig-zagging persists in the non-relativistic limit, which will be considered here, and which is described the Pauli equation for a nonuniform external magnetic field. Our results clarify the different meanings of \"spin\" as a property of the wave function and as a random variable in the Stern-Gerlach setup, and they illustrate the notion of effective collapse. We also examine the case of an EPR-pair. By letting one of the entangled particles pass through a Stern-Gerlach device, the nonlocal influence (action-at-a-distance) on the other particle is manifest in its trajectory, e.g. by initiating its zig-zagging."}
{"title":"Curvature of the energy per particle in neutron stars","authors":["Micha\u0142 Marczenko","Krzysztof Redlich","Chihiro Sasaki"],"raw_abstract":"Neutron stars (NSs) serve as laboratories for probing strongly interacting\nmatter at the most extreme densities. Their inner cores are expected to be\ndense enough to host deconfined quark matter. Utilizing state-of-the-art\ntheoretical and multi-messenger constraints, we statistically determine the\nbulk properties of dense NS matter. We show that the speed of sound can be\nexpressed in terms of the slope and curvature of the energy per particle. We\ndemonstrate that the restoration of conformal symmetry requires changing the\nsign of the curvature of the bulk energy per particle as a function of energy\ndensity. Furthermore, we find that such a sign change is closely related to the\npeak in the speed of sound. We argue that the curvature of the energy per\nparticle may serve as an approximate order parameter that signifies the onset\nof strongly coupled conformal matter in the NS core.","publication_date":1700661374,"paper_link":"http://arxiv.org/pdf/2311.13401v1","categories":["Physics"],"abstract":"Neutron stars (NSs) serve as laboratories for probing strongly interacting matter at the most extreme densities. Their inner cores are expected to be dense enough to host deconfined quark matter. Utilizing state-of-the-art theoretical and multi-messenger constraints, we statistically determine the bulk properties of dense NS matter. We show that the speed of sound can be expressed in terms of the slope and curvature of the energy per particle. We demonstrate that the restoration of conformal symmetry requires changing the sign of the curvature of the bulk energy per particle as a function of energy density. Furthermore, we find that such a sign change is closely related to the peak in the speed of sound. We argue that the curvature of the energy per particle may serve as an approximate order parameter that signifies the onset of strongly coupled conformal matter in the NS core."}
{"title":"Wetting and Strain Engineering of 2D Materials on Nanopatterned Substrates","authors":["Davoud Adinehloo","Joshua R. Hendrickson","Vasili Perebeinos"],"raw_abstract":"The fascinating realm of strain engineering and wetting transitions in\ntwo-dimensional (2D) materials takes place when placed on a two-dimensional\narray of nanopillars or one-dimensional rectangular grated substrates. Our\ninvestigation encompasses a diverse set of atomically thin 2D materials,\nincluding transition metal dichalcogenides, hexagonal boron nitride, and\ngraphene, with a keen focus on the impact of van der Waals adhesion energies to\nthe substrate on the wetting/dewetting behavior on nanopatterned substrates. We\nfind a critical aspect ratio of the nanopillar or grating heights to the period\nof the pattern when the wetting/dewetting transition occurs. Furthermore,\nenergy hysteresis analysis reveals dynamic detachment and re-engagement events\nduring height adjustments, shedding light on energy barriers of 2D monolayer\ntransferred on patterned substrates. Our findings offer avenues for strain\nengineering in 2D materials, leading to promising prospects for future\ntechnological applications.","publication_date":1700661211,"paper_link":"http://arxiv.org/pdf/2311.13399v1","categories":["Physics"],"abstract":"The fascinating realm of strain engineering and wetting transitions in two-dimensional (2D) materials takes place when placed on a two-dimensional array of nanopillars or one-dimensional rectangular grated substrates. Our investigation encompasses a diverse set of atomically thin 2D materials, including transition metal dichalcogenides, hexagonal boron nitride, and graphene, with a keen focus on the impact of van der Waals adhesion energies to the substrate on the wetting/dewetting behavior on nanopatterned substrates. We find a critical aspect ratio of the nanopillar or grating heights to the period of the pattern when the wetting/dewetting transition occurs. Furthermore, energy hysteresis analysis reveals dynamic detachment and re-engagement events during height adjustments, shedding light on energy barriers of 2D monolayer transferred on patterned substrates. Our findings offer avenues for strain engineering in 2D materials, leading to promising prospects for future technological applications."}
{"title":"Status and Prospects of the PandaX-III Experiment","authors":["Wenming Zhang","Heng Lin","Yuanchun Liu","Ke Han","Kaixiang Ni","Shaobo Wang","Wenchang Zhai"],"raw_abstract":"The PandaX-III experiment searches the neutrinoless double beta decay of\n$^{136}$Xe with a high-pressure xenon gaseous time projection chamber~(TPC).\nThermal-bonding Micromegas modules are used for charge collection. Benefitting\nfrom the excellent energy resolution and imaging capability, the background\nrate can be significantly suppressed through the topological information of\nevents. The technology is successfully demonstrated by a prototype detector.\nThe final detector has been constructed. In this paper, we will report the\nstatus of the PandaX-III experiment, including the construction and\ncommissioning of the final detector, and the Micromegas-based TPC performance\ntest in the prototype detector.","publication_date":1700661096,"paper_link":"http://arxiv.org/pdf/2311.13396v1","categories":["Physics"],"abstract":"The PandaX-III experiment searches the neutrinoless double beta decay of __FORMULA__Xe with a high-pressure xenon gaseous time projection chamber~(TPC). Thermal-bonding Micromegas modules are used for charge collection. Benefitting from the excellent energy resolution and imaging capability, the background rate can be significantly suppressed through the topological information of events. The technology is successfully demonstrated by a prototype detector. The final detector has been constructed. In this paper, we will report the status of the PandaX-III experiment, including the construction and commissioning of the final detector, and the Micromegas-based TPC performance test in the prototype detector."}
{"title":"The GUAPOS project: G31.41+0.31 Unbiased ALMA sPectral Observational Survey. IV. Phosphorus-bearing molecules and their relation with shock tracers","authors":["F. Fontani","C. Mininni","M. T. Beltr\u00e1n","V. M. Rivilla","L. Colzi","I. Jim\u00e9nez-Serra","\u00c1. L\u00f3pez-Gallifa","\u00c1. S\u00e1nchez-Monge","S. Viti"],"raw_abstract":"The astrochemistry of the important biogenic element phosphorus (P) is still\npoorly understood, but observational evidence indicates that P-bearing\nmolecules are likely associated with shocks. We study P-bearing molecules, as\nwell as some shock tracers, towards one of the chemically richest hot molecular\ncore, G31.41+0.31, in the framework of the project \"G31.41+0.31 Unbiased ALMA\nsPectral Observational Survey\" (GUAPOS), observed with the Atacama Large\nMillimeter Array (ALMA). We have observed the molecules PN, PO, SO, SO2, SiO,\nand SiS, through their rotational lines in the spectral range 84.05-115.91 GHz,\ncovered by the GUAPOS project. PN is clearly detected while PO is tentatively\ndetected. The PN emission arises from two regions southwest of the hot core\npeak, \"1\" and \"2\", and is undetected or tentatively detected towards the hot\ncore peak. the PN and SiO lines are very similar both in spatial emission\nmorphology and spectral shape. Region \"1\" is in part overlapping with the hot\ncore and it is warmer than region \"2\", which is well separated from the hot\ncore and located along the outflows identified in previous studies. The column\ndensity ratio SiO/PN remains constant in regions \"1\" and \"2\", while SO/PN,\nSiS/PN, and SO2/PN decrease by about an order of magnitude from region \"1\" to\nregion \"2\", indicating that SiO and PN have a common origin even in regions\nwith different physical conditions. Our study firmly confirms previous\nobservational evidence that PN emission is tightly associated with SiO and it\nis likely a product of shock-chemistry, as the lack of a clear detection of PN\ntowards the hot-core allows to rule out relevant formation pathways in hot gas.\nWe propose the PN emitting region \"2\" as a new astrophysical laboratory for\nshock-chemistry studies","publication_date":1700658407,"paper_link":"http://arxiv.org/pdf/2311.13367v1","categories":["Physics"],"abstract":"The astrochemistry of the important biogenic element phosphorus (P) is still poorly understood, but observational evidence indicates that P-bearing molecules are likely associated with shocks. We study P-bearing molecules, as well as some shock tracers, towards one of the chemically richest hot molecular core, G31.41+0.31, in the framework of the project \"G31.41+0.31 Unbiased ALMA sPectral Observational Survey\" (GUAPOS), observed with the Atacama Large Millimeter Array (ALMA). We have observed the molecules PN, PO, SO, SO2, SiO, and SiS, through their rotational lines in the spectral range 84.05-115.91 GHz, covered by the GUAPOS project. PN is clearly detected while PO is tentatively detected. The PN emission arises from two regions southwest of the hot core peak, \"1\" and \"2\", and is undetected or tentatively detected towards the hot core peak. the PN and SiO lines are very similar both in spatial emission morphology and spectral shape. Region \"1\" is in part overlapping with the hot core and it is warmer than region \"2\", which is well separated from the hot core and located along the outflows identified in previous studies. The column density ratio SiO/PN remains constant in regions \"1\" and \"2\", while SO/PN, SiS/PN, and SO2/PN decrease by about an order of magnitude from region \"1\" to region \"2\", indicating that SiO and PN have a common origin even in regions with different physical conditions. Our study firmly confirms previous observational evidence that PN emission is tightly associated with SiO and it is likely a product of shock-chemistry, as the lack of a clear detection of PN towards the hot-core allows to rule out relevant formation pathways in hot gas. We propose the PN emitting region \"2\" as a new astrophysical laboratory for shock-chemistry studies"}
{"title":"Direct observations of general geothermal convection in deep Mediterranean waters","authors":["Hans van Haren"],"raw_abstract":"Like elsewhere in the deep-sea, life in the deep Mediterranean depends on\nturbulent exchange across the stable vertical density stratification for supply\nof nutrients and oxygen. Commonly modelled, turbulent exchange is inversely\nproportional to the stratification rate. However, this proportionality depends\non the particular turbulence type, whether it is driven by vertical current\ndifferences (shear) or by buoyancy (convection). While shear-turbulence is well\nobserved in stratified seas, direct observations of convection-turbulence are\nlimited. In this paper, high-resolution moored temperature observations show\nthat Mediterranean Sea waters are not stagnant in the lower 109 m above the\nseafloor at 2480 m, although variations are in the range of only 0.0001-0.001\ndegrC. In winter, convection-turbulence is regularly observed. Fortnightly\naveraged spectra show a collapse to the inertial-subrange scaling of dominant\nshear-turbulence for data from about 100 m above the seafloor, and to the\nbuoyancy-subrange scaling of dominant convection-turbulence at about 10 m above\nthe seafloor. Time-depth images reveal details of convection-turbulence driven\nfrom below, which is considered primarily due to general geothermal heating\nthrough the Earth crust not related to volcanic vents. When its observation is\nnot masked by (sub-)mesoscale eddies that advect warmer waters from above, the\ngeothermal heat flux matches the deep-sea turbulence dissipation rate, if in\nthe calculations a mixing efficiency of 0.5 is taken typical for natural\nconvection, integration is over 250 m above the seafloor as confirmed from\nshipborne CTD, and if maximum 2-m-scale buoyancy frequency replaces its\n100-m-scale mean equivalent.","publication_date":1700658353,"paper_link":"http://arxiv.org/pdf/2311.13366v1","categories":["Physics"],"abstract":"Like elsewhere in the deep-sea, life in the deep Mediterranean depends on turbulent exchange across the stable vertical density stratification for supply of nutrients and oxygen. Commonly modelled, turbulent exchange is inversely proportional to the stratification rate. However, this proportionality depends on the particular turbulence type, whether it is driven by vertical current differences (shear) or by buoyancy (convection). While shear-turbulence is well observed in stratified seas, direct observations of convection-turbulence are limited. In this paper, high-resolution moored temperature observations show that Mediterranean Sea waters are not stagnant in the lower 109 m above the seafloor at 2480 m, although variations are in the range of only 0.0001-0.001 degrC. In winter, convection-turbulence is regularly observed. Fortnightly averaged spectra show a collapse to the inertial-subrange scaling of dominant shear-turbulence for data from about 100 m above the seafloor, and to the buoyancy-subrange scaling of dominant convection-turbulence at about 10 m above the seafloor. Time-depth images reveal details of convection-turbulence driven from below, which is considered primarily due to general geothermal heating through the Earth crust not related to volcanic vents. When its observation is not masked by (sub-)mesoscale eddies that advect warmer waters from above, the geothermal heat flux matches the deep-sea turbulence dissipation rate, if in the calculations a mixing efficiency of 0.5 is taken typical for natural convection, integration is over 250 m above the seafloor as confirmed from shipborne CTD, and if maximum 2-m-scale buoyancy frequency replaces its 100-m-scale mean equivalent."}
{"title":"Upgrade of Belle II Vertex Detector with CMOS Pixel Technology","authors":["M. Schwickardi","M. Babeluk","M. Barbero","J. Baudot","T. Bergauer","G. Bertolone","S. Bettarini","F. Bosi","P. Breugnon","Y. Buch","G. Casarosa","G. Dujany","C. Finck","F. Forti","A. Frey","A. Himmi","C. Irmler","A. Kumar","C. Marinas","M. Massa","L. Massaccesi","J. Mazzora de Cos","M. Minuti","S. Mondal","P. Pangaud","H. Pham","I. Ripp-Baudot","G. Rizzo","B. Schwenker","I. Valin"],"raw_abstract":"The Belle II experiment at KEK in Japan considers upgrading its vertex\ndetector system to address the challenges posed by high background levels\ncaused by the increased luminosity of the SuperKEKB collider. One proposal for\nupgrading the vertex detector aims to install a 5-layer all monolithic pixel\nvertex detector based on fully depleted CMOS sensors in 2027. The new system\nwill use the OBELIX MAPS chips to improve background robustness and reduce\noccupancy levels through small and fast pixels. This causes better track\nfinding, especially for low transverse momenta tracks. This text will focus on\nthe predecessor of the OBELIX sensor, the TJ-Monopix2, presenting laboratory\nand test beam results on pixel response, efficiency, and spatial resolution.","publication_date":1700657683,"paper_link":"http://arxiv.org/pdf/2311.13360v1","categories":["Physics"],"abstract":"The Belle II experiment at KEK in Japan considers upgrading its vertex detector system to address the challenges posed by high background levels caused by the increased luminosity of the SuperKEKB collider. One proposal for upgrading the vertex detector aims to install a 5-layer all monolithic pixel vertex detector based on fully depleted CMOS sensors in 2027. The new system will use the OBELIX MAPS chips to improve background robustness and reduce occupancy levels through small and fast pixels. This causes better track finding, especially for low transverse momenta tracks. This text will focus on the predecessor of the OBELIX sensor, the TJ-Monopix2, presenting laboratory and test beam results on pixel response, efficiency, and spatial resolution."}
{"title":"Experimental determination of axion signal power of dish antennas and dielectric haloscopes using the reciprocity approach","authors":["J. Egge","M. Ekmed\u017ei\u0107","A. Gardikiotis","E. Garutti","S. Heyminck","C. Kasemann","M. Kramer","C. Krieger","D. Leppla-Weber","S. Martens","E. \u00d6z","N. Salama","A. Schmidt","H. Wang","G. Wieching"],"raw_abstract":"The reciprocity approach is a powerful method to determine the expected\nsignal power of axion haloscopes in a model-independent way. Especially for\nopen and broadband setups like the MADMAX dielectric haloscope the sensitivity\nto the axion field is difficult to calibrate since they do not allow discrete\neigenmode analysis and are optically too large to fully simulate. The central\nidea of the reciprocity approach is to measure a reflection-induced test field\nin the setup instead of trying to simulate the axion-induced field. In this\narticle, the reciprocity approach is used to determine the expected signal\npower of a dish antenna and a minimal dielectric haloscope directly from\nmeasurements. The results match expectations from simulation but also include\nimportant systematic effects that are too difficult to simulate. In particular,\nthe effect of antenna standing waves and higher order mode perturbations can be\nquantified for the first time in a dielectric haloscope.","publication_date":1700657557,"paper_link":"http://arxiv.org/pdf/2311.13359v1","categories":["Physics"],"abstract":"The reciprocity approach is a powerful method to determine the expected signal power of axion haloscopes in a model-independent way. Especially for open and broadband setups like the MADMAX dielectric haloscope the sensitivity to the axion field is difficult to calibrate since they do not allow discrete eigenmode analysis and are optically too large to fully simulate. The central idea of the reciprocity approach is to measure a reflection-induced test field in the setup instead of trying to simulate the axion-induced field. In this article, the reciprocity approach is used to determine the expected signal power of a dish antenna and a minimal dielectric haloscope directly from measurements. The results match expectations from simulation but also include important systematic effects that are too difficult to simulate. In particular, the effect of antenna standing waves and higher order mode perturbations can be quantified for the first time in a dielectric haloscope."}
{"title":"Irregular Fibonacci Conformal Blocks","authors":["Xia Gu","Babak Haghighat","Kevin Loo"],"raw_abstract":"This work studies Liouville conformal blocks of irregular type with the\ninsertion of at least one level-$3$ degenerate field admitting a Fibonacci\nfusion rule. We algebraically derive the corresponding third-order BPZ\nequations for regular blocks and their modifications when a rank one irregular\noperator is inserted. Employing Lefschetz thimbles as integration cycles, we\nthen successively proceed to construct integral representations and prove that\nthey satisfy the corresponding BPZ equations. Finally, we show that taking a\nsemiclassical limit, these integral representations can be expressed in terms\nof Heun functions and have correct leading behaviors consistent with conformal\nweights and fusion rules.","publication_date":1700657419,"paper_link":"http://arxiv.org/pdf/2311.13358v1","categories":["Physics"],"abstract":"This work studies Liouville conformal blocks of irregular type with the insertion of at least one level-__FORMULA__ degenerate field admitting a Fibonacci fusion rule. We algebraically derive the corresponding third-order BPZ equations for regular blocks and their modifications when a rank one irregular operator is inserted. Employing Lefschetz thimbles as integration cycles, we then successively proceed to construct integral representations and prove that they satisfy the corresponding BPZ equations. Finally, we show that taking a semiclassical limit, these integral representations can be expressed in terms of Heun functions and have correct leading behaviors consistent with conformal weights and fusion rules."}
{"title":"\"Energy transfers in surface wave-averaged equations\"","authors":["Lars Czeschel","Carsten Eden"],"raw_abstract":"Ocean surface gravity waves play an important role for the air-sea momentum\nfluxes and the upper ocean mixing, and knowledge of the sea state leads in\ngeneral circulation models to improved estimates of the ocean energy budget and\nallows to incorporate surface wave impacts, such as Langmuir turbulence.\nHowever, including the Stokes drift, in phase-averaged equations for the\nEulerian mean motion leads to an Eulerian energy budget which is physically\ndifficult to interpret. In this note, we show that a Lagrangian energy budget\nallows for a closed energy budget, in which all terms connecting the different\nenergy compartments correspond to well known energy transfer terms. We show\nthat the so-called Coriolis-Stokes force does not lead to an energy transfer\nbetween surface gravity waves and oceanic mean motions as previously suggested.\nIn an energy budget for the Lagrangian mean kinetic energy, the work done by\nthe Coriolis-Stokes force does not contribute, and should be used to estimate\nthe kinetic energy balance in the wave affected surface mixed layer. The\nLagrangian energy budget is used to discuss an energetically consistent\nframework, which can be used to couple a general circulation ocean model to a\nsurface wave model.","publication_date":1700657150,"paper_link":"http://arxiv.org/pdf/2311.13354v1","categories":["Physics"],"abstract":"Ocean surface gravity waves play an important role for the air-sea momentum fluxes and the upper ocean mixing, and knowledge of the sea state leads in general circulation models to improved estimates of the ocean energy budget and allows to incorporate surface wave impacts, such as Langmuir turbulence. However, including the Stokes drift, in phase-averaged equations for the Eulerian mean motion leads to an Eulerian energy budget which is physically difficult to interpret. In this note, we show that a Lagrangian energy budget allows for a closed energy budget, in which all terms connecting the different energy compartments correspond to well known energy transfer terms. We show that the so-called Coriolis-Stokes force does not lead to an energy transfer between surface gravity waves and oceanic mean motions as previously suggested. In an energy budget for the Lagrangian mean kinetic energy, the work done by the Coriolis-Stokes force does not contribute, and should be used to estimate the kinetic energy balance in the wave affected surface mixed layer. The Lagrangian energy budget is used to discuss an energetically consistent framework, which can be used to couple a general circulation ocean model to a surface wave model."}
{"title":"Many-body physics-induced selection rules: application to Raman spectroscopy","authors":["Igor Benek-Lins","Saurabh Maiti"],"raw_abstract":"Spectroscopic measurements in quantum systems are subject to selection rules,\nusually based on space-time symmetries, that allow or disallow transitions\nbetween states. In many-body systems, in addition to the single-particle\nstates, there emerge new ones due to collective excitations of the system. Here\nwe demonstrate the existence of a \"fragile\" selection rule that emerges as a\nmanifestation of many-body effects and outlines the conditions for collective\nexcitations to couple to a given spectroscopic probe beyond the usual symmetry\nconsiderations. As an example, we apply the rule to Raman spectroscopy of\nmultiband superconductors and settle some unresolved features in experiments.","publication_date":1700655652,"paper_link":"http://arxiv.org/pdf/2311.13345v1","categories":["Physics"],"abstract":"Spectroscopic measurements in quantum systems are subject to selection rules, usually based on space-time symmetries, that allow or disallow transitions between states. In many-body systems, in addition to the single-particle states, there emerge new ones due to collective excitations of the system. Here we demonstrate the existence of a \"fragile\" selection rule that emerges as a manifestation of many-body effects and outlines the conditions for collective excitations to couple to a given spectroscopic probe beyond the usual symmetry considerations. As an example, we apply the rule to Raman spectroscopy of multiband superconductors and settle some unresolved features in experiments."}
{"title":"Measurements of chemical potentials in Pb-Pb collisions at $\\sqrt{s_{\\rm NN}} = 5.02$ TeV","authors":["ALICE Collaboration"],"raw_abstract":"This Letter presents the most precise measurement to date of the\nmatter/antimatter imbalance at midrapidity in Pb-Pb collisions at a\ncenter-of-mass energy per nucleon pair $\\sqrt{s_{\\rm NN}} = 5.02$ TeV. Using\nthe Statistical Hadronization framework, it is possible to obtain the value of\nthe electric charge and baryon chemical potentials, $\\mu_Q=-0.18\\pm0.90$ MeV\nand $\\mu_B=0.71\\pm0.45$ MeV, with unprecedented precision. A\ncentrality-differential study of the antiparticle-to-particle yield ratios of\ncharged pions, protons, $\\Omega$-baryons, and light (hyper)nuclei is performed.\nThese results indicate that the system created in Pb-Pb collisions at the LHC\nis on average baryon-free and electrically neutral at midrapidity.","publication_date":1700654036,"paper_link":"http://arxiv.org/pdf/2311.13332v1","categories":["Physics"],"abstract":"This Letter presents the most precise measurement to date of the matter/antimatter imbalance at midrapidity in Pb-Pb collisions at a center-of-mass energy per nucleon pair __FORMULA__ TeV. Using the Statistical Hadronization framework, it is possible to obtain the value of the electric charge and baryon chemical potentials, __FORMULA__ MeV and __FORMULA__ MeV, with unprecedented precision. A centrality-differential study of the antiparticle-to-particle yield ratios of charged pions, protons, __FORMULA__-baryons, and light (hyper)nuclei is performed. These results indicate that the system created in Pb-Pb collisions at the LHC is on average baryon-free and electrically neutral at midrapidity."}
{"title":"Multiplicity dependence of charged-particle intra-jet properties in pp collisions at $\\sqrt{s}$ = 13 TeV","authors":["ALICE Collaboration"],"raw_abstract":"The first measurement of the multiplicity dependence of intra-jet properties\nof leading charged-particle jets in proton-proton (pp) collisions is reported.\nThe mean charged-particle multiplicity and jet fragmentation distributions are\nmeasured in minimum-bias and high-multiplicity pp collisions at $\\sqrt{s}$ = 13\nTeV using the ALICE detector. Jets are reconstructed from charged particles\nproduced in the midrapidity region ($|\\eta| < 0.9$) using the sequential\nrecombination anti-$k_{\\rm T}$ algorithm with jet resolution parameters $R$ =\n0.2, 0.3, and 0.4 for the transverse momentum ($p_{\\rm T}$) interval 5$-$110\nGeV/$c$. High-multiplicity events are selected by the forward V0 scintillator\ndetectors. The mean charged-particle multiplicity inside the leading jet cone\nrises monotonically with increasing jet $p_{\\rm T}$ in qualitative agreement\nwith previous measurements at lower energies. The distributions of jet\nfragmentation functions $z^{\\rm ch}$ and $\\xi^{\\rm ch}$ are measured for\ndifferent jet-$p_{\\rm T}$ intervals. Jet-$p_{\\rm T}$ independent fragmentation\nof leading jets is observed for wider jets except at high- and low-$z^{\\rm\nch}$. The observed \"hump-backed plateau\" structure in the $\\xi^{\\rm ch}$\ndistribution indicates suppression of low-$p_{\\rm T}$ particles. In\nhigh-multiplicity events, an enhancement of the fragmentation probability of\nlow-$z^{\\rm ch}$ particles accompanied by a suppression of high-$z^{\\rm ch}$\nparticles is observed compared to minimum-bias events. This behavior becomes\nmore prominent for low-$p_{\\rm T}$ jets with larger jet radius. The results are\ncompared with predictions of QCD-inspired event generators, PYTHIA 8 with\nMonash 2013 tune and EPOS LHC. It is found that PYTHIA 8 qualitatively\nreproduces the jet modification in high-multiplicity events except at high jet\n$p_{\\rm T}$. These measurements provide important constraints to models of jet\nfragmentation.","publication_date":1700652251,"paper_link":"http://arxiv.org/pdf/2311.13322v1","categories":["Physics"],"abstract":"The first measurement of the multiplicity dependence of intra-jet properties of leading charged-particle jets in proton-proton (pp) collisions is reported. The mean charged-particle multiplicity and jet fragmentation distributions are measured in minimum-bias and high-multiplicity pp collisions at __FORMULA__ = 13 TeV using the ALICE detector. Jets are reconstructed from charged particles produced in the midrapidity region (__FORMULA__) using the sequential recombination anti-__FORMULA__ algorithm with jet resolution parameters __FORMULA__ = 0.2, 0.3, and 0.4 for the transverse momentum (__FORMULA__) interval 5__FORMULA__110 GeV/__FORMULA__. High-multiplicity events are selected by the forward V0 scintillator detectors. The mean charged-particle multiplicity inside the leading jet cone rises monotonically with increasing jet __FORMULA__ in qualitative agreement with previous measurements at lower energies. The distributions of jet fragmentation functions __FORMULA__ and __FORMULA__ are measured for different jet-__FORMULA__ intervals. Jet-__FORMULA__ independent fragmentation of leading jets is observed for wider jets except at high- and low-__FORMULA__. The observed \"hump-backed plateau\" structure in the __FORMULA__ distribution indicates suppression of low-__FORMULA__ particles. In high-multiplicity events, an enhancement of the fragmentation probability of low-__FORMULA__ particles accompanied by a suppression of high-__FORMULA__ particles is observed compared to minimum-bias events. This behavior becomes more prominent for low-__FORMULA__ jets with larger jet radius. The results are compared with predictions of QCD-inspired event generators, PYTHIA 8 with Monash 2013 tune and EPOS LHC. It is found that PYTHIA 8 qualitatively reproduces the jet modification in high-multiplicity events except at high jet __FORMULA__. These measurements provide important constraints to models of jet fragmentation."}
{"title":"All-Dielectric Structural Coloration Empowered by Bound States in the Continuum","authors":["Hong Zheng","Haiyang Hu","Thomas Weber","Juan Wang","Lin Nan","Bingsuo Zou","Stefan A. Maier","Andreas Tittl"],"raw_abstract":"The technological requirements of low-power and high-fidelity color displays\nhave been instrumental in driving research into advanced coloration\ntechnologies. At the forefront of these developments is the implementation of\ndye-free coloration techniques, which overcome previous constraints related to\ninsufficient resolution and color fading. In this context, resonant dielectric\nnanostructures have emerged as a promising paradigm, showing great potential\nfor high efficiency, remarkably high color saturation, wide gamut palette, and\nrealistic image reproduction. However, they still face limitations related to\ncolor accuracy, purity, and simultaneous brightness tunability. Here, we\ndemonstrate an all-dielectric metasurface empowered by photonic bound states in\nthe continuum (BICs), which supports sharp resonances throughout the visible\nspectral range, ideally suited for producing a wide range of structural colors.\nThe metasurface design consists of titanium dioxide (TiO2) ellipses with\ncarefully controlled sizes and geometrical asymmetry, allowing versatile and\non-demand variation of the brightness and hue of the output colors,\nrespectively.","publication_date":1700651378,"paper_link":"http://arxiv.org/pdf/2311.13315v1","categories":["Physics"],"abstract":"The technological requirements of low-power and high-fidelity color displays have been instrumental in driving research into advanced coloration technologies. At the forefront of these developments is the implementation of dye-free coloration techniques, which overcome previous constraints related to insufficient resolution and color fading. In this context, resonant dielectric nanostructures have emerged as a promising paradigm, showing great potential for high efficiency, remarkably high color saturation, wide gamut palette, and realistic image reproduction. However, they still face limitations related to color accuracy, purity, and simultaneous brightness tunability. Here, we demonstrate an all-dielectric metasurface empowered by photonic bound states in the continuum (BICs), which supports sharp resonances throughout the visible spectral range, ideally suited for producing a wide range of structural colors. The metasurface design consists of titanium dioxide (TiO2) ellipses with carefully controlled sizes and geometrical asymmetry, allowing versatile and on-demand variation of the brightness and hue of the output colors, respectively."}
{"title":"Liquid-solid friction on crystalline surfaces: a perspective","authors":["Mathieu Liz\u00e9e","Alessandro Siria"],"raw_abstract":"Liquids flowing on solid surfaces experience a friction force. Whereas solid\nfriction is familiar to anyone gifted with the sense of touch, liquid friction\nis much more exotic. Although it was long believed to be infinite, meaning that\ninterfacial liquid molecules stick to solid surfaces, we have known for a few\ndecades that this is not the case and that some materials show extreme liquid\nslippage, which implies a dramatic enhancement of nanoscale pipes' permeability\nto liquid flows. Harnessing liquid friction bears the promise of\nhigh-efficiency membrane separation processes, heat recovery systems, or blue\nenergy harvesting, turning it into a highly strategic field to reduce carbon\nemissions and meet the climate emergency.","publication_date":1700650981,"paper_link":"http://arxiv.org/pdf/2311.13311v1","categories":["Physics"],"abstract":"Liquids flowing on solid surfaces experience a friction force. Whereas solid friction is familiar to anyone gifted with the sense of touch, liquid friction is much more exotic. Although it was long believed to be infinite, meaning that interfacial liquid molecules stick to solid surfaces, we have known for a few decades that this is not the case and that some materials show extreme liquid slippage, which implies a dramatic enhancement of nanoscale pipes' permeability to liquid flows. Harnessing liquid friction bears the promise of high-efficiency membrane separation processes, heat recovery systems, or blue energy harvesting, turning it into a highly strategic field to reduce carbon emissions and meet the climate emergency."}
{"title":"Buildup and dephasing of Floquet-Bloch bands on subcycle time scales","authors":["S. Ito","M. Sch\u00fcler","M. Meierhofer","S. Schlauderer","J. Freudenstein","J. Reimann","D. Afanasiev","K. A. Kokh","O. E. Tereshchenko","J. G\u00fcdde","M. A. Sentef","U. H\u00f6fer","R. Huber"],"raw_abstract":"Strong light fields have created spectacular opportunities to tailor novel\nfunctionalities of solids. Floquet-Bloch states can form under periodic driving\nof electrons and enable exotic quantum phases. On subcycle time scales,\nlightwaves can simultaneously drive intraband currents and interband\ntransitions, which enable high-harmonic generation (HHG) and pave the way\ntowards ultrafast electronics. Yet, the interplay of intra- and interband\nexcitations as well as their relation with Floquet physics have been key open\nquestions as dynamical aspects of Floquet states have remained elusive. Here we\nprovide this pivotal link by pioneering the ultrafast buildup of Floquet-Bloch\nbands with time- and angle-resolved photoemission spectroscopy. We drive\nsurface states on a topological insulator with mid-infrared fields - strong\nenough for HHG - and directly monitor the transient band structure with\nsubcycle time resolution. Starting with strong intraband currents, we observe\nhow Floquet sidebands emerge within a single optical cycle; intraband\nacceleration simultaneously proceeds in multiple sidebands until high-energy\nelectrons scatter into bulk states and dissipation destroys the Floquet bands.\nQuantum nonequilibrium calculations explain the simultaneous occurrence of\nFloquet states with intra- and interband dynamics. Our joint experiment-theory\nstudy opens up a direct time-domain view of Floquet physics and explores the\nfundamental frontiers of ultrafast band-structure engineering.","publication_date":1700650796,"paper_link":"http://arxiv.org/pdf/2311.13309v1","categories":["Physics"],"abstract":"Strong light fields have created spectacular opportunities to tailor novel functionalities of solids. Floquet-Bloch states can form under periodic driving of electrons and enable exotic quantum phases. On subcycle time scales, lightwaves can simultaneously drive intraband currents and interband transitions, which enable high-harmonic generation (HHG) and pave the way towards ultrafast electronics. Yet, the interplay of intra- and interband excitations as well as their relation with Floquet physics have been key open questions as dynamical aspects of Floquet states have remained elusive. Here we provide this pivotal link by pioneering the ultrafast buildup of Floquet-Bloch bands with time- and angle-resolved photoemission spectroscopy. We drive surface states on a topological insulator with mid-infrared fields - strong enough for HHG - and directly monitor the transient band structure with subcycle time resolution. Starting with strong intraband currents, we observe how Floquet sidebands emerge within a single optical cycle; intraband acceleration simultaneously proceeds in multiple sidebands until high-energy electrons scatter into bulk states and dissipation destroys the Floquet bands. Quantum nonequilibrium calculations explain the simultaneous occurrence of Floquet states with intra- and interband dynamics. Our joint experiment-theory study opens up a direct time-domain view of Floquet physics and explores the fundamental frontiers of ultrafast band-structure engineering."}
{"title":"Superconductivity and Charge-density-wave-like Transition in Th2Cu4As5","authors":["Qing-Chen Duan","Shao-Hua Liu","Bai-Zhuo Li","Jiao-Jiao Meng","Wu-Zhang Yang","Yi Liu","Yi-Qiang Lin","Si-Qi Wu","Jia-Yi Lu","Jin-Ke Bao","Yu-Sen Xiao","Xin-Yu Zhao","Yu-Xue Mei","Yu-Ping Sun","Dan Yu","Shu-Gang Tan","Qiang Jing","Rui-Dan Zhong","Yong-Liang Chen","Yong Zhao","Zhi Ren","Cao Wang","Guang-Han Cao"],"raw_abstract":"We report the synthesis, crystal structure, and physical properties of a\nnovel ternary compound, Th$_2$Cu$_4$As$_5$. The material crystallizes in a\ntetragonal structure with lattice parameters $a=4.0716(1)$ {\\AA} and\n$c=24.8131(4)$ {\\AA}. Its structure can be described as an alternating stacking\nof fluorite-type Th$_2$As$_2$ layers with antifluorite-type double-layered\nCu$_4$As$_3$ slabs. The measurement of electrical resistivity, magnetic\nsusceptibility and specific heat reveals that Th$_2$Cu$_4$As$_5$ undergoes bulk\nsuperconducting transition at 4.2 K. Moreover, all these physical quantities\nexhibit anomalies at 48 K, where the Hall coefficient change the sign. These\nfindings suggest a charge-density-wave-like (CDW) transition, making\nTh$_2$Cu$_4$As$_5$ a rare example for studying the interplay between CDW and\nsuperconductivity.","publication_date":1700650566,"paper_link":"http://arxiv.org/pdf/2311.13308v1","categories":["Physics"],"abstract":"We report the synthesis, crystal structure, and physical properties of a novel ternary compound, Th__FORMULA__Cu__FORMULA__As__FORMULA__. The material crystallizes in a tetragonal structure with lattice parameters __FORMULA__ {\\AA} and __FORMULA__ {\\AA}. Its structure can be described as an alternating stacking of fluorite-type Th__FORMULA__As__FORMULA__ layers with antifluorite-type double-layered Cu__FORMULA__As__FORMULA__ slabs. The measurement of electrical resistivity, magnetic susceptibility and specific heat reveals that Th__FORMULA__Cu__FORMULA__As__FORMULA__ undergoes bulk superconducting transition at 4.2 K. Moreover, all these physical quantities exhibit anomalies at 48 K, where the Hall coefficient change the sign. These findings suggest a charge-density-wave-like (CDW) transition, making Th__FORMULA__Cu__FORMULA__As__FORMULA__ a rare example for studying the interplay between CDW and superconductivity."}
{"title":"A tale of many $H_0$","authors":["Licia Verde","Nils Sch\u00f6neberg","H\u00e9ctor Gil-Mar\u00edn"],"raw_abstract":"The Hubble parameter $H_0$, is not a univocally-defined quantity: it relates\nredshifts to distances in the near Universe, but is also a key parameter of the\n$\\Lambda$CDM standard cosmological model. As such, $H_0$ affects several\nphysical processes at different cosmic epochs, and multiple observables. We\nhave counted more than a dozen $H_0$'s which are expected to agree if a) there\nare no significant systematics in the data and their interpretation and b) the\nadopted cosmological model is correct.\n  With few exceptions (proverbially confirming the rule) these determinations\ndo not agree at high statistical significance; their values cluster around two\ncamps: the low (68 km/s/Mpc) and high (73 km/s/Mpc) camp. It appears to be a\nmatter of anchors: the shape of the Universe expansion history agrees with the\nmodel, it is the normalizations that disagree.\n  Beyond systematics in the data/analysis, if the model is incorrect there are\nonly two viable ways to \"fix\" it: by changing the early time ($z\\gtrsim 1100$)\nphysics and thus the early time normalization, or by a global modification,\npossibly touching the model's fundamental assumptions (e.g., homogeneity,\nisotropy, gravity). None of these three options has the consensus of the\ncommunity.\n  The research community has been actively looking for deviations from\n$\\Lambda$CDM for two decades; the one we might have found makes us wish we\ncould put the genie back in the bottle.","publication_date":1700650439,"paper_link":"http://arxiv.org/pdf/2311.13305v1","categories":["Physics"],"abstract":"The Hubble parameter __FORMULA__, is not a univocally-defined quantity: it relates redshifts to distances in the near Universe, but is also a key parameter of the __FORMULA__CDM standard cosmological model. As such, __FORMULA__ affects several physical processes at different cosmic epochs, and multiple observables. We have counted more than a dozen __FORMULA__'s which are expected to agree if a) there are no significant systematics in the data and their interpretation and b) the adopted cosmological model is correct.   With few exceptions (proverbially confirming the rule) these determinations do not agree at high statistical significance; their values cluster around two camps: the low (68 km/s/Mpc) and high (73 km/s/Mpc) camp. It appears to be a matter of anchors: the shape of the Universe expansion history agrees with the model, it is the normalizations that disagree.   Beyond systematics in the data/analysis, if the model is incorrect there are only two viable ways to \"fix\" it: by changing the early time (__FORMULA__) physics and thus the early time normalization, or by a global modification, possibly touching the model's fundamental assumptions (e.g., homogeneity, isotropy, gravity). None of these three options has the consensus of the community.   The research community has been actively looking for deviations from __FORMULA__CDM for two decades; the one we might have found makes us wish we could put the genie back in the bottle."}
{"title":"Observation of magnetically-induced transition intensity redistribution in the onset of the hyperfine Paschen-Back regime","authors":["Armen Sargsyan","Emmanuel Klinger","Ara Tonoyan","David Sarkisyan"],"raw_abstract":"The Zeeman effect is an important topic in atomic spectroscopy. The induced\nchange in transition frequencies and amplitudes finds applications in the\nEarth-field-range magnetometry. At intermediate magnetic field amplitude $B\\sim\nB_0 = A_\\text{hfs}/\\mu_B$, where $A_\\text{hfs}$ is the magnetic dipole constant\nof the ground state, and $\\mu_B$ is the Bohr magneton ($B_0\\approx 1.7$ kG for\nCs), the rigorous rule $\\Delta F = 0, \\pm1$ is affected by the coupling between\nmagnetic sub-levels induced by the field. Transitions satisfying $\\Delta F =\n\\pm2$, referred to as magnetically-induced transitions, can be observed. Here,\nwe show that a significant redistribution of the Cs $6\\text{S}_{1/2}\\rightarrow\n6\\text{P}_{3/2}$ magnetically-induced transition intensities occurs with\nincreasing magnetic field. We observe that the strongest transition in the\ngroup $F_g=3\\rightarrow F_e=5$ ($\\sigma^+$ polarization) for $B<B_0$ cease to\nbe the strongest for $B>3 B_0$. On the other hand, the strongest transition in\nthe group $F_g=2\\rightarrow F_e=4$ ($\\sigma^-$ polarization) remains so for all\nour measurements with magnetic fields up to 9 kG. These results are in\nagreement with a theoretical model. The model predicts that similar\nobservations can be made for all alkali metals, including Na, K and Rb atoms.\nOur findings are important for magnetometers utilizing the Zeeman effect above\nEarth field, following the rapid development of micro-machined vapor-cell-based\nsensors.","publication_date":1700648024,"paper_link":"http://arxiv.org/pdf/2311.13288v1","categories":["Physics"],"abstract":"The Zeeman effect is an important topic in atomic spectroscopy. The induced change in transition frequencies and amplitudes finds applications in the Earth-field-range magnetometry. At intermediate magnetic field amplitude __FORMULA__, where __FORMULA__ is the magnetic dipole constant of the ground state, and __FORMULA__ is the Bohr magneton (__FORMULA__ kG for Cs), the rigorous rule __FORMULA__ is affected by the coupling between magnetic sub-levels induced by the field. Transitions satisfying __FORMULA__, referred to as magnetically-induced transitions, can be observed. Here, we show that a significant redistribution of the Cs __FORMULA__ magnetically-induced transition intensities occurs with increasing magnetic field. We observe that the strongest transition in the group __FORMULA__ (__FORMULA__ polarization) for __FORMULA__ cease to be the strongest for __FORMULA__. On the other hand, the strongest transition in the group __FORMULA__ (__FORMULA__ polarization) remains so for all our measurements with magnetic fields up to 9 kG. These results are in agreement with a theoretical model. The model predicts that similar observations can be made for all alkali metals, including Na, K and Rb atoms. Our findings are important for magnetometers utilizing the Zeeman effect above Earth field, following the rapid development of micro-machined vapor-cell-based sensors."}
{"title":"Improving performance of heart rate time series classification by grouping subjects","authors":["Michael Beekhuizen","Arman Naseri","David Tax","Ivo van der Bilt","Marcel Reinders"],"raw_abstract":"Unlike the more commonly analyzed ECG or PPG data for activity\nclassification, heart rate time series data is less detailed, often noisier and\ncan contain missing data points. Using the BigIdeasLab_STEP dataset, which\nincludes heart rate time series annotated with specific tasks performed by\nindividuals, we sought to determine if general classification was achievable.\nOur analyses showed that the accuracy is sensitive to the choice of\nwindow/stride size. Moreover, we found variable classification performances\nbetween subjects due to differences in the physical structure of their hearts.\nVarious techniques were used to minimize this variability. First of all,\nnormalization proved to be a crucial step and significantly improved the\nperformance. Secondly, grouping subjects and performing classification inside a\ngroup helped to improve performance and decrease inter-subject variability.\nFinally, we show that including handcrafted features as input to a deep\nlearning (DL) network improves the classification performance further.\nTogether, these findings indicate that heart rate time series can be utilized\nfor classification tasks like predicting activity. However, normalization or\ngrouping techniques need to be chosen carefully to minimize the issue of\nsubject variability.","publication_date":1700647713,"paper_link":"http://arxiv.org/pdf/2311.13285v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Unlike the more commonly analyzed ECG or PPG data for activity classification, heart rate time series data is less detailed, often noisier and can contain missing data points. Using the BigIdeasLab_STEP dataset, which includes heart rate time series annotated with specific tasks performed by individuals, we sought to determine if general classification was achievable. Our analyses showed that the accuracy is sensitive to the choice of window/stride size. Moreover, we found variable classification performances between subjects due to differences in the physical structure of their hearts. Various techniques were used to minimize this variability. First of all, normalization proved to be a crucial step and significantly improved the performance. Secondly, grouping subjects and performing classification inside a group helped to improve performance and decrease inter-subject variability. Finally, we show that including handcrafted features as input to a deep learning (DL) network improves the classification performance further. Together, these findings indicate that heart rate time series can be utilized for classification tasks like predicting activity. However, normalization or grouping techniques need to be chosen carefully to minimize the issue of subject variability."}
{"title":"Spectroscopic and Interferometric Sum-Frequency Imaging of Strongly Coupled Phonon Polaritons in SiC Metasurfaces","authors":["Richarda Niemann","Niclas S. Mueller","S\u00f6ren Wasserroth","Guanyu Lu","Martin Wolf","Joshua D. Caldwell","Alexander Paarmann"],"raw_abstract":"Phonon polaritons enable waveguiding and localization of infrared light with\nextreme confinement and low losses. The spatial propagation and spectral\nresonances of such polaritons are usually probed with complementary techniques\nsuch as near-field optical microscopy and far-field reflection spectroscopy.\nHere, we introduce infrared-visible sum-frequency spectro-microscopy as a tool\nfor spectroscopic imaging of phonon polaritons. The technique simultaneously\nprovides sub-wavelength spatial resolution and highly-resolved spectral\nresonance information. This is implemented by resonantly exciting polaritons\nusing a tunable infrared laser and wide-field microscopic detection of the\nupconverted light. We employ this technique to image hybridization and strong\ncoupling of localized and propagating surface phonon polaritons in metasurfaces\nof SiC micropillars. Spectro-microscopy allows us to measure the polariton\ndispersion simultaneously in momentum space by angle-dependent resonance\nimaging, and in real space by polariton interferometry. Notably, we directly\nvisualize how strong coupling affects the spatial localization of polaritons,\ninaccessible with conventional spectroscopic techniques. We further observe the\nformation of edge states at excitation frequencies where strong coupling\nprevents polariton propagation into the metasurface. Our approach is applicable\nto the wide range of polaritonic materials with broken inversion symmetry and\ncan be used as a fast and non-perturbative tool to image polariton\nhybridization and propagation.","publication_date":1700647655,"paper_link":"http://arxiv.org/pdf/2311.13284v1","categories":["Physics"],"abstract":"Phonon polaritons enable waveguiding and localization of infrared light with extreme confinement and low losses. The spatial propagation and spectral resonances of such polaritons are usually probed with complementary techniques such as near-field optical microscopy and far-field reflection spectroscopy. Here, we introduce infrared-visible sum-frequency spectro-microscopy as a tool for spectroscopic imaging of phonon polaritons. The technique simultaneously provides sub-wavelength spatial resolution and highly-resolved spectral resonance information. This is implemented by resonantly exciting polaritons using a tunable infrared laser and wide-field microscopic detection of the upconverted light. We employ this technique to image hybridization and strong coupling of localized and propagating surface phonon polaritons in metasurfaces of SiC micropillars. Spectro-microscopy allows us to measure the polariton dispersion simultaneously in momentum space by angle-dependent resonance imaging, and in real space by polariton interferometry. Notably, we directly visualize how strong coupling affects the spatial localization of polaritons, inaccessible with conventional spectroscopic techniques. We further observe the formation of edge states at excitation frequencies where strong coupling prevents polariton propagation into the metasurface. Our approach is applicable to the wide range of polaritonic materials with broken inversion symmetry and can be used as a fast and non-perturbative tool to image polariton hybridization and propagation."}
{"title":"Impact of primordial black holes on heavy neutral leptons searches in the framework of resonant leptogenesis","authors":["Roberta Calabrese","Marco Chianese","Jacob Gunn","Gennaro Miele","Stefano Morisi","Ninetta Saviano"],"raw_abstract":"We investigate the effects on sub-TeV resonant leptogenesis of Primordial\nBlack Holes with masses from $10^6$ to $\\sim 10^9$ g. The latter might dominate\nthe energy content of the Universe altering its evolution and, eventually,\ndiluting the final baryon asymmetry. We find that, depending on the mass and\nabundance of Primordial Black Holes, the parameter space of sub-TeV resonant\nleptogenesis shrinks towards higher Right-Handed Neutrino masses and smaller\nactive-sterile mixing. Remarkably, this translates into important implications\nfor the experimental searches of heavy neutral leptons. Conversely, we\ndemonstrate that a possible future detection of sub-TeV heavy neutral leptons\nwould disfavour regions of the parameter space of Primordial Black Holes\ncurrently allowed.","publication_date":1700646721,"paper_link":"http://arxiv.org/pdf/2311.13276v1","categories":["Physics"],"abstract":"We investigate the effects on sub-TeV resonant leptogenesis of Primordial Black Holes with masses from __FORMULA__ to __FORMULA__ g. The latter might dominate the energy content of the Universe altering its evolution and, eventually, diluting the final baryon asymmetry. We find that, depending on the mass and abundance of Primordial Black Holes, the parameter space of sub-TeV resonant leptogenesis shrinks towards higher Right-Handed Neutrino masses and smaller active-sterile mixing. Remarkably, this translates into important implications for the experimental searches of heavy neutral leptons. Conversely, we demonstrate that a possible future detection of sub-TeV heavy neutral leptons would disfavour regions of the parameter space of Primordial Black Holes currently allowed."}
{"title":"First results of the LARES 2 space experiment to test the general theory of relativity","authors":["Ignazio Ciufolini","Claudio Paris","Erricos C. Pavlis","John Ries","Richard Matzner","Antonio Paolozzi","Emiliano Ortore","Giuseppe Bianco","Magdalena Kuzmicz-Cieslak","Vahe Gurzadyan","Roger Penrose"],"raw_abstract":"The LAGEOS 3 (today LARES 2) space experiment was proposed in the eighties by\nthe Physics Department and by the Center of Space Research (CSR) of the\nUniversity of Texas (UT) at Austin and by the Italian Space Agency (ASI) to\ntest and accurately measure frame-dragging, with the strong support of John\nArchibald Wheeler, director of the Center for Theoretical Physics of UT Austin.\nFrame-dragging is an intriguing phenomenon predicted by Einstein's theory of\ngeneral relativity which has fundamental implications in high energy\nastrophysics and in the generation of gravitational waves by spinning black\nholes. LAGEOS 3 was reproposed in 2016 to the Italian Space Agency and to the\nEuropean Space Agency as a technologically much improved version of LAGEOS 3\nunder the name LARES 2 (LAres RElativity Satellite 2) and then successfully\nlaunched in 2022 with the new launch vehicle VEGA C of ASI, ESA and AVIO.\nToday, after almost forty years since the original proposal, we report the\nfirst results of the LARES 2 space experiment to test general relativity. The\nresults are in complete agreement with the predictions of Einstein's\ngravitational theory. Whereas previous results already confirmed the\nframe-dragging prediction, the conceptual relative simplicity of the LARES 2\nexperiment with respect to the previous tests with the LARES and LAGEOS\nsatellites provides a significant advance in the field of tests of general\nrelativity.","publication_date":1700645929,"paper_link":"http://arxiv.org/pdf/2311.13268v1","categories":["Physics"],"abstract":"The LAGEOS 3 (today LARES 2) space experiment was proposed in the eighties by the Physics Department and by the Center of Space Research (CSR) of the University of Texas (UT) at Austin and by the Italian Space Agency (ASI) to test and accurately measure frame-dragging, with the strong support of John Archibald Wheeler, director of the Center for Theoretical Physics of UT Austin. Frame-dragging is an intriguing phenomenon predicted by Einstein's theory of general relativity which has fundamental implications in high energy astrophysics and in the generation of gravitational waves by spinning black holes. LAGEOS 3 was reproposed in 2016 to the Italian Space Agency and to the European Space Agency as a technologically much improved version of LAGEOS 3 under the name LARES 2 (LAres RElativity Satellite 2) and then successfully launched in 2022 with the new launch vehicle VEGA C of ASI, ESA and AVIO. Today, after almost forty years since the original proposal, we report the first results of the LARES 2 space experiment to test general relativity. The results are in complete agreement with the predictions of Einstein's gravitational theory. Whereas previous results already confirmed the frame-dragging prediction, the conceptual relative simplicity of the LARES 2 experiment with respect to the previous tests with the LARES and LAGEOS satellites provides a significant advance in the field of tests of general relativity."}
{"title":"A model-free approach to fingertip slip and disturbance detection for grasp stability inference","authors":["Dounia Kitouni","Mahdi Khoramshahi","Veronique Perdereau"],"raw_abstract":"Robotic capacities in object manipulation are incomparable to those of\nhumans. Besides years of learning, humans rely heavily on the richness of\ninformation from physical interaction with the environment. In particular,\ntactile sensing is crucial in providing such rich feedback. Despite its\npotential contributions to robotic manipulation, tactile sensing is less\nexploited; mainly due to the complexity of the time series provided by tactile\nsensors. In this work, we propose a method for assessing grasp stability using\ntactile sensing. More specifically, we propose a methodology to extract\ntask-relevant features and design efficient classifiers to detect object\nslippage with respect to individual fingertips. We compare two classification\nmodels: support vector machine and logistic regression. We use highly sensitive\nUskin tactile sensors mounted on an Allegro hand to test and validate our\nmethod. Our results demonstrate that the proposed method is effective in\nslippage detection in an online fashion.","publication_date":1700643866,"paper_link":"http://arxiv.org/pdf/2311.13245v1","categories":["Statistics","Electrical Engineering and Systems Science"],"abstract":"Robotic capacities in object manipulation are incomparable to those of humans. Besides years of learning, humans rely heavily on the richness of information from physical interaction with the environment. In particular, tactile sensing is crucial in providing such rich feedback. Despite its potential contributions to robotic manipulation, tactile sensing is less exploited; mainly due to the complexity of the time series provided by tactile sensors. In this work, we propose a method for assessing grasp stability using tactile sensing. More specifically, we propose a methodology to extract task-relevant features and design efficient classifiers to detect object slippage with respect to individual fingertips. We compare two classification models: support vector machine and logistic regression. We use highly sensitive Uskin tactile sensors mounted on an Allegro hand to test and validate our method. Our results demonstrate that the proposed method is effective in slippage detection in an online fashion."}
{"title":"Stability of nanoparticle laden aerosol liquid droplets","authors":["A. J. Archer","B. D. Goddard","R. Roth"],"raw_abstract":"We develop a model for the thermodynamics and evaporation dynamics of aerosol\ndroplets of a liquid such as water, surrounded by the gas. When the temperature\nand the chemical potential (or equivalently the humidity) are such that the\nvapour phase is the thermodynamic equilibrium state, then of course droplets of\nthe pure liquid evaporate over a relatively short time. However, if the\ndroplets also contain nanoparticles or any other non-volatile solute, then the\ndroplets can become thermodynamically stable. We show that the equilibrium\ndroplet size depends strongly on the amount and solubility of the nanoparticles\nwithin, i.e. on the nature of the particle interactions with the liquid, and of\ncourse also on the vapour temperature and chemical potential. We develop a\nsimple thermodynamic model for such droplets and compare predictions with\nresults from a lattice density functional theory that takes as input the same\nparticle interaction properties, finding very good agreement. We also use\ndynamical density functional theory to study the evaporation/condensation\ndynamics of liquid from/to droplets as they equilibrate with the vapour,\nthereby demonstrating droplet stability.","publication_date":1700643113,"paper_link":"http://arxiv.org/pdf/2311.13236v1","categories":["Physics"],"abstract":"We develop a model for the thermodynamics and evaporation dynamics of aerosol droplets of a liquid such as water, surrounded by the gas. When the temperature and the chemical potential (or equivalently the humidity) are such that the vapour phase is the thermodynamic equilibrium state, then of course droplets of the pure liquid evaporate over a relatively short time. However, if the droplets also contain nanoparticles or any other non-volatile solute, then the droplets can become thermodynamically stable. We show that the equilibrium droplet size depends strongly on the amount and solubility of the nanoparticles within, i.e. on the nature of the particle interactions with the liquid, and of course also on the vapour temperature and chemical potential. We develop a simple thermodynamic model for such droplets and compare predictions with results from a lattice density functional theory that takes as input the same particle interaction properties, finding very good agreement. We also use dynamical density functional theory to study the evaporation/condensation dynamics of liquid from/to droplets as they equilibrate with the vapour, thereby demonstrating droplet stability."}
{"title":"Strong Light-Matter Coupling Facilitated Charge Carrier Transport in Cavity Organic Solar Cells","authors":["Yahui Tang","Alexandra Stuart","Timothy van der Laan","Girish Lakhwani"],"raw_abstract":"Strong light-matter coupling has shown great potential for modifying the\nelectro-optical properties of semiconducting materials in recent years. In the\nstrong coupling regime, excitons and cavity photons form new states named\nexciton-polaritons, with their properties a hybrid of each constituent. Herein,\nwe report strong coupling observed in solution-processed donor:acceptor\nbulk-heterojunction organic solar cells (OSCs) evidenced by the observed Rabi\nsplitting of ~300 meV and the effects of strong coupling on OSC operations.\nCombining the transient photovoltage decay measurement and nanosecond transient\nabsorption spectroscopy, our results reveal that the effective charge carrier\nlifetimes are longer in cavity devices, attributed to the reduced bimolecular\nrecombination. It is also found that access to CT state(s) of higher energy is\nenabled in cavity devices. This study demonstrates that strong coupling can\neffectively modify the device- and photo-physics in OSCs and opens a new\npathway for engineering more efficient OSCs.","publication_date":1700643046,"paper_link":"http://arxiv.org/pdf/2311.13235v1","categories":["Physics"],"abstract":"Strong light-matter coupling has shown great potential for modifying the electro-optical properties of semiconducting materials in recent years. In the strong coupling regime, excitons and cavity photons form new states named exciton-polaritons, with their properties a hybrid of each constituent. Herein, we report strong coupling observed in solution-processed donor:acceptor bulk-heterojunction organic solar cells (OSCs) evidenced by the observed Rabi splitting of ~300 meV and the effects of strong coupling on OSC operations. Combining the transient photovoltage decay measurement and nanosecond transient absorption spectroscopy, our results reveal that the effective charge carrier lifetimes are longer in cavity devices, attributed to the reduced bimolecular recombination. It is also found that access to CT state(s) of higher energy is enabled in cavity devices. This study demonstrates that strong coupling can effectively modify the device- and photo-physics in OSCs and opens a new pathway for engineering more efficient OSCs."}
{"title":"Ecosystem transformations in response to environmental fluctuations","authors":["Ikumi Kobayashi"],"raw_abstract":"Ecosystems, which are intricate amalgams of biological communities and their\nsurrounding environments, continually evolve under the influence of their\nmyriad interactions. The world is currently facing intensifying environmental\nfluctuations. Understanding general trends in ecosystem transformations in\nresponse to environmental fluctuations and elucidating the underlying\nmechanisms are thus critical challenges. In this study, we used a model\necosystem approach to investigate ecosystem alterations caused by escalating\nenvironmental fluctuations. We analyzed two distinct models: a stochastic\necosystem model with a spatial structure, and a differential equation model for\nresource competition. We found that environmental fluctuations tend to shift\nmulti-species coexistence toward the dominance of specific species. We also\ncategorized biological species as specialists or generalists and discovered\nthat which of these groups becomes the dominant species depends on the\nintensity and frequency of environmental fluctuations. We also determined that\na qualitative change in the diversity-stability relationship depends on the\nperiod of environmental fluctuations. These results underscore the need to\nexplicitly consider the type of perturbation when discussing ecological\ntransitions and the stability of ecosystems. Our findings advance understanding\nof the mechanisms underlying how environmental changes reshape ecosystems and\noffer insights into ecosystem sustainability in the face of future\nenvironmental perturbations.","publication_date":1700642600,"paper_link":"http://arxiv.org/pdf/2311.13232v1","categories":["Physics"],"abstract":"Ecosystems, which are intricate amalgams of biological communities and their surrounding environments, continually evolve under the influence of their myriad interactions. The world is currently facing intensifying environmental fluctuations. Understanding general trends in ecosystem transformations in response to environmental fluctuations and elucidating the underlying mechanisms are thus critical challenges. In this study, we used a model ecosystem approach to investigate ecosystem alterations caused by escalating environmental fluctuations. We analyzed two distinct models: a stochastic ecosystem model with a spatial structure, and a differential equation model for resource competition. We found that environmental fluctuations tend to shift multi-species coexistence toward the dominance of specific species. We also categorized biological species as specialists or generalists and discovered that which of these groups becomes the dominant species depends on the intensity and frequency of environmental fluctuations. We also determined that a qualitative change in the diversity-stability relationship depends on the period of environmental fluctuations. These results underscore the need to explicitly consider the type of perturbation when discussing ecological transitions and the stability of ecosystems. Our findings advance understanding of the mechanisms underlying how environmental changes reshape ecosystems and offer insights into ecosystem sustainability in the face of future environmental perturbations."}
{"title":"Heat dissipation mechanisms in hybrid superconductor-semiconductor devices revealed by Joule spectroscopy","authors":["Angel Ibabe","Gorm O. Steffensen","Ignacio Casal","Mario Gomez","Thomas Kanne","Jesper Nygard","Alfredo Levy Yeyati","Eduardo J. H. Lee"],"raw_abstract":"Understanding heating and cooling mechanisms in mesoscopic\nsuperconductor-semiconductor hybrid devices is crucial for their application in\nquantum technologies. Owing to the poor thermal conductivity of typical\ndevices, heating effects can drive superconducting-to-normal phase transitions\neven at low applied bias, observed as sharp conductance dips through the loss\nof Andreev excess currents. Tracking such dips across magnetic field, cryostat\ntemperature, and applied microwave power, which constitutes Joule spectroscopy,\nallows to uncover the underlying cooling bottlenecks in different parts of a\ndevice. By applying this technique, we analyze heat dissipation in devices\nbased on full-shell InAs-Al nanowires and reveal that superconducting islands\nare strongly susceptible to heating as their cooling is limited by the rather\ninefficient electron-phonon coupling, as opposed to grounded superconductors\nthat primarily cool by quasiparticle diffusion. Our measurements indicate that\npowers as low as 50-150 pW are able to fully suprpress the superconductivity of\nan island. Finally, we show that applied microwaves lead to similar heating\neffects as DC signals, and explore the interplay of the microwave frequency and\nthe effective electron-phonon relaxation time.","publication_date":1700642104,"paper_link":"http://arxiv.org/pdf/2311.13229v1","categories":["Physics"],"abstract":"Understanding heating and cooling mechanisms in mesoscopic superconductor-semiconductor hybrid devices is crucial for their application in quantum technologies. Owing to the poor thermal conductivity of typical devices, heating effects can drive superconducting-to-normal phase transitions even at low applied bias, observed as sharp conductance dips through the loss of Andreev excess currents. Tracking such dips across magnetic field, cryostat temperature, and applied microwave power, which constitutes Joule spectroscopy, allows to uncover the underlying cooling bottlenecks in different parts of a device. By applying this technique, we analyze heat dissipation in devices based on full-shell InAs-Al nanowires and reveal that superconducting islands are strongly susceptible to heating as their cooling is limited by the rather inefficient electron-phonon coupling, as opposed to grounded superconductors that primarily cool by quasiparticle diffusion. Our measurements indicate that powers as low as 50-150 pW are able to fully suprpress the superconductivity of an island. Finally, we show that applied microwaves lead to similar heating effects as DC signals, and explore the interplay of the microwave frequency and the effective electron-phonon relaxation time."}
{"title":"The behavior of rich-club coefficient in scale-free networks","authors":["Zhihang Liu","Wei Li"],"raw_abstract":"The rich-club phenomenon, which provides information about the association\nbetween nodes, is a useful method to study the hierarchy structure of networks.\nIn this work, we explore the behavior of rich-club coefficient (RCC) in\nscale-free networks, and find that RCC is a power function of degree centrality\nwith exponent eta and a linear function of betweenness centrality with slope\ntheta. Moreover, we calculate the value of RCC in a BA network by deleting\nnodes and obtain a general expression for RCC as a function of node sequence.\nOn this basis, the solution of RCC for centrality is also obtained, which shows\nthat the curve of RCC is determined by the centrality distribution. In order to\nfind out what affects eta and theta, we conduct numerical simulations in\nscale-free networks, and observe: eta and gamma (the degree distribution\nexponent) increase together, theta increases with m (the number of new edges)\nand decreases to convergence as gamma increases.","publication_date":1700641413,"paper_link":"http://arxiv.org/pdf/2311.13221v1","categories":["Physics"],"abstract":"The rich-club phenomenon, which provides information about the association between nodes, is a useful method to study the hierarchy structure of networks. In this work, we explore the behavior of rich-club coefficient (RCC) in scale-free networks, and find that RCC is a power function of degree centrality with exponent eta and a linear function of betweenness centrality with slope theta. Moreover, we calculate the value of RCC in a BA network by deleting nodes and obtain a general expression for RCC as a function of node sequence. On this basis, the solution of RCC for centrality is also obtained, which shows that the curve of RCC is determined by the centrality distribution. In order to find out what affects eta and theta, we conduct numerical simulations in scale-free networks, and observe: eta and gamma (the degree distribution exponent) increase together, theta increases with m (the number of new edges) and decreases to convergence as gamma increases."}
{"title":"Alleviating Barren Plateaus in Parameterized Quantum Machine Learning Circuits: Investigating Advanced Parameter Initialization Strategies","authors":["Muhammad Kashif","Muhammad Rashid","Saif Al-Kuwari","Muhammad Shafique"],"raw_abstract":"Parameterized quantum circuits (PQCs) have emerged as a foundational element\nin the development and applications of quantum algorithms. However, when\ninitialized with random parameter values, PQCs often exhibit barren plateaus\n(BP). These plateaus, characterized by vanishing gradients with an increasing\nnumber of qubits, hinder optimization in quantum algorithms. In this paper, we\nanalyze the impact of state-of-the-art parameter initialization strategies from\nclassical machine learning in random PQCs from the aspect of BP phenomenon. Our\ninvestigation encompasses a spectrum of initialization techniques, including\nrandom, Xavier (both normal and uniform variants), He, LeCun, and Orthogonal\nmethods. Empirical assessment reveals a pronounced reduction in variance decay\nof gradients across all these methodologies compared to the randomly\ninitialized PQCs. Specifically, the Xavier initialization technique outperforms\nthe rest, showing a 62\\% improvement in variance decay compared to the random\ninitialization. The He, Lecun, and orthogonal methods also display\nimprovements, with respective enhancements of 32\\%, 28\\%, and 26\\%. This\ncompellingly suggests that the adoption of these existing initialization\ntechniques holds the potential to significantly amplify the training efficacy\nof Quantum Neural Networks (QNNs), a subclass of PQCs. Demonstrating this\neffect, we employ the identified techniques to train QNNs for learning the\nidentity function, effectively mitigating the adverse effects of BPs. The\ntraining performance, ranked from the best to the worst, aligns with the\nvariance decay enhancement as outlined above. This paper underscores the role\nof tailored parameter initialization in mitigating the BP problem and\neventually enhancing the training dynamics of QNNs.","publication_date":1700640473,"paper_link":"http://arxiv.org/pdf/2311.13218v1","categories":["Physics"],"abstract":"Parameterized quantum circuits (PQCs) have emerged as a foundational element in the development and applications of quantum algorithms. However, when initialized with random parameter values, PQCs often exhibit barren plateaus (BP). These plateaus, characterized by vanishing gradients with an increasing number of qubits, hinder optimization in quantum algorithms. In this paper, we analyze the impact of state-of-the-art parameter initialization strategies from classical machine learning in random PQCs from the aspect of BP phenomenon. Our investigation encompasses a spectrum of initialization techniques, including random, Xavier (both normal and uniform variants), He, LeCun, and Orthogonal methods. Empirical assessment reveals a pronounced reduction in variance decay of gradients across all these methodologies compared to the randomly initialized PQCs. Specifically, the Xavier initialization technique outperforms the rest, showing a 62\\% improvement in variance decay compared to the random initialization. The He, Lecun, and orthogonal methods also display improvements, with respective enhancements of 32\\%, 28\\%, and 26\\%. This compellingly suggests that the adoption of these existing initialization techniques holds the potential to significantly amplify the training efficacy of Quantum Neural Networks (QNNs), a subclass of PQCs. Demonstrating this effect, we employ the identified techniques to train QNNs for learning the identity function, effectively mitigating the adverse effects of BPs. The training performance, ranked from the best to the worst, aligns with the variance decay enhancement as outlined above. This paper underscores the role of tailored parameter initialization in mitigating the BP problem and eventually enhancing the training dynamics of QNNs."}
{"title":"$b \\to s\u03b3$, electron EDM and electroweak baryogenesis: a study in general 2HDM","authors":["George Wei-Shu Hou"],"raw_abstract":"We study inclusive $b \\to s\\gamma$ decay in the context of electron EDM and\nbaryogenesis. The general 2HDM (i.e. without $Z_2$) that possesses an extra set\nof Yukawa matrices can drive electroweak baryogensis via $\\lambda_t {\\rm\nIm}\\rho_{tt}$, where $\\rho_{tt}$ is the extra diagonal top Yukawa coupling,\nwith the $e$EDM constraint evaded by an exquisite flavor cancellation\nmechanism. We touch upon the current status of direct search for exotic $H$,\n$A$ and $H^+$ scalars at the LHC, while for the plethora of flavor observables\nin $g$2HDM, we focus on $b\\to s\\gamma$, pointing out $chiral$ enhancement of a\n$\\rho_{tt}\\rho_{bb}$ effect in $g$2HDM, which can bring in a CPV phase. We\nfirst explore the inclusive ${\\cal B}(b \\to s\\gamma)$ rate, then showcase the\nprogress that Belle II can make in the future, illustrating a potential\n3$\\sigma$ effect for the inclusive $B^+ \\to X_s^+\\gamma$ vs $B^0 \\to\nX_s^0\\gamma$ CPV rate difference, $\\Delta A_{\\rm CP}$. Especially if $e$EDM\nemerges swiftly, perhaps one should pursue further upgrade beyond Belle II.","publication_date":1700639289,"paper_link":"http://arxiv.org/pdf/2311.13210v1","categories":["Physics"],"abstract":"We study inclusive __FORMULA__ decay in the context of electron EDM and baryogenesis. The general 2HDM (i.e. without __FORMULA__) that possesses an extra set of Yukawa matrices can drive electroweak baryogensis via __FORMULA__, where __FORMULA__ is the extra diagonal top Yukawa coupling, with the __FORMULA__EDM constraint evaded by an exquisite flavor cancellation mechanism. We touch upon the current status of direct search for exotic __FORMULA__, __FORMULA__ and __FORMULA__ scalars at the LHC, while for the plethora of flavor observables in __FORMULA__2HDM, we focus on __FORMULA__, pointing out __FORMULA__ enhancement of a __FORMULA__ effect in __FORMULA__2HDM, which can bring in a CPV phase. We first explore the inclusive __FORMULA__ rate, then showcase the progress that Belle II can make in the future, illustrating a potential 3__FORMULA__ effect for the inclusive __FORMULA__ vs __FORMULA__ CPV rate difference, __FORMULA__. Especially if __FORMULA__EDM emerges swiftly, perhaps one should pursue further upgrade beyond Belle II."}
{"title":"Bunch lengthening affected by the short-range effect of resonant modes in radio-frequency cavities","authors":["Tianlong He","Weiwei Li","Zhenghe Bai","Weimin Li"],"raw_abstract":"Longitudinal bunch lengthening via higher harmonic cavities is essential for\nthe new state-of-the-art 4th generation of synchrotron light storage rings, as\nit can effectively improve the Touschek lifetime and mitigate the transverse\nemittance growth due to intrabeam scattering. In general, the optimum or\nnear-optimum bunch lengthening condition is widely adopted for the double\nradio-frequency system. This paper reveals, under this optimum lengthening\ncondition, that the short-range effect of resonant modes of the main and\nharmonic cavities has the potential to enhance or suppress the bunch\nlengthening significantly. Using the planned Hefei Advanced Light Facility\nstorage ring as an example, it is particularly demonstrated that the\nshort-range effects of the main and harmonic fundamental modes can dramatically\ndegrade the bunch lengthening for the assumed case of high-charge bunches. This\ndegradation of bunch lengthening is again presented with a realistic example of\nPETRA-IV that operated in timing mode with high bunch charge. It is found that\nthere exists a setting of harmonic voltage and phase quite different from the\nconventional optimum lengthening setting, to get optimum bunch lengthening.","publication_date":1700638467,"paper_link":"http://arxiv.org/pdf/2311.13207v1","categories":["Physics"],"abstract":"Longitudinal bunch lengthening via higher harmonic cavities is essential for the new state-of-the-art 4th generation of synchrotron light storage rings, as it can effectively improve the Touschek lifetime and mitigate the transverse emittance growth due to intrabeam scattering. In general, the optimum or near-optimum bunch lengthening condition is widely adopted for the double radio-frequency system. This paper reveals, under this optimum lengthening condition, that the short-range effect of resonant modes of the main and harmonic cavities has the potential to enhance or suppress the bunch lengthening significantly. Using the planned Hefei Advanced Light Facility storage ring as an example, it is particularly demonstrated that the short-range effects of the main and harmonic fundamental modes can dramatically degrade the bunch lengthening for the assumed case of high-charge bunches. This degradation of bunch lengthening is again presented with a realistic example of PETRA-IV that operated in timing mode with high bunch charge. It is found that there exists a setting of harmonic voltage and phase quite different from the conventional optimum lengthening setting, to get optimum bunch lengthening."}
{"title":"Analytic formulas for the D-mode Robinson instability","authors":["Tianlong He","Weiwei Li","Zhenghe Bai","Weimin Li"],"raw_abstract":"The passive superconducting harmonic cavity (PSHC) scheme is adopted by\nseveral existing and future synchrotron light source storage rings, as it has a\nrelatively smaller R/Q and a relatively larger quality factor (Q), which can\neffectively reduce the beam-loading effect and suppress the mode-one\ninstability. Based on the mode-zero Robinson instability equation of uniformly\nfilled rigid bunches and a search algorithm for minimum, we have revealed that\nthe PSHC fundamental mode with a large loaded-Q possibly triggers the D-mode\nRobinson instability [T. He, et al., Mode-zero Robinson instability in the\npresence of passive superconducting harmonic cavities, PRAB 26, 064403 (2023)].\nThis D-mode Robinson instability is unique because it is anti-damped by the\nradiation-damping effect. In this paper, analytical formulas for the frequency\nand growth rate of the D-mode Robinson instability are derived with several\nappropriate approximations. These analytical formulas will facilitate analyzing\nand understanding the D-mode Robinson instability. Most importantly, useful\nformulas for the D-mode threshold detuning calculation have finally been found.","publication_date":1700638146,"paper_link":"http://arxiv.org/pdf/2311.13205v1","categories":["Physics"],"abstract":"The passive superconducting harmonic cavity (PSHC) scheme is adopted by several existing and future synchrotron light source storage rings, as it has a relatively smaller R/Q and a relatively larger quality factor (Q), which can effectively reduce the beam-loading effect and suppress the mode-one instability. Based on the mode-zero Robinson instability equation of uniformly filled rigid bunches and a search algorithm for minimum, we have revealed that the PSHC fundamental mode with a large loaded-Q possibly triggers the D-mode Robinson instability [T. He, et al., Mode-zero Robinson instability in the presence of passive superconducting harmonic cavities, PRAB 26, 064403 (2023)]. This D-mode Robinson instability is unique because it is anti-damped by the radiation-damping effect. In this paper, analytical formulas for the frequency and growth rate of the D-mode Robinson instability are derived with several appropriate approximations. These analytical formulas will facilitate analyzing and understanding the D-mode Robinson instability. Most importantly, useful formulas for the D-mode threshold detuning calculation have finally been found."}
{"title":"Reshaping and Enzymatic Activity allow Viruses to move through the Mucus","authors":["Falko Ziebert","Kenan G. Dokonon","Igor M. Kuli\u0107"],"raw_abstract":"Filamentous viruses like influenza and torovirus often display systematic\nbends and arcs of mysterious physical origin. We propose that such viruses\nundergo an instability from a cylindrically symmetric to a toroidally curved\nstate. This \"toro-elastic\" state emerges via a spontaneous symmetry breaking\nunder prestress, induced via short range spike protein interactions and\nmagnified by the filament's surface topography. Once surface stresses become\nsufficiently large, the filament buckles and the toroidal, curved state\nconstitutes a soft mode that can propagate through the filament's material\nframe around a \"mexican-hat\" potential. In the mucus of our airways, glycan\nchains are omnipresent that influenza's spike proteins can bind to and cut. We\nshow that when coupled to such a non-equilibrium chemical reaction, the curved\ntoro-elastic state can attain a spontaneous rotation for sufficiently strong\nenzymatic activity, leading to a whole body reshaping propulsion similar to --\nbut different from -- eukaryotic flagella and spirochetes.","publication_date":1700637562,"paper_link":"http://arxiv.org/pdf/2311.13203v1","categories":["Quantitative Biology","Physics"],"abstract":"Filamentous viruses like influenza and torovirus often display systematic bends and arcs of mysterious physical origin. We propose that such viruses undergo an instability from a cylindrically symmetric to a toroidally curved state. This \"toro-elastic\" state emerges via a spontaneous symmetry breaking under prestress, induced via short range spike protein interactions and magnified by the filament's surface topography. Once surface stresses become sufficiently large, the filament buckles and the toroidal, curved state constitutes a soft mode that can propagate through the filament's material frame around a \"mexican-hat\" potential. In the mucus of our airways, glycan chains are omnipresent that influenza's spike proteins can bind to and cut. We show that when coupled to such a non-equilibrium chemical reaction, the curved toro-elastic state can attain a spontaneous rotation for sufficiently strong enzymatic activity, leading to a whole body reshaping propulsion similar to -- but different from -- eukaryotic flagella and spirochetes."}
{"title":"Broadband dielectric studies in linseed oil: Supercriticality and the New Insight into Melting/Freezing Discontinuous Transition","authors":["Aleksandra Drozd-Rzoska","Sylwester J. Rzoska","Joanna \u0141o\u015b"],"raw_abstract":"The long-range supercritical changes of dielectric constant, resembling ones\nobserved in the isotropic phase of rod-like liquid crystalline compounds, are\nevidenced on approaching liquid-solid discontinuous phase transition. The\n'supercriticality' can be an additional factor for supporting the unique\npro-health properties of linseed oil. It can also be significant for numerous\ntechnological applications. The report also reveals properties significant for\nmelting/freezing discontinuous phase transition cognitive puzzle. Broadband\ndielectric spectroscopy studies revealed long-range premelting (heating from\nthe solid phase) and post-freezing (cooling from the liquid phase) effects with\ncritical-like parametrizations. They can be correlated with the 'grain model'\nfor premelting and its development by the Lipovsky model. The evidence for the\npost-freezing effect, with the critical-like portrayal, may indicate a specific\ngranular solidification associated with pretransitional fluctuations. Numerous\nhallmarks of liquid-liquid and solid-solid phase transitions have also been\nfound. Notably, the melting temperature surrounding is related to the minimum\nand the freezing temperature to the maximum of dielectric loss factor\nD=tan(delta). Regarding dynamics, three primary relaxation processes have been\nfound. Their changes in subsequent temperature intervals, related to the\napparent activation enthalpy, follow critical-like patterns with the same\nsingular temperature. For relaxation times evolution, it leads to optimal\nparameterizations via the 'critical and activated' equation, recently proposed.","publication_date":1700636641,"paper_link":"http://arxiv.org/pdf/2311.13197v1","categories":["Physics"],"abstract":"The long-range supercritical changes of dielectric constant, resembling ones observed in the isotropic phase of rod-like liquid crystalline compounds, are evidenced on approaching liquid-solid discontinuous phase transition. The 'supercriticality' can be an additional factor for supporting the unique pro-health properties of linseed oil. It can also be significant for numerous technological applications. The report also reveals properties significant for melting/freezing discontinuous phase transition cognitive puzzle. Broadband dielectric spectroscopy studies revealed long-range premelting (heating from the solid phase) and post-freezing (cooling from the liquid phase) effects with critical-like parametrizations. They can be correlated with the 'grain model' for premelting and its development by the Lipovsky model. The evidence for the post-freezing effect, with the critical-like portrayal, may indicate a specific granular solidification associated with pretransitional fluctuations. Numerous hallmarks of liquid-liquid and solid-solid phase transitions have also been found. Notably, the melting temperature surrounding is related to the minimum and the freezing temperature to the maximum of dielectric loss factor D=tan(delta). Regarding dynamics, three primary relaxation processes have been found. Their changes in subsequent temperature intervals, related to the apparent activation enthalpy, follow critical-like patterns with the same singular temperature. For relaxation times evolution, it leads to optimal parameterizations via the 'critical and activated' equation, recently proposed."}
{"title":"From integrability to chaos: the quantum-classical correspondence in a triple well bosonic model","authors":["Erick R. Castro","Karin Wittmann W.","Jorge Ch\u00e1vez-Carlos","Itzhak Roditi","Angela Foerster","Jorge G. Hirsch"],"raw_abstract":"In this work, we investigate the semiclassical limit of a simple bosonic\nquantum many-body system exhibiting both integrable and chaotic behavior. A\nclassical Hamiltonian is derived using coherent states. The transition from\nregularity to chaos in classical dynamics is visualized through Poincar\\'e\nsections. Classical trajectories in phase space closely resemble the\nprojections of the Husimi functions of eigenstates with similar energy, even in\nchaotic cases. It is demonstrated that this correlation is more evident when\nprojecting the eigenstates onto the Fock states. The analysis is carried out at\na critical energy where the eigenstates are maximally delocalized in the Fock\nbasis. Despite the imperfect delocalization, its influence is present in the\nclassical quantum properties under investigation. The study systematically\nestablishes quantum-classical correspondence for a bosonic many-body system\nwith more than two wells, even within the chaotic region.","publication_date":1700634660,"paper_link":"http://arxiv.org/pdf/2311.13189v1","categories":["Physics"],"abstract":"In this work, we investigate the semiclassical limit of a simple bosonic quantum many-body system exhibiting both integrable and chaotic behavior. A classical Hamiltonian is derived using coherent states. The transition from regularity to chaos in classical dynamics is visualized through Poincar\\'e sections. Classical trajectories in phase space closely resemble the projections of the Husimi functions of eigenstates with similar energy, even in chaotic cases. It is demonstrated that this correlation is more evident when projecting the eigenstates onto the Fock states. The analysis is carried out at a critical energy where the eigenstates are maximally delocalized in the Fock basis. Despite the imperfect delocalization, its influence is present in the classical quantum properties under investigation. The study systematically establishes quantum-classical correspondence for a bosonic many-body system with more than two wells, even within the chaotic region."}
{"title":"Fractional quantum Hall interface induced by geometric singularity","authors":["Qi Li","Yi Yang","Zhou Li","Hao Wang","Zi-Xiang Hu"],"raw_abstract":"The geometric response of quantum Hall liquids is an important aspect to\nunderstand their topological characteristics in addition to the electromagnetic\nresponse. According to the Wen-Zee theory, the topological spin is coupled to\nthe curvature of the space in which the electrons reside. The presence of\nconical geometry provides a local isolated geometric singularity, making it\nsuitable for exploring the geometric response. In the context of\ntwo-dimensional electrons in a perpendicular magnetic field, each Landau orbit\noccupies the same area. The cone geometry naturally provides a structure in\nwhich the distances between two adjacent orbits gradually change and can be\neasily adjusted by altering the tip angle. The presence of a cone tip\nintroduces a geometric singularity that affects the electron density and\ninteracts with the motion of electrons, which has been extensively studied.\nFurthermore, this type of geometry can automatically create a smooth interface\nor crossover between the crystalline charge-density-wave state and the\nliquid-like fractional quantum Hall state. In this work, the properties of this\ninterface are studied from multiple perspectives, shedding light on the\nbehavior of quantum Hall liquids in such geometric configurations.","publication_date":1700633260,"paper_link":"http://arxiv.org/pdf/2311.13181v1","categories":["Physics"],"abstract":"The geometric response of quantum Hall liquids is an important aspect to understand their topological characteristics in addition to the electromagnetic response. According to the Wen-Zee theory, the topological spin is coupled to the curvature of the space in which the electrons reside. The presence of conical geometry provides a local isolated geometric singularity, making it suitable for exploring the geometric response. In the context of two-dimensional electrons in a perpendicular magnetic field, each Landau orbit occupies the same area. The cone geometry naturally provides a structure in which the distances between two adjacent orbits gradually change and can be easily adjusted by altering the tip angle. The presence of a cone tip introduces a geometric singularity that affects the electron density and interacts with the motion of electrons, which has been extensively studied. Furthermore, this type of geometry can automatically create a smooth interface or crossover between the crystalline charge-density-wave state and the liquid-like fractional quantum Hall state. In this work, the properties of this interface are studied from multiple perspectives, shedding light on the behavior of quantum Hall liquids in such geometric configurations."}
{"title":"Bubble dynamics in the Polyakov quark-meson model","authors":["Junrong Wang","Jinshuang Jin","Hong Mao"],"raw_abstract":"We investigate the dynamics of a first-order quark-hadron transition via\nhomogeneous thermal nucleation in the Polyakov quark-meson model for the\ntwo-quark flavor case. The contribution of fermionic vacuum loop in pressure\nand phase diagram together with the location of critical end point (CEP) have\nbeen obtained in the temperature and chemical potential plane. We develop an\nalternative geometric approach to search the minima of effective potential,\nwhich can be tunnelled through a bounce interpolated between a higher\nmetastable vacuum to an adjacent lower energy vacuum. By separating our\ndiscussions into a weak and strong first-order hadron quark phase transition,\nthe bubble profiles, the surface tension, the typical radius of the bounce and\nthe saddle point action in the presence of a nucleation bubble as a function of\ntemperature are calculated in detail when fixing chemical potentials at\n$\\mu=306 \\mathrm{MeV}$ and $\\mu=310 \\mathrm{MeV}$. our results show that the\nsurface tension remains a small value even when the chemical potential is very\nhigh and phase boundary for a hadron phase or a quark phase should be resized\naccording to the saddle point action at finite temperature. Compared with\nprevious results based on the quark meson model, it is shown that the inclusion\nof the deconfinement phase transition in term of the Polyakov loop does not\nchange chiral phase transition dramatically for light quarks.","publication_date":1700631601,"paper_link":"http://arxiv.org/pdf/2311.13175v1","categories":["Physics"],"abstract":"We investigate the dynamics of a first-order quark-hadron transition via homogeneous thermal nucleation in the Polyakov quark-meson model for the two-quark flavor case. The contribution of fermionic vacuum loop in pressure and phase diagram together with the location of critical end point (CEP) have been obtained in the temperature and chemical potential plane. We develop an alternative geometric approach to search the minima of effective potential, which can be tunnelled through a bounce interpolated between a higher metastable vacuum to an adjacent lower energy vacuum. By separating our discussions into a weak and strong first-order hadron quark phase transition, the bubble profiles, the surface tension, the typical radius of the bounce and the saddle point action in the presence of a nucleation bubble as a function of temperature are calculated in detail when fixing chemical potentials at __FORMULA__ and __FORMULA__. our results show that the surface tension remains a small value even when the chemical potential is very high and phase boundary for a hadron phase or a quark phase should be resized according to the saddle point action at finite temperature. Compared with previous results based on the quark meson model, it is shown that the inclusion of the deconfinement phase transition in term of the Polyakov loop does not change chiral phase transition dramatically for light quarks."}
{"title":"Control of open quantum systems via dynamical invariants","authors":["Loris Maria Cangemi","Hilario Espin\u00f3s","Ricardo Puebla","Erik Torrontegui","Amikam Levy"],"raw_abstract":"In this work, we confront the challenge of controlling quantum systems that\nare influenced by their environment, utilizing the theory of dynamical\ninvariants. Our strategy involves a reverse engineering method for formulating\ncontrol protocols like Shortcuts to Adiabaticity (STA), tailored to be\nresilient against environmental noise and dissipation. This technique offers\ntwo main advantages compared to other quantum control methods: firstly, it\nincorporates the time-varying aspect of the dissipation factor in the master\nequation, which arises from driving the system's Hamiltonian (the control\nfields). Secondly, our method eliminates the need for iterative propagation of\nthe system state, a process that is typically resource-intensive. The efficacy\nand practicality of our approach are demonstrated through the application to\ntwo fundamental models: a two-level quantum system and a quantum harmonic\noscillator, each interacting with a thermal bath.","publication_date":1700629793,"paper_link":"http://arxiv.org/pdf/2311.13164v1","categories":["Physics"],"abstract":"In this work, we confront the challenge of controlling quantum systems that are influenced by their environment, utilizing the theory of dynamical invariants. Our strategy involves a reverse engineering method for formulating control protocols like Shortcuts to Adiabaticity (STA), tailored to be resilient against environmental noise and dissipation. This technique offers two main advantages compared to other quantum control methods: firstly, it incorporates the time-varying aspect of the dissipation factor in the master equation, which arises from driving the system's Hamiltonian (the control fields). Secondly, our method eliminates the need for iterative propagation of the system state, a process that is typically resource-intensive. The efficacy and practicality of our approach are demonstrated through the application to two fundamental models: a two-level quantum system and a quantum harmonic oscillator, each interacting with a thermal bath."}
{"title":"Mass spectra of $0^{--}$ and $0^{+-}$ hidden-heavy baryoniums","authors":["Bing-Dong Wan"],"raw_abstract":"In this work, the spectra of the prospective exotic hidden-charm and\nhidden-bottom baryonium, viz. the baryon-antibaryon states, with\n$J^{PC}=0^{--}$ and $0^{+-}$ are investigated in the framework of QCD sum\nrules. The non-perturbative contributions up to dimension 12 are taken into\naccount. Numerical results indicate that there might exist 3 possible $0^{--}$\nhidden-charm baryonium states with masses $(5.21\\pm0.15)$, $(5.53\\pm0.15)$, and\n$(5.45\\pm0.12)$ GeV, and 5 possible $0^{+-}$ hidden-charm baryonium states with\nmasses $(4.76\\pm0.15)$, $(5.25\\pm0.15)$, $(5.16\\pm0.17)$, $(5.51\\pm0.14)$, and\n$(5.69\\pm0.14)$ GeV, respectively. The corresponding hidden-bottom partners are\nfound lying in the range of $11.68-12.28$ GeV and $11.38-12.33$ GeV,\nrespectively. The possible baryonium decay modes are analyzed, which are\nhopefully measurable in LHC experiments.","publication_date":1700629661,"paper_link":"http://arxiv.org/pdf/2311.13161v1","categories":["Physics"],"abstract":"In this work, the spectra of the prospective exotic hidden-charm and hidden-bottom baryonium, viz. the baryon-antibaryon states, with __FORMULA__ and __FORMULA__ are investigated in the framework of QCD sum rules. The non-perturbative contributions up to dimension 12 are taken into account. Numerical results indicate that there might exist 3 possible __FORMULA__ hidden-charm baryonium states with masses __FORMULA__, __FORMULA__, and __FORMULA__ GeV, and 5 possible __FORMULA__ hidden-charm baryonium states with masses __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, and __FORMULA__ GeV, respectively. The corresponding hidden-bottom partners are found lying in the range of __FORMULA__ GeV and __FORMULA__ GeV, respectively. The possible baryonium decay modes are analyzed, which are hopefully measurable in LHC experiments."}
{"title":"Fundamental limits of few-layer NbSe$_2$ microbolometers at terahertz frequencies","authors":["K. Shein","E. Zharkova","M. A. Kashchenko","A. I. Kolbatova","A. Lyubchak","L. Elesin","E. Nguyen","A. Semenov","I. Charaev","A. Schilling","G. N. Goltsman","K. S. Novoselov","I. Gayduchenko","D. A. Bandurin"],"raw_abstract":"The rapid development of infrared spectroscopy, observational astronomy, and\nscanning near-field microscopy has been enabled by the emergence of sensitive\nmid- and far-infrared photodetectors. Owing to their exceptional\nsignal-to-noise ratio and fast photoresponse, superconducting hot-electron\nbolometers (HEBs) have become a critical component in these applications. While\nsuperconducting HEBs are traditionally made from sputtered superconducting thin\nfilms like Nb or NbN, the potential of layered van der Waals (vdW)\nsuperconductors is untapped at THz frequencies. Here, we report the fabrication\nof superconducting HEBs out of few-layer NbSe$_2$ microwires. By improving the\ninterface between NbSe$_2$ and metal leads connected to a broadband antenna, we\novercome the impedance mismatch between this vdW superconductor and the radio\nfrequency (RF) readout circuitry that allowed us to achieve large responsivity\nTHz detection over the range from 0.13 to 2.5 THz with minimum noise equivalent\npower of 7~pW$\\sqrt{Hz}$. Using the heterodyne sub-THz mixing technique, we\nreveal that NbSe$_2$ superconducting HEBs are relatively fast and feature a\ncharacteristic response time in the nanosecond range limited by the slow heat\nescape to the bath through a SiO$_2$ layer, on which they are assembled, in\nagreement with energy relaxation model. Our work expands the family of\nmaterials for superconducting HEBs technology, reveals NbSe$_2$ as a promising\nplatform, and offers a reliable protocol for the in-lab production of custom\nbolometers using the vdW assembly technique.","publication_date":1700627022,"paper_link":"http://arxiv.org/pdf/2311.13150v1","categories":["Physics"],"abstract":"The rapid development of infrared spectroscopy, observational astronomy, and scanning near-field microscopy has been enabled by the emergence of sensitive mid- and far-infrared photodetectors. Owing to their exceptional signal-to-noise ratio and fast photoresponse, superconducting hot-electron bolometers (HEBs) have become a critical component in these applications. While superconducting HEBs are traditionally made from sputtered superconducting thin films like Nb or NbN, the potential of layered van der Waals (vdW) superconductors is untapped at THz frequencies. Here, we report the fabrication of superconducting HEBs out of few-layer NbSe__FORMULA__ microwires. By improving the interface between NbSe__FORMULA__ and metal leads connected to a broadband antenna, we overcome the impedance mismatch between this vdW superconductor and the radio frequency (RF) readout circuitry that allowed us to achieve large responsivity THz detection over the range from 0.13 to 2.5 THz with minimum noise equivalent power of 7~pW__FORMULA__. Using the heterodyne sub-THz mixing technique, we reveal that NbSe__FORMULA__ superconducting HEBs are relatively fast and feature a characteristic response time in the nanosecond range limited by the slow heat escape to the bath through a SiO__FORMULA__ layer, on which they are assembled, in agreement with energy relaxation model. Our work expands the family of materials for superconducting HEBs technology, reveals NbSe__FORMULA__ as a promising platform, and offers a reliable protocol for the in-lab production of custom bolometers using the vdW assembly technique."}
{"title":"Page Curves in Holographic Superconductors","authors":["Yuanceng Xu","Dong Wang","Qiyuan Pan"],"raw_abstract":"Considering a doubly holographic model, we study the black hole information\nparadox for the eternal AdSd-RN black hole coupled to and in equilibrium with a\nd-dimensional conformal bath whose state has been deformed by the charged\nscalar field coupled to a U(1) gauge field. Without a brane, the spontaneous\nsymmetry breaking of the gauge field on boundary systems can induce a\nsecond-order phase transition of the charged scalar field at the critical\ntemperature, known as holographic superconductors. The bath deformation can\nsignificantly change its entanglement dynamics with the black hole, resulting\nin variations in the Page curve and Page time. Our results indicate that\ncharacteristic parameters of the Page curve, such as entanglement velocity,\ninitial area difference and Page time, can be used as suitable probes to detect\nsuperconducting phase transitions. In particular, the entanglement velocity can\nalso probe both Kasner flows and Josephson oscillations. When keeping the\nendpoint of the radiation region fixed at twice the critical Page point, the\nentanglement velocity (the internal backreaction) has a more significant\ninfluence on the Page time compared to the initial area difference (the\nexternal backreaction).","publication_date":1700626584,"paper_link":"http://arxiv.org/pdf/2311.13145v1","categories":["Physics"],"abstract":"Considering a doubly holographic model, we study the black hole information paradox for the eternal AdSd-RN black hole coupled to and in equilibrium with a d-dimensional conformal bath whose state has been deformed by the charged scalar field coupled to a U(1) gauge field. Without a brane, the spontaneous symmetry breaking of the gauge field on boundary systems can induce a second-order phase transition of the charged scalar field at the critical temperature, known as holographic superconductors. The bath deformation can significantly change its entanglement dynamics with the black hole, resulting in variations in the Page curve and Page time. Our results indicate that characteristic parameters of the Page curve, such as entanglement velocity, initial area difference and Page time, can be used as suitable probes to detect superconducting phase transitions. In particular, the entanglement velocity can also probe both Kasner flows and Josephson oscillations. When keeping the endpoint of the radiation region fixed at twice the critical Page point, the entanglement velocity (the internal backreaction) has a more significant influence on the Page time compared to the initial area difference (the external backreaction)."}
{"title":"Bulk--boundary correspondence in a non-Hermitian quantum spin-Hall insulator","authors":["Chihiro Ishii","Yositake Takane"],"raw_abstract":"We focus on a scenario of non-Hermitian bulk--boundary correspondence that\nuses a topological invariant defined in a bulk geometry under a modified\nperiodic boundary condition. Although this has succeeded in describing the\ntopological nature of various one-dimensional non-Hermitian systems, its\napplication to two-dimensional systems has been limited to a non-Hermitian\nChern insulator. Here, we adapt the scenario to a non-Hermitian quantum\nspin-Hall insulator to extend its applicability. We show that it properly\ndescribes the bulk--boundary correspondence in the non-Hermitian quantum\nspin-Hall insulator. A phase diagram derived from the bulk--boundary\ncorrespondence is shown to be consistent with spectra of the system under an\nopen boundary condition.","publication_date":1700626215,"paper_link":"http://arxiv.org/pdf/2311.13142v1","categories":["Physics"],"abstract":"We focus on a scenario of non-Hermitian bulk--boundary correspondence that uses a topological invariant defined in a bulk geometry under a modified periodic boundary condition. Although this has succeeded in describing the topological nature of various one-dimensional non-Hermitian systems, its application to two-dimensional systems has been limited to a non-Hermitian Chern insulator. Here, we adapt the scenario to a non-Hermitian quantum spin-Hall insulator to extend its applicability. We show that it properly describes the bulk--boundary correspondence in the non-Hermitian quantum spin-Hall insulator. A phase diagram derived from the bulk--boundary correspondence is shown to be consistent with spectra of the system under an open boundary condition."}
{"title":"Effect of rainbow function on the structural properties of dark energy star","authors":["A. Bagheri Tudeshki","G. H. Bordbar","B. Eslam Panah"],"raw_abstract":"Confirming the existence of compact objects with a mass greater than\n$2.5M_{\\odot}$ by observational results such as GW190814 makes that is possible\nto provide theories to justify these observational results using modified\ngravity. This motivates us to use gravity's rainbow, which is the appropriate\ncase for dense objects, to investigate the dark energy star structure as a\nsuggested alternative case to the mass gap between neutron stars and black\nholes in the perspective of quantum gravity. Hence, in the present work, we\nderive the modified hydrostatic equilibrium equation for an anisotropic fluid,\nrepresented by the extended Chaplygin equation of state in gravity's rainbow.\nThen, for two isotropic and anisotropic cases, using the numerical solution, we\nobtain energy-dependent maximum mass and its corresponding radius, and the\nother properties of the dark energy star including the pressure, energy\ndensity, stability, etc. In the following, using the observational data, we\ncompare the obtained results in two frameworks of general relativity and\ngravity's rainbow.","publication_date":1700625353,"paper_link":"http://arxiv.org/pdf/2311.13138v1","categories":["Physics"],"abstract":"Confirming the existence of compact objects with a mass greater than __FORMULA__ by observational results such as GW190814 makes that is possible to provide theories to justify these observational results using modified gravity. This motivates us to use gravity's rainbow, which is the appropriate case for dense objects, to investigate the dark energy star structure as a suggested alternative case to the mass gap between neutron stars and black holes in the perspective of quantum gravity. Hence, in the present work, we derive the modified hydrostatic equilibrium equation for an anisotropic fluid, represented by the extended Chaplygin equation of state in gravity's rainbow. Then, for two isotropic and anisotropic cases, using the numerical solution, we obtain energy-dependent maximum mass and its corresponding radius, and the other properties of the dark energy star including the pressure, energy density, stability, etc. In the following, using the observational data, we compare the obtained results in two frameworks of general relativity and gravity's rainbow."}
{"title":"Bile dynamics within the biliary tract and microfluidic-based bile component detection: A review","authors":["Tao Peng","Chenxiao Zhou","Zhexin Zhang","Yingying Liu","Xiaodong Lin","Yongqing","Yunlong Zhong","Ping Wang","Yanwei Jia"],"raw_abstract":"Bilestones are solid masses found in the gallbladder or biliary tract, which\nblock the normal bile flow and eventually result in severe life-threatening\ncomplications. Studies have shown that bilestone formation may be related to\nbile flow dynamics and the concentration level of bile components. The bile\nflow dynamics in the biliary tract play a critical role in disclosing the\nmechanism of bile stasis and transportation. The concentration of bile\ncomposition is closely associated with processes such as nucleation and\ncrystallization. Recently, microfluidic-based biosensors have been favored for\nmultiple advantages over traditional bench-top detection assays for their less\nsample consumption, portability, low cost, and high sensitivity for real-time\ndetection. Here, we reviewed the developments in bile dynamics study and\nmicrofluidics-based bile component detection methods. These studies may provide\nvaluable insights into the bilestone formation mechanisms and better treatment,\nalongside our opinions on the future development of in vitro lithotriptic drug\nscreening of bilestones and bile characterization tests.","publication_date":1700620874,"paper_link":"http://arxiv.org/pdf/2311.13117v1","categories":["Physics"],"abstract":"Bilestones are solid masses found in the gallbladder or biliary tract, which block the normal bile flow and eventually result in severe life-threatening complications. Studies have shown that bilestone formation may be related to bile flow dynamics and the concentration level of bile components. The bile flow dynamics in the biliary tract play a critical role in disclosing the mechanism of bile stasis and transportation. The concentration of bile composition is closely associated with processes such as nucleation and crystallization. Recently, microfluidic-based biosensors have been favored for multiple advantages over traditional bench-top detection assays for their less sample consumption, portability, low cost, and high sensitivity for real-time detection. Here, we reviewed the developments in bile dynamics study and microfluidics-based bile component detection methods. These studies may provide valuable insights into the bilestone formation mechanisms and better treatment, alongside our opinions on the future development of in vitro lithotriptic drug screening of bilestones and bile characterization tests."}
{"title":"Powerful Quantum Circuit Resizing with Resource Efficient Synthesis","authors":["Siyuan Niu","Akel Hashim","Costin Iancu","Wibe Albert de Jong","Ed Younis"],"raw_abstract":"In the noisy intermediate-scale quantum era, mid-circuit measurement and\nreset operations facilitate novel circuit optimization strategies by reducing a\ncircuit's qubit count in a method called resizing. This paper introduces two\nsuch algorithms. The first one leverages gate-dependency rules to reduce qubit\ncount by 61.6% or 45.3% when optimizing depth as well. Based on numerical\ninstantiation and synthesis, the second algorithm finds resizing opportunities\nin previously unresizable circuits via dependency rules and other\nstate-of-the-art tools. This resizing algorithm reduces qubit count by 20.7% on\naverage for these previously impossible-to-resize circuits.","publication_date":1700619514,"paper_link":"http://arxiv.org/pdf/2311.13107v1","categories":["Physics"],"abstract":"In the noisy intermediate-scale quantum era, mid-circuit measurement and reset operations facilitate novel circuit optimization strategies by reducing a circuit's qubit count in a method called resizing. This paper introduces two such algorithms. The first one leverages gate-dependency rules to reduce qubit count by 61.6% or 45.3% when optimizing depth as well. Based on numerical instantiation and synthesis, the second algorithm finds resizing opportunities in previously unresizable circuits via dependency rules and other state-of-the-art tools. This resizing algorithm reduces qubit count by 20.7% on average for these previously impossible-to-resize circuits."}
{"title":"Analysis of the data for $\u03b3p \\to f_1(1285) p$ photoproduction","authors":["Ai-Chao Wang","Neng-Chang Wei","Fei Huang"],"raw_abstract":"The photoproduction of $f_1(1285)$ meson off proton is investigated within an\neffective Lagrangian approach. The $t$-channel $\\rho$- and $\\omega$-exchange\ndiagrams, $u$-channel nucleon-exchange diagram, generalized contact term, and\n$s$-channel pole diagrams of nucleon and a minimal number of nucleon resonances\nare taken into account in constructing the reaction amplitudes to describe the\nexperimental data. Three different models, i.e., the Feynman model, the Regge\nmodel, and the interpolated Regge model, are employed where the $t$-channel\nreaction amplitudes are constructed in Feynman type, Regge type, and\ninterpolated Regge type, respectively. The results show that in neither Feynman\nmodel with two nucleon resonances introduced nor interpolated Regge model with\none nucleon resonances included, can the available data for $\\gamma p \\to\nf_1(1285) p$ be satisfactorily reproduced. Nevertheless, in the Regge model,\nwhen any one of the $N(1990){7/2}^+$, $N(2000){5/2}^+$, $N(2040){3/2}^+$,\n$N(2060){5/2}^-$, $N(2100){1/2}^+$, $N(2120){3/2}^-$, $N(2190){7/2}^-$,\n$N(2300){1/2}^+$, and $N(2570){5/2}^-$ resonances is considered, the data can\nbe well described. The resulted resonance parameters are consistent with those\nadvocated in Particle Data Group (PDG) review. Further analysis shows that in\nhigh-energy region, the peaks of $\\gamma p \\to f_1(1285) p$ differential cross\nsections at forward angles are dominated by the contributions from $t$-channel\n$\\rho$- and $\\omega$-exchange diagrams, while in low-energy region, the\n$s$-channel pole diagrams of resonances also provide significant contributions\nto the $\\gamma p \\to f_1(1285) p$ cross sections.","publication_date":1700618625,"paper_link":"http://arxiv.org/pdf/2311.13101v1","categories":["Physics"],"abstract":"The photoproduction of __FORMULA__ meson off proton is investigated within an effective Lagrangian approach. The __FORMULA__-channel __FORMULA__- and __FORMULA__-exchange diagrams, __FORMULA__-channel nucleon-exchange diagram, generalized contact term, and __FORMULA__-channel pole diagrams of nucleon and a minimal number of nucleon resonances are taken into account in constructing the reaction amplitudes to describe the experimental data. Three different models, i.e., the Feynman model, the Regge model, and the interpolated Regge model, are employed where the __FORMULA__-channel reaction amplitudes are constructed in Feynman type, Regge type, and interpolated Regge type, respectively. The results show that in neither Feynman model with two nucleon resonances introduced nor interpolated Regge model with one nucleon resonances included, can the available data for __FORMULA__ be satisfactorily reproduced. Nevertheless, in the Regge model, when any one of the __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, __FORMULA__, and __FORMULA__ resonances is considered, the data can be well described. The resulted resonance parameters are consistent with those advocated in Particle Data Group (PDG) review. Further analysis shows that in high-energy region, the peaks of __FORMULA__ differential cross sections at forward angles are dominated by the contributions from __FORMULA__-channel __FORMULA__- and __FORMULA__-exchange diagrams, while in low-energy region, the __FORMULA__-channel pole diagrams of resonances also provide significant contributions to the __FORMULA__ cross sections."}
{"title":"Multiphoton-pumped UV-Vis transient absorption spectroscopy of 2D materials: basic concepts and recent applications","authors":["Yuri D Glinka"],"raw_abstract":"2D materials are considered a key element in the development of\nnext-generation electronics (nanoelectronics) due to their extreme thickness in\nthe nanometer range and unique physical properties. The ultrafast dynamics of\nphotoexcited carriers in such materials is strongly influenced by their\ninterfaces, since the thickness of 2D materials is much smaller than the\ntypical depth of light penetration into them and the mean free path of\nphotoexcited carriers. The resulting collisions of photoexcited carriers with\ninterfacial potential barriers of 2D materials in the presence of a strong\nlaser field significantly alter the overall dynamics of photoexcitation,\nallowing laser light to be directly absorbed by carriers in the\nconduction/valence band through the inverse bremsstrahlung mechanism. The\ncorresponding ultrafast carrier dynamics can be monitored using\nmultiphoton-pumped UV-Vis transient absorption spectroscopy. In this review, we\ndiscuss the basic concepts and recent applications of this spectroscopy for a\nvariety of 2D materials, including transition-metal dichalcogenide monolayers,\ntopological insulators, and other 2D semiconductor structures.","publication_date":1700618252,"paper_link":"http://arxiv.org/pdf/2311.13098v1","categories":["Physics"],"abstract":"2D materials are considered a key element in the development of next-generation electronics (nanoelectronics) due to their extreme thickness in the nanometer range and unique physical properties. The ultrafast dynamics of photoexcited carriers in such materials is strongly influenced by their interfaces, since the thickness of 2D materials is much smaller than the typical depth of light penetration into them and the mean free path of photoexcited carriers. The resulting collisions of photoexcited carriers with interfacial potential barriers of 2D materials in the presence of a strong laser field significantly alter the overall dynamics of photoexcitation, allowing laser light to be directly absorbed by carriers in the conduction/valence band through the inverse bremsstrahlung mechanism. The corresponding ultrafast carrier dynamics can be monitored using multiphoton-pumped UV-Vis transient absorption spectroscopy. In this review, we discuss the basic concepts and recent applications of this spectroscopy for a variety of 2D materials, including transition-metal dichalcogenide monolayers, topological insulators, and other 2D semiconductor structures."}
{"title":"Mechanistic Insights into the Hydrazine-induced Chemical Reduction Pathway of Graphene Oxide","authors":["Shu Chen","Jianqiang Guo"],"raw_abstract":"Hydrazine stands out as the most generally used chemical-reducing agent for\nreducing graphene oxide. Despite numerous experimental and theoretical\ninvestigations into the reduction reaction, the reduction mechanism remains\nunclear. In this study, we propose that, in aqueous hydrazine solutions, both\nhydrazine and hydroxide ions could initiate the reduction of graphene oxide. We\nintroduce a chemical reaction pathway involving C-H cleavage and a\ndehydroxylation process for the reduction of graphene oxide. By utilizing\ndensity functional theory calculations, the reduction reactions mediated by\nhydrazine and hydroxide ions are separately investigated. The reaction routes\non the basal plane and edge regions of graphene oxide are discussed\nindependently. The density functional theory calculations demonstrate that the\nproposed mechanism is both thermodynamically and dynamically feasible. This\nwork might contribute to an atomic-level comprehension of a longstanding\nchallenge in the field of graphene oxide.","publication_date":1700616571,"paper_link":"http://arxiv.org/pdf/2311.13086v1","categories":["Physics"],"abstract":"Hydrazine stands out as the most generally used chemical-reducing agent for reducing graphene oxide. Despite numerous experimental and theoretical investigations into the reduction reaction, the reduction mechanism remains unclear. In this study, we propose that, in aqueous hydrazine solutions, both hydrazine and hydroxide ions could initiate the reduction of graphene oxide. We introduce a chemical reaction pathway involving C-H cleavage and a dehydroxylation process for the reduction of graphene oxide. By utilizing density functional theory calculations, the reduction reactions mediated by hydrazine and hydroxide ions are separately investigated. The reaction routes on the basal plane and edge regions of graphene oxide are discussed independently. The density functional theory calculations demonstrate that the proposed mechanism is both thermodynamically and dynamically feasible. This work might contribute to an atomic-level comprehension of a longstanding challenge in the field of graphene oxide."}
{"title":"Contextual quantum metrology","authors":["Jeongwoo Jae","Jiwon Lee","M. S. Kim","Kwang-Geol Lee","Jinhyoung Lee"],"raw_abstract":"Quantum metrology promises higher precision measurements than classical\nmethods. Entanglement has been identified as one of quantum resources to\nenhance metrological precision. However, generating entangled states with high\nfidelity presents considerable challenges, and thus attaining metrological\nenhancement through entanglement is generally difficult. Here, we show that\ncontextuality of measurement selection can enhance metrological precision, and\nthis enhancement is attainable with a simple linear optical experiment. We call\nour methodology \"contextual quantum metrology\" (coQM). Contextuality is a\nnonclassical property known as a resource for various quantum information\nprocessing tasks. Until now, it has remained an open question whether\ncontextuality can be a resource for quantum metrology. We answer this question\nin the affirmative by showing that the coQM can elevate precision of an optical\npolarimetry by a factor of 1.4 to 6.0, much higher than the one by quantum\nFisher information, known as the limit of conventional quantum metrology. We\nachieve the contextuality-enabled enhancement with two polarization\nmeasurements which are mutually complementary, whereas, in the conventional\nmethod, some optimal measurements to achieve the precision limit are either\ntheoretically difficult to find or experimentally infeasible. These results\nhighlight that the contextuality of measurement selection is applicable in\npractice for quantum metrology.","publication_date":1700616399,"paper_link":"http://arxiv.org/pdf/2311.13084v1","categories":["Physics"],"abstract":"Quantum metrology promises higher precision measurements than classical methods. Entanglement has been identified as one of quantum resources to enhance metrological precision. However, generating entangled states with high fidelity presents considerable challenges, and thus attaining metrological enhancement through entanglement is generally difficult. Here, we show that contextuality of measurement selection can enhance metrological precision, and this enhancement is attainable with a simple linear optical experiment. We call our methodology \"contextual quantum metrology\" (coQM). Contextuality is a nonclassical property known as a resource for various quantum information processing tasks. Until now, it has remained an open question whether contextuality can be a resource for quantum metrology. We answer this question in the affirmative by showing that the coQM can elevate precision of an optical polarimetry by a factor of 1.4 to 6.0, much higher than the one by quantum Fisher information, known as the limit of conventional quantum metrology. We achieve the contextuality-enabled enhancement with two polarization measurements which are mutually complementary, whereas, in the conventional method, some optimal measurements to achieve the precision limit are either theoretically difficult to find or experimentally infeasible. These results highlight that the contextuality of measurement selection is applicable in practice for quantum metrology."}
{"title":"Coherent control of molecular rotation in superfluid helium","authors":["Alexander A. Milner","Ian MacPhail-Bartley","Katarina Preocanin","Shroyon Dasgupta","Xuanshan Peng","Valery Milner"],"raw_abstract":"We experimentally demonstrate control of molecular rotation in bulk\nsuperfluid $^4\\mathrm{He}$. Metastable helium dimers, $\\mathrm{He}_2^*$, are\nrotationally excited by a periodic train of linearly polarized femtosecond\nlaser pulses. We show that the degree of rotational excitation of\n$\\mathrm{He}_2^*$ can be enhanced or suppressed by varying the period of the\npulse train, whereas the directionality of molecular rotation can be controlled\nby the relative angle between the polarization vectors of pulses in the train.\nThe experimental results are in agreement with numerical calculations, based on\na simple model, in which $\\mathrm{He}_2^*$ molecules do not interact with the\nsuperfluid.","publication_date":1700613618,"paper_link":"http://arxiv.org/pdf/2311.13077v1","categories":["Physics"],"abstract":"We experimentally demonstrate control of molecular rotation in bulk superfluid __FORMULA__. Metastable helium dimers, __FORMULA__, are rotationally excited by a periodic train of linearly polarized femtosecond laser pulses. We show that the degree of rotational excitation of __FORMULA__ can be enhanced or suppressed by varying the period of the pulse train, whereas the directionality of molecular rotation can be controlled by the relative angle between the polarization vectors of pulses in the train. The experimental results are in agreement with numerical calculations, based on a simple model, in which __FORMULA__ molecules do not interact with the superfluid."}
{"title":"Non-equilibrium dynamics of topological defects in the 3d O(2) model","authors":["Edgar L\u00f3pez-Contreras","Jaime Fabi\u00e1n Nieto Castellanos","El\u00edas Natanael Polanco-Eu\u00e1n","Wolfgang Bietenholz"],"raw_abstract":"We present a study of the 3d O(2) non-linear $\\sigma$-model on the lattice,\nwhich exhibits topological defects in the form of vortices. They tend to\norganize into vortex lines that bear close analogies with global cosmic\nstrings. Therefore, this model serves as a testbed for studying the dynamics of\ntopological defects. It undergoes a second order phase transition, hence it is\nappropriate for investigating the Kibble-Zurek mechanism. In this regard, we\nexplore the persistence of topological defects when the temperature is rapidly\nreduced from above to below the critical temperature; this cooling (or\n\"quenching\") process takes the system out of equilibrium. We probe a wide range\nof inverse cooling rates $\\tau_{\\rm Q}$ and final temperatures, employing\ndistinct Monte Carlo algorithms. The results consistently show that the density\nof persisting topological defects follows a power-law in $\\tau_{\\rm Q}$, in\nagreement with Zurek's conjecture. On the other hand, at this point our results\ndo not confirm Zurek's prediction for the exponent in this power-law, but its\nfinal test is still under investigation.","publication_date":1700613132,"paper_link":"http://arxiv.org/pdf/2311.13074v1","categories":["Physics"],"abstract":"We present a study of the 3d O(2) non-linear __FORMULA__-model on the lattice, which exhibits topological defects in the form of vortices. They tend to organize into vortex lines that bear close analogies with global cosmic strings. Therefore, this model serves as a testbed for studying the dynamics of topological defects. It undergoes a second order phase transition, hence it is appropriate for investigating the Kibble-Zurek mechanism. In this regard, we explore the persistence of topological defects when the temperature is rapidly reduced from above to below the critical temperature; this cooling (or \"quenching\") process takes the system out of equilibrium. We probe a wide range of inverse cooling rates __FORMULA__ and final temperatures, employing distinct Monte Carlo algorithms. The results consistently show that the density of persisting topological defects follows a power-law in __FORMULA__, in agreement with Zurek's conjecture. On the other hand, at this point our results do not confirm Zurek's prediction for the exponent in this power-law, but its final test is still under investigation."}
{"title":"Magnetic field dependence of the neutral pion longitudinal screening mass in the linear sigma model with quarks","authors":["Alejandro Ayala","Ricardo L. S. Farias","L. A. Hern\u00e1ndez","Ana Julia Mizher","Javier Rend\u00f3n","Cristian Villavicencio","R. Zamora"],"raw_abstract":"We use the Linear Sigma Model with quarks to study the magnetic field-induced\nmodifications on the longitudinal screening mass for the neutral pion at\none-loop level. The effects of the magnetic field are introduced into the\nself-energy which contains the contributions from all the model particles. We\nfind that to obtain a reasonable description for the behavior with the field\nstrength, the magnetic field dependence of the particle masses need to be\naccounted for. We also find that the couplings need to decrease fast enough\nwith the field strength to then reach constant and smaller values as compared\nto their vacuum ones. The results illustrate the need to treat the magnetic\ncorrections to the particle masses and couplings in a self-consistent manner,\naccounting for the back reaction of the field effects for the magnetic field\ndependence of the rest of the particle species and couplings in the model.","publication_date":1700611298,"paper_link":"http://arxiv.org/pdf/2311.13068v1","categories":["Physics"],"abstract":"We use the Linear Sigma Model with quarks to study the magnetic field-induced modifications on the longitudinal screening mass for the neutral pion at one-loop level. The effects of the magnetic field are introduced into the self-energy which contains the contributions from all the model particles. We find that to obtain a reasonable description for the behavior with the field strength, the magnetic field dependence of the particle masses need to be accounted for. We also find that the couplings need to decrease fast enough with the field strength to then reach constant and smaller values as compared to their vacuum ones. The results illustrate the need to treat the magnetic corrections to the particle masses and couplings in a self-consistent manner, accounting for the back reaction of the field effects for the magnetic field dependence of the rest of the particle species and couplings in the model."}
{"title":"Gaussian-basis many-body theory calculations of positron binding to negative ions and atoms","authors":["J. Hofierka","B. Cunningham","C. M. Rawlins","C. H. Patterson","D. G. Green"],"raw_abstract":"Positron binding energies in the negative ions H$^-$, F$^-$, Cl$^-$ and\nBr$^-$, and the closed-shell atoms Be, Mg, Zn and Ca, are calculated via a\nmany-body theory approach developed by the authors [J.~Hofierka \\emph{et al.}\nNature~{\\bf 608}, 688-693 (2022)]. Specifically, the Dyson equation is solved\nusing a Gaussian basis, with the positron self energy constructed from three\ninfinite classes of diagrams that account for the strong positron-atom\ncorrelations that characterise the system including the positron-induced\npolarization of the electron cloud, screening of the electron-positron Coulomb\ninteraction, virtual-positronium formation and electron-hole and positron-hole\ninteractions. For the negative ions, binding occurs at the static level of\ntheory, and the correlations are found to enhance the binding energies by\n$\\sim$25--50\\%, yielding results in good agreement with ($\\lesssim$5\\% larger\nthan) calculations from a number of distinct methods. For the atoms, for which\nbinding is enabled exclusively by correlations, most notably virtual-Ps\nformation, the binding energies are found to be of similar order to (but\n$\\sim$10--30\\% larger than) relativistic coupled-cluster calculations of [C.\nHarabati, V.~A.~Dzuba and V.~V. Flambaum, Phys.~Rev.~A {\\bf 89}, 022517\n(2014)], both of which are systematically larger than stochastic variational\ncalculations of [M.~Bromley and J.~Mitroy, Phys.~Rev.~A {\\bf 73} (2005);\nJ.~Mitroy, J.~At.~Mol.~Sci.~{\\bf 1}, 275 (2010)].","publication_date":1700610947,"paper_link":"http://arxiv.org/pdf/2311.13066v1","categories":["Physics"],"abstract":"Positron binding energies in the negative ions H__FORMULA__, F__FORMULA__, Cl__FORMULA__ and Br__FORMULA__, and the closed-shell atoms Be, Mg, Zn and Ca, are calculated via a many-body theory approach developed by the authors [J.~Hofierka et al. Nature~{\\bf 608}, 688-693 (2022)]. Specifically, the Dyson equation is solved using a Gaussian basis, with the positron self energy constructed from three infinite classes of diagrams that account for the strong positron-atom correlations that characterise the system including the positron-induced polarization of the electron cloud, screening of the electron-positron Coulomb interaction, virtual-positronium formation and electron-hole and positron-hole interactions. For the negative ions, binding occurs at the static level of theory, and the correlations are found to enhance the binding energies by __FORMULA__25--50\\%, yielding results in good agreement with (__FORMULA__5\\% larger than) calculations from a number of distinct methods. For the atoms, for which binding is enabled exclusively by correlations, most notably virtual-Ps formation, the binding energies are found to be of similar order to (but __FORMULA__10--30\\% larger than) relativistic coupled-cluster calculations of [C. Harabati, V.~A.~Dzuba and V.~V. Flambaum, Phys.~Rev.~A {\\bf 89}, 022517 (2014)], both of which are systematically larger than stochastic variational calculations of [M.~Bromley and J.~Mitroy, Phys.~Rev.~A {\\bf 73} (2005); J.~Mitroy, J.~At.~Mol.~Sci.~{\\bf 1}, 275 (2010)]."}
{"title":"On-Demand Quantum Light Sources for Underwater Communications","authors":["Dominic Scognamiglio","Angus Gale","Ali Al-Juboori","Milos Toth","Igor Aharonovich"],"raw_abstract":"Quantum communication has been at the forefront of modern research for\ndecades, however it is severely hampered in underwater applications, where the\nproperties of water absorb nearly all useful optical wavelengths and prevent\nthem from propagating more than, in most cases, a few metres. This research\nreports on-demand quantum light sources, suitable for underwater optical\ncommunication. The single photon emitters, which can be engineered using an\nelectron beam, are based on impurities in hexagonal boron nitride. They have a\nzero phonon line at ~ 436 nm, near the minimum value of water absorption and\nare shown to suffer negligible transmission and purity loss when travelling\nthrough water channels. These emitters are also shown to possess exceptional\nunderwater transmission properties compared to emitters at other optical\nwavelengths and are utilised in a proof of principle underwater communication\nlink with rates of several kbits/s.","publication_date":1700610915,"paper_link":"http://arxiv.org/pdf/2311.13065v1","categories":["Physics"],"abstract":"Quantum communication has been at the forefront of modern research for decades, however it is severely hampered in underwater applications, where the properties of water absorb nearly all useful optical wavelengths and prevent them from propagating more than, in most cases, a few metres. This research reports on-demand quantum light sources, suitable for underwater optical communication. The single photon emitters, which can be engineered using an electron beam, are based on impurities in hexagonal boron nitride. They have a zero phonon line at ~ 436 nm, near the minimum value of water absorption and are shown to suffer negligible transmission and purity loss when travelling through water channels. These emitters are also shown to possess exceptional underwater transmission properties compared to emitters at other optical wavelengths and are utilised in a proof of principle underwater communication link with rates of several kbits/s."}
{"title":"Training Deep 3D Convolutional Neural Networks to Extract BSM Physics Parameters Directly from HEP Data: a Proof-of-Concept Study Using Monte Carlo Simulations","authors":["S. Dubey","T. E. Browder","S. Kohani","R. Mandal","A. Sibidanov","R. Sinha"],"raw_abstract":"We report on a novel application of computer vision techniques to extract\nbeyond the Standard Model (BSM) parameters directly from high energy physics\n(HEP) flavor data. We develop a method of transforming angular and kinematic\ndistributions into \"quasi-images\" that can be used to train a convolutional\nneural network to perform regression tasks, similar to fitting. This contrasts\nwith the usual classification functions performed using ML/AI in HEP. As a\nproof-of-concept, we train a 34-layer Residual Neural Network to regress on\nthese images and determine the Wilson Coefficient $C_{9}$ in MC (Monte Carlo)\nsimulations of $B \\rightarrow K^{*}\\mu^{+}\\mu^{-}$ decays. The technique\ndescribed here can be generalized and may find applicability across various HEP\nexperiments and elsewhere.","publication_date":1700610591,"paper_link":"http://arxiv.org/pdf/2311.13060v1","categories":["Physics"],"abstract":"We report on a novel application of computer vision techniques to extract beyond the Standard Model (BSM) parameters directly from high energy physics (HEP) flavor data. We develop a method of transforming angular and kinematic distributions into \"quasi-images\" that can be used to train a convolutional neural network to perform regression tasks, similar to fitting. This contrasts with the usual classification functions performed using ML/AI in HEP. As a proof-of-concept, we train a 34-layer Residual Neural Network to regress on these images and determine the Wilson Coefficient __FORMULA__ in MC (Monte Carlo) simulations of __FORMULA__ decays. The technique described here can be generalized and may find applicability across various HEP experiments and elsewhere."}
{"title":"Camera-Independent Single Image Depth Estimation from Defocus Blur","authors":["Lahiru Wijayasingha","Homa Alemzadeh","John A. Stankovic"],"raw_abstract":"Monocular depth estimation is an important step in many downstream tasks in\nmachine vision. We address the topic of estimating monocular depth from defocus\nblur which can yield more accurate results than the semantic based depth\nestimation methods. The existing monocular depth from defocus techniques are\nsensitive to the particular camera that the images are taken from. We show how\nseveral camera-related parameters affect the defocus blur using optical physics\nequations and how they make the defocus blur depend on these parameters. The\nsimple correction procedure we propose can alleviate this problem which does\nnot require any retraining of the original model. We created a synthetic\ndataset which can be used to test the camera independent performance of depth\nfrom defocus blur models. We evaluate our model on both synthetic and real\ndatasets (DDFF12 and NYU depth V2) obtained with different cameras and show\nthat our methods are significantly more robust to the changes of cameras. Code:\nhttps://github.com/sleekEagle/defocus_camind.git","publication_date":1700608482,"paper_link":"http://arxiv.org/pdf/2311.13045v1","categories":["Physics"],"abstract":"Monocular depth estimation is an important step in many downstream tasks in machine vision. We address the topic of estimating monocular depth from defocus blur which can yield more accurate results than the semantic based depth estimation methods. The existing monocular depth from defocus techniques are sensitive to the particular camera that the images are taken from. We show how several camera-related parameters affect the defocus blur using optical physics equations and how they make the defocus blur depend on these parameters. The simple correction procedure we propose can alleviate this problem which does not require any retraining of the original model. We created a synthetic dataset which can be used to test the camera independent performance of depth from defocus blur models. We evaluate our model on both synthetic and real datasets (DDFF12 and NYU depth V2) obtained with different cameras and show that our methods are significantly more robust to the changes of cameras. Code: https://github.com/sleekEagle/defocus_camind.git"}
{"title":"Fast Adaptive Optics for High-Dimensional Quantum Communications in Turbulent Channels","authors":["Lukas Scarfe","Felix Hufnagel","Manuel F. Ferrer-Garcia","Alessio D'Errico","Khabat Heshami","Ebrahim Karimi"],"raw_abstract":"Quantum Key Distribution (QKD) promises a provably secure method to transmit\ninformation from one party to another. Free-space QKD allows for this\ninformation to be sent over great distances and in places where fibre-based\ncommunications cannot be implemented, such as ground-satellite. The primary\nlimiting factor for free-space links is the effect of atmospheric turbulence,\nwhich can result in significant error rates and increased losses in QKD\nchannels. Here, we employ the use of a high-speed Adaptive Optics (AO) system\nto make real-time corrections to the wavefront distortions on spatial modes\nthat are used for high-dimensional QKD in our turbulent channel. First, we\ndemonstrate the effectiveness of the AO system in improving the coupling\nefficiency of a Gaussian mode that has propagated through turbulence. Through\nprocess tomography, we show that our system is capable of significantly\nreducing the crosstalk of spatial modes in the channel. Finally, we show that\nemploying AO reduces the quantum dit error rate for a high-dimensional orbital\nangular momentum-based QKD protocol, allowing for secure communication in a\nchannel where it would otherwise be impossible. These results are promising for\nestablishing long-distance free-space QKD systems.","publication_date":1700607911,"paper_link":"http://arxiv.org/pdf/2311.13041v1","categories":["Physics"],"abstract":"Quantum Key Distribution (QKD) promises a provably secure method to transmit information from one party to another. Free-space QKD allows for this information to be sent over great distances and in places where fibre-based communications cannot be implemented, such as ground-satellite. The primary limiting factor for free-space links is the effect of atmospheric turbulence, which can result in significant error rates and increased losses in QKD channels. Here, we employ the use of a high-speed Adaptive Optics (AO) system to make real-time corrections to the wavefront distortions on spatial modes that are used for high-dimensional QKD in our turbulent channel. First, we demonstrate the effectiveness of the AO system in improving the coupling efficiency of a Gaussian mode that has propagated through turbulence. Through process tomography, we show that our system is capable of significantly reducing the crosstalk of spatial modes in the channel. Finally, we show that employing AO reduces the quantum dit error rate for a high-dimensional orbital angular momentum-based QKD protocol, allowing for secure communication in a channel where it would otherwise be impossible. These results are promising for establishing long-distance free-space QKD systems."}
{"title":"Note on an extended chiral bosons system contextualized in a modified gauge-unfixing formalism","authors":["Gabriella V. Ambrosio","Cleber N. Costa","Paulo R. F. Alves","Jorge Ananias Neto","Ronaldo Thibes"],"raw_abstract":"We analyze the Hamiltonian structure of an extended chiral bosons theory in\nwhich the self-dual constraint is introduced via a control $\\alpha$-parameter.\nThe system has two second-class constraints in the non-critical regime and an\nadditional one in the critical regime. We use a modified gauge unfixing\nformalism to derive a first-class system, disclosing hidden symmetries. To this\nend, we choose one of the second-class constraints to build a corresponding\ngauge symmetry generator. The worked out procedure converts second-class\nvariables into first-class ones allowing the lifting of gauge symmetry. Any\nfunction of these GU variables will also be invariant. We obtain the GU\nHamiltonian and Lagrangian densities in a generalized context containing the\nSrivastava and Floreanini-Jackiw models as particular cases. Additionally, we\nobserve that the resulting GU Lagrangian presents similaries to the Siegel\ninvariant Lagrangian which is known to be suitable for describing chiral bosons\ntheory with classical gauge invariance, however broken at quantul level. The\nfinal results signal a possible equivalence between our invariant Lagrangian\nobtained from the modified GU formalism and the Siegel invariant Lagrangian,\nwith a distinct gauge symmetry.","publication_date":1700607808,"paper_link":"http://arxiv.org/pdf/2311.13039v1","categories":["Physics"],"abstract":"We analyze the Hamiltonian structure of an extended chiral bosons theory in which the self-dual constraint is introduced via a control __FORMULA__-parameter. The system has two second-class constraints in the non-critical regime and an additional one in the critical regime. We use a modified gauge unfixing formalism to derive a first-class system, disclosing hidden symmetries. To this end, we choose one of the second-class constraints to build a corresponding gauge symmetry generator. The worked out procedure converts second-class variables into first-class ones allowing the lifting of gauge symmetry. Any function of these GU variables will also be invariant. We obtain the GU Hamiltonian and Lagrangian densities in a generalized context containing the Srivastava and Floreanini-Jackiw models as particular cases. Additionally, we observe that the resulting GU Lagrangian presents similaries to the Siegel invariant Lagrangian which is known to be suitable for describing chiral bosons theory with classical gauge invariance, however broken at quantul level. The final results signal a possible equivalence between our invariant Lagrangian obtained from the modified GU formalism and the Siegel invariant Lagrangian, with a distinct gauge symmetry."}
{"title":"Accessing new physics with an undoped, cryogenic CsI CEvNS detector for COHERENT at the SNS","authors":["P. S. Barbeau","V. Belov","I. Bernardi","C. Bock","A. Bolozdynya","R. Bouabid","J. Browning","B. Cabrera-Palmer","E. Conley","V. da Silva","J. Daughhetee","J. Detwiler","K. Ding","M. R. Durand","Y. Efremenko","S. R. Elliott","A. Erlandson","L. Fabris","M. Febbraro","A. Galindo-Uribarri","M. P. Green","J. Hakenm\u00fcller","M. R. Heath","S. Hedges","B. A. Johnson","T. Johnson","A. Khromov","A. Konovalov","E. Kozlova","A. Kumpan","O. Kyzylova","J. M. Link","J. Liu","A. Major","K. Mann","D. M. Markoff","J. Mattingly","P. E. Mueller","J. Newby","N. Ogoi","J. O'Reilly","D. S. Parno","D. P\u00e9rez-Loureiro","S. I. Penttila","D. Pershey","C. G. Prior","J. Queen","R. Rapp","H. Ray","O. Razuvaeva","D. Reyna","G. C. Rich","D. Rudik","J. Runge","D. J. Salvat","J. Sander","K. Scholberg","A. Shakirov","G. Simakov","W. M. Snow","V. Sosnovtsev","M. Stringer","T. Subedi","B. Suh","B. Sur","R. Tayloe","K. Tellez-Giron-Flores","Y. -T. Tsai","J. Vanderwerp","E. E. van Nieuwenhuizen","R. L. Varner","C. J. Virtue","G. Visser","K. Walkup","E. M. Ward","T. Wongjirad","Y. Yang","J. Yoo","C. -H. Yu","A. Zaalishvili"],"raw_abstract":"We consider the potential for a 10-kg undoped cryogenic CsI detector\noperating at the Spallation Neutron Source to measure coherent elastic\nneutrino-nucleus scattering and its sensitivity to discover new physics beyond\nthe standard model. Through a combination of increased event rate, lower\nthreshold, and good timing resolution, such a detector would significantly\nimprove on past measurements. We considered tests of several\nbeyond-the-standard-model scenarios such as neutrino non-standard interactions\nand accelerator-produced dark matter. This detector's performance was also\nstudied for relevant questions in nuclear physics and neutrino astronomy,\nnamely the weak charge distribution of CsI nuclei and detection of neutrinos\nfrom a core-collapse supernova.","publication_date":1700606444,"paper_link":"http://arxiv.org/pdf/2311.13032v1","categories":["Physics"],"abstract":"We consider the potential for a 10-kg undoped cryogenic CsI detector operating at the Spallation Neutron Source to measure coherent elastic neutrino-nucleus scattering and its sensitivity to discover new physics beyond the standard model. Through a combination of increased event rate, lower threshold, and good timing resolution, such a detector would significantly improve on past measurements. We considered tests of several beyond-the-standard-model scenarios such as neutrino non-standard interactions and accelerator-produced dark matter. This detector's performance was also studied for relevant questions in nuclear physics and neutrino astronomy, namely the weak charge distribution of CsI nuclei and detection of neutrinos from a core-collapse supernova."}
{"title":"Deformation Dynamics of Nanopores upon Water Imbibition","authors":["Juan Sanchez","Lars Dammann","Laura Gallardo","Zhuoqing Li","Michael Fr\u00f6ba","Robert Meissner","Howard A. Stone","Patrick Huber"],"raw_abstract":"Capillarity-driven transport in nanoporous solids is ubiquitous in nature and\nis of increasing importance for the functionality of modern liquid-infused\nengineering materials. During imbibition, highly curved menisci are driven by\nnegative Laplace pressures of several hundred atmospheres, exerting an enormous\ncontractile load on an increasing portion of the porous matrix. Due to the\nchallenge of simultaneously monitoring imbibition and deformation with high\nspatial resolution, the resulting coupling of solid elasticity to liquid\ncapillarity has remained largely unexplored. Here, we study water imbibition in\nmesoporous silica using optical imaging, gravimetry, and high-resolution\ndilatometry. In contrast to an expected Laplace pressure-induced contraction,\nwe find a square-root-of-time expansion and an additional abrupt length\nincrease when the menisci reach the top surface. The final expansion is absent\nwhen we stop the imbibition front inside the porous medium in a dynamic\nimbibition-evaporation equilibrium, as is typical for water transport and\ntranspiration in plants. These peculiar deformation behaviors are validated by\nsingle-nanopore molecular dynamics simulations and described by a continuum\nmodel that highlights the importance of expansive surface stresses at the pore\nwalls (Bangham effect) and the buildup or release of contractile Laplace\npressures as nanoscale menisci collectively advance, arrest, or disappear. Our\nmodel predicts that these observations are valid not only for water imbibition\nin silica, but for any imbibition process in nanopores, regardless of the\nliquid/solid combination. This also suggests that simple deformation\nmeasurements can be used to quantify surface stresses and Laplace pressures or\ntransport in a wide variety of natural and artificial porous media.","publication_date":1700604741,"paper_link":"http://arxiv.org/pdf/2311.13025v1","categories":["Physics"],"abstract":"Capillarity-driven transport in nanoporous solids is ubiquitous in nature and is of increasing importance for the functionality of modern liquid-infused engineering materials. During imbibition, highly curved menisci are driven by negative Laplace pressures of several hundred atmospheres, exerting an enormous contractile load on an increasing portion of the porous matrix. Due to the challenge of simultaneously monitoring imbibition and deformation with high spatial resolution, the resulting coupling of solid elasticity to liquid capillarity has remained largely unexplored. Here, we study water imbibition in mesoporous silica using optical imaging, gravimetry, and high-resolution dilatometry. In contrast to an expected Laplace pressure-induced contraction, we find a square-root-of-time expansion and an additional abrupt length increase when the menisci reach the top surface. The final expansion is absent when we stop the imbibition front inside the porous medium in a dynamic imbibition-evaporation equilibrium, as is typical for water transport and transpiration in plants. These peculiar deformation behaviors are validated by single-nanopore molecular dynamics simulations and described by a continuum model that highlights the importance of expansive surface stresses at the pore walls (Bangham effect) and the buildup or release of contractile Laplace pressures as nanoscale menisci collectively advance, arrest, or disappear. Our model predicts that these observations are valid not only for water imbibition in silica, but for any imbibition process in nanopores, regardless of the liquid/solid combination. This also suggests that simple deformation measurements can be used to quantify surface stresses and Laplace pressures or transport in a wide variety of natural and artificial porous media."}
{"title":"Modeling of Energy Distributions in Pseudo-Rest Frame Analyses of Two-Body Decays with Missing Particles","authors":["J. A. Colorado-Caicedo","C. Lizama-Garcia","E. De La Cruz-Burelo"],"raw_abstract":"In this study, we introduce a parametric function designed to describe the\nenergy distribution of the observed particle within the framework of two-body\ndecays involving one undetected particle, analyzed using the pseudo-rest frame\napproximation. While we illustrate its effectiveness through the specific case\nstudy of the Lepton Flavor Violating decay $\\tau \\rightarrow l+\\alpha$, this\nparametric function is broadly applicable to a wide range of pseudo-rest frame\nmethod-related searches involving undetected particles. Remarkably, it requires\nonly a single simulation to account for the smearing effects resulting from the\npseudo-rest frame approximation. The uniqueness of this function lies in its\ndependency on the mass of the undetected particle, enabling continuous\nexploration of the mass parameter space. We validate the performance of our\nparametric function using simulated datasets and find that it exhibits\ncomparable performance to traditional simulation-based methods. Notably, our\napproach offers the distinct advantage of accommodating any mass value for the\nundetected particle without the need for multiple simulations.","publication_date":1700604586,"paper_link":"http://arxiv.org/pdf/2311.13023v1","categories":["Physics"],"abstract":"In this study, we introduce a parametric function designed to describe the energy distribution of the observed particle within the framework of two-body decays involving one undetected particle, analyzed using the pseudo-rest frame approximation. While we illustrate its effectiveness through the specific case study of the Lepton Flavor Violating decay __FORMULA__, this parametric function is broadly applicable to a wide range of pseudo-rest frame method-related searches involving undetected particles. Remarkably, it requires only a single simulation to account for the smearing effects resulting from the pseudo-rest frame approximation. The uniqueness of this function lies in its dependency on the mass of the undetected particle, enabling continuous exploration of the mass parameter space. We validate the performance of our parametric function using simulated datasets and find that it exhibits comparable performance to traditional simulation-based methods. Notably, our approach offers the distinct advantage of accommodating any mass value for the undetected particle without the need for multiple simulations."}
{"title":"Unveiling the cosmic dawn and epoch of reionization using cosmic 21-cm signal","authors":["Ankita Bera"],"raw_abstract":"The cosmological 21-cm signal from neutral hydrogen, which is considered as a\npromising tool, is being used to observe and study the cosmic dawn (CD) and\nepoch of reionization (EoR). A significant part of this thesis focuses on the\nsemi-analytical modeling of the global HI 21-cm signal from CD considering\nseveral physical processes. Further, it investigates the nature of galaxies\nthat dominate during CD and EoR using current available observations. In our\nwork, we study the redshift evolution of the primordial magnetic field (PMF)\nduring the dark ages and cosmic dawn, and prospects of constraining it in light\nof EDGES 21-cm signal in the `colder IGM' background. We find that the IGM\nheating rate due to the PMF enhances compared to the standard scenario.\nHowever, PMF is an unlikely candidate for explaining the rise of the EDGES\nabsorption signal at lower redshift. We further consider, in detail, the\nheating of the IGM owing to cosmic ray protons generated by the supernovae from\nboth early Pop~III and Pop~II stars. We show that the EDGES signal can be well\nfitted by the cosmic ray heating along with the Lyman-$\\alpha$ coupling and the\ndark matter-baryon interaction. We, further, explore the conditions by which\nthe EDGES detection is consistent with current reionization and\npost-reionization observations. By coupling a physically motivated source model\nderived from radiative transfer hydrodynamic simulations of reionization to a\nMCMC sampler, we find that high contribution from low-mass halos along with\nhigh photon escape fractions are required to simultaneously reproduce the\nexisting constraints. With the extreme effort in building more advanced and\nsophisticated telescopes, the future 21-cm signal detection would be able to\nprovide better constraints on the amplitude of PMF and the efficiencies on\ncosmic ray protons, and consequently on early star formation rates.","publication_date":1700603748,"paper_link":"http://arxiv.org/pdf/2311.13019v1","categories":["Physics"],"abstract":"The cosmological 21-cm signal from neutral hydrogen, which is considered as a promising tool, is being used to observe and study the cosmic dawn (CD) and epoch of reionization (EoR). A significant part of this thesis focuses on the semi-analytical modeling of the global HI 21-cm signal from CD considering several physical processes. Further, it investigates the nature of galaxies that dominate during CD and EoR using current available observations. In our work, we study the redshift evolution of the primordial magnetic field (PMF) during the dark ages and cosmic dawn, and prospects of constraining it in light of EDGES 21-cm signal in the `colder IGM' background. We find that the IGM heating rate due to the PMF enhances compared to the standard scenario. However, PMF is an unlikely candidate for explaining the rise of the EDGES absorption signal at lower redshift. We further consider, in detail, the heating of the IGM owing to cosmic ray protons generated by the supernovae from both early Pop~III and Pop~II stars. We show that the EDGES signal can be well fitted by the cosmic ray heating along with the Lyman-__FORMULA__ coupling and the dark matter-baryon interaction. We, further, explore the conditions by which the EDGES detection is consistent with current reionization and post-reionization observations. By coupling a physically motivated source model derived from radiative transfer hydrodynamic simulations of reionization to a MCMC sampler, we find that high contribution from low-mass halos along with high photon escape fractions are required to simultaneously reproduce the existing constraints. With the extreme effort in building more advanced and sophisticated telescopes, the future 21-cm signal detection would be able to provide better constraints on the amplitude of PMF and the efficiencies on cosmic ray protons, and consequently on early star formation rates."}
{"title":"Ancilla quantum measurements on interacting chains: Sensitivity of entanglement dynamics to the type and concentration of detectors","authors":["Elmer V. H. Doggen","Igor V. Gornyi","Alexander D. Mirlin"],"raw_abstract":"We consider a quantum many-body lattice system that is coupled to ancillary\ndegrees of freedom (\"detectors\"), which are periodically measured by means of\nstrong projective measurements. The concentration $\\rho_a$ of ancillae and\ntheir coupling $M$ to the main system are considered as parameters. We explore\nthe dynamics of density and of entanglement entropy in the chain, for various\nvalues of $\\rho_a$ and $M$ for two models of the detector-chain interaction\nthat couple the local density in the chain to a detector degree of freedom. It\nis found that, for the density-density ($S_z s_z$-type in spin language)\ncoupling, the critical values $M_c$ for the measurement-induced entanglement\ntransition depends sensitively on $\\rho_a$. Moreover, our results indicate that\nfor a sufficiently small $\\rho_a$ the transition in this model disappears,\ni.e., a finite density of detectors is needed to reach a disentangling phase.\nThe behavior is qualitatively different for the second model, with\ndensity-hopping ($S_z s_x$-type) coupling. Specifically, the dynamics is much\nless sensitive to the concentration $\\rho_a$ of detectors than in the first\nmodel. Furthermore, the dependence of entanglement on the coupling strength $M$\nis strongly non-monotonic, indicating re-entrance of the entangling phase at\nlarge $M$.","publication_date":1700602871,"paper_link":"http://arxiv.org/pdf/2311.13011v1","categories":["Physics"],"abstract":"We consider a quantum many-body lattice system that is coupled to ancillary degrees of freedom (\"detectors\"), which are periodically measured by means of strong projective measurements. The concentration __FORMULA__ of ancillae and their coupling __FORMULA__ to the main system are considered as parameters. We explore the dynamics of density and of entanglement entropy in the chain, for various values of __FORMULA__ and __FORMULA__ for two models of the detector-chain interaction that couple the local density in the chain to a detector degree of freedom. It is found that, for the density-density (__FORMULA__-type in spin language) coupling, the critical values __FORMULA__ for the measurement-induced entanglement transition depends sensitively on __FORMULA__. Moreover, our results indicate that for a sufficiently small __FORMULA__ the transition in this model disappears, i.e., a finite density of detectors is needed to reach a disentangling phase. The behavior is qualitatively different for the second model, with density-hopping (__FORMULA__-type) coupling. Specifically, the dynamics is much less sensitive to the concentration __FORMULA__ of detectors than in the first model. Furthermore, the dependence of entanglement on the coupling strength __FORMULA__ is strongly non-monotonic, indicating re-entrance of the entangling phase at large __FORMULA__."}
{"title":"Modeling and characterization of TES-based detectors for the Ricochet experiment","authors":["R. Chen","E. Figueroa-Feliciano","G. Bratrud","C. L. Chang","L. Chaplinsky","E. Cudmore","W. Van De Pontseele","J. A. Formaggio","P. Harrington","S. A. Hertel","Z. Hong","K. T. Kennard","M. Li","M. Lisovenko","L. O. Mateo","D. W. Mayer","V. Novati","P. K. Patel","H. D. Pinckney","N. Raha","F. C. Reyes","A. Rodriguez","B. Schmidt","J. Stachurska","C. Veihmeyer","G. Wang","L. Winslow","V. G. Yefremenko","J. Zhang"],"raw_abstract":"Coherent elastic neutrino-nucleus scattering (CE$\\nu$NS) offers a valuable\napproach in searching for physics beyond the Standard Model. The Ricochet\nexperiment aims to perform a precision measurement of the CE$\\nu$NS spectrum at\nthe Institut Laue-Langevin nuclear reactor with cryogenic solid-state\ndetectors. The experiment plans to employ an array of cryogenic thermal\ndetectors, each with a mass around 30 g and an energy threshold of sub-100 eV.\nThe array includes nine detectors read out by Transition-Edge Sensors (TES).\nThese TES based detectors will also serve as demonstrators for future neutrino\nexperiments with thousands of detectors. In this article we present an update\nin the characterization and modeling of a prototype TES detector.","publication_date":1700602431,"paper_link":"http://arxiv.org/pdf/2311.13007v1","categories":["Physics"],"abstract":"Coherent elastic neutrino-nucleus scattering (CE__FORMULA__NS) offers a valuable approach in searching for physics beyond the Standard Model. The Ricochet experiment aims to perform a precision measurement of the CE__FORMULA__NS spectrum at the Institut Laue-Langevin nuclear reactor with cryogenic solid-state detectors. The experiment plans to employ an array of cryogenic thermal detectors, each with a mass around 30 g and an energy threshold of sub-100 eV. The array includes nine detectors read out by Transition-Edge Sensors (TES). These TES based detectors will also serve as demonstrators for future neutrino experiments with thousands of detectors. In this article we present an update in the characterization and modeling of a prototype TES detector."}
{"title":"Convex optimization of contour deformations","authors":["Scott Lawrence","Yukari Yamauchi"],"raw_abstract":"We discuss various formal aspects of contour deformations used to alleviate\nsign problems; most importantly, relating these contour deformations to a\ncertain convex optimization problem. As a consequence of this connection we\ndescribe a general method for proving upper bounds on the average phase\nachievable by the contour deformation method. Using this method we show that\nAbelian lattice Yang-Mills in two spacetime dimensions possesses, for many\nvalues of the complex coupling, an exponential sign problem that cannot be\nremoved via any contour deformation.","publication_date":1700601972,"paper_link":"http://arxiv.org/pdf/2311.13002v1","categories":["Physics"],"abstract":"We discuss various formal aspects of contour deformations used to alleviate sign problems; most importantly, relating these contour deformations to a certain convex optimization problem. As a consequence of this connection we describe a general method for proving upper bounds on the average phase achievable by the contour deformation method. Using this method we show that Abelian lattice Yang-Mills in two spacetime dimensions possesses, for many values of the complex coupling, an exponential sign problem that cannot be removed via any contour deformation."}
{"title":"Systematic Review Protocol: Requirements Engineering in Quantum Computing","authors":["Samuel Sep\u00falveda","Ania Cravero"],"raw_abstract":"Context: Quantum computing (QC) represents a paradigm shift in computational\ncapabilities, presenting unique challenges in requirements engineering (RE).\nThe complexity of quantum systems and rapid technological advancements\nnecessitate a comprehensive understanding of the current state and future\ntrajectories in RE for QC. Objective: A protocol for carrying out a systematic\nliterature review about the evidence for identifying and analyzing the\nchallenges in RE for QC software. It seeks to evaluate the current\nmethodologies employed in this domain and propose a forward-looking perspective\non the evolution of these methodologies to meet future industry and academic\nneeds. Method: This protocol employs a structured approach to search and\nanalyze relevant literature systematically, according to Barbara Kitchenham's\nguidelines. Results: A validated protocol to conduct a systematic review. The\nprotocol is expected to yield diverse literature spanning theoretical\nframeworks, empirical studies, and methodological advancements in RE for QC. It\nwill highlight the current challenges, opportunities, and future directions,\noffering insights into the field's academic and practical aspects. Conclusions:\nThe systematic review aims to provide a nuanced understanding of the RE\nlandscape in QC. It will offer valuable insights for academic researchers,\nindustry professionals, software engineers, industry analysts, and educators,\nshaping the future discourse in QC development.","publication_date":1700601465,"paper_link":"http://arxiv.org/pdf/2311.12998v1","categories":["Physics"],"abstract":"Context: Quantum computing (QC) represents a paradigm shift in computational capabilities, presenting unique challenges in requirements engineering (RE). The complexity of quantum systems and rapid technological advancements necessitate a comprehensive understanding of the current state and future trajectories in RE for QC. Objective: A protocol for carrying out a systematic literature review about the evidence for identifying and analyzing the challenges in RE for QC software. It seeks to evaluate the current methodologies employed in this domain and propose a forward-looking perspective on the evolution of these methodologies to meet future industry and academic needs. Method: This protocol employs a structured approach to search and analyze relevant literature systematically, according to Barbara Kitchenham's guidelines. Results: A validated protocol to conduct a systematic review. The protocol is expected to yield diverse literature spanning theoretical frameworks, empirical studies, and methodological advancements in RE for QC. It will highlight the current challenges, opportunities, and future directions, offering insights into the field's academic and practical aspects. Conclusions: The systematic review aims to provide a nuanced understanding of the RE landscape in QC. It will offer valuable insights for academic researchers, industry professionals, software engineers, industry analysts, and educators, shaping the future discourse in QC development."}
{"title":"GPDs and gravitational form factors of nucleons (quark and gluon contributions)","authors":["O. V. Selyugin","O. V. Teryaev"],"raw_abstract":"Taking into account the recent parameterizations of parton distribution\nfunctions (PDFs), % obtained in the recent time, the momentum transfer\ndependence of generalized parton distributions (GPDs) of nucleons is obtained\nin the limit $\\xi \\rightarrow 0$. The gravitational quark and gluon form\nfactors of nucleons are calculated. It is shown that the gluon gravitational\nradius of the nucleon is comparable to the electromagnetic radius of the\nproton. The power dependence of form factors is investigated. As a result, it\nwas obtained that the quark gravitational form factor is reproduced by the\ndipole form, while the form of the gluon gravitation form factor corresponds to\nthe triple form.","publication_date":1700599770,"paper_link":"http://arxiv.org/pdf/2311.12988v1","categories":["Physics"],"abstract":"Taking into account the recent parameterizations of parton distribution functions (PDFs), % obtained in the recent time, the momentum transfer dependence of generalized parton distributions (GPDs) of nucleons is obtained in the limit __FORMULA__. The gravitational quark and gluon form factors of nucleons are calculated. It is shown that the gluon gravitational radius of the nucleon is comparable to the electromagnetic radius of the proton. The power dependence of form factors is investigated. As a result, it was obtained that the quark gravitational form factor is reproduced by the dipole form, while the form of the gluon gravitation form factor corresponds to the triple form."}
{"title":"Gravitational Waves from Thurston Geometries","authors":["Daniel Flores-Alfonso","Cesar S. Lopez-Monsalvo","Marco Maceda"],"raw_abstract":"It has been established that the famous Thurston geometries of\nlow-dimensional topology have four intrinsically Lorentzian analogs. We propose\nstudying these spacetimes in the framework of general relativity and find\nmatter content sourcing them. Three of these spacetimes support electromagnetic\nradiation, while, the other is partially sourced by a nonnull field.\nFurthermore, new configurations are found which do without symmetry\ninheritance. Some of these fields go undetected by the background due to\nnonminimal coupling and provide a novel type of gravitational Cheshire effect.","publication_date":1700599425,"paper_link":"http://arxiv.org/pdf/2311.12985v1","categories":["Physics"],"abstract":"It has been established that the famous Thurston geometries of low-dimensional topology have four intrinsically Lorentzian analogs. We propose studying these spacetimes in the framework of general relativity and find matter content sourcing them. Three of these spacetimes support electromagnetic radiation, while, the other is partially sourced by a nonnull field. Furthermore, new configurations are found which do without symmetry inheritance. Some of these fields go undetected by the background due to nonminimal coupling and provide a novel type of gravitational Cheshire effect."}
{"title":"How to distinguish white dwarf and neutron star X-ray binaries during their X-ray outbursts?","authors":["Lev Titarchuk","Elena Seifina"],"raw_abstract":"Neutron stars (NSs) and white dwarfs (WDs) are characterized by different\ngeometric and physical properties, but their observed properties are often\nsimilar, making them difficult to distinguish. Therefore, it is desirable to\nsearch for their spectral features that could be easily identified from\nobservations. We present spectral and timing signatures of NSs and WDs hosted\nin accreting X-ray binaries that can be easily identified from X-ray\nobservations. We perform spectral and timing analysis of 4U~1636--53 and\nSS~Cygni, as typical representatives of such NS and WD binaries, based on their\nX-ray observations by RXTE, ASCA, Suzaku and BeppoSAX uising {\\it\nComptonization} spectral model. As a result, we formulate a criterion that\nmakes it easy to distinguish NS from WD in such binaries: NS X-rays exhibits\nclear quasi-stable behavior with the index $\\Gamma\\to2$ and is characterized by\nquasi periodic oscillations (QPOs) at $\\nu_{QPO} >0.5$~Hz, although WD X-rays\nis stable with $\\Gamma \\to1.85$ and is accompanied by QPOs at\n$\\nu_{QPO}<0.05$~Hz during source outbursts. In addition, we revealed that in\n4U~1636--53 the mHz QPOs anti-correlate with the plasma temperature, $T_e$ of\nCompton cloud (or the corona around a NS. This allowed us to associate mHz-QPOs\nwith the corona dynamics during outburst cycle. The above index effect, now\nwell established for 4U~1636--53 and SS~Cygni using extensive observations, has\npreviously been found in other low-mass X-ray NS and WD binaries and agrees\nwell with the criterion for distinguishing NSs and WDs presented here.","publication_date":1700598835,"paper_link":"http://arxiv.org/pdf/2311.12982v1","categories":["Physics"],"abstract":"Neutron stars (NSs) and white dwarfs (WDs) are characterized by different geometric and physical properties, but their observed properties are often similar, making them difficult to distinguish. Therefore, it is desirable to search for their spectral features that could be easily identified from observations. We present spectral and timing signatures of NSs and WDs hosted in accreting X-ray binaries that can be easily identified from X-ray observations. We perform spectral and timing analysis of 4U~1636--53 and SS~Cygni, as typical representatives of such NS and WD binaries, based on their X-ray observations by RXTE, ASCA, Suzaku and BeppoSAX uising {\\it Comptonization} spectral model. As a result, we formulate a criterion that makes it easy to distinguish NS from WD in such binaries: NS X-rays exhibits clear quasi-stable behavior with the index __FORMULA__ and is characterized by quasi periodic oscillations (QPOs) at __FORMULA__~Hz, although WD X-rays is stable with __FORMULA__ and is accompanied by QPOs at __FORMULA__~Hz during source outbursts. In addition, we revealed that in 4U~1636--53 the mHz QPOs anti-correlate with the plasma temperature, __FORMULA__ of Compton cloud (or the corona around a NS. This allowed us to associate mHz-QPOs with the corona dynamics during outburst cycle. The above index effect, now well established for 4U~1636--53 and SS~Cygni using extensive observations, has previously been found in other low-mass X-ray NS and WD binaries and agrees well with the criterion for distinguishing NSs and WDs presented here."}
{"title":"Nonlinear Self-Calibrated Spectrometer with Single GeSe-InSe Heterojunction Device","authors":["Rana Darweesh","Rajesh Kumar Yadav","Elior Adler","Michal Poplinger","Adi Levi","Jea-Jung Lee","Amir Leshem","Ashwin Ramasubramaniam","Fengnian Xia","Doron Naveh"],"raw_abstract":"Optical spectroscopy the measurement of electromagnetic spectra is\nfundamental to various scientific domains and serves as the building block of\nnumerous technologies. Computational spectrometry is an emerging field that\nemploys an array of photodetectors with different spectral responses or a\nsingle photodetector device with tunable spectral response, in conjunction with\nnumerical algorithms, for spectroscopic measurements. Compact single\nphotodetectors made from layered materials are particularly attractive, since\nthey eliminate the need for bulky mechanical and optical components used in\ntraditional spectrometers and can easily be engineered as heterostructures to\noptimize device performance. However, compact tunable photodetectors are\ntypically nonlinear devices and this adds complexity to extracting optical\nspectra from the device response. Here, we report on the training of an\nartificial neural network (ANN) to recover the full nonlinear spectral\nphotoresponse of a nonlinear problem of high dimensionality of a single\nGeSe-InSe p-n heterojunction device. We demonstrate the functionality of a\ncalibrated spectrometer in the spectral range of 400-1100 nm, with a small\ndevice footprint of ~25X25 micrometers, and we achieve a mean reconstruction\nerror of 0.0002 for the power-spectrum at a spectral resolution of 0.35 nm.\nUsing our device, we demonstrate a solution to metamerism, an apparent matching\nof colors with different power spectral distributions, which is a fundamental\nproblem in optical imaging.","publication_date":1700598709,"paper_link":"http://arxiv.org/pdf/2311.12980v1","categories":["Physics"],"abstract":"Optical spectroscopy the measurement of electromagnetic spectra is fundamental to various scientific domains and serves as the building block of numerous technologies. Computational spectrometry is an emerging field that employs an array of photodetectors with different spectral responses or a single photodetector device with tunable spectral response, in conjunction with numerical algorithms, for spectroscopic measurements. Compact single photodetectors made from layered materials are particularly attractive, since they eliminate the need for bulky mechanical and optical components used in traditional spectrometers and can easily be engineered as heterostructures to optimize device performance. However, compact tunable photodetectors are typically nonlinear devices and this adds complexity to extracting optical spectra from the device response. Here, we report on the training of an artificial neural network (ANN) to recover the full nonlinear spectral photoresponse of a nonlinear problem of high dimensionality of a single GeSe-InSe p-n heterojunction device. We demonstrate the functionality of a calibrated spectrometer in the spectral range of 400-1100 nm, with a small device footprint of ~25X25 micrometers, and we achieve a mean reconstruction error of 0.0002 for the power-spectrum at a spectral resolution of 0.35 nm. Using our device, we demonstrate a solution to metamerism, an apparent matching of colors with different power spectral distributions, which is a fundamental problem in optical imaging."}
{"title":"Physics-Informed Priors with Application to Boundary Layer Velocity","authors":["Luca Menicali","David H. Richter","Stefano Castruccio"],"raw_abstract":"One of the most popular recent areas of machine learning predicates the use\nof neural networks augmented by information about the underlying process in the\nform of Partial Differential Equations (PDEs). These physics-informed neural\nnetworks are obtained by penalizing the inference with a PDE, and have been\ncast as a minimization problem currently lacking a formal approach to quantify\nthe uncertainty. In this work, we propose a novel model-based framework which\nregards the PDE as a prior information of a deep Bayesian neural network. The\nprior is calibrated without data to resemble the PDE solution in the prior\nmean, while our degree in confidence on the PDE with respect to the data is\nexpressed in terms of the prior variance. The information embedded in the PDE\nis then propagated to the posterior yielding physics-informed forecasts with\nuncertainty quantification. We apply our approach to a simulated viscous fluid\nand to experimentally-obtained turbulent boundary layer velocity in a wind\ntunnel using an appropriately simplified Navier-Stokes equation. Our approach\nrequires very few observations to produce physically-consistent forecasts as\nopposed to non-physical forecasts stemming from non-informed priors, thereby\nallowing forecasting complex systems where some amount of data as well as some\ncontextual knowledge is available.","publication_date":1700598350,"paper_link":"http://arxiv.org/pdf/2311.12978v1","categories":["Statistics","Physics"],"abstract":"One of the most popular recent areas of machine learning predicates the use of neural networks augmented by information about the underlying process in the form of Partial Differential Equations (PDEs). These physics-informed neural networks are obtained by penalizing the inference with a PDE, and have been cast as a minimization problem currently lacking a formal approach to quantify the uncertainty. In this work, we propose a novel model-based framework which regards the PDE as a prior information of a deep Bayesian neural network. The prior is calibrated without data to resemble the PDE solution in the prior mean, while our degree in confidence on the PDE with respect to the data is expressed in terms of the prior variance. The information embedded in the PDE is then propagated to the posterior yielding physics-informed forecasts with uncertainty quantification. We apply our approach to a simulated viscous fluid and to experimentally-obtained turbulent boundary layer velocity in a wind tunnel using an appropriately simplified Navier-Stokes equation. Our approach requires very few observations to produce physically-consistent forecasts as opposed to non-physical forecasts stemming from non-informed priors, thereby allowing forecasting complex systems where some amount of data as well as some contextual knowledge is available."}
{"title":"Conformal Perturbation Theory on K3: The Quartic Gepner Point","authors":["Christoph A. Keller"],"raw_abstract":"The Gepner model (2)^4 describes the sigma model of the Fermat quartic K3\nsurface. Moving through the nearby moduli space using conformal perturbation\ntheory, we investigate how the conformal weights of its fields change at first\nand second order and find approximate minima. This serves as a toy model for a\nmechanism that could produce new chiral fields and possibly new nearby rational\nCFTs.","publication_date":1700598233,"paper_link":"http://arxiv.org/pdf/2311.12974v1","categories":["Physics"],"abstract":"The Gepner model (2)^4 describes the sigma model of the Fermat quartic K3 surface. Moving through the nearby moduli space using conformal perturbation theory, we investigate how the conformal weights of its fields change at first and second order and find approximate minima. This serves as a toy model for a mechanism that could produce new chiral fields and possibly new nearby rational CFTs."}
{"title":"Magneto-interferometry of multiterminal Josephson junctions","authors":["R\u00e9gis M\u00e9lin","Clemens B. Winkelmann","Romain Danneau"],"raw_abstract":"We report a theoretical study of multiterminal Josephson junctions under the\ninfluence of a magnetic field $B$. We consider a ballistic rectangular\ntwo-dimensional metal $N_0$ connected by the edges to the left, right, top and\nbottom superconductors $S_L$, $S_R$, $S_T$ and $S_B$, respectively. We\nnumerically calculate in the long-junction limit the critical current $I_c$\nversus $B$ between $S_L$ and $S_R$ for various aspect ratios, with $S_T$ and\n$S_B$ playing the role of superconducting mirrors. We find $I_c$ to be enhanced\nby orders of magnitude, especially at long distance, due to the phase rigidity\nprovided by the mirrors. We obtain superconducting quantum interference\ndevice-like magnetic oscillations. With symmetric couplings, the\nself-consistent superconducting phase variables of the top and bottom mirrors\ntake the values $0$ or $\\pi$, as for emerging Ising degrees of freedom. We\npropose a simple effective Josephson junction circuit model that is compatible\nwith these microscopic numerical calculations. From the $I_c(B)$ patterns we\ninfer where the supercurrent flows in various device geometries. In particular\nin the elongated geometry, we show that the supercurrent flows between all\npairs of contacts, which allows exploring the full phase space of the relevant\nphase differences.","publication_date":1700597144,"paper_link":"http://arxiv.org/pdf/2311.12964v1","categories":["Physics"],"abstract":"We report a theoretical study of multiterminal Josephson junctions under the influence of a magnetic field __FORMULA__. We consider a ballistic rectangular two-dimensional metal __FORMULA__ connected by the edges to the left, right, top and bottom superconductors __FORMULA__, __FORMULA__, __FORMULA__ and __FORMULA__, respectively. We numerically calculate in the long-junction limit the critical current __FORMULA__ versus __FORMULA__ between __FORMULA__ and __FORMULA__ for various aspect ratios, with __FORMULA__ and __FORMULA__ playing the role of superconducting mirrors. We find __FORMULA__ to be enhanced by orders of magnitude, especially at long distance, due to the phase rigidity provided by the mirrors. We obtain superconducting quantum interference device-like magnetic oscillations. With symmetric couplings, the self-consistent superconducting phase variables of the top and bottom mirrors take the values __FORMULA__ or __FORMULA__, as for emerging Ising degrees of freedom. We propose a simple effective Josephson junction circuit model that is compatible with these microscopic numerical calculations. From the __FORMULA__ patterns we infer where the supercurrent flows in various device geometries. In particular in the elongated geometry, we show that the supercurrent flows between all pairs of contacts, which allows exploring the full phase space of the relevant phase differences."}
{"title":"Teaching Quantum Computing using Microsoft Quantum Development Kit and Azure Quantum","authors":["Mariia Mykhailova"],"raw_abstract":"This report describes my experience teaching a graduate-level quantum\ncomputing course at Northeastern University in the academic year 2022-23. The\ncourse takes a practical, software-driven approach to the course, teaching\nbasic quantum concepts and algorithms through hands-on programming assignments\nand a software-focused final project. The course guides learners through all\nstages of the quantum software development process, from solving quantum\ncomputing problems and implementing solutions to debugging quantum programs,\noptimizing the code, and running the code on quantum hardware. This report\noffers instructors who want to adopt a similar practical approach to teaching\nquantum computing a comprehensive guide to getting started.","publication_date":1700596523,"paper_link":"http://arxiv.org/pdf/2311.12960v1","categories":["Physics"],"abstract":"This report describes my experience teaching a graduate-level quantum computing course at Northeastern University in the academic year 2022-23. The course takes a practical, software-driven approach to the course, teaching basic quantum concepts and algorithms through hands-on programming assignments and a software-focused final project. The course guides learners through all stages of the quantum software development process, from solving quantum computing problems and implementing solutions to debugging quantum programs, optimizing the code, and running the code on quantum hardware. This report offers instructors who want to adopt a similar practical approach to teaching quantum computing a comprehensive guide to getting started."}
{"title":"A nonlocal equation describing tumor growth","authors":["Rafael Granero-Belinch\u00f3n","Martina Magliocca"],"raw_abstract":"Cancer is a very complex phenomenon that involves many different scales and\nsituations. In this paper we consider a free boundary problem describing the\nevolution of a tumor colony and we derive a new asymptotic model for tumor\ngrowth. We focus on the case of a single phase tumor colony taking into account\nchemotactic effects in an early stage where there is no necrotic inner region.\nThus, our model is valid for the case of multilayer avascular tumors with very\nlittle access to both nutrients and inhibitors or the case where the amount of\nnutrients and inhibitors is very similar to the amount consumed by the\nmultilayer tumor cells. Our model takes the form of a single nonlocal and\nnonlinear partial differential equation. Besides deriving the model, we also\nprove a well-posedness result.","publication_date":1700596413,"paper_link":"http://arxiv.org/pdf/2311.12958v1","categories":["Mathematics","Physics"],"abstract":"Cancer is a very complex phenomenon that involves many different scales and situations. In this paper we consider a free boundary problem describing the evolution of a tumor colony and we derive a new asymptotic model for tumor growth. We focus on the case of a single phase tumor colony taking into account chemotactic effects in an early stage where there is no necrotic inner region. Thus, our model is valid for the case of multilayer avascular tumors with very little access to both nutrients and inhibitors or the case where the amount of nutrients and inhibitors is very similar to the amount consumed by the multilayer tumor cells. Our model takes the form of a single nonlocal and nonlinear partial differential equation. Besides deriving the model, we also prove a well-posedness result."}
{"title":"Differential rotation in convecting spherical shells with non-uniform viscosity and entropy diffusivity","authors":["Parag Gupta","David MacTaggart","Radostin D. Simitev"],"raw_abstract":"Contemporary three-dimensional physics-based simulations of the solar\nconvection zone disagree with observations. They feature differential rotation\nsubstantially different from the true rotation inferred by solar\nhelioseismology and exhibit a conveyor belt of convective \"Busse\" columns not\nfound in observations. To help unravel this so-called \"convection conundrum\",\nwe use a three-dimensional pseudospectral simulation code to investigate how\nradially non-uniform viscosity and entropy diffusivity affect differential\nrotation and convective flow patterns in density-stratified rotating spherical\nfluid shells. We find that radial non-uniformity in fluid properties enhances\npolar convection, which, in turn, induces non-negligible lateral entropy\ngradients that lead to large deviations from differential rotation geostrophy\ndue to thermal wind balance. We report simulations wherein this mechanism\nmaintains differential rotation patterns very similar to the true solar profile\noutside the tangent cylinder, although discrepancies remain at high latitudes.\nThis is significant because differential rotation plays a key role in\nsustaining solar-like cyclic dipolar dynamos.","publication_date":1700596247,"paper_link":"http://arxiv.org/pdf/2311.12957v1","categories":["Physics"],"abstract":"Contemporary three-dimensional physics-based simulations of the solar convection zone disagree with observations. They feature differential rotation substantially different from the true rotation inferred by solar helioseismology and exhibit a conveyor belt of convective \"Busse\" columns not found in observations. To help unravel this so-called \"convection conundrum\", we use a three-dimensional pseudospectral simulation code to investigate how radially non-uniform viscosity and entropy diffusivity affect differential rotation and convective flow patterns in density-stratified rotating spherical fluid shells. We find that radial non-uniformity in fluid properties enhances polar convection, which, in turn, induces non-negligible lateral entropy gradients that lead to large deviations from differential rotation geostrophy due to thermal wind balance. We report simulations wherein this mechanism maintains differential rotation patterns very similar to the true solar profile outside the tangent cylinder, although discrepancies remain at high latitudes. This is significant because differential rotation plays a key role in sustaining solar-like cyclic dipolar dynamos."}
{"title":"Spectroscopy of a single-carrier bilayer graphene quantum dot from time-resolved charge detection","authors":["Hadrien Duprez","Solenn Cances","Andraz Omahen","Michele Masseroni","Max J. Ruckriegel","Christoph Adam","Chuyao Tong","Jonas Gerber","Rebekka Garreis","Wister Huang","Lisa G\u00e4chter","Takashi Taniguchi","Kenji Watanabe","Thomas Ihn","Klaus Ensslin"],"raw_abstract":"We measured the spectrum of a single-carrier bilayer graphene quantum dot as\na function of both parallel and perpendicular magnetic fields, using a\ntime-resolved charge detection technique that gives access to individual tunnel\nevents. Thanks to our unprecedented energy resolution of 4$\\mu~$eV, we could\ndistinguish all four levels of the dot's first orbital, in particular in the\nrange of magnetic fields where the first and second excited states cross\n($B_\\perp\\lesssim 100~$mT). We thereby experimentally establish, the hitherto\nextrapolated, single-charge carrier spectrum picture and provide a new upper\nbound for the inter-valley mixing, equal to our energy resolution.","publication_date":1700594894,"paper_link":"http://arxiv.org/pdf/2311.12949v1","categories":["Physics"],"abstract":"We measured the spectrum of a single-carrier bilayer graphene quantum dot as a function of both parallel and perpendicular magnetic fields, using a time-resolved charge detection technique that gives access to individual tunnel events. Thanks to our unprecedented energy resolution of 4__FORMULA__eV, we could distinguish all four levels of the dot's first orbital, in particular in the range of magnetic fields where the first and second excited states cross (__FORMULA__mT). We thereby experimentally establish, the hitherto extrapolated, single-charge carrier spectrum picture and provide a new upper bound for the inter-valley mixing, equal to our energy resolution."}
{"title":"PINNs-Based Uncertainty Quantification for Transient Stability Analysis","authors":["Ren Wang","Ming Zhong","Kaidi Xu","Lola Gir\u00e1ldez S\u00e1nchez-Cort\u00e9s","Ignacio de Cominges Guerra"],"raw_abstract":"This paper addresses the challenge of transient stability in power systems\nwith missing parameters and uncertainty propagation in swing equations. We\nintroduce a novel application of Physics-Informed Neural Networks (PINNs),\nspecifically an Ensemble of PINNs (E-PINNs), to estimate critical parameters\nlike rotor angle and inertia coefficient with enhanced accuracy and reduced\ncomputational load. E-PINNs capitalize on the underlying physical principles of\nswing equations to provide a robust solution. Our approach not only facilitates\nefficient parameter estimation but also quantifies uncertainties, delivering\nprobabilistic insights into the system behavior. The efficacy of E-PINNs is\ndemonstrated through the analysis of $1$-bus and $2$-bus systems, highlighting\nthe model's ability to handle parameter variability and data scarcity. The\nstudy advances the application of machine learning in power system stability,\npaving the way for reliable and computationally efficient transient stability\nanalysis.","publication_date":1700594509,"paper_link":"http://arxiv.org/pdf/2311.12947v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper addresses the challenge of transient stability in power systems with missing parameters and uncertainty propagation in swing equations. We introduce a novel application of Physics-Informed Neural Networks (PINNs), specifically an Ensemble of PINNs (E-PINNs), to estimate critical parameters like rotor angle and inertia coefficient with enhanced accuracy and reduced computational load. E-PINNs capitalize on the underlying physical principles of swing equations to provide a robust solution. Our approach not only facilitates efficient parameter estimation but also quantifies uncertainties, delivering probabilistic insights into the system behavior. The efficacy of E-PINNs is demonstrated through the analysis of __FORMULA__-bus and __FORMULA__-bus systems, highlighting the model's ability to handle parameter variability and data scarcity. The study advances the application of machine learning in power system stability, paving the way for reliable and computationally efficient transient stability analysis."}
{"title":"Synthetic X-ray emission from white dwarf accreting planetary material","authors":["S. Estrada-Dorado","V. Lora","J. A. Toal\u00e1","A. Esquivel","M. A. Guerrero","R. F. Maldonado","Y. -H. Chu"],"raw_abstract":"The emission of hard X-rays associated with white dwarfs (WD) can be\ngenerated by the presence of a stellar companion either by the companion's\ncoronal emission or by an accretion disk formed by material stripped from the\ncompanion. Recent studies have suggested that a Jupiter-like planet can also be\ndonor of material whose accretion onto the WD can generate hard X-rays. We use\nthe {\\sc guacho} code to reproduce the conditions of this WD-planet scenario.\nWith the example of the hard X-ray WD KPD\\,0005+5106, we explore different\nterminal wind velocities and mass-loss rates of a donor planet for a future\nnetwork of simulations to investigate the luminosity and the spectral and\ntemporal properties of the hard X-ray emission in WD-planet systems. Our\nsimulations show that the material stripped from the planet forms a disk and\naccretes onto the WD to reach temperatures high enough to generate hard X-rays\nas usually seen in X-ray binaries with low-mass companions. For high terminal\nwind velocities, the planet material does not form a disk, but it rather\naccretes directly onto the WD surface. The simulations reproduce the X-ray\nluminosity of another X-ray accreting WD (G\\,29$-$38), and only for some times\nreaches the hard X-ray luminosity of KPD\\,0005+5106. The X-ray variability is\nstochastic and does not reproduce the period of KPD\\,0005+5106, suggesting that\nadditional physical processes (e.g., hot spots resulting from magnetic\nchannelling of the accreting material) need to be explored.","publication_date":1700593705,"paper_link":"http://arxiv.org/pdf/2311.12942v1","categories":["Physics"],"abstract":"The emission of hard X-rays associated with white dwarfs (WD) can be generated by the presence of a stellar companion either by the companion's coronal emission or by an accretion disk formed by material stripped from the companion. Recent studies have suggested that a Jupiter-like planet can also be donor of material whose accretion onto the WD can generate hard X-rays. We use the {\\sc guacho} code to reproduce the conditions of this WD-planet scenario. With the example of the hard X-ray WD KPD\\,0005+5106, we explore different terminal wind velocities and mass-loss rates of a donor planet for a future network of simulations to investigate the luminosity and the spectral and temporal properties of the hard X-ray emission in WD-planet systems. Our simulations show that the material stripped from the planet forms a disk and accretes onto the WD to reach temperatures high enough to generate hard X-rays as usually seen in X-ray binaries with low-mass companions. For high terminal wind velocities, the planet material does not form a disk, but it rather accretes directly onto the WD surface. The simulations reproduce the X-ray luminosity of another X-ray accreting WD (G\\,29__FORMULA__38), and only for some times reaches the hard X-ray luminosity of KPD\\,0005+5106. The X-ray variability is stochastic and does not reproduce the period of KPD\\,0005+5106, suggesting that additional physical processes (e.g., hot spots resulting from magnetic channelling of the accreting material) need to be explored."}
{"title":"Controlled Interlayer Exciton Ionization in an Electrostatic Trap in Atomically Thin Heterostructures","authors":["Andrew Y. Joe","Andr\u00e9s M. Mier Valdivia","Luis A. Jauregui","Kateryna Pistunova","Dapeng Ding","You Zhou","Giovanni Scuri","Kristiaan De Greve","Andrey Sushko","Bumho Kim","Takashi Taniguchi","Kenji Watanabe","James C. Hone","Mikhail D. Lukin","Hongkun Park","Philip Kim"],"raw_abstract":"Atomically thin semiconductor heterostructures provide a two-dimensional (2D)\ndevice platform for creating high densities of cold, controllable excitons.\nInterlayer excitons (IEs), bound electrons and holes localized to separate 2D\nquantum well layers, have permanent out-of-plane dipole moments and long\nlifetimes, allowing their spatial distribution to be tuned on demand. Here, we\nemploy electrostatic gates to trap IEs and control their density. By\nelectrically modulating the IE Stark shift, electron-hole pair concentrations\nabove $2\\times10^{12}$ cm$^{-2}$ can be achieved. At this high IE density, we\nobserve an exponentially increasing linewidth broadening indicative of an IE\nionization transition, independent of the trap depth. This runaway threshold\nremains constant at low temperatures, but increases above 20 K, consistent with\nthe quantum dissociation of a degenerate IE gas. Our demonstration of the IE\nionization in a tunable electrostatic trap represents an important step towards\nthe realization of dipolar exciton condensates in solid-state optoelectronic\ndevices.","publication_date":1700593690,"paper_link":"http://arxiv.org/pdf/2311.12941v1","categories":["Physics"],"abstract":"Atomically thin semiconductor heterostructures provide a two-dimensional (2D) device platform for creating high densities of cold, controllable excitons. Interlayer excitons (IEs), bound electrons and holes localized to separate 2D quantum well layers, have permanent out-of-plane dipole moments and long lifetimes, allowing their spatial distribution to be tuned on demand. Here, we employ electrostatic gates to trap IEs and control their density. By electrically modulating the IE Stark shift, electron-hole pair concentrations above __FORMULA__ cm__FORMULA__ can be achieved. At this high IE density, we observe an exponentially increasing linewidth broadening indicative of an IE ionization transition, independent of the trap depth. This runaway threshold remains constant at low temperatures, but increases above 20 K, consistent with the quantum dissociation of a degenerate IE gas. Our demonstration of the IE ionization in a tunable electrostatic trap represents an important step towards the realization of dipolar exciton condensates in solid-state optoelectronic devices."}
{"title":"Shock waves, black hole interiors and holographic RG flows","authors":["Elena C\u00e1ceres","Ayan K. Patra","Juan F. Pedraza"],"raw_abstract":"We study holographic renormalization group (RG) flows perturbed by a shock\nwave in dimensions $d\\geq 2$. The flows are obtained by deforming a holographic\nconformal field theory with a relevant operator, altering the interior geometry\nfrom AdS-Schwarzschild to a more general Kasner universe near the spacelike\nsingularity. We introduce null matter in the form of a shock wave into this\ngeometry and scrutinize its impact on the near-horizon and interior dynamics of\nthe black hole. Using out-of-time-order correlators, we find that the\nscrambling time increases as we increase the strength of the deformation,\nwhereas the butterfly velocity displays a non-monotonic behavior. We examine\nother observables that are more sensitive to the black hole interior, such as\nthe thermal $a$-function and the entanglement velocity. Notably, the\n$a$-function experiences a discontinuous jump across the shock wave, signaling\nan instantaneous loss of degrees of freedom due to the infalling matter. This\njump is interpreted as a `cosmological time skip' which arises from an\ninfinitely boosted length contraction. The entanglement velocity exhibits\nsimilar dependence to the butterfly velocity as we vary the strength of the\ndeformation. Lastly, we extend our analyses to a model where the interior\ngeometry undergoes an infinite sequence of bouncing Kasner epochs.","publication_date":1700593416,"paper_link":"http://arxiv.org/pdf/2311.12940v1","categories":["Physics"],"abstract":"We study holographic renormalization group (RG) flows perturbed by a shock wave in dimensions __FORMULA__. The flows are obtained by deforming a holographic conformal field theory with a relevant operator, altering the interior geometry from AdS-Schwarzschild to a more general Kasner universe near the spacelike singularity. We introduce null matter in the form of a shock wave into this geometry and scrutinize its impact on the near-horizon and interior dynamics of the black hole. Using out-of-time-order correlators, we find that the scrambling time increases as we increase the strength of the deformation, whereas the butterfly velocity displays a non-monotonic behavior. We examine other observables that are more sensitive to the black hole interior, such as the thermal __FORMULA__-function and the entanglement velocity. Notably, the __FORMULA__-function experiences a discontinuous jump across the shock wave, signaling an instantaneous loss of degrees of freedom due to the infalling matter. This jump is interpreted as a `cosmological time skip' which arises from an infinitely boosted length contraction. The entanglement velocity exhibits similar dependence to the butterfly velocity as we vary the strength of the deformation. Lastly, we extend our analyses to a model where the interior geometry undergoes an infinite sequence of bouncing Kasner epochs."}
{"title":"A nonlocal charge for cylindrical gravitational waves","authors":["Robert F. Penna"],"raw_abstract":"The classical scattering of cylindrical gravitational waves is exactly\nsolvable. The motivation for this paper is to understand if the quantum\nscattering problem is also exactly solvable. The classical dynamics is governed\nby a two dimensional sigma model. We study this sigma model's $S$-matrix. We\nconstruct a conserved nonlocal charge and derive the associated tree-level\n$S$-matrix conservation law. We check our conservation law directly using\nFeynman diagrams. The existence of this symmetry is a hint that cylindrical\ngravitational waves might have an exactly solvable $S$-matrix.","publication_date":1700593408,"paper_link":"http://arxiv.org/pdf/2311.12939v1","categories":["Physics"],"abstract":"The classical scattering of cylindrical gravitational waves is exactly solvable. The motivation for this paper is to understand if the quantum scattering problem is also exactly solvable. The classical dynamics is governed by a two dimensional sigma model. We study this sigma model's __FORMULA__-matrix. We construct a conserved nonlocal charge and derive the associated tree-level __FORMULA__-matrix conservation law. We check our conservation law directly using Feynman diagrams. The existence of this symmetry is a hint that cylindrical gravitational waves might have an exactly solvable __FORMULA__-matrix."}
{"title":"Slowly moving black holes in khrono-metric model","authors":["Andrew Kovachik","Sergey Sibiryakov"],"raw_abstract":"We search for solutions describing slowly moving black holes in the\nkhrono-metric model, a modified gravity theory with preferred time (khronon)\nwhich arises at low energies from the non-projectable Horava gravity. We work\nin the decoupling limit when the back-reaction of the khronon on the metric is\nsmall and can be treated perturbatively. For a given black hole velocity, we\nfind a family of solutions parameterized by the khronon propagation speed and\nregular everywhere outside the universal horizon. On the universal horizon they\nhave a weak singularity manifesting itself in a non-analyticity of the khronon\nfield. Using the behavior of khronon at infinity we extract the leading black\nhole sensitivity for which we obtain a simple analytic expression valid\nthroughout the phenomenologically allowed parameter space.","publication_date":1700593264,"paper_link":"http://arxiv.org/pdf/2311.12936v1","categories":["Physics"],"abstract":"We search for solutions describing slowly moving black holes in the khrono-metric model, a modified gravity theory with preferred time (khronon) which arises at low energies from the non-projectable Horava gravity. We work in the decoupling limit when the back-reaction of the khronon on the metric is small and can be treated perturbatively. For a given black hole velocity, we find a family of solutions parameterized by the khronon propagation speed and regular everywhere outside the universal horizon. On the universal horizon they have a weak singularity manifesting itself in a non-analyticity of the khronon field. Using the behavior of khronon at infinity we extract the leading black hole sensitivity for which we obtain a simple analytic expression valid throughout the phenomenologically allowed parameter space."}
{"title":"Sampling-accelerated First-principles Prediction of Phonon Scattering Rates for Converged Thermal Conductivity and Radiative Properties","authors":["Ziqi Guo","Zherui Han","Dudong Feng","Guang Lin","Xiulin Ruan"],"raw_abstract":"First-principles prediction of thermal conductivity and radiative properties\nis crucial. However, computing phonon scattering, especially for four-phonon\nscattering, could be prohibitively expensive, and the thermal conductivity even\nfor silicon was still under-predicted and not converged in the literature. Here\nwe propose a method to estimate scattering rates from a small sample of\nscattering processes using maximum likelihood estimation. The computational\ncost of estimating scattering rates and associated thermal conductivity and\nradiative properties is dramatically reduced by over 99%. This allows us to use\nan unprecedented q-mesh of 32*32*32 for silicon and achieve a converged thermal\nconductivity value that agrees much better with experiments. The accuracy and\nefficiency of our approach make it ideal for the high-throughput screening of\nmaterials for thermal and optical applications.","publication_date":1700593244,"paper_link":"http://arxiv.org/pdf/2311.12935v1","categories":["Physics"],"abstract":"First-principles prediction of thermal conductivity and radiative properties is crucial. However, computing phonon scattering, especially for four-phonon scattering, could be prohibitively expensive, and the thermal conductivity even for silicon was still under-predicted and not converged in the literature. Here we propose a method to estimate scattering rates from a small sample of scattering processes using maximum likelihood estimation. The computational cost of estimating scattering rates and associated thermal conductivity and radiative properties is dramatically reduced by over 99%. This allows us to use an unprecedented q-mesh of 32*32*32 for silicon and achieve a converged thermal conductivity value that agrees much better with experiments. The accuracy and efficiency of our approach make it ideal for the high-throughput screening of materials for thermal and optical applications."}
{"title":"Long-lived Searches of Vector-like Lepton and Its Accompanying Scalar at Colliders","authors":["Qing-Hong Cao","Jinhui Guo","Jia Liu","Yan Luo","Xiao-Ping Wang"],"raw_abstract":"Recently, the vector-like leptons (VLLs) as a simple extension to the\nstandard model (SM) have attracted widespread attention both in theory and\nexperiments. The present collider searches mainly focus on the studies of their\nprompt decays, which prefer a relatively large coupling. In this paper, we\nconcentrate on searches for long-lived signatures of the singlet VLLs $F$ or\ntheir accompanying scalar particles $\\phi$ both in the hadronic and electronic\ncolliders. The long-lived signatures are naturally induced from small chiral\nmass mixing between VLLs and SM leptons. Two specific models distinguished by\nwhether the VLLs couple to scalar particles are introduced to realize the\naforementioned features. For long-lived VLLs case, we find that with the kink\ntrack method the sensitivities at future HL-LHC with $\\sqrt{s}=13~\\text{TeV}$\ncan reach the regions for VLL mass $m_F \\in [200,950]~\\text{GeV}$ and the mass\nmixing parameter $\\theta_L \\in [10^{-10},10^{-7}]$. For the long-lived\naccompanying scalar particle case, by fixing VLLs or scalar mass, or the mass\nratio between VLL and the accompanying scalar, we explore the projected\nsensitivities through the time delay and displaced vertex strategies, which can\nprobe the regions for $m_F \\in [200,1200]~\\text{GeV}$ and coupling\n$y\\theta_L\\in [10^{-11},10^{-6}]$. Furthermore, we also explore the long-lived\naccompanying scalars at the future CEPC provided that the VLLs can couple to\nthe SM first-generation leptons. We find that CEPC has good performances for\n$m_\\phi < 70~\\text{GeV}$ and $m_F<1000~\\text{GeV}$. These long-lived searches\nare complementary to previous studies, which opens the door towards the smaller\ncoupling regions.","publication_date":1700593213,"paper_link":"http://arxiv.org/pdf/2311.12934v1","categories":["Physics"],"abstract":"Recently, the vector-like leptons (VLLs) as a simple extension to the standard model (SM) have attracted widespread attention both in theory and experiments. The present collider searches mainly focus on the studies of their prompt decays, which prefer a relatively large coupling. In this paper, we concentrate on searches for long-lived signatures of the singlet VLLs __FORMULA__ or their accompanying scalar particles __FORMULA__ both in the hadronic and electronic colliders. The long-lived signatures are naturally induced from small chiral mass mixing between VLLs and SM leptons. Two specific models distinguished by whether the VLLs couple to scalar particles are introduced to realize the aforementioned features. For long-lived VLLs case, we find that with the kink track method the sensitivities at future HL-LHC with __FORMULA__ can reach the regions for VLL mass __FORMULA__ and the mass mixing parameter __FORMULA__. For the long-lived accompanying scalar particle case, by fixing VLLs or scalar mass, or the mass ratio between VLL and the accompanying scalar, we explore the projected sensitivities through the time delay and displaced vertex strategies, which can probe the regions for __FORMULA__ and coupling __FORMULA__. Furthermore, we also explore the long-lived accompanying scalars at the future CEPC provided that the VLLs can couple to the SM first-generation leptons. We find that CEPC has good performances for __FORMULA__ and __FORMULA__. These long-lived searches are complementary to previous studies, which opens the door towards the smaller coupling regions."}
{"title":"Determining the spin of light primordial black holes with Hawking radiation II: high spin regime","authors":["Marco Calz\u00e0","Jo\u00e3o G. Rosa"],"raw_abstract":"We propose a method to determine the mass and spin of primordial black holes\nbased on measuring the energy and emission rate at the dipolar and quadrupolar\npeaks in the primary photon Hawking spectrum, applicable for dimensionless spin\nparameters $\\tilde{a}\\gtrsim 0.6$. In particular, we show that the ratio\nbetween the energies of the two peaks is only a function of the black hole\nspin, while the ratio between their emission rates depends also on the\nline-of-sight inclination. The black hole mass and distance from the Earth may\nthen be inferred from the absolute values of the peak energies and emission\nrates. This method is relevant for primordial black holes born with large spin\nparameters that are presently still in the early stages of their evaporation\nprocess.","publication_date":1700593205,"paper_link":"http://arxiv.org/pdf/2311.12930v1","categories":["Physics"],"abstract":"We propose a method to determine the mass and spin of primordial black holes based on measuring the energy and emission rate at the dipolar and quadrupolar peaks in the primary photon Hawking spectrum, applicable for dimensionless spin parameters __FORMULA__. In particular, we show that the ratio between the energies of the two peaks is only a function of the black hole spin, while the ratio between their emission rates depends also on the line-of-sight inclination. The black hole mass and distance from the Earth may then be inferred from the absolute values of the peak energies and emission rates. This method is relevant for primordial black holes born with large spin parameters that are presently still in the early stages of their evaporation process."}
{"title":"Hierarchical Learning for Quantum ML: Novel Training Technique for Large-Scale Variational Quantum Circuits","authors":["Hrant Gharibyan","Vincent Su","Hayk Tepanyan"],"raw_abstract":"We present hierarchical learning, a novel variational architecture for\nefficient training of large-scale variational quantum circuits. We test and\nbenchmark our technique for distribution loading with quantum circuit born\nmachines (QCBMs). With QCBMs, probability distributions are loaded into the\nsquared amplitudes of computational basis vectors represented by bitstrings.\nOur key insight is to take advantage of the fact that the most significant\n(qu)bits have a greater effect on the final distribution and can be learned\nfirst. One can think of it as a generalization of layerwise learning, where\nsome parameters of the variational circuit are learned first to prevent the\nphenomena of barren plateaus. We briefly review adjoint methods for computing\nthe gradient, in particular for loss functions that are not expectation values\nof observables. We first compare the role of connectivity in the variational\nansatz for the task of loading a Gaussian distribution on nine qubits, finding\nthat 2D connectivity greatly outperforms qubits arranged on a line. Based on\nour observations, we then implement this strategy on large-scale numerical\nexperiments with GPUs, training a QCBM to reproduce a 3-dimensional\nmultivariate Gaussian distribution on 27 qubits up to $\\sim4\\%$ total variation\ndistance. Though barren plateau arguments do not strictly apply here due to the\nobjective function not being tied to an observable, this is to our knowledge\nthe first practical demonstration of variational learning on large numbers of\nqubits. We also demonstrate hierarchical learning as a resource-efficient way\nto load distributions for existing quantum hardware (IBM's 7 and 27 qubit\ndevices) in tandem with Fire Opal optimizations.","publication_date":1700593203,"paper_link":"http://arxiv.org/pdf/2311.12929v1","categories":["Physics"],"abstract":"We present hierarchical learning, a novel variational architecture for efficient training of large-scale variational quantum circuits. We test and benchmark our technique for distribution loading with quantum circuit born machines (QCBMs). With QCBMs, probability distributions are loaded into the squared amplitudes of computational basis vectors represented by bitstrings. Our key insight is to take advantage of the fact that the most significant (qu)bits have a greater effect on the final distribution and can be learned first. One can think of it as a generalization of layerwise learning, where some parameters of the variational circuit are learned first to prevent the phenomena of barren plateaus. We briefly review adjoint methods for computing the gradient, in particular for loss functions that are not expectation values of observables. We first compare the role of connectivity in the variational ansatz for the task of loading a Gaussian distribution on nine qubits, finding that 2D connectivity greatly outperforms qubits arranged on a line. Based on our observations, we then implement this strategy on large-scale numerical experiments with GPUs, training a QCBM to reproduce a 3-dimensional multivariate Gaussian distribution on 27 qubits up to __FORMULA__ total variation distance. Though barren plateau arguments do not strictly apply here due to the objective function not being tied to an observable, this is to our knowledge the first practical demonstration of variational learning on large numbers of qubits. We also demonstrate hierarchical learning as a resource-efficient way to load distributions for existing quantum hardware (IBM's 7 and 27 qubit devices) in tandem with Fire Opal optimizations."}
{"title":"Nonlinear Quantum Photonics with a Tin-Vacancy Center Coupled to a One-Dimensional Diamond Waveguide","authors":["Matteo Pasini","Nina Codreanu","Tim Turan","Adri\u00e0 Riera Moral","Christian F. Primavera","Lorenzo De Santis","Hans K. C. Beukers","Julia M. Brevoord","Christopher Waas","Johannes Borregaard","Ronald Hanson"],"raw_abstract":"Color-centers integrated with nanophotonic devices have emerged as a\ncompelling platform for quantum science and technology. Here we integrate\ntin-vacancy centers in a diamond waveguide and investigate the interaction with\nlight at the single-photon level. We observe single-emitter induced extinction\nof the transmitted light up to 25% and measure the nonlinear effect on the\nphoton statistics. Furthermore, we demonstrate fully tunable interference\nbetween the reflected single-photon field and laser light back-scattered at the\nfiber end and show the corresponding controlled change between bunched and\nanti-bunched photon statistics in the reflected field.","publication_date":1700593202,"paper_link":"http://arxiv.org/pdf/2311.12927v1","categories":["Physics"],"abstract":"Color-centers integrated with nanophotonic devices have emerged as a compelling platform for quantum science and technology. Here we integrate tin-vacancy centers in a diamond waveguide and investigate the interaction with light at the single-photon level. We observe single-emitter induced extinction of the transmitted light up to 25% and measure the nonlinear effect on the photon statistics. Furthermore, we demonstrate fully tunable interference between the reflected single-photon field and laser light back-scattered at the fiber end and show the corresponding controlled change between bunched and anti-bunched photon statistics in the reflected field."}
{"title":"Controlled expansion for transport in a class of non-Fermi liquids","authors":["Zhengyan Darius Shi"],"raw_abstract":"Non-Fermi liquids arise when strong interactions destroy stable fermionic\nquasiparticles. The simplest models featuring this phenomenon involve a Fermi\nsurface coupled to fluctuating gapless bosonic order parameter fields, broadly\nreferred to as \"Hertz-Millis\" models. We revisit a controlled approach to\nHertz-Millis models that combines an expansion in the inverse number ($N$) of\nfermion species with an expansion in the deviation of the boson dynamical\ncritical exponent $z$ from 2. The structure of the expansion is found to be\nqualitatively different in the quantum critical regime $\\Omega \\ll q$ and in\nthe transport regime $\\Omega \\gg q$. In particular, correlation functions in\nthe transport regime involve infinitely many diagrams at each order in\nperturbation theory. We provide an explicit and tractable recipe to classify\nand resum these diagrams. For the simplest Hertz-Millis models, we show that\nthis recipe is consistent with non-perturbative anomaly arguments and correctly\ncaptures the fixed point optical conductivity as well as leading corrections\nfrom irrelevant operators. We comment on potential applications of this\nexpansion to transport in more complicated Hertz-Millis models as well as\ncertain beyond-Landau metallic quantum critical points.","publication_date":1700593201,"paper_link":"http://arxiv.org/pdf/2311.12922v1","categories":["Physics"],"abstract":"Non-Fermi liquids arise when strong interactions destroy stable fermionic quasiparticles. The simplest models featuring this phenomenon involve a Fermi surface coupled to fluctuating gapless bosonic order parameter fields, broadly referred to as \"Hertz-Millis\" models. We revisit a controlled approach to Hertz-Millis models that combines an expansion in the inverse number (__FORMULA__) of fermion species with an expansion in the deviation of the boson dynamical critical exponent __FORMULA__ from 2. The structure of the expansion is found to be qualitatively different in the quantum critical regime __FORMULA__ and in the transport regime __FORMULA__. In particular, correlation functions in the transport regime involve infinitely many diagrams at each order in perturbation theory. We provide an explicit and tractable recipe to classify and resum these diagrams. For the simplest Hertz-Millis models, we show that this recipe is consistent with non-perturbative anomaly arguments and correctly captures the fixed point optical conductivity as well as leading corrections from irrelevant operators. We comment on potential applications of this expansion to transport in more complicated Hertz-Millis models as well as certain beyond-Landau metallic quantum critical points."}
{"title":"Non-resonant Anomaly Detection with Background Extrapolation","authors":["Kehang Bai","Radha Mastandrea","Benjamin Nachman"],"raw_abstract":"Complete anomaly detection strategies that are both signal sensitive and\ncompatible with background estimation have largely focused on resonant signals.\nNon-resonant new physics scenarios are relatively under-explored and may arise\nfrom off-shell effects or final states with significant missing energy. In this\npaper, we extend a class of weakly supervised anomaly detection strategies\ndeveloped for resonant physics to the non-resonant case. Machine learning\nmodels are trained to reweight, generate, or morph the background, extrapolated\nfrom a control region. A classifier is then trained in a signal region to\ndistinguish the estimated background from the data. The new methods are\ndemonstrated using a semi-visible jet signature as a benchmark signal model,\nand are shown to automatically identify the anomalous events without specifying\nthe signal ahead of time.","publication_date":1700593201,"paper_link":"http://arxiv.org/pdf/2311.12924v1","categories":["Physics"],"abstract":"Complete anomaly detection strategies that are both signal sensitive and compatible with background estimation have largely focused on resonant signals. Non-resonant new physics scenarios are relatively under-explored and may arise from off-shell effects or final states with significant missing energy. In this paper, we extend a class of weakly supervised anomaly detection strategies developed for resonant physics to the non-resonant case. Machine learning models are trained to reweight, generate, or morph the background, extrapolated from a control region. A classifier is then trained in a signal region to distinguish the estimated background from the data. The new methods are demonstrated using a semi-visible jet signature as a benchmark signal model, and are shown to automatically identify the anomalous events without specifying the signal ahead of time."}
{"title":"Are $f(R, {\\rm Matter})$ theories really relevant to cosmology?","authors":["Osmin Lacombe","Shinji Mukohyama","Josef Seitz"],"raw_abstract":"We examine $f(R, {\\rm Matter})$ theories that directly couple the curvature\n$R$ or $R_{\\mu\\nu}$ with the matter sector in the action, in addition to the\nuniversal coupling. We argue that if the matter sector includes the Standard\nModel (SM), such theories are either inconsistent or already excluded by\nexperiments unless they are a rewriting of $f(R)$ gravity or general\nrelativity. If these theories genuinely couple the SM to curvature, they suffer\nfrom the presence of ghost states at energies within their domain of\napplication for cosmological purposes. Therefore, we raise questions about\ntheir relevance to cosmology.","publication_date":1700593201,"paper_link":"http://arxiv.org/pdf/2311.12925v1","categories":["Physics"],"abstract":"We examine __FORMULA__ theories that directly couple the curvature __FORMULA__ or __FORMULA__ with the matter sector in the action, in addition to the universal coupling. We argue that if the matter sector includes the Standard Model (SM), such theories are either inconsistent or already excluded by experiments unless they are a rewriting of __FORMULA__ gravity or general relativity. If these theories genuinely couple the SM to curvature, they suffer from the presence of ghost states at energies within their domain of application for cosmological purposes. Therefore, we raise questions about their relevance to cosmology."}
{"title":"Moir\u00e9 Fractional Chern Insulators II: First-principles Calculations and Continuum Models of Rhombohedral Graphene Superlattices","authors":["Jonah Herzog-Arbeitman","Yuzhi Wang","Jiaxuan Liu","Pok Man Tam","Ziyue Qi","Yujin Jia","Dmitri K. Efetov","Oskar Vafek","Nicolas Regnault","Hongming Weng","Quansheng Wu","B. Andrei Bernevig","Jiabin Yu"],"raw_abstract":"The experimental discovery of fractional Chern insulators (FCIs) in\nrhombohedral pentalayer graphene twisted on hexagonal boron nitride (hBN) has\npreceded theoretical prediction. Supported by large-scale first principles\nrelaxation calculations at the experimental twist angle of $0.77^\\circ$, we\nobtain an accurate continuum model of $n=3,4,5,6,7$ layer rhombohedral\ngraphene-hBN moir\\'e systems. Focusing on the pentalayer case, we analytically\nexplain the robust $|C|=0,5$ Chern numbers seen in the low-energy\nsingle-particle bands and their flattening with displacement field, making use\nof a minimal two-flavor continuum Hamiltonian derived from the full model. We\nthen predict nonzero valley Chern numbers at the $\\nu = -4,0$ insulators\nobserved in experiment. Our analysis makes clear the importance of displacement\nfield and the moir\\'e potential in producing localized \"heavy fermion\" charge\ndensity in the top valence band, in addition to the nearly free conduction\nband. Lastly, we study doubly aligned devices as additional platforms for\nmoir\\'e FCIs with higher Chern number bands.","publication_date":1700593200,"paper_link":"http://arxiv.org/pdf/2311.12920v1","categories":["Physics"],"abstract":"The experimental discovery of fractional Chern insulators (FCIs) in rhombohedral pentalayer graphene twisted on hexagonal boron nitride (hBN) has preceded theoretical prediction. Supported by large-scale first principles relaxation calculations at the experimental twist angle of __FORMULA__, we obtain an accurate continuum model of __FORMULA__ layer rhombohedral graphene-hBN moir\\'e systems. Focusing on the pentalayer case, we analytically explain the robust __FORMULA__ Chern numbers seen in the low-energy single-particle bands and their flattening with displacement field, making use of a minimal two-flavor continuum Hamiltonian derived from the full model. We then predict nonzero valley Chern numbers at the __FORMULA__ insulators observed in experiment. Our analysis makes clear the importance of displacement field and the moir\\'e potential in producing localized \"heavy fermion\" charge density in the top valence band, in addition to the nearly free conduction band. Lastly, we study doubly aligned devices as additional platforms for moir\\'e FCIs with higher Chern number bands."}
{"title":"Bulk reconstruction and non-isometry in the backwards-forwards holographic black hole map","authors":["Oliver DeWolfe","Kenneth Higginbotham"],"raw_abstract":"The backwards-forwards map, introduced as a generalization of the\nnon-isometric holographic maps of the black hole interior of Akers, Engelhardt,\nHarlow, Penington, and Vardhan to include non-trivial dynamics in the effective\ndescription, has two possible formulations differing in when the post-selection\nis performed. While these two forms are equivalent on the set of dynamically\ngenerated states -- states formed from unitary time evolution acting on\nwell-defined initial configurations of infalling matter -- they differ on the\ngeneric set of states necessary to describe the apparent world of the infalling\nobserver. We show that while both versions successfully reproduce the Page\ncurve, the version involving post-selection as the final step, dubbed the\nbackwards-forwards-post-selection (BFP) map, has the desirable properties of\nbeing non-isometric but isometric on average and providing state-dependent\nreconstruction of bulk operators, while the other version does not. Thus the\nBFP map is a suitable non-isometric code describing the black hole interior\nincluding interior interactions.","publication_date":1700593200,"paper_link":"http://arxiv.org/pdf/2311.12921v1","categories":["Physics"],"abstract":"The backwards-forwards map, introduced as a generalization of the non-isometric holographic maps of the black hole interior of Akers, Engelhardt, Harlow, Penington, and Vardhan to include non-trivial dynamics in the effective description, has two possible formulations differing in when the post-selection is performed. While these two forms are equivalent on the set of dynamically generated states -- states formed from unitary time evolution acting on well-defined initial configurations of infalling matter -- they differ on the generic set of states necessary to describe the apparent world of the infalling observer. We show that while both versions successfully reproduce the Page curve, the version involving post-selection as the final step, dubbed the backwards-forwards-post-selection (BFP) map, has the desirable properties of being non-isometric but isometric on average and providing state-dependent reconstruction of bulk operators, while the other version does not. Thus the BFP map is a suitable non-isometric code describing the black hole interior including interior interactions."}
{"title":"Tilted Dirac superconductor at quantum criticality: Restoration of Lorentz symmetry","authors":["Pablo Reiser","Vladimir Juricic"],"raw_abstract":"Lorentz symmetry appears as a quite robust feature of the strongly\ninteracting Dirac materials even though the lattice interactions break such a\nsymmetry. We here demonstrate that the Lorentz symmetry is restored at the\nquantum-critical point (QCP) separating the tilted Dirac semimetal, breaking\nthis symmetry already at the noninteracting level, from a gapped $s-$wave\nsuperconducting instability. To this end, we employ a one-loop\n$\\epsilon=(3-D)-$expansion close to the $D=3$ upper critical dimension of the\ncorresponding Gross-Neveu-Yukawa field theory. In particular, we show that the\ntilt parameter is marginally irrelevant and ultimately vanishes at the QCP\nseparating the two phases. In fact, as we argue here, such a Lorentz symmetry\nrestoration may be generic for the strongly interacting tilted Dirac\nsemimetals, irrespective of whether they feature mirror-symmetric or\nmirror-asymmetric tilting, and is also insensitive to whether the instability\nrepresents an insulator or a gapped superconductor. The proposed scenario can\nbe tested in the quantum Monte Carlo simulations of the interacting tilted\nDirac fermion lattice models.","publication_date":1700593199,"paper_link":"http://arxiv.org/pdf/2311.12797v1","categories":["Physics"],"abstract":"Lorentz symmetry appears as a quite robust feature of the strongly interacting Dirac materials even though the lattice interactions break such a symmetry. We here demonstrate that the Lorentz symmetry is restored at the quantum-critical point (QCP) separating the tilted Dirac semimetal, breaking this symmetry already at the noninteracting level, from a gapped __FORMULA__wave superconducting instability. To this end, we employ a one-loop __FORMULA__expansion close to the __FORMULA__ upper critical dimension of the corresponding Gross-Neveu-Yukawa field theory. In particular, we show that the tilt parameter is marginally irrelevant and ultimately vanishes at the QCP separating the two phases. In fact, as we argue here, such a Lorentz symmetry restoration may be generic for the strongly interacting tilted Dirac semimetals, irrespective of whether they feature mirror-symmetric or mirror-asymmetric tilting, and is also insensitive to whether the instability represents an insulator or a gapped superconductor. The proposed scenario can be tested in the quantum Monte Carlo simulations of the interacting tilted Dirac fermion lattice models."}
{"title":"Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models","authors":["David Stotko","Nils Wandel","Reinhard Klein"],"raw_abstract":"3D reconstruction of dynamic scenes is a long-standing problem in computer\ngraphics and increasingly difficult the less information is available.\nShape-from-Template (SfT) methods aim to reconstruct a template-based geometry\nfrom RGB images or video sequences, often leveraging just a single monocular\ncamera without depth information, such as regular smartphone recordings.\nUnfortunately, existing reconstruction methods are either unphysical and noisy\nor slow in optimization. To solve this problem, we propose a novel SfT\nreconstruction algorithm for cloth using a pre-trained neural surrogate model\nthat is fast to evaluate, stable, and produces smooth reconstructions due to a\nregularizing physics simulation. Differentiable rendering of the simulated mesh\nenables pixel-wise comparisons between the reconstruction and a target video\nsequence that can be used for a gradient-based optimization procedure to\nextract not only shape information but also physical parameters such as\nstretching, shearing, or bending stiffness of the cloth. This allows to retain\na precise, stable, and smooth reconstructed geometry while reducing the runtime\nby a factor of 400-500 compared to $\\phi$-SfT, a state-of-the-art physics-based\nSfT approach.","publication_date":1700593198,"paper_link":"http://arxiv.org/pdf/2311.12796v1","categories":["Physics"],"abstract":"3D reconstruction of dynamic scenes is a long-standing problem in computer graphics and increasingly difficult the less information is available. Shape-from-Template (SfT) methods aim to reconstruct a template-based geometry from RGB images or video sequences, often leveraging just a single monocular camera without depth information, such as regular smartphone recordings. Unfortunately, existing reconstruction methods are either unphysical and noisy or slow in optimization. To solve this problem, we propose a novel SfT reconstruction algorithm for cloth using a pre-trained neural surrogate model that is fast to evaluate, stable, and produces smooth reconstructions due to a regularizing physics simulation. Differentiable rendering of the simulated mesh enables pixel-wise comparisons between the reconstruction and a target video sequence that can be used for a gradient-based optimization procedure to extract not only shape information but also physical parameters such as stretching, shearing, or bending stiffness of the cloth. This allows to retain a precise, stable, and smooth reconstructed geometry while reducing the runtime by a factor of 400-500 compared to __FORMULA__-SfT, a state-of-the-art physics-based SfT approach."}
{"title":"MadQCI: a heterogeneous and scalable SDN QKD network deployed in production facilities","authors":["V. Martin","J. P. Brito","L. Ort\u00edz","R. Brito-M\u00e9ndez","J. S\u00e1ez-Buruaga","R. Vicente","A. Sebasti\u00e1n-Lombra\u00f1a","D. Rinc\u00f3n","F. P\u00e9rez","C. S\u00e1nchez","M. Peev","H. H. Brunner","F. Fung","A. Poppe","F. Fr\u00f6wis","A. J. Shields","R. I. Woodward","H. Griesser","S. Roehrich","F. De La Iglesia","C. Abell\u00e1n","M. Hentschel","J. M. Rivas-Moscoso","A. Pastor Perales","J. Folgueira","D. L\u00f3pez"],"raw_abstract":"Current quantum key distribution (QKD) networks focus almost exclusively on\ntransporting secret keys with the highest possible rate. Consequently, they are\nbuilt as mostly fixed, ad hoc, logically, and physically isolated\ninfrastructures designed to avoid any penalty to the quantum channel. This\narchitecture is neither scalable nor cost-effective and future, real-world\ndeployments will differ considerably. The structure of the MadQCI QKD network\npresented here is based on disaggregated components and modern paradigms\nespecially designed for flexibility, upgradability, and facilitating the\nintegration of QKD in the security and telecommunications-networks ecosystem.\nThese underlying ideas have been tested by deploying many QKD systems from\nseveral manufacturers in a real-world, multi-tenant telecommunications network,\ninstalled in production facilities and sharing the infrastructure with\ncommercial traffic. Different technologies have been used in different links to\naddress the variety of situations and needs that arise in real networks,\nexploring a wide range of possibilities. Finally, a set of realistic use cases\nhave been implemented to demonstrate the validity and performance of the\nnetwork. The testing took place during a period close to three years, where\nmost of the nodes were continuously active.","publication_date":1700593041,"paper_link":"http://arxiv.org/pdf/2311.12791v1","categories":["Physics"],"abstract":"Current quantum key distribution (QKD) networks focus almost exclusively on transporting secret keys with the highest possible rate. Consequently, they are built as mostly fixed, ad hoc, logically, and physically isolated infrastructures designed to avoid any penalty to the quantum channel. This architecture is neither scalable nor cost-effective and future, real-world deployments will differ considerably. The structure of the MadQCI QKD network presented here is based on disaggregated components and modern paradigms especially designed for flexibility, upgradability, and facilitating the integration of QKD in the security and telecommunications-networks ecosystem. These underlying ideas have been tested by deploying many QKD systems from several manufacturers in a real-world, multi-tenant telecommunications network, installed in production facilities and sharing the infrastructure with commercial traffic. Different technologies have been used in different links to address the variety of situations and needs that arise in real networks, exploring a wide range of possibilities. Finally, a set of realistic use cases have been implemented to demonstrate the validity and performance of the network. The testing took place during a period close to three years, where most of the nodes were continuously active."}
{"title":"Propagator zeros and lattice chiral gauge theories","authors":["Maarten Golterman","Yigal Shamir"],"raw_abstract":"Symmetric mass generation (SMG) has been advocated as a mechanism to render\nmirror fermions massive without symmetry breaking, ultimately aiming for the\nconstruction of lattice chiral gauge theories. It has been argued that in an\nSMG phase, the poles in the mirror fermion propagators are replaced by zeros.\nUsing an effective lagrangian approach, we investigate the role of propagator\nzeros when the gauge field is turned on, finding that they act as coupled ghost\nstates. In four dimensions, a propagator zero makes an opposite-sign\ncontribution to the one-loop beta function as compared to a normal fermion. In\ntwo dimensional abelian theories, a propagator zero makes a negative\ncontribution to the photon mass squared. In addition, propagator zeros generate\nthe same anomaly as propagator poles. Thus, gauge invariance will always be\nmaintained in an SMG phase, in fact, even if the target chiral gauge theory is\nanomalous, but unitarity of the gauge theory is lost.","publication_date":1700592985,"paper_link":"http://arxiv.org/pdf/2311.12790v1","categories":["Physics"],"abstract":"Symmetric mass generation (SMG) has been advocated as a mechanism to render mirror fermions massive without symmetry breaking, ultimately aiming for the construction of lattice chiral gauge theories. It has been argued that in an SMG phase, the poles in the mirror fermion propagators are replaced by zeros. Using an effective lagrangian approach, we investigate the role of propagator zeros when the gauge field is turned on, finding that they act as coupled ghost states. In four dimensions, a propagator zero makes an opposite-sign contribution to the one-loop beta function as compared to a normal fermion. In two dimensional abelian theories, a propagator zero makes a negative contribution to the photon mass squared. In addition, propagator zeros generate the same anomaly as propagator poles. Thus, gauge invariance will always be maintained in an SMG phase, in fact, even if the target chiral gauge theory is anomalous, but unitarity of the gauge theory is lost."}
{"title":"On the emergence of memory in equilibrium versus non-equilibrium systems","authors":["Xizhu Zhao","David Hartich","Alja\u017e Godec"],"raw_abstract":"Experiments often probe observables that correspond to low-dimensional\nprojections of highdimensional dynamics. In such situations distinct\nmicroscopic configurations become lumped into the same observable state. It is\nwell known that correlations between the observable and the hidden degrees of\nfreedom give rise to memory effects. However, how and under which conditions\nthese correlations emerge remains poorly understood. Here we shed light on two\nfundamentally different scenarios of the emergence of memory in minimal\nstationary systems, where observed and hidden degrees of freedom evolve either\ncooperatively or are coupled by a hidden non-equilibrium current. In the\nreversible setting strongest memory manifests when the time-scales of hidden\nand observed dynamics overlap, whereas, strikingly, in the driven setting\nmaximal memory emerges under a clear time-scale separation. Our results hint at\nthe possibility of fundamental differences in the way memory emerges in\nequilibrium versus driven systems that may be utilized as a \"diagnostic\" of the\nunderlying hidden transport mechanism.","publication_date":1700592730,"paper_link":"http://arxiv.org/pdf/2311.12788v1","categories":["Physics"],"abstract":"Experiments often probe observables that correspond to low-dimensional projections of highdimensional dynamics. In such situations distinct microscopic configurations become lumped into the same observable state. It is well known that correlations between the observable and the hidden degrees of freedom give rise to memory effects. However, how and under which conditions these correlations emerge remains poorly understood. Here we shed light on two fundamentally different scenarios of the emergence of memory in minimal stationary systems, where observed and hidden degrees of freedom evolve either cooperatively or are coupled by a hidden non-equilibrium current. In the reversible setting strongest memory manifests when the time-scales of hidden and observed dynamics overlap, whereas, strikingly, in the driven setting maximal memory emerges under a clear time-scale separation. Our results hint at the possibility of fundamental differences in the way memory emerges in equilibrium versus driven systems that may be utilized as a \"diagnostic\" of the underlying hidden transport mechanism."}
{"title":"Molecular simulation approaches to probing the effects of mechanical forces in the actin cytoskeleton","authors":["Fatemah Mukadum","Willmor J. Pena Ccoa","Glen M. Hocky"],"raw_abstract":"In this article we give our perspective on the successes and promise of\nvarious molecular and coarse-grained simulation approaches to probing the\neffect of mechanical forces in the actin cytoskeleton.","publication_date":1700592668,"paper_link":"http://arxiv.org/pdf/2311.12787v1","categories":["Physics"],"abstract":"In this article we give our perspective on the successes and promise of various molecular and coarse-grained simulation approaches to probing the effect of mechanical forces in the actin cytoskeleton."}
{"title":"Prompting Frameworks for Large Language Models: A Survey","authors":["Xiaoxia Liu","Jingyi Wang","Jun Sun","Xiaohan Yuan","Guoliang Dong","Peng Di","Wenhai Wang","Dongxia Wang"],"raw_abstract":"Since the launch of ChatGPT, a powerful AI Chatbot developed by OpenAI, large\nlanguage models (LLMs) have made significant advancements in both academia and\nindustry, bringing about a fundamental engineering paradigm shift in many\nareas. While LLMs are powerful, it is also crucial to best use their power\nwhere \"prompt'' plays a core role. However, the booming LLMs themselves,\nincluding excellent APIs like ChatGPT, have several inherent limitations: 1)\ntemporal lag of training data, and 2) the lack of physical capabilities to\nperform external actions. Recently, we have observed the trend of utilizing\nprompt-based tools to better utilize the power of LLMs for downstream tasks,\nbut a lack of systematic literature and standardized terminology, partly due to\nthe rapid evolution of this field. Therefore, in this work, we survey related\nprompting tools and promote the concept of the \"Prompting Framework\" (PF), i.e.\nthe framework for managing, simplifying, and facilitating interaction with\nlarge language models. We define the lifecycle of the PF as a hierarchical\nstructure, from bottom to top, namely: Data Level, Base Level, Execute Level,\nand Service Level. We also systematically depict the overall landscape of the\nemerging PF field and discuss potential future research and challenges. To\ncontinuously track the developments in this area, we maintain a repository at\nhttps://github.com/lxx0628/Prompting-Framework-Survey, which can be a useful\nresource sharing platform for both academic and industry in this field.","publication_date":1700592663,"paper_link":"http://arxiv.org/pdf/2311.12785v1","categories":["Physics"],"abstract":"Since the launch of ChatGPT, a powerful AI Chatbot developed by OpenAI, large language models (LLMs) have made significant advancements in both academia and industry, bringing about a fundamental engineering paradigm shift in many areas. While LLMs are powerful, it is also crucial to best use their power where \"prompt'' plays a core role. However, the booming LLMs themselves, including excellent APIs like ChatGPT, have several inherent limitations: 1) temporal lag of training data, and 2) the lack of physical capabilities to perform external actions. Recently, we have observed the trend of utilizing prompt-based tools to better utilize the power of LLMs for downstream tasks, but a lack of systematic literature and standardized terminology, partly due to the rapid evolution of this field. Therefore, in this work, we survey related prompting tools and promote the concept of the \"Prompting Framework\" (PF), i.e. the framework for managing, simplifying, and facilitating interaction with large language models. We define the lifecycle of the PF as a hierarchical structure, from bottom to top, namely: Data Level, Base Level, Execute Level, and Service Level. We also systematically depict the overall landscape of the emerging PF field and discuss potential future research and challenges. To continuously track the developments in this area, we maintain a repository at https://github.com/lxx0628/Prompting-Framework-Survey, which can be a useful resource sharing platform for both academic and industry in this field."}
{"title":"Quantum Imaging Beyond the Standard-Quantum Limit and Phase Distillation","authors":["Simon Schaffrath","Daniel Derr","Markus Gr\u00e4fe","Enno Giese"],"raw_abstract":"Quantum sensing using non-linear interferometers offers the possibility of\nbicolour imaging, using light that never interacted with the object of\ninterest, and provides a way to achieve phase supersensitivity, i.e. a\nHeisenberg-type scaling of the phase uncertainty. Such a scaling behaviour is\nextremely susceptible to noise and only arises at specific phases that define\nthe optimal working point of the device. While phase-shifting algorithms are to\nsome degree robust against the deleterious effects induced by noise they\nextract an image by tuning the interferometer phase over a broad range,\nimplying an operation beyond the working point. In our theoretical study, we\ninvestigate both the spontaneous and the high-gain regime of operation of a\nnon-linear interferometer. In fact, in the spontaneous regime using a\ndistillation technique and operating at the working point leads to a\nqualitatively similar behaviour. In the high-gain regime, however, typical\ndistillation techniques inherently forbid a scaling better than the\nstandard-quantum limit, as a consequence of the photon statistics of squeezed\nvacuum. In contrast, an operation at the working point still may lead to a\nsensitivity below shot noise, even in the presence of noise. Therefore, this\nprocedure opens the perspective of bicolour imaging with a better than\nshot-noise phase uncertainty by working in the vicinity of the working point.\nOur results transfer quantum imaging distillation in a noisy environment to the\nhigh-gain regime with the ultimate goal of harnessing its full potential by\ncombining bicolour imaging and phase supersensitivity.","publication_date":1700592521,"paper_link":"http://arxiv.org/pdf/2311.12782v1","categories":["Physics"],"abstract":"Quantum sensing using non-linear interferometers offers the possibility of bicolour imaging, using light that never interacted with the object of interest, and provides a way to achieve phase supersensitivity, i.e. a Heisenberg-type scaling of the phase uncertainty. Such a scaling behaviour is extremely susceptible to noise and only arises at specific phases that define the optimal working point of the device. While phase-shifting algorithms are to some degree robust against the deleterious effects induced by noise they extract an image by tuning the interferometer phase over a broad range, implying an operation beyond the working point. In our theoretical study, we investigate both the spontaneous and the high-gain regime of operation of a non-linear interferometer. In fact, in the spontaneous regime using a distillation technique and operating at the working point leads to a qualitatively similar behaviour. In the high-gain regime, however, typical distillation techniques inherently forbid a scaling better than the standard-quantum limit, as a consequence of the photon statistics of squeezed vacuum. In contrast, an operation at the working point still may lead to a sensitivity below shot noise, even in the presence of noise. Therefore, this procedure opens the perspective of bicolour imaging with a better than shot-noise phase uncertainty by working in the vicinity of the working point. Our results transfer quantum imaging distillation in a noisy environment to the high-gain regime with the ultimate goal of harnessing its full potential by combining bicolour imaging and phase supersensitivity."}
{"title":"One consistency relation for all single-field inflationary models","authors":["Mohammad Hossein Namjoo"],"raw_abstract":"In this paper, we present a non-Gaussianity consistency relation that enables\nthe calculation of the squeezed limit bispectrum of the curvature perturbation\nin single-field inflationary models by carefully inspecting the background\nevolution and the linear perturbation theory. The consistency relation is more\ngeneral than others in the literature since it does not require any specific\nsymmetry, conservation of the curvature perturbation at large scales, attractor\nbackground evolution or canonical kinetic energy of the inflaton field. We\ndemonstrate that all known examples of the squeezed limit bispectrum in\nsingle-field models of inflation can be reproduced within this framework.","publication_date":1700591919,"paper_link":"http://arxiv.org/pdf/2311.12777v1","categories":["Physics"],"abstract":"In this paper, we present a non-Gaussianity consistency relation that enables the calculation of the squeezed limit bispectrum of the curvature perturbation in single-field inflationary models by carefully inspecting the background evolution and the linear perturbation theory. The consistency relation is more general than others in the literature since it does not require any specific symmetry, conservation of the curvature perturbation at large scales, attractor background evolution or canonical kinetic energy of the inflaton field. We demonstrate that all known examples of the squeezed limit bispectrum in single-field models of inflation can be reproduced within this framework."}
{"title":"Large-scale Package Deliveries with Unmanned Aerial Vehicles using Collective Learning","authors":["Arun Narayanan","Evangelos Pournaras","Pedro H. J. Nardelli"],"raw_abstract":"Unmanned aerial vehicles (UAVs) have significant practical advantages for\ndelivering packages, and many logistics companies have begun deploying UAVs for\ncommercial package deliveries. To deliver packages quickly and\ncost-effectively, the routes taken by UAVs from depots to customers must be\noptimized. This route optimization problem, a type of capacitated vehicle\nrouting problem, has recently attracted considerable research interest.\nHowever, few papers have dealt with large-scale deliveries, where the number of\ncustomers exceed 1000. We present an innovative, practical package delivery\nmodel wherein multiple UAVs deliver multiple packages to customers who are\ncompensated for late deliveries. Further, we propose an innovative methodology\nthat combines a new plan-generation algorithm with a collective-learning\nheuristic to quickly determine cost-effective paths of UAVs even for\nlarge-scale deliveries up to 10000 customers. Specialized settings are applied\nto a collective-learning heuristic, the Iterative Economic Planning and\nOptimized Selections (I-EPOS) in order to coordinate collective actions of the\nUAVs. To demonstrate our methodology, we applied our highly flexible approach\nto a depot in Heathrow Airport, London. We show that a coordinated approach, in\nwhich the UAVs collectively determine their flight paths, leads to lower\noperational costs than an uncoordinated approach. Further, the coordinated\napproach enables large-scale package deliveries.","publication_date":1700669079,"paper_link":"http://arxiv.org/pdf/2311.13489v1","categories":["Economics"],"abstract":"Unmanned aerial vehicles (UAVs) have significant practical advantages for delivering packages, and many logistics companies have begun deploying UAVs for commercial package deliveries. To deliver packages quickly and cost-effectively, the routes taken by UAVs from depots to customers must be optimized. This route optimization problem, a type of capacitated vehicle routing problem, has recently attracted considerable research interest. However, few papers have dealt with large-scale deliveries, where the number of customers exceed 1000. We present an innovative, practical package delivery model wherein multiple UAVs deliver multiple packages to customers who are compensated for late deliveries. Further, we propose an innovative methodology that combines a new plan-generation algorithm with a collective-learning heuristic to quickly determine cost-effective paths of UAVs even for large-scale deliveries up to 10000 customers. Specialized settings are applied to a collective-learning heuristic, the Iterative Economic Planning and Optimized Selections (I-EPOS) in order to coordinate collective actions of the UAVs. To demonstrate our methodology, we applied our highly flexible approach to a depot in Heathrow Airport, London. We show that a coordinated approach, in which the UAVs collectively determine their flight paths, leads to lower operational costs than an uncoordinated approach. Further, the coordinated approach enables large-scale package deliveries."}
{"title":"Belief identification by proxy","authors":["Elias Tsakas"],"raw_abstract":"It is well known that individual beliefs cannot be identified using\ntraditional choice data, unless we exogenously assume state-independent\nutilities. In this paper, I propose a novel methodology that solves this\nlong-standing identification problem in a simple way. This method relies on the\nextending the state space by introducing a proxy, for which the agent has no\nstakes conditional on the original state space. The latter allows us to\nidentify the agent's conditional beliefs about the proxy given each state\nrealization, which in turn suffices for indirectly identifying her beliefs\nabout the original state space. This approach is analogous to the one of\ninstrumental variables in econometrics. Similarly to instrumental variables,\nthe appeal of this method comes from the flexibility in selecting a proxy.","publication_date":1700660650,"paper_link":"http://arxiv.org/pdf/2311.13394v1","categories":["Economics"],"abstract":"It is well known that individual beliefs cannot be identified using traditional choice data, unless we exogenously assume state-independent utilities. In this paper, I propose a novel methodology that solves this long-standing identification problem in a simple way. This method relies on the extending the state space by introducing a proxy, for which the agent has no stakes conditional on the original state space. The latter allows us to identify the agent's conditional beliefs about the proxy given each state realization, which in turn suffices for indirectly identifying her beliefs about the original state space. This approach is analogous to the one of instrumental variables in econometrics. Similarly to instrumental variables, the appeal of this method comes from the flexibility in selecting a proxy."}
{"title":"Regressions under Adverse Conditions","authors":["Timo Dimitriadis","Yannick Hoga"],"raw_abstract":"We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, given the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\neconomic scenarios, which receive increasing interest in managing macroeconomic\nand financial risks, among many others. In the terminology of the systemic risk\nliterature, our method can be interpreted as a regression for the Marginal\nExpected Shortfall. We propose a two-step procedure to estimate the new models,\nshow consistency and asymptotic normality of the estimator, and propose\nfeasible inference under weak conditions allowing for cross-sectional and time\nseries applications. The accuracy of the asymptotic approximations of the\ntwo-step estimator is verified in simulations. Two empirical applications show\nthat our regressions under adverse conditions are valuable in such diverse\nfields as the study of the relation between systemic risk and asset price\nbubbles, and dissecting macroeconomic growth vulnerabilities into individual\ncomponents.","publication_date":1700653660,"paper_link":"http://arxiv.org/pdf/2311.13327v1","categories":["Economics","Statistics"],"abstract":"We introduce a new regression method that relates the mean of an outcome variable to covariates, given the \"adverse condition\" that a distress variable falls in its tail. This allows to tailor classical mean regressions to adverse economic scenarios, which receive increasing interest in managing macroeconomic and financial risks, among many others. In the terminology of the systemic risk literature, our method can be interpreted as a regression for the Marginal Expected Shortfall. We propose a two-step procedure to estimate the new models, show consistency and asymptotic normality of the estimator, and propose feasible inference under weak conditions allowing for cross-sectional and time series applications. The accuracy of the asymptotic approximations of the two-step estimator is verified in simulations. Two empirical applications show that our regressions under adverse conditions are valuable in such diverse fields as the study of the relation between systemic risk and asset price bubbles, and dissecting macroeconomic growth vulnerabilities into individual components."}
{"title":"Randomisation with moral hazard: a path to existence of optimal contracts","authors":["Daniel Kr\u0161ek","Dylan Possama\u00ef"],"raw_abstract":"We study a generic principal-agent problem in continuous time on a finite\ntime horizon. We introduce a framework in which the agent is allowed to employ\nmeasure-valued controls and characterise the continuation utility as a solution\nto a specific form of a backward stochastic differential equation driven by a\nmartingale measure. We leverage this characterisation to prove that, under\nappropriate conditions, an optimal solution to the principal's problem exists,\neven when constraints on the contract are imposed. In doing so, we employ\ncompactification techniques and, as a result, circumvent the typical challenge\nof showing well-posedness for a degenerate partial differential equation with\npotential boundary conditions, where regularity problems often arise.","publication_date":1700646874,"paper_link":"http://arxiv.org/pdf/2311.13278v1","categories":["Mathematics","Economics","Quantitative Finance"],"abstract":"We study a generic principal-agent problem in continuous time on a finite time horizon. We introduce a framework in which the agent is allowed to employ measure-valued controls and characterise the continuation utility as a solution to a specific form of a backward stochastic differential equation driven by a martingale measure. We leverage this characterisation to prove that, under appropriate conditions, an optimal solution to the principal's problem exists, even when constraints on the contract are imposed. In doing so, we employ compactification techniques and, as a result, circumvent the typical challenge of showing well-posedness for a degenerate partial differential equation with potential boundary conditions, where regularity problems often arise."}
{"title":"Terminal Phase Navigation for AUV Docking: An Innovative Electromagnetic Approach","authors":["Yevgeni Gutnik","Morel Groper"],"raw_abstract":"This study introduces a groundbreaking approach for real-time 3D\nlocalization, specifically focusing on achieving seamless and precise\nlocalization during an AUV's terminal guidance phase as it approaches an\nomnidirectional docking component in an automated Launch and Recovery System\n(LARS). Through the use of the AUV's magnetometer, an economical\nelectromagnetic beacon embedded in the docking station, and an advanced signal\nprocessing algorithm, this novel approach ensures the accurate localization of\nthe docking component in three dimensions without the need for direct\nline-of-sight contact. The method's real-time capabilities were rigorously\nevaluated via simulations, prototype experiments in a controlled lab setting,\nand extensive full-scale pool experiments. These assessments consistently\ndemonstrated an exceptional average positioning accuracy of under 3 cm.,\nmarking a significant advancement in AUV guidance systems.","publication_date":1700613990,"paper_link":"http://arxiv.org/pdf/2311.13078v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This study introduces a groundbreaking approach for real-time 3D localization, specifically focusing on achieving seamless and precise localization during an AUV's terminal guidance phase as it approaches an omnidirectional docking component in an automated Launch and Recovery System (LARS). Through the use of the AUV's magnetometer, an economical electromagnetic beacon embedded in the docking station, and an advanced signal processing algorithm, this novel approach ensures the accurate localization of the docking component in three dimensions without the need for direct line-of-sight contact. The method's real-time capabilities were rigorously evaluated via simulations, prototype experiments in a controlled lab setting, and extensive full-scale pool experiments. These assessments consistently demonstrated an exceptional average positioning accuracy of under 3 cm., marking a significant advancement in AUV guidance systems."}
{"title":"Do we listen to what we are told? An empirical study on human behaviour during the COVID-19 pandemic: neural networks vs. regression analysis","authors":["Yuxi Heluo","Kexin Wang","Charles W. Robson"],"raw_abstract":"In this work, we contribute the first visual open-source empirical study on\nhuman behaviour during the COVID-19 pandemic, in order to investigate how\ncompliant a general population is to mask-wearing-related public-health policy.\nObject-detection-based convolutional neural networks, regression analysis and\nmultilayer perceptrons are combined to analyse visual data of the Viennese\npublic during 2020. We find that mask-wearing-related government regulations\nand public-transport announcements encouraged correct mask-wearing-behaviours\nduring the COVID-19 pandemic. Importantly, changes in announcement and\nregulation contents led to heterogeneous effects on people's behaviour.\nComparing the predictive power of regression analysis and neural networks, we\ndemonstrate that the latter produces more accurate predictions of population\nreactions during the COVID-19 pandemic. Our use of regression modelling also\nallows us to unearth possible causal pathways underlying societal behaviour.\nSince our findings highlight the importance of appropriate communication\ncontents, our results will facilitate more effective non-pharmaceutical\ninterventions to be developed in future. Adding to the literature, we\ndemonstrate that regression modelling and neural networks are not mutually\nexclusive but instead complement each other.","publication_date":1700608487,"paper_link":"http://arxiv.org/pdf/2311.13046v1","categories":["Statistics","Economics","Quantitative Finance"],"abstract":"In this work, we contribute the first visual open-source empirical study on human behaviour during the COVID-19 pandemic, in order to investigate how compliant a general population is to mask-wearing-related public-health policy. Object-detection-based convolutional neural networks, regression analysis and multilayer perceptrons are combined to analyse visual data of the Viennese public during 2020. We find that mask-wearing-related government regulations and public-transport announcements encouraged correct mask-wearing-behaviours during the COVID-19 pandemic. Importantly, changes in announcement and regulation contents led to heterogeneous effects on people's behaviour. Comparing the predictive power of regression analysis and neural networks, we demonstrate that the latter produces more accurate predictions of population reactions during the COVID-19 pandemic. Our use of regression modelling also allows us to unearth possible causal pathways underlying societal behaviour. Since our findings highlight the importance of appropriate communication contents, our results will facilitate more effective non-pharmaceutical interventions to be developed in future. Adding to the literature, we demonstrate that regression modelling and neural networks are not mutually exclusive but instead complement each other."}
{"title":"Comment on \"Ironing, sweeping, and multidimensional screening''","authors":["Robert J. McCann","Kelvin Shuangjian Zhang"],"raw_abstract":"In their study of price discrimination for a monopolist selling heterogeneous\nproducts to consumers having private information about their own\nmultidimensional types, Rochet and Chon\\'e (1998) discovered a new form of\nscreening in which consumers with intermediate types are bunched together into\nisochoice groups of various dimensions incentivized to purchase the same\nproduct. They analyzed a particular example involving customer types\ndistributed uniformly over the unit square. For this example, we prove that\ntheir proposed solution cannot be correct, and explain how it can be corrected.","publication_date":1700602912,"paper_link":"http://arxiv.org/pdf/2311.13012v1","categories":["Mathematics","Economics"],"abstract":"In their study of price discrimination for a monopolist selling heterogeneous products to consumers having private information about their own multidimensional types, Rochet and Chon\\'e (1998) discovered a new form of screening in which consumers with intermediate types are bunched together into isochoice groups of various dimensions incentivized to purchase the same product. They analyzed a particular example involving customer types distributed uniformly over the unit square. For this example, we prove that their proposed solution cannot be correct, and explain how it can be corrected."}
{"title":"Speed limits in traffic emission models using multi-objective optimization","authors":["Simone G\u00f6ttlich","Michael Herty","Alena Ulke"],"raw_abstract":"Climate change compels a reduction of greenhouse gas emissions, yet vehicular\ntraffic still contributes significantly to the emission of air pollutants.\nHence, in this paper we focus on the optimization of traffic flow while\nsimultaneously minimizing air pollution using speed limits as controllable\nparameters. We introduce a framework of traffic emission models to simulate the\ntraffic dynamic as well as the production and spread of air pollutants. We\nformulate a multi-objective optimization problem for the optimization of\nmultiple aspects of vehicular traffic. The results show that multi-objective\noptimization can be a valuable tool in traffic emission modeling as it allows\nto find optimal compromises between ecological and economic objectives.","publication_date":1700587933,"paper_link":"http://arxiv.org/pdf/2311.12744v1","categories":["Mathematics"],"abstract":"Climate change compels a reduction of greenhouse gas emissions, yet vehicular traffic still contributes significantly to the emission of air pollutants. Hence, in this paper we focus on the optimization of traffic flow while simultaneously minimizing air pollution using speed limits as controllable parameters. We introduce a framework of traffic emission models to simulate the traffic dynamic as well as the production and spread of air pollutants. We formulate a multi-objective optimization problem for the optimization of multiple aspects of vehicular traffic. The results show that multi-objective optimization can be a valuable tool in traffic emission modeling as it allows to find optimal compromises between ecological and economic objectives."}
{"title":"Power System Capacity Planning Considering Seasonal Hydrogen Storage by Salt Caverns","authors":["Xueqian He","Tianguang Lu","Jing Li","Wanxing Sheng","Rui Li"],"raw_abstract":"In China, air conditioning in summer and electric heating in winter lead to\nseasonal volatility in load power. Therefore, it is urgent to develop economic\nand efficient long-term energy storage systems to enhance peak regulation.\nPower-to-hydrogen technology is a perspective solution to balance seasonal\npower fluctuation. However, current hydrogen storage methods have shortcomings\nsuch as small storage capacity, high levelized cost and low operation safety,\nwhich the salt cavern hydrogen storage could overcome. This paper considers the\nuse of hydrogen storage in salt caverns as a means of peak shaving. To minimize\nthe overall operating cost, a comprehensive power system capacity planning\nmodel is proposed with the consideration of hydrogen storage in salt caverns,\nwhich is implemented by adopting an improved fast unit commitment method.\nConsidering the seasonal characteristics of the load power in Jiangsu Province,\nthe capacity of the power system in 2050 has been planned. According to the\ncase study, after the optimal deployment of the salt cavern hydrogen storage\nsystem (SCHSS), the construction capacity of renewable units (especially wind\npower) will be significantly increased with environmental friendliness and\nlower costs. Compared with the current energy storage method, the overall cost\nof SCHSS-incorporated power system will be reduced by 22.2% with a carbon\nemission reduction of 24.4%, and the amount of curtailed wind and solar power\nwill be reduced by 27.0% and 13.6%, respectively.","publication_date":1700565118,"paper_link":"http://arxiv.org/pdf/2311.12525v1","categories":["Electrical Engineering and Systems Science"],"abstract":"In China, air conditioning in summer and electric heating in winter lead to seasonal volatility in load power. Therefore, it is urgent to develop economic and efficient long-term energy storage systems to enhance peak regulation. Power-to-hydrogen technology is a perspective solution to balance seasonal power fluctuation. However, current hydrogen storage methods have shortcomings such as small storage capacity, high levelized cost and low operation safety, which the salt cavern hydrogen storage could overcome. This paper considers the use of hydrogen storage in salt caverns as a means of peak shaving. To minimize the overall operating cost, a comprehensive power system capacity planning model is proposed with the consideration of hydrogen storage in salt caverns, which is implemented by adopting an improved fast unit commitment method. Considering the seasonal characteristics of the load power in Jiangsu Province, the capacity of the power system in 2050 has been planned. According to the case study, after the optimal deployment of the salt cavern hydrogen storage system (SCHSS), the construction capacity of renewable units (especially wind power) will be significantly increased with environmental friendliness and lower costs. Compared with the current energy storage method, the overall cost of SCHSS-incorporated power system will be reduced by 22.2% with a carbon emission reduction of 24.4%, and the amount of curtailed wind and solar power will be reduced by 27.0% and 13.6%, respectively."}
{"title":"Underreaction and dynamic inconsistency in communication games under noise","authors":["Gerrit Bauch"],"raw_abstract":"Communication is rarely perfect, but rather prone to error of transmission\nand reception. Often the origin of these errors cannot be properly quantified\nand is thus imprecisely known. We analyze the impact of an ambiguous noise\nwhich may alter the received message on a communication game of common\ninterest. The noise is ambiguous in the sense that the parameters of the\nerror-generating process and thus the likelihood to receive a message by\nmistake are Knightianly unknown. Ex-ante and interim responses are\ncharacterized under maxmin preferences. While the sender can disregard\nambiguity, the receiver reveals a dynamically inconsistent, but astonishing\nbehavior under a quadratic loss. Their interim actions will be closer to the\npooling action than their ex-ante ones, as if facing a higher likelihood of an\noccurring error.","publication_date":1700561645,"paper_link":"http://arxiv.org/pdf/2311.12496v1","categories":["Economics"],"abstract":"Communication is rarely perfect, but rather prone to error of transmission and reception. Often the origin of these errors cannot be properly quantified and is thus imprecisely known. We analyze the impact of an ambiguous noise which may alter the received message on a communication game of common interest. The noise is ambiguous in the sense that the parameters of the error-generating process and thus the likelihood to receive a message by mistake are Knightianly unknown. Ex-ante and interim responses are characterized under maxmin preferences. While the sender can disregard ambiguity, the receiver reveals a dynamically inconsistent, but astonishing behavior under a quadratic loss. Their interim actions will be closer to the pooling action than their ex-ante ones, as if facing a higher likelihood of an occurring error."}
{"title":"Successive Incentives","authors":["Jens Gudmundsson","Jens Leth Hougaard","Juan D. Moreno-Ternero","Lars Peter \u00d8sterdal"],"raw_abstract":"We study the design of optimal incentives in sequential processes. To do so,\nwe consider a basic and fundamental model in which an agent initiates a\nvalue-creating sequential process through costly investment with random\nsuccess. If unsuccessful, the process stops. If successful, a new agent\nthereafter faces a similar investment decision, and so forth. For any outcome\nof the process, the total value is distributed among the agents using a reward\nrule. Reward rules thus induce a game among the agents. By design, the reward\nrule may lead to an asymmetric game, yet we are able to show equilibrium\nexistence with optimal symmetric equilibria. We characterize optimal reward\nrules that yield the highest possible welfare created by the process, and the\nhighest possible expected payoff for the initiator of the process. Our findings\nshow that simple reward rules invoking short-run incentives are sufficient to\nmeet long-run objectives.","publication_date":1700561449,"paper_link":"http://arxiv.org/pdf/2311.12494v1","categories":["Economics"],"abstract":"We study the design of optimal incentives in sequential processes. To do so, we consider a basic and fundamental model in which an agent initiates a value-creating sequential process through costly investment with random success. If unsuccessful, the process stops. If successful, a new agent thereafter faces a similar investment decision, and so forth. For any outcome of the process, the total value is distributed among the agents using a reward rule. Reward rules thus induce a game among the agents. By design, the reward rule may lead to an asymmetric game, yet we are able to show equilibrium existence with optimal symmetric equilibria. We characterize optimal reward rules that yield the highest possible welfare created by the process, and the highest possible expected payoff for the initiator of the process. Our findings show that simple reward rules invoking short-run incentives are sufficient to meet long-run objectives."}
{"title":"Uniformly Strict Equilibrium for Repeated Games with Private Monitoring and Communication","authors":["Richard McLean","Ichiro Obara","Andrew Postlewaite"],"raw_abstract":"Cooperation through repetition is an important theme in game theory. In this\nregard, various celebrated ``folk theorems'' have been proposed for repeated\ngames in increasingly more complex environments. There has, however, been\ninsufficient attention paid to the robustness of a large set of equilibria that\nis needed for such folk theorems. Starting with perfect public equilibrium as\nour starting point, we study uniformly strict equilibria in repeated games with\nprivate monitoring and direct communication (cheap talk). We characterize the\nlimit equilibrium payoff set and identify the conditions for the folk theorem\nto hold with uniformly strict equilibrium.","publication_date":1700524046,"paper_link":"http://arxiv.org/pdf/2311.12242v1","categories":["Economics"],"abstract":"Cooperation through repetition is an important theme in game theory. In this regard, various celebrated ``folk theorems'' have been proposed for repeated games in increasingly more complex environments. There has, however, been insufficient attention paid to the robustness of a large set of equilibria that is needed for such folk theorems. Starting with perfect public equilibrium as our starting point, we study uniformly strict equilibria in repeated games with private monitoring and direct communication (cheap talk). We characterize the limit equilibrium payoff set and identify the conditions for the folk theorem to hold with uniformly strict equilibrium."}
{"title":"Day-Ahead Programming of Energy Communities Participating in Pay-as-Bid Service Markets","authors":["F. Conte","S. Massucco","G. Natrella","M. Saviozzi","F. Silvestro"],"raw_abstract":"This paper proposes an optimal strategy for a Renewable Energy Community\nparticipating in the Italian pay-as-bid ancillary service market. The community\nis composed by a group of residential customers sharing a common facility\nequipped with a PV power plant and a battery energy storage system. This\nbattery is employed to maximize the community cash flow obtained by the\nparticipation in the service market. A scenario-based optimization problem is\ndefined to size the bids to be submitted to the market and to define the\ncorresponding optimal battery energy exchange profile for the day ahead. The\nproposed optimization scheme is able to take in to account the probability of\nacceptance of the service market bids and the uncertainties on the forecasts of\nPV generation and energy demands. Results prove the effectiveness of the\napproach and the economic advantages on participating in the service market.","publication_date":1700516704,"paper_link":"http://arxiv.org/pdf/2311.12203v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper proposes an optimal strategy for a Renewable Energy Community participating in the Italian pay-as-bid ancillary service market. The community is composed by a group of residential customers sharing a common facility equipped with a PV power plant and a battery energy storage system. This battery is employed to maximize the community cash flow obtained by the participation in the service market. A scenario-based optimization problem is defined to size the bids to be submitted to the market and to define the corresponding optimal battery energy exchange profile for the day ahead. The proposed optimization scheme is able to take in to account the probability of acceptance of the service market bids and the uncertainties on the forecasts of PV generation and energy demands. Results prove the effectiveness of the approach and the economic advantages on participating in the service market."}
{"title":"Materials for Fusion Energy: Key Challenges and Guiding Principles for Developing Under High Design Uncertainty","authors":["David Cohen-Tanugi","Myles G. Stapelberg","Michael P. Short","Sara E. Ferry","Dennis G. Whyte","Zachary S. Hartwig","Tonio Buonassisi"],"raw_abstract":"Fusion energy is at an important inflection point in its development:\nmultiple government agencies and private companies are now planning fusion\npilot plants to deliver electricity to the grid in the next decade. However,\nrealizing fusion as a technically and economically viable energy source depends\non developing and qualifying materials that can withstand the extreme\nenvironment inside a fusion power plant. This Perspective seeks to engage the\nmaterials science community in this effort. We first outline the principal\nmaterials challenges and research opportunities for fusion. Next, we argue that\nfusion is distinct from other energy applications with respect to materials,\nnot just in the magnitude and complexity of the technical challenges but also\nin the present level of uncertainty in materials design requirements. To\naddress this, we finally propose a research framework based on an iterative\nco-evolution of materials science and fusion power plant design requirements.","publication_date":1700514294,"paper_link":"http://arxiv.org/pdf/2311.12187v1","categories":["Physics"],"abstract":"Fusion energy is at an important inflection point in its development: multiple government agencies and private companies are now planning fusion pilot plants to deliver electricity to the grid in the next decade. However, realizing fusion as a technically and economically viable energy source depends on developing and qualifying materials that can withstand the extreme environment inside a fusion power plant. This Perspective seeks to engage the materials science community in this effort. We first outline the principal materials challenges and research opportunities for fusion. Next, we argue that fusion is distinct from other energy applications with respect to materials, not just in the magnitude and complexity of the technical challenges but also in the present level of uncertainty in materials design requirements. To address this, we finally propose a research framework based on an iterative co-evolution of materials science and fusion power plant design requirements."}
{"title":"Continuity of Filters for Discrete-Time Control Problems Defined by Explicit Equations","authors":["Eugene A. Feinberg","Sayaka Ishizawa","Pavlo O. Kasyanov","David N. Kraemer"],"raw_abstract":"Discrete time control systems whose dynamics and observations are described\nby stochastic equations are common in engineering, operations research, health\ncare, and economics. For example, stochastic filtering problems are usually\ndefined via stochastic equations. These problems can be reduced to Markov\ndecision processes (MDPs) whose states are posterior state distributions, and\nsuch MDPs are sometimes called filters. This paper investigates sufficient\nconditions on transition and observation functions for the original problems to\nguarantee weak continuity of the transition probabilities of the filter MDP.\nUnder mild conditions on cost functions, weak continuity implies the existence\nof optimal policies minimizing the expected total costs, the validity of\noptimality equations, and convergence of value iterations to optimal values.\nThis paper uses recent results on weak continuity of filters for partially\nobservable MDPs defined by transition and observation probabilities. It\ndevelops a criterion of weak continuity of transition probabilities and a\nsufficient condition for continuity in total variation of transition\nprobabilities. The results are illustrated with applications to filtering\nproblems.","publication_date":1700514097,"paper_link":"http://arxiv.org/pdf/2311.12184v1","categories":["Mathematics"],"abstract":"Discrete time control systems whose dynamics and observations are described by stochastic equations are common in engineering, operations research, health care, and economics. For example, stochastic filtering problems are usually defined via stochastic equations. These problems can be reduced to Markov decision processes (MDPs) whose states are posterior state distributions, and such MDPs are sometimes called filters. This paper investigates sufficient conditions on transition and observation functions for the original problems to guarantee weak continuity of the transition probabilities of the filter MDP. Under mild conditions on cost functions, weak continuity implies the existence of optimal policies minimizing the expected total costs, the validity of optimality equations, and convergence of value iterations to optimal values. This paper uses recent results on weak continuity of filters for partially observable MDPs defined by transition and observation probabilities. It develops a criterion of weak continuity of transition probabilities and a sufficient condition for continuity in total variation of transition probabilities. The results are illustrated with applications to filtering problems."}
{"title":"Adaptive Bayesian Learning with Action and State-Dependent Signal Variance","authors":["Kaiwen Hou"],"raw_abstract":"This manuscript presents an advanced framework for Bayesian learning by\nincorporating action and state-dependent signal variances into decision-making\nmodels. This framework is pivotal in understanding complex data-feedback loops\nand decision-making processes in various economic systems. Through a series of\nexamples, we demonstrate the versatility of this approach in different\ncontexts, ranging from simple Bayesian updating in stable environments to\ncomplex models involving social learning and state-dependent uncertainties. The\npaper uniquely contributes to the understanding of the nuanced interplay\nbetween data, actions, outcomes, and the inherent uncertainty in economic\nmodels.","publication_date":1700503170,"paper_link":"http://arxiv.org/pdf/2311.12878v1","categories":["Mathematics","Economics","Statistics"],"abstract":"This manuscript presents an advanced framework for Bayesian learning by incorporating action and state-dependent signal variances into decision-making models. This framework is pivotal in understanding complex data-feedback loops and decision-making processes in various economic systems. Through a series of examples, we demonstrate the versatility of this approach in different contexts, ranging from simple Bayesian updating in stable environments to complex models involving social learning and state-dependent uncertainties. The paper uniquely contributes to the understanding of the nuanced interplay between data, actions, outcomes, and the inherent uncertainty in economic models."}
{"title":"Theory coherent shrinkage of Time-Varying Parameters in VARs","authors":["Andrea Renzetti"],"raw_abstract":"Time-Varying Parameters Vector Autoregressive (TVP-VAR) models are frequently\nused in economics to capture evolving relationships among the macroeconomic\nvariables. However, TVP-VARs have the tendency of overfitting the data,\nresulting in inaccurate forecasts and imprecise estimates of typical objects of\ninterests such as the impulse response functions. This paper introduces a\nTheory Coherent Time-Varying Parameters Vector Autoregressive Model\n(TC-TVP-VAR), which leverages on an arbitrary theoretical framework derived by\nan underlying economic theory to form a prior for the time varying parameters.\nThis \"theory coherent\" shrinkage prior significantly improves inference\nprecision and forecast accuracy over the standard TVP-VAR. Furthermore, the\nTC-TVP-VAR can be used to perform indirect posterior inference on the deep\nparameters of the underlying economic theory. The paper reveals that using the\nclassical 3-equation New Keynesian block to form a prior for the TVP- VAR\nsubstantially enhances forecast accuracy of output growth and of the inflation\nrate in a standard model of monetary policy. Additionally, the paper shows that\nthe TC-TVP-VAR can be used to address the inferential challenges during the\nZero Lower Bound period.","publication_date":1700495542,"paper_link":"http://arxiv.org/pdf/2311.11858v1","categories":["Economics"],"abstract":"Time-Varying Parameters Vector Autoregressive (TVP-VAR) models are frequently used in economics to capture evolving relationships among the macroeconomic variables. However, TVP-VARs have the tendency of overfitting the data, resulting in inaccurate forecasts and imprecise estimates of typical objects of interests such as the impulse response functions. This paper introduces a Theory Coherent Time-Varying Parameters Vector Autoregressive Model (TC-TVP-VAR), which leverages on an arbitrary theoretical framework derived by an underlying economic theory to form a prior for the time varying parameters. This \"theory coherent\" shrinkage prior significantly improves inference precision and forecast accuracy over the standard TVP-VAR. Furthermore, the TC-TVP-VAR can be used to perform indirect posterior inference on the deep parameters of the underlying economic theory. The paper reveals that using the classical 3-equation New Keynesian block to form a prior for the TVP- VAR substantially enhances forecast accuracy of output growth and of the inflation rate in a standard model of monetary policy. Additionally, the paper shows that the TC-TVP-VAR can be used to address the inferential challenges during the Zero Lower Bound period."}
{"title":"Would Monetary Incentives to COVID-19 vaccination reduce motivation?","authors":["Eiji Yamamura","Yoshiro Tsutsui","Fumio Ohtake"],"raw_abstract":"Some people did not get the COVID-19 vaccine even though it was offered at no\ncost. A monetary incentive might lead people to vaccinate, although existing\nstudies have provided different findings about this effect. We investigate how\nmonetary incentives differ according to individual characteristics. Using panel\ndata with online experiments, we found that (1) subsidies reduced vaccine\nintention but increased it after controlling heterogeneity; (2) the stronger\nthe social image against the vaccination, the lower the monetary incentive; and\n(3) persistently unvaccinated people would intend to be vaccinated only if a\nlarge subsidy was provided.","publication_date":1700492666,"paper_link":"http://arxiv.org/pdf/2311.11828v1","categories":["Economics","Quantitative Finance"],"abstract":"Some people did not get the COVID-19 vaccine even though it was offered at no cost. A monetary incentive might lead people to vaccinate, although existing studies have provided different findings about this effect. We investigate how monetary incentives differ according to individual characteristics. Using panel data with online experiments, we found that (1) subsidies reduced vaccine intention but increased it after controlling heterogeneity; (2) the stronger the social image against the vaccination, the lower the monetary incentive; and (3) persistently unvaccinated people would intend to be vaccinated only if a large subsidy was provided."}
{"title":"Multi-stage optimisation towards transformation pathways for municipal energy systems","authors":["Paul Maximilian R\u00f6hrig","Nils K\u00f6rber","Julius Zocher","Andreas Ulbig"],"raw_abstract":"An essential facet of achieving climate neutrality by 2045 is the\ndecarbonization of municipal energy systems. To accomplish this, it is\nnecessary to establish implementation concepts that detail the timing,\nlocation, and specific measures required to achieve decarbonization. This\nrestructuring process involves identifying the measures that offer the most\ncompelling techno-economic and ecological advantages. In particular, measures\nthat contribute to the interconnection of energy vectors and domains, e.g.\nheating, cooling, and electricity supply, in the sense of decentralized\nmulti-energy systems are a promising future development option. Due to the high\ncomplexity resulting from a multitude of decision options as well as a temporal\ncoupling across the transformation path, the use of optimization methods is\nrequired, which enable a bottom-up identification of suitable transformation\nsolutions in a high spatial resolution. For the design of reasonable concepts,\nwe develop a multistage optimization problem for the derivation of\ntransformation pathways in the context of a multi-location structure,\nexpansion, and operation problem. The results show that the heat supply in the\nfuture will mainly be provided by heat pumps with a share of 60%. It can also\nbe shown that an early dismantling of the gas network will lead to the need for\ntransitional technologies such as pellet heating. Overall, the conversion of\nthe municipal energy system can significantly reduce emissions (97%).","publication_date":1700464854,"paper_link":"http://arxiv.org/pdf/2311.11576v1","categories":["Quantitative Finance","Economics","Electrical Engineering and Systems Science"],"abstract":"An essential facet of achieving climate neutrality by 2045 is the decarbonization of municipal energy systems. To accomplish this, it is necessary to establish implementation concepts that detail the timing, location, and specific measures required to achieve decarbonization. This restructuring process involves identifying the measures that offer the most compelling techno-economic and ecological advantages. In particular, measures that contribute to the interconnection of energy vectors and domains, e.g. heating, cooling, and electricity supply, in the sense of decentralized multi-energy systems are a promising future development option. Due to the high complexity resulting from a multitude of decision options as well as a temporal coupling across the transformation path, the use of optimization methods is required, which enable a bottom-up identification of suitable transformation solutions in a high spatial resolution. For the design of reasonable concepts, we develop a multistage optimization problem for the derivation of transformation pathways in the context of a multi-location structure, expansion, and operation problem. The results show that the heat supply in the future will mainly be provided by heat pumps with a share of 60%. It can also be shown that an early dismantling of the gas network will lead to the need for transitional technologies such as pellet heating. Overall, the conversion of the municipal energy system can significantly reduce emissions (97%)."}
{"title":"Data-driven project planning: An integrated network learning and constraint relaxation approach in favor of scheduling","authors":["Izack Cohen"],"raw_abstract":"Our focus is on projects, i.e., business processes, which are emerging as the\neconomic drivers of our times. Differently from day-to-day operational\nprocesses that do not require detailed planning, a project requires planning\nand resource-constrained scheduling for coordinating resources across sub- or\nrelated projects and organizations. A planner in charge of project planning has\nto select a set of activities to perform, determine their precedence\nconstraints, and schedule them according to temporal project constraints. We\nsuggest a data-driven project planning approach for classes of projects such as\ninfrastructure building and information systems development projects. A project\nnetwork is first learned from historical records. The discovered network\nrelaxes temporal constraints embedded in individual projects, thus uncovering\nwhere planning and scheduling flexibility can be exploited for greater benefit.\nThen, the network, which contains multiple project plan variations, from which\none has to be selected, is enriched by identifying decision rules and frequent\npaths. The planner can rely on the project network for: 1) decoding a project\nvariation such that it forms a new project plan, and 2) applying\nresource-constrained project scheduling procedures to determine the project's\nschedule and resource allocation. Using two real-world project datasets, we\nshow that the suggested approach may provide the planner with significant\nflexibility (up to a 26% reduction of the critical path of a real project) to\nadjust the project plan and schedule. We believe that the proposed approach can\nplay an important part in supporting decision making towards automated\ndata-driven project planning.","publication_date":1700457197,"paper_link":"http://arxiv.org/pdf/2311.11542v1","categories":["Economics"],"abstract":"Our focus is on projects, i.e., business processes, which are emerging as the economic drivers of our times. Differently from day-to-day operational processes that do not require detailed planning, a project requires planning and resource-constrained scheduling for coordinating resources across sub- or related projects and organizations. A planner in charge of project planning has to select a set of activities to perform, determine their precedence constraints, and schedule them according to temporal project constraints. We suggest a data-driven project planning approach for classes of projects such as infrastructure building and information systems development projects. A project network is first learned from historical records. The discovered network relaxes temporal constraints embedded in individual projects, thus uncovering where planning and scheduling flexibility can be exploited for greater benefit. Then, the network, which contains multiple project plan variations, from which one has to be selected, is enriched by identifying decision rules and frequent paths. The planner can rely on the project network for: 1) decoding a project variation such that it forms a new project plan, and 2) applying resource-constrained project scheduling procedures to determine the project's schedule and resource allocation. Using two real-world project datasets, we show that the suggested approach may provide the planner with significant flexibility (up to a 26% reduction of the critical path of a real project) to adjust the project plan and schedule. We believe that the proposed approach can play an important part in supporting decision making towards automated data-driven project planning."}
{"title":"Benefiting from Bias: Delegating to Encourage Information Acquisition","authors":["Ian Ball","Xin Gao"],"raw_abstract":"A principal delegates decisions to a biased agent. Payoffs depend on a state\nthat the principal cannot observe. Initially, the agent does not observe the\nstate, but he can acquire information about it at a cost. We characterize the\nprincipal's optimal delegation set. This set features a cap on high decisions\nand a gap around the agent's ex ante favorite decision. It may even induce\nex-post Pareto-dominated decisions. Under certain conditions on the cost of\ninformation acquisition, we show that the principal prefers delegating to an\nagent with a small bias than to an unbiased agent.","publication_date":1700453696,"paper_link":"http://arxiv.org/pdf/2311.11526v1","categories":["Economics"],"abstract":"A principal delegates decisions to a biased agent. Payoffs depend on a state that the principal cannot observe. Initially, the agent does not observe the state, but he can acquire information about it at a cost. We characterize the principal's optimal delegation set. This set features a cap on high decisions and a gap around the agent's ex ante favorite decision. It may even induce ex-post Pareto-dominated decisions. Under certain conditions on the cost of information acquisition, we show that the principal prefers delegating to an agent with a small bias than to an unbiased agent."}
{"title":"Spatial Arbitrage Through Bidirectional Electric Vehicle Charging with Delivery Fleets","authors":["Mostafa Mohammadian","Constance Crozier","Kyri Baker"],"raw_abstract":"The adoption of electric vehicles (EVs), including electric taxis and buses,\nas a mode of transportation, is rapidly increasing in cities. In addition to\nproviding economic and environmental benefits, these fleets can potentially\nparticipate in the energy arbitrage market by leveraging their mobile energy\nstorage capabilities. This presents an opportunity for EV owners to contribute\nto a more sustainable and efficient energy system while also reducing their\noperational costs. The present study introduces deterministic and single-stage\nstochastic optimization frameworks that aim to maximize revenue by optimizing\nthe charging, discharging, and travel of a fleet of electric vehicles in the\ncontext of uncertainty surrounding both spatial and temporal energy prices. The\nsimulations are performed on a fleet of electric delivery trucks, which have to\nmake deliveries to certain locations on specific dates.\n  The findings indicate the promising potential of bidirectional electric\nvehicle charging as a mobile grid asset. However, it is important to note that\nsignificant revenue is only realized in scenarios where there is substantial\nvariation in prices between different areas, and when these price variations\ncan be accurately forecasted with a high level of confidence.","publication_date":1700439599,"paper_link":"http://arxiv.org/pdf/2311.11464v1","categories":["Mathematics"],"abstract":"The adoption of electric vehicles (EVs), including electric taxis and buses, as a mode of transportation, is rapidly increasing in cities. In addition to providing economic and environmental benefits, these fleets can potentially participate in the energy arbitrage market by leveraging their mobile energy storage capabilities. This presents an opportunity for EV owners to contribute to a more sustainable and efficient energy system while also reducing their operational costs. The present study introduces deterministic and single-stage stochastic optimization frameworks that aim to maximize revenue by optimizing the charging, discharging, and travel of a fleet of electric vehicles in the context of uncertainty surrounding both spatial and temporal energy prices. The simulations are performed on a fleet of electric delivery trucks, which have to make deliveries to certain locations on specific dates.   The findings indicate the promising potential of bidirectional electric vehicle charging as a mobile grid asset. However, it is important to note that significant revenue is only realized in scenarios where there is substantial variation in prices between different areas, and when these price variations can be accurately forecasted with a high level of confidence."}
{"title":"Actors in Multi-Sector Transitions -- Discourse Analysis on Hydrogen in Germany","authors":["Nils Ohlendorf","Meike L\u00f6hr","Jochen Markard"],"raw_abstract":"With net-zero emission goals, low-carbon transitions enter a new phase of\ndevelopment, leading to new challenges for policymaking and research. Multiple\ntransitions unfold in parallel across different sectors. This involves a broad\nrange of technologies, while actors engage in increasingly complex discourses.\nHere, we study the discourses on hydrogen in Germany. Based on the analysis of\n179 newspaper articles from 2016 to 2020, we find that a diverse set of actors,\nincluding many industry incumbents, speak favorably about hydrogen, emphasizing\neconomic opportunities and its relevance for the energy transition, whereas\nskeptics highlight its low energy efficiency and expected scarcity. With the\nhelp of discourse network analysis, we identify three emerging conflicts around\nthe use, production, and import of hydrogen. We explain these conflicts and the\nwidespread support of incumbents with a conceptual framework that captures the\ncomplex interplay of sectoral contexts, specific technologies and actor\ninterests.","publication_date":1700426594,"paper_link":"http://arxiv.org/pdf/2311.11421v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"With net-zero emission goals, low-carbon transitions enter a new phase of development, leading to new challenges for policymaking and research. Multiple transitions unfold in parallel across different sectors. This involves a broad range of technologies, while actors engage in increasingly complex discourses. Here, we study the discourses on hydrogen in Germany. Based on the analysis of 179 newspaper articles from 2016 to 2020, we find that a diverse set of actors, including many industry incumbents, speak favorably about hydrogen, emphasizing economic opportunities and its relevance for the energy transition, whereas skeptics highlight its low energy efficiency and expected scarcity. With the help of discourse network analysis, we identify three emerging conflicts around the use, production, and import of hydrogen. We explain these conflicts and the widespread support of incumbents with a conceptual framework that captures the complex interplay of sectoral contexts, specific technologies and actor interests."}
{"title":"Architecting the Future: A Model for Enterprise Integration in the Metaverse","authors":["Amirmohammad Nateghi","Maedeh Mosharraf"],"raw_abstract":"Although it has a history that goes back about three decades, Metaverse has\ngrown to be one of the most talked-about subjects today. Metaverse gradually\nincreased its influence in the realm of business discourse after initially\nbeing restricted to discussions about entertainment. Before getting deep into\nthe Metaverse, it should be noted that failure and deviating from the business\npath are highly likely for an enterprise that relies heavily on information\ntechnology (IT) because of improper use and thinking about IT. The idea of\nenterprise architecture (EA) emerged as a management strategy to address this\nissue. As the first school of thought of EA, it sought to transform IT from an\nunnecessary burden in an enterprise to a guiding and supporting force. Then an\nextended EA model is suggested as a result of the attempt made in this paper to\nuse the idea of EA to steer virtual enterprises on Metaverse-based platforms.\nFinally, to evaluate the conceptual model and demonstrate that the Metaverse\ncan support businesses, three case studies Decentraland, Battle Infinity, and\nRooom were utilized.","publication_date":1700422337,"paper_link":"http://arxiv.org/pdf/2311.11406v1","categories":["Economics","Quantitative Finance"],"abstract":"Although it has a history that goes back about three decades, Metaverse has grown to be one of the most talked-about subjects today. Metaverse gradually increased its influence in the realm of business discourse after initially being restricted to discussions about entertainment. Before getting deep into the Metaverse, it should be noted that failure and deviating from the business path are highly likely for an enterprise that relies heavily on information technology (IT) because of improper use and thinking about IT. The idea of enterprise architecture (EA) emerged as a management strategy to address this issue. As the first school of thought of EA, it sought to transform IT from an unnecessary burden in an enterprise to a guiding and supporting force. Then an extended EA model is suggested as a result of the attempt made in this paper to use the idea of EA to steer virtual enterprises on Metaverse-based platforms. Finally, to evaluate the conceptual model and demonstrate that the Metaverse can support businesses, three case studies Decentraland, Battle Infinity, and Rooom were utilized."}
{"title":"Ambiguity aversion as a route to randomness in a duopoly game","authors":["Davide Radi","Laura Gardini"],"raw_abstract":"The global dynamics is investigated for a duopoly game where the perfect\nforesight hypothesis is relaxed and firms are worst-case maximizers.\nOverlooking the degree of product substitutability as well as the sensitivity\nof price to quantity, the unique and globally stable Cournot-Nash equilibrium\nof the complete-information duopoly game, loses stability when firms are not\naware if they are playing a duopoly game, as it is, or an oligopoly game with\nmore than two competitors. This finding resembles Theocharis' condition for the\nstability of the Cournot-Nash equilibrium in oligopolies without uncertainty.\nAs opposed to complete-information oligopoly games, coexisting attractors,\ndisconnected basins of attractions and chaotic dynamics emerge when the\nCournot-Nash equilibrium loses stability. This difference in the global\ndynamics is due to the nonlinearities introduced by the worst-case approach to\nuncertainty, which mirror in bimodal best-reply functions. Conducted with\ntechniques that require a symmetric setting of the game, the investigation of\nthe dynamics reveals that a chaotic regime prevents firms from being ambiguity\naverse, that is, firms are worst-case maximizers only in the\nquantity-expectation space. Therefore, chaotic dynamics are the result and at\nthe same time the source of profit uncertainty.","publication_date":1700411097,"paper_link":"http://arxiv.org/pdf/2311.11366v1","categories":["Mathematics","Economics"],"abstract":"The global dynamics is investigated for a duopoly game where the perfect foresight hypothesis is relaxed and firms are worst-case maximizers. Overlooking the degree of product substitutability as well as the sensitivity of price to quantity, the unique and globally stable Cournot-Nash equilibrium of the complete-information duopoly game, loses stability when firms are not aware if they are playing a duopoly game, as it is, or an oligopoly game with more than two competitors. This finding resembles Theocharis' condition for the stability of the Cournot-Nash equilibrium in oligopolies without uncertainty. As opposed to complete-information oligopoly games, coexisting attractors, disconnected basins of attractions and chaotic dynamics emerge when the Cournot-Nash equilibrium loses stability. This difference in the global dynamics is due to the nonlinearities introduced by the worst-case approach to uncertainty, which mirror in bimodal best-reply functions. Conducted with techniques that require a symmetric setting of the game, the investigation of the dynamics reveals that a chaotic regime prevents firms from being ambiguity averse, that is, firms are worst-case maximizers only in the quantity-expectation space. Therefore, chaotic dynamics are the result and at the same time the source of profit uncertainty."}
{"title":"Workforce pDEI: Productivity Coupled with DEI","authors":["Lanqing Du","Jinwook Lee"],"raw_abstract":"Ranking pertaining to the human-centered tasks -- underscoring their\nparamount significance in these domains such as evaluation and hiring process\n-- exhibits widespread prevalence across various industries. Consequently,\ndecision-makers are taking proactive measurements to promote diversity,\nunderscore equity, and advance inclusion. Their unwavering commitment to these\nideals emanates from the following convictions: (i) Diversity encompasses a\nbroad spectrum of differences; (ii) Equity involves the assurance of equitable\nopportunities; and (iii) Inclusion revolves around the cultivation of a sense\nof value and impartiality, concurrently empowering individuals. Data-driven AI\ntools have been used for screening and ranking processes. However, there is a\ngrowing concern that the presence of pre-existing biases in databases may be\nexacerbated, particularly in the context of imbalanced datasets or the\nblack-box-schema. In this research, we propose a model-driven recruitment\ndecision support tool that addresses fairness together with equity in the\nscreening phase. We introduce the term ``pDEI\" to represent the output-input\noriented production efficiency adjusted by socioeconomic disparity. Taking into\naccount various aspects of interpreting socioeconomic disparity, our goals are\n(i) maximizing the relative efficiency of underrepresented groups and (ii)\nunderstanding how socioeconomic disparity affects the cultivation of a\nDEI-positive workplace.","publication_date":1700370915,"paper_link":"http://arxiv.org/pdf/2311.11231v1","categories":["Economics","Quantitative Finance"],"abstract":"Ranking pertaining to the human-centered tasks -- underscoring their paramount significance in these domains such as evaluation and hiring process -- exhibits widespread prevalence across various industries. Consequently, decision-makers are taking proactive measurements to promote diversity, underscore equity, and advance inclusion. Their unwavering commitment to these ideals emanates from the following convictions: (i) Diversity encompasses a broad spectrum of differences; (ii) Equity involves the assurance of equitable opportunities; and (iii) Inclusion revolves around the cultivation of a sense of value and impartiality, concurrently empowering individuals. Data-driven AI tools have been used for screening and ranking processes. However, there is a growing concern that the presence of pre-existing biases in databases may be exacerbated, particularly in the context of imbalanced datasets or the black-box-schema. In this research, we propose a model-driven recruitment decision support tool that addresses fairness together with equity in the screening phase. We introduce the term ``pDEI\" to represent the output-input oriented production efficiency adjusted by socioeconomic disparity. Taking into account various aspects of interpreting socioeconomic disparity, our goals are (i) maximizing the relative efficiency of underrepresented groups and (ii) understanding how socioeconomic disparity affects the cultivation of a DEI-positive workplace."}
{"title":"Modelling the Formation of Peer-to-Peer Trading Coalitions and Prosumer Participation Incentives in Transactive Energy Communities","authors":["Ying Zhang","Valentin Robu","Sho Cremers","Sonam Norbu","Benoit Couraud","Merlinda Andoni","David Flynn","H. Vincent Poor"],"raw_abstract":"Peer-to-peer (P2P) energy trading and energy communities have garnered much\nattention over in recent years due to increasing investments in local energy\ngeneration and storage assets. However, the efficiency to be gained from P2P\ntrading, and the structure of local energy markets raise many important\nchallenges. To analyse the efficiency of P2P energy markets, in this work, we\nconsider two different popular approaches to peer-to-peer trading: centralised\n(through a central market maker/clearing entity) vs. fully decentralised (P2P),\nand explore the comparative economic benefits of these models. We focus on the\nmetric of Gains from Trade (GT), given optimal P2P trading schedule computed by\na schedule optimiser. In both local market models, benefits from trading are\nrealised mainly due to the diversity in consumption behaviour and renewable\nenergy generation between prosumers in an energy community. Both market models\nwill lead to the most promising P2P contracts (the ones with the highest Gains\nfrom Trade) to be established first. Yet, we find diversity decreases quickly\nas more peer-to-peer energy contracts are established and more prosumers join\nthe market, leading to significantly diminishing returns. In this work, we aim\nto quantify this effect using real-world data from two large-scale smart energy\ntrials in the UK, i.e. the Low Carbon London project and the Thames Valley\nVision project. Our experimental study shows that, for both market models, only\na small number of P2P contracts, and only a fraction of total prosumers in the\ncommunity are required to achieve the majority of the maximal potential Gains\nfrom Trade. We also study the effect that diversity in consumption profiles has\non overall trading potential and dynamics in an energy community.","publication_date":1700355032,"paper_link":"http://arxiv.org/pdf/2311.11192v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"Peer-to-peer (P2P) energy trading and energy communities have garnered much attention over in recent years due to increasing investments in local energy generation and storage assets. However, the efficiency to be gained from P2P trading, and the structure of local energy markets raise many important challenges. To analyse the efficiency of P2P energy markets, in this work, we consider two different popular approaches to peer-to-peer trading: centralised (through a central market maker/clearing entity) vs. fully decentralised (P2P), and explore the comparative economic benefits of these models. We focus on the metric of Gains from Trade (GT), given optimal P2P trading schedule computed by a schedule optimiser. In both local market models, benefits from trading are realised mainly due to the diversity in consumption behaviour and renewable energy generation between prosumers in an energy community. Both market models will lead to the most promising P2P contracts (the ones with the highest Gains from Trade) to be established first. Yet, we find diversity decreases quickly as more peer-to-peer energy contracts are established and more prosumers join the market, leading to significantly diminishing returns. In this work, we aim to quantify this effect using real-world data from two large-scale smart energy trials in the UK, i.e. the Low Carbon London project and the Thames Valley Vision project. Our experimental study shows that, for both market models, only a small number of P2P contracts, and only a fraction of total prosumers in the community are required to achieve the majority of the maximal potential Gains from Trade. We also study the effect that diversity in consumption profiles has on overall trading potential and dynamics in an energy community."}
{"title":"Economic Viability of the Energy-Water-Hydrogen Nexus for Power System Decarbonization","authors":["Mostafa Goodarzi","Qifeng Li"],"raw_abstract":"This paper aims to evaluate the economic viability of the\nenergy-water-hydrogen (EWH) nexus as a new solution for reducing carbon\nemissions from power systems. The urgency around climate change emphasizes the\npressing need to mitigate carbon emissions, especially from the electricity\nsector, which accounts for a significant portion of total emissions in the US.\nIn response, incorporating more renewable energy sources (RESs) and green\nhydrogen, created through water electrolysis and RES, stands out as a crucial\nstrategy to combat climate challenges. We delve into various aspects of the EWH\nnexus, including carbon emissions from different power plants, capturing these\nemissions, and potential options for their reuse or storage. This paper\ninvolves modeling different sections of the EWH nexus and conducting an\neconomic analysis across scenarios in power plants to determine optimal water\nsupply methods, suitable chemical products for carbon reuse, and an appropriate\ncarbon emission penalty to encourage emission reduction through the EWH nexus.\nThe results indicate that reusing captured carbon emissions emerges as the most\nbeneficial option across all power plant types. This finding underscores the\npotential of carbon reuse as a pivotal strategy within the EWH nexus framework\nfor addressing carbon emissions.","publication_date":1700323712,"paper_link":"http://arxiv.org/pdf/2311.11111v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper aims to evaluate the economic viability of the energy-water-hydrogen (EWH) nexus as a new solution for reducing carbon emissions from power systems. The urgency around climate change emphasizes the pressing need to mitigate carbon emissions, especially from the electricity sector, which accounts for a significant portion of total emissions in the US. In response, incorporating more renewable energy sources (RESs) and green hydrogen, created through water electrolysis and RES, stands out as a crucial strategy to combat climate challenges. We delve into various aspects of the EWH nexus, including carbon emissions from different power plants, capturing these emissions, and potential options for their reuse or storage. This paper involves modeling different sections of the EWH nexus and conducting an economic analysis across scenarios in power plants to determine optimal water supply methods, suitable chemical products for carbon reuse, and an appropriate carbon emission penalty to encourage emission reduction through the EWH nexus. The results indicate that reusing captured carbon emissions emerges as the most beneficial option across all power plant types. This finding underscores the potential of carbon reuse as a pivotal strategy within the EWH nexus framework for addressing carbon emissions."}
{"title":"Optimal Investment and Fair Sharing Rules of the Incentives for Renewable Energy Communities","authors":["Almendra Awerkin","Paolo Falbo","Tiziano Vargiolu"],"raw_abstract":"The focus on Renewable Energy Communities (REC) is fastly growing after the\nEuropean Union (EU) has introduced a dedicated regulation in 2018. The idea of\ncreating local groups of citizens, small- and medium-sized companies, and\npublic institutions, which self-produce and self-consume energy from renewable\nsources is at the same time a way to save money for the participants, increase\nefficiency of the energy system, and reduce CO$_2$ emissions. Member states\ninside the EU are fixing more detailed regulations, which describe, how public\nincentives are measured. A natural objective for the incentive policies is of\ncourse to promote the self-consumption of a REC. A sophisticated incentive\npolicy is that based on the so called 'virtual framework'. Under this framework\nall the energy produced by a REC is sold to the market, and all the energy\nconsumed must be paid to retailers: self-consumption occurs only 'virtually',\nthanks a money compensation (paid by a central authority) for every MWh\nproduced and consumed by the REC in the same hour. In this context, two\nproblems have to be solved: the optimal investment in new technologies and a\nfair division of the incentive among the community members. We address these\nproblems by considering a particular type of REC, composed by a representative\nhousehold and a biogas producer, where the potential demand of the community is\ngiven by the household's demand, while both members produce renewable energy.\nWe set the problem as a leader-follower problem: the leader decide how to share\nthe incentive for the self-consumed energy, while the followers decide their\nown optimal installation strategy. We solve the leader's problem by searching\nfor a Nash bargaining solution for the incentive's fair division, while the\nfollower problem is solved by finding the Nash equilibria of a static\ncompetitive game between the members.","publication_date":1700311565,"paper_link":"http://arxiv.org/pdf/2311.12055v1","categories":["Economics","Quantitative Finance"],"abstract":"The focus on Renewable Energy Communities (REC) is fastly growing after the European Union (EU) has introduced a dedicated regulation in 2018. The idea of creating local groups of citizens, small- and medium-sized companies, and public institutions, which self-produce and self-consume energy from renewable sources is at the same time a way to save money for the participants, increase efficiency of the energy system, and reduce CO__FORMULA__ emissions. Member states inside the EU are fixing more detailed regulations, which describe, how public incentives are measured. A natural objective for the incentive policies is of course to promote the self-consumption of a REC. A sophisticated incentive policy is that based on the so called 'virtual framework'. Under this framework all the energy produced by a REC is sold to the market, and all the energy consumed must be paid to retailers: self-consumption occurs only 'virtually', thanks a money compensation (paid by a central authority) for every MWh produced and consumed by the REC in the same hour. In this context, two problems have to be solved: the optimal investment in new technologies and a fair division of the incentive among the community members. We address these problems by considering a particular type of REC, composed by a representative household and a biogas producer, where the potential demand of the community is given by the household's demand, while both members produce renewable energy. We set the problem as a leader-follower problem: the leader decide how to share the incentive for the self-consumed energy, while the followers decide their own optimal installation strategy. We solve the leader's problem by searching for a Nash bargaining solution for the incentive's fair division, while the follower problem is solved by finding the Nash equilibria of a static competitive game between the members."}
{"title":"Ruin probabilities with investments in random environment: smoothness","authors":["Viktor Antipov","Yuri Kabanov"],"raw_abstract":"The paper deals with the ruin problem of an insurance company investing its\ncapital reserve in a risky asset with the price dynamics given by a conditional\ngeometric Brownian motion whose parameters depend on a Markov process\ndescribing a random variations in the economic and financial environments. We\nprove smoothness of the ruin probability as a function of the initial capital\nand obtain for it an integro-differential equation.","publication_date":1700301193,"paper_link":"http://arxiv.org/pdf/2311.11023v1","categories":["Mathematics"],"abstract":"The paper deals with the ruin problem of an insurance company investing its capital reserve in a risky asset with the price dynamics given by a conditional geometric Brownian motion whose parameters depend on a Markov process describing a random variations in the economic and financial environments. We prove smoothness of the ruin probability as a function of the initial capital and obtain for it an integro-differential equation."}
{"title":"Research on the Dynamic Evolution and Influencing Factors of Energy Resilience in China","authors":["Tie Wei","Youqi Chen","Zhicheng Duan"],"raw_abstract":"Energy security is the guarantee for achieving the goal of carbon peaking and\ncarbon neutrality, and exploring energy resilience is one of the important ways\nto promote energy security transition and adapt to changes in international and\ndomestic energy markets. This paper applies the combined dynamic evaluation\nmethod to measure China's energy resilience level from 2004-2021, analyses the\nspatio-temporal dynamic evolution of China's energy resilience through the\ncenter of gravity-standard deviation ellipse and kernel density estimation, and\nemploys geo-detectors to detect the main influencing factors and interactions\nof China's energy resilience. The study finds that:(1)China's energy resilience\nlevel generally shows a zigzagging forward development trend, and the spatial\nimbalance characteristic of China's energy resilience is more obvious.(2)The\nspatial dynamics of China's energy resilience level evolves in a\nnortheast-southwest direction, and the whole moves towards the southwest, with\nan overall counterclockwise trend of constant offset.(3)When the energy\nresilience level of neighboring provinces is too low or too high, it has little\neffect on the improvement of the energy resilience level of the province; when\nthe energy resilience level of neighboring provinces is 1-1.4, it has a\npositive spatial correlation with the energy resilience level of the province,\nand the synergistic development of the provinces can improve the energy\nresilience level together.(4)GDP, the number of employees, the number of\nemployees enrolled in basic pension and medical insurance, and the number of\npatent applications in high-tech industries have a more significant impact on\nChina's energy resilience, while China's energy resilience is affected by the\ninteraction of multiple factors.","publication_date":1700289910,"paper_link":"http://arxiv.org/pdf/2311.10987v1","categories":["Economics","Quantitative Finance"],"abstract":"Energy security is the guarantee for achieving the goal of carbon peaking and carbon neutrality, and exploring energy resilience is one of the important ways to promote energy security transition and adapt to changes in international and domestic energy markets. This paper applies the combined dynamic evaluation method to measure China's energy resilience level from 2004-2021, analyses the spatio-temporal dynamic evolution of China's energy resilience through the center of gravity-standard deviation ellipse and kernel density estimation, and employs geo-detectors to detect the main influencing factors and interactions of China's energy resilience. The study finds that:(1)China's energy resilience level generally shows a zigzagging forward development trend, and the spatial imbalance characteristic of China's energy resilience is more obvious.(2)The spatial dynamics of China's energy resilience level evolves in a northeast-southwest direction, and the whole moves towards the southwest, with an overall counterclockwise trend of constant offset.(3)When the energy resilience level of neighboring provinces is too low or too high, it has little effect on the improvement of the energy resilience level of the province; when the energy resilience level of neighboring provinces is 1-1.4, it has a positive spatial correlation with the energy resilience level of the province, and the synergistic development of the provinces can improve the energy resilience level together.(4)GDP, the number of employees, the number of employees enrolled in basic pension and medical insurance, and the number of patent applications in high-tech industries have a more significant impact on China's energy resilience, while China's energy resilience is affected by the interaction of multiple factors."}
{"title":"Formation-Flying Interferometry in Geocentric Orbits","authors":["Takahiro Ito"],"raw_abstract":"Spacecraft formation flying serves as a method of astronomical\ninstrumentation that enables the construction of large virtual structures in\nspace. The formation-flying interferometry generally requires very-high control\naccuracy, and beyond-Earth orbits are typically selected. By contrast, this\nstudy proposes the use of geocentric orbits for formation-flying\ninterferometry. A geocentric orbit is beneficial because of its economic\naccessibility and the availability of flight-proven technologies for\nformation-flying autonomy, safety, and management. Its feasibility depends on\nthe existence of specific orbits that satisfy a small-disturbance environment\nwith favorable observation conditions. This theory, developed based on\ncelestial mechanics, indicates that small-perturbation regions tend to appear\nin higher-altitude and shorter-separation regions. Candidate orbits are\nidentified in high Earth orbit for the triangular laser-interferometric\ngravitational-wave telescope, which is 100 km in size, and in middle Earth\norbit for the linear astronomical interferometer, which is 0.5 km in size. A\nlow Earth orbit with a separation of approximately 0.1 km may be suitable for\nexperimental purposes. As shown in these examples, geocentric orbits are\npotentially applicable for various types of formation-flying interferometry.","publication_date":1700282235,"paper_link":"http://arxiv.org/pdf/2311.10970v1","categories":["Physics"],"abstract":"Spacecraft formation flying serves as a method of astronomical instrumentation that enables the construction of large virtual structures in space. The formation-flying interferometry generally requires very-high control accuracy, and beyond-Earth orbits are typically selected. By contrast, this study proposes the use of geocentric orbits for formation-flying interferometry. A geocentric orbit is beneficial because of its economic accessibility and the availability of flight-proven technologies for formation-flying autonomy, safety, and management. Its feasibility depends on the existence of specific orbits that satisfy a small-disturbance environment with favorable observation conditions. This theory, developed based on celestial mechanics, indicates that small-perturbation regions tend to appear in higher-altitude and shorter-separation regions. Candidate orbits are identified in high Earth orbit for the triangular laser-interferometric gravitational-wave telescope, which is 100 km in size, and in middle Earth orbit for the linear astronomical interferometer, which is 0.5 km in size. A low Earth orbit with a separation of approximately 0.1 km may be suitable for experimental purposes. As shown in these examples, geocentric orbits are potentially applicable for various types of formation-flying interferometry."}
{"title":"Short-term Volatility Estimation for High Frequency Trades using Gaussian processes (GPs)","authors":["Leonard Mushunje","Maxwell Mashasha","Edina Chandiwana"],"raw_abstract":"The fundamental theorem behind financial markets is that stock prices are\nintrinsically complex and stochastic. One of the complexities is the volatility\nassociated with stock prices. Volatility is a tendency for prices to change\nunexpectedly [1]. Price volatility is often detrimental to the return\neconomics, and thus, investors should factor it in whenever making investment\ndecisions, choices, and temporal or permanent moves. It is, therefore, crucial\nto make necessary and regular short and long-term stock price volatility\nforecasts for the safety and economics of investors returns. These forecasts\nshould be accurate and not misleading. Different models and methods, such as\nARCH GARCH models, have been intuitively implemented to make such forecasts.\nHowever, such traditional means fail to capture the short-term volatility\nforecasts effectively. This paper, therefore, investigates and implements a\ncombination of numeric and probabilistic models for short-term volatility and\nreturn forecasting for high-frequency trades. The essence is that one-day-ahead\nvolatility forecasts were made with Gaussian Processes (GPs) applied to the\noutputs of a Numerical market prediction (NMP) model. Firstly, the stock price\ndata from NMP was corrected by a GP. Since it is not easy to set price limits\nin a market due to its free nature and randomness, a Censored GP was used to\nmodel the relationship between the corrected stock prices and returns.\nForecasting errors were evaluated using the implied and estimated data.","publication_date":1700273028,"paper_link":"http://arxiv.org/pdf/2311.10935v1","categories":["Quantitative Finance","Statistics"],"abstract":"The fundamental theorem behind financial markets is that stock prices are intrinsically complex and stochastic. One of the complexities is the volatility associated with stock prices. Volatility is a tendency for prices to change unexpectedly [1]. Price volatility is often detrimental to the return economics, and thus, investors should factor it in whenever making investment decisions, choices, and temporal or permanent moves. It is, therefore, crucial to make necessary and regular short and long-term stock price volatility forecasts for the safety and economics of investors returns. These forecasts should be accurate and not misleading. Different models and methods, such as ARCH GARCH models, have been intuitively implemented to make such forecasts. However, such traditional means fail to capture the short-term volatility forecasts effectively. This paper, therefore, investigates and implements a combination of numeric and probabilistic models for short-term volatility and return forecasting for high-frequency trades. The essence is that one-day-ahead volatility forecasts were made with Gaussian Processes (GPs) applied to the outputs of a Numerical market prediction (NMP) model. Firstly, the stock price data from NMP was corrected by a GP. Since it is not easy to set price limits in a market due to its free nature and randomness, a Censored GP was used to model the relationship between the corrected stock prices and returns. Forecasting errors were evaluated using the implied and estimated data."}
{"title":"Modeling trading games in a stochastic non-life insurance market","authors":["Leonard Mushunje","David Edmund Allen"],"raw_abstract":"We studied the behavior and variation of utility between the two conflicting\nplayers in a closed Nash-equilibrium loop. Our modeling approach also captured\nthe nexus between optimal premium strategizing and firm performance using the\nLotka-Volterra completion model. Our model robustly modeled the two main cases,\ninsurer-insurer and insurer-policyholder, which we accompanied by numerical\nexamples of premium movements and their relationship to the market equilibrium\npoint. We found that insurers with high claim exposures tend to set high\npremiums. The other competitors either set a competitive premium or adopt the\nfixed premium charge to remain in the game; otherwise, they will operate below\nthe optimal point. We also noted an inverse link between trading premiums and\nclaims in general insurance games due to self-interest and utility\nindifferences. We concluded that while an insurer aims to charge high premiums\nto enjoy more, policyholders are willing to avoid these charges by paying less.","publication_date":1700265835,"paper_link":"http://arxiv.org/pdf/2311.10917v1","categories":["Economics"],"abstract":"We studied the behavior and variation of utility between the two conflicting players in a closed Nash-equilibrium loop. Our modeling approach also captured the nexus between optimal premium strategizing and firm performance using the Lotka-Volterra completion model. Our model robustly modeled the two main cases, insurer-insurer and insurer-policyholder, which we accompanied by numerical examples of premium movements and their relationship to the market equilibrium point. We found that insurers with high claim exposures tend to set high premiums. The other competitors either set a competitive premium or adopt the fixed premium charge to remain in the game; otherwise, they will operate below the optimal point. We also noted an inverse link between trading premiums and claims in general insurance games due to self-interest and utility indifferences. We concluded that while an insurer aims to charge high premiums to enjoy more, policyholders are willing to avoid these charges by paying less."}
{"title":"First, Do No Harm: Algorithms, AI, and Digital Product Liability","authors":["Marc J. Pfeiffer"],"raw_abstract":"The ethical imperative for technology should be first, do no harm. But\ndigital innovations like AI and social media increasingly enable societal\nharms, from bias to misinformation. As these technologies grow ubiquitous, we\nneed solutions to address unintended consequences. This report proposes a model\nto incentivize developers to prevent foreseeable algorithmic harms. It does\nthis by expanding negligence and product liability laws. Digital product\ndevelopers would be incentivized to mitigate potential algorithmic risks before\ndeployment to protect themselves and investors. Standards and penalties would\nbe set proportional to harm. Insurers would require harm mitigation during\ndevelopment in order to obtain coverage. This shifts tech ethics from move fast\nand break things to first, do no harm. Details would need careful refinement\nbetween stakeholders to enact reasonable guardrails without stifling\ninnovation. Policy and harm prevention frameworks would likely evolve over\ntime. Similar accountability schemes have helped address workplace,\nenvironmental, and product safety. Introducing algorithmic harm negligence\nliability would acknowledge the real societal costs of unethical tech. The\ntiming is right for reform. This proposal provides a model to steer the digital\nrevolution toward human rights and dignity. Harm prevention must be prioritized\nover reckless growth. Vigorous liability policies are essential to stop\ntechnologists from breaking things","publication_date":1700254061,"paper_link":"http://arxiv.org/pdf/2311.10861v1","categories":["Economics","Quantitative Finance"],"abstract":"The ethical imperative for technology should be first, do no harm. But digital innovations like AI and social media increasingly enable societal harms, from bias to misinformation. As these technologies grow ubiquitous, we need solutions to address unintended consequences. This report proposes a model to incentivize developers to prevent foreseeable algorithmic harms. It does this by expanding negligence and product liability laws. Digital product developers would be incentivized to mitigate potential algorithmic risks before deployment to protect themselves and investors. Standards and penalties would be set proportional to harm. Insurers would require harm mitigation during development in order to obtain coverage. This shifts tech ethics from move fast and break things to first, do no harm. Details would need careful refinement between stakeholders to enact reasonable guardrails without stifling innovation. Policy and harm prevention frameworks would likely evolve over time. Similar accountability schemes have helped address workplace, environmental, and product safety. Introducing algorithmic harm negligence liability would acknowledge the real societal costs of unethical tech. The timing is right for reform. This proposal provides a model to steer the digital revolution toward human rights and dignity. Harm prevention must be prioritized over reckless growth. Vigorous liability policies are essential to stop technologists from breaking things"}
{"title":"Religious Competition, Culture and Domestic Violence: Evidence from Colombia","authors":["Hector Galindo-Silva","Guy Tchuente"],"raw_abstract":"This paper studies how religious competition, as measured by the emergence of\nreligious organizations with innovative worship styles and cultural practices,\nimpacts domestic violence. Using data from Colombia, the study estimates a\ntwo-way fixed-effects model and reveals that the establishment of the first\nnon-Catholic church in a predominantly Catholic municipality leads to a\nsignificant decrease in reported cases of domestic violence. This effect\npersists in the long run, indicating that religious competition introduces\nvalues and practices that discourage domestic violence, such as household\nstability and reduced male dominance. Additionally, the effect is more\npronounced in municipalities with less clustered social networks, suggesting\nthe diffusion of these values and practices through social connections. This\nresearch contributes to the understanding of how culture influences domestic\nviolence, emphasizing the role of religious competition as a catalyst for\ncultural change.","publication_date":1700248878,"paper_link":"http://arxiv.org/pdf/2311.10831v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper studies how religious competition, as measured by the emergence of religious organizations with innovative worship styles and cultural practices, impacts domestic violence. Using data from Colombia, the study estimates a two-way fixed-effects model and reveals that the establishment of the first non-Catholic church in a predominantly Catholic municipality leads to a significant decrease in reported cases of domestic violence. This effect persists in the long run, indicating that religious competition introduces values and practices that discourage domestic violence, such as household stability and reduced male dominance. Additionally, the effect is more pronounced in municipalities with less clustered social networks, suggesting the diffusion of these values and practices through social connections. This research contributes to the understanding of how culture influences domestic violence, emphasizing the role of religious competition as a catalyst for cultural change."}
{"title":"Unveiling spatial patterns of population in Italian municipalities","authors":["Davide Fiaschi","Angela Parenti","Cristiano Ricci"],"raw_abstract":"We study the evolution of population density across Italian municipalities on\nthe based of their trajectories in the Moran space. We find evidence of spatial\ndynamical patterns of concentrated urban growth, urban sprawl, agglomeration,\nand depopulation. Over the long run, three distinct settlement systems emerge:\nurban, suburban, and rural. We discuss how estimating these demographic trends\nat the municipal level can help the design and validation of policies\ncontrasting the socio-economic decline in specific Italian areas, as in the\ncase of the Italian National Strategy for Inner Areas (Strategia Nazionale per\nle Aree Interne, SNAI).","publication_date":1700228528,"paper_link":"http://arxiv.org/pdf/2311.10520v1","categories":["Economics","Quantitative Finance"],"abstract":"We study the evolution of population density across Italian municipalities on the based of their trajectories in the Moran space. We find evidence of spatial dynamical patterns of concentrated urban growth, urban sprawl, agglomeration, and depopulation. Over the long run, three distinct settlement systems emerge: urban, suburban, and rural. We discuss how estimating these demographic trends at the municipal level can help the design and validation of policies contrasting the socio-economic decline in specific Italian areas, as in the case of the Italian National Strategy for Inner Areas (Strategia Nazionale per le Aree Interne, SNAI)."}
{"title":"Deriving Weeklong Activity-Travel Dairy from Google Location History: Survey Tool Development and A Field Test in Toronto","authors":["Melvyn Li","Kaili Wang","Yicong Liu","Khandker Nurul Habib"],"raw_abstract":"This paper introduces an innovative travel survey methodology that utilizes\nGoogle Location History (GLH) data to generate travel diaries for\ntransportation demand analysis. By leveraging the accuracy and omnipresence\namong smartphone users of GLH, the proposed methodology avoids the need for\nproprietary GPS tracking applications to collect smartphone-based GPS data.\nThis research enhanced an existing travel survey designer, Travel Activity\nInternet Survey Interface (TRAISI), to make it capable of deriving travel\ndiaries from the respondents' GLH. The feasibility of this data collection\napproach is showcased through the Google Timeline Travel Survey (GTTS)\nconducted in the Greater Toronto Area, Canada. The resultant dataset from the\nGTTS is demographically representative and offers detailed and accurate travel\nbehavioural insights.","publication_date":1700171763,"paper_link":"http://arxiv.org/pdf/2311.10210v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper introduces an innovative travel survey methodology that utilizes Google Location History (GLH) data to generate travel diaries for transportation demand analysis. By leveraging the accuracy and omnipresence among smartphone users of GLH, the proposed methodology avoids the need for proprietary GPS tracking applications to collect smartphone-based GPS data. This research enhanced an existing travel survey designer, Travel Activity Internet Survey Interface (TRAISI), to make it capable of deriving travel diaries from the respondents' GLH. The feasibility of this data collection approach is showcased through the Google Timeline Travel Survey (GTTS) conducted in the Greater Toronto Area, Canada. The resultant dataset from the GTTS is demographically representative and offers detailed and accurate travel behavioural insights."}
{"title":"Linear-scaling local natural orbital CCSD(T) approach for open-shell systems: algorithm, benchmarks, and large-scale applications","authors":["P. Bern\u00e1t Szab\u00f3","J\u00f3zsef Cs\u00f3ka","Mih\u00e1ly K\u00e1llay","P\u00e9ter R. Nagy"],"raw_abstract":"The extension of the highly-optimized local natural orbital (LNO) CCSD(T)\nmethod is presented for high-spin open-shell molecules. The techniques enabling\nthe outstanding efficiency of the closed-shell LNO-CCSD(T) variant are adopted,\nincluding the iteration- and redundancy-free MP2 and (T) formulations, as well\nas the integral-direct, memory- and disk use economic, and OpenMP-parallel\nalgorithms. For large molecules, the efficiency of our open-shell LNO-CCSD(T)\nmethod approaches that of its closed-shell parent method due to a novel\napproximation for higher-order long-range spin-polarization effects. The\naccuracy of open-shell LNO-CCSD(T) is extensively tested for radicals and\nreactions thereof, ionization processes, as well as spin-state splittings and\ntransition-metal compounds. At the size range, where the canonical CCSD(T)\nreference is accessible (up to 20-30 atoms) the average open-shell LNO-CCSD(T)\ncorrelation energies are found to be 99.9-99.95% accurate, which translates\ninto average absolute deviations of a few tenth of a kcal/mol in the\ninvestigated energy differences already with the default settings. This enables\nthe accurate modeling of large systems with complex electronic structure, as\nillustrated on open-shell organic radicals and transition metal complexes of up\nto 179 atoms, as well as on challenging biochemical systems, including up to\n601 atoms and 11,000 basis functions. While the protein models involve\ndifficulties for local approximations, such as the spin states of a bounded\niron ion or an extremely delocalized singly occupied orbital, the corresponding\nsingle-node LNO-CCSD(T) computations were feasible in a matter of days with 10s\nto a 100 GB of memory use. Therefore, the new LNO-CCSD(T) implementation\nenables highly-accurate computations for open-shell systems of unprecedented\nsize and complexity with widely accessible hardware.","publication_date":1700156673,"paper_link":"http://arxiv.org/pdf/2311.10048v1","categories":["Physics"],"abstract":"The extension of the highly-optimized local natural orbital (LNO) CCSD(T) method is presented for high-spin open-shell molecules. The techniques enabling the outstanding efficiency of the closed-shell LNO-CCSD(T) variant are adopted, including the iteration- and redundancy-free MP2 and (T) formulations, as well as the integral-direct, memory- and disk use economic, and OpenMP-parallel algorithms. For large molecules, the efficiency of our open-shell LNO-CCSD(T) method approaches that of its closed-shell parent method due to a novel approximation for higher-order long-range spin-polarization effects. The accuracy of open-shell LNO-CCSD(T) is extensively tested for radicals and reactions thereof, ionization processes, as well as spin-state splittings and transition-metal compounds. At the size range, where the canonical CCSD(T) reference is accessible (up to 20-30 atoms) the average open-shell LNO-CCSD(T) correlation energies are found to be 99.9-99.95% accurate, which translates into average absolute deviations of a few tenth of a kcal/mol in the investigated energy differences already with the default settings. This enables the accurate modeling of large systems with complex electronic structure, as illustrated on open-shell organic radicals and transition metal complexes of up to 179 atoms, as well as on challenging biochemical systems, including up to 601 atoms and 11,000 basis functions. While the protein models involve difficulties for local approximations, such as the spin states of a bounded iron ion or an extremely delocalized singly occupied orbital, the corresponding single-node LNO-CCSD(T) computations were feasible in a matter of days with 10s to a 100 GB of memory use. Therefore, the new LNO-CCSD(T) implementation enables highly-accurate computations for open-shell systems of unprecedented size and complexity with widely accessible hardware."}
{"title":"A Framework for Modeling, Analyzing, and Decision-Making in Disease Spread Dynamics and Medicine/Vaccine Distribution","authors":["Zenin Easa Panthakkalakath","Neeraj","Jimson Mathew"],"raw_abstract":"The challenges posed by epidemics and pandemics are immense, especially if\nthe causes are novel. This article introduces a versatile open-source\nsimulation framework designed to model intricate dynamics of infectious\ndiseases across diverse population centres. Taking inspiration from historical\nprecedents such as the Spanish flu and COVID-19, and geographical economic\ntheories such as Central place theory, the simulation integrates agent-based\nmodelling to depict the movement and interactions of individuals within\ndifferent settlement hierarchies. Additionally, the framework provides a tool\nfor decision-makers to assess and strategize optimal distribution plans for\nlimited resources like vaccines or cures as well as to impose mobility\nrestrictions.","publication_date":1700150617,"paper_link":"http://arxiv.org/pdf/2311.09984v1","categories":["Economics"],"abstract":"The challenges posed by epidemics and pandemics are immense, especially if the causes are novel. This article introduces a versatile open-source simulation framework designed to model intricate dynamics of infectious diseases across diverse population centres. Taking inspiration from historical precedents such as the Spanish flu and COVID-19, and geographical economic theories such as Central place theory, the simulation integrates agent-based modelling to depict the movement and interactions of individuals within different settlement hierarchies. Additionally, the framework provides a tool for decision-makers to assess and strategize optimal distribution plans for limited resources like vaccines or cures as well as to impose mobility restrictions."}
{"title":"Natural Disaster Analysis using Satellite Imagery and Social-Media Data for Emergency Response Situations","authors":["Sukeerthi Mandyam","Shanmuga Priya MG","Shalini Suresh","Kavitha Srinivasan"],"raw_abstract":"Disaster Management is one of the most promising research areas because of\nits significant economic, environmental and social repercussions. This research\nfocuses on analyzing different types of data (pre and post satellite images and\ntwitter data) related to disaster management for in-depth analysis of\nlocation-wise emergency requirements. This research has been divided into two\nstages, namely, satellite image analysis and twitter data analysis followed by\nintegration using location. The first stage involves pre and post disaster\nsatellite image analysis of the location using multi-class land cover\nsegmentation technique based on U-Net architecture. The second stage focuses on\nmapping the region with essential information about the disaster situation and\nimmediate requirements for relief operations. The severely affected regions are\ndemarcated and twitter data is extracted using keywords respective to that\nlocation. The extraction of situational information from a large corpus of raw\ntweets adopts Content Word based Tweet Summarization (COWTS) technique. An\nintegration of these modules using real-time location-based mapping and\nfrequency analysis technique gathers multi-dimensional information in the\nadvent of disaster occurrence such as the Kerala and Mississippi floods that\nwere analyzed and validated as test cases. The novelty of this research lies in\nthe application of segmented satellite images for disaster relief using\nhighlighted land cover changes and integration of twitter data by mapping these\nregion-specific filters for obtaining a complete overview of the disaster.","publication_date":1700146886,"paper_link":"http://arxiv.org/pdf/2311.09947v1","categories":["Economics"],"abstract":"Disaster Management is one of the most promising research areas because of its significant economic, environmental and social repercussions. This research focuses on analyzing different types of data (pre and post satellite images and twitter data) related to disaster management for in-depth analysis of location-wise emergency requirements. This research has been divided into two stages, namely, satellite image analysis and twitter data analysis followed by integration using location. The first stage involves pre and post disaster satellite image analysis of the location using multi-class land cover segmentation technique based on U-Net architecture. The second stage focuses on mapping the region with essential information about the disaster situation and immediate requirements for relief operations. The severely affected regions are demarcated and twitter data is extracted using keywords respective to that location. The extraction of situational information from a large corpus of raw tweets adopts Content Word based Tweet Summarization (COWTS) technique. An integration of these modules using real-time location-based mapping and frequency analysis technique gathers multi-dimensional information in the advent of disaster occurrence such as the Kerala and Mississippi floods that were analyzed and validated as test cases. The novelty of this research lies in the application of segmented satellite images for disaster relief using highlighted land cover changes and integration of twitter data by mapping these region-specific filters for obtaining a complete overview of the disaster."}
{"title":"The efficacy of the sugar-free labels is reduced by the health-sweetness tradeoff","authors":["Ksenia Panidi","Yaroslava Grebenschikova","Vasily Klucharev"],"raw_abstract":"In the present study, we use an experimental setting to explore the effects\nof sugar-free labels on the willingness to pay for food products. In our\nexperiment, participants placed bids for sugar-containing and analogous\nsugar-free products in a Becker-deGroot-Marschak auction to determine the\nwillingness to pay. Additionally, they rated each product on the level of\nperceived healthiness, sweetness, tastiness and familiarity with the product.\nWe then used structural equation modelling to estimate the direct, indirect and\ntotal effect of the label on the willingness to pay. The results suggest that\nsugar-free labels significantly increase the willingness to pay due to the\nperception of sugar-free products as healthier than sugar-containing ones.\nHowever, this positive effect is overridden by a significant decrease in\nperceived sweetness (and hence, tastiness) of products labelled as sugar-free\ncompared to sugar-containing products. As in our sample, healthiness and\ntastiness are positively related, while healthiness and sweetness are related\nnegatively, these results suggest that it is health-sweetness rather than\nhealth-tastiness tradeoff that decreases the efficiency of the sugar-free\nlabelling in nudging consumers towards healthier options.","publication_date":1700141197,"paper_link":"http://arxiv.org/pdf/2311.09885v1","categories":["Economics","Quantitative Finance"],"abstract":"In the present study, we use an experimental setting to explore the effects of sugar-free labels on the willingness to pay for food products. In our experiment, participants placed bids for sugar-containing and analogous sugar-free products in a Becker-deGroot-Marschak auction to determine the willingness to pay. Additionally, they rated each product on the level of perceived healthiness, sweetness, tastiness and familiarity with the product. We then used structural equation modelling to estimate the direct, indirect and total effect of the label on the willingness to pay. The results suggest that sugar-free labels significantly increase the willingness to pay due to the perception of sugar-free products as healthier than sugar-containing ones. However, this positive effect is overridden by a significant decrease in perceived sweetness (and hence, tastiness) of products labelled as sugar-free compared to sugar-containing products. As in our sample, healthiness and tastiness are positively related, while healthiness and sweetness are related negatively, these results suggest that it is health-sweetness rather than health-tastiness tradeoff that decreases the efficiency of the sugar-free labelling in nudging consumers towards healthier options."}
{"title":"Load Data Valuation in Multi-Energy Systems: An End-to-End Approach","authors":["Yangze Zhou","Qingsong Wen","Jie Song","Xueyuan Cui","Yi Wang"],"raw_abstract":"Accurate load forecasting serves as the foundation for the flexible operation\nof multi-energy systems (MES). Multi-energy loads are tightly coupled and\nexhibit significant uncertainties. Many works focus on enhancing forecasting\naccuracy by leveraging cross-sector information. However, data owners may not\nbe motivated to share their data unless it leads to substantial benefits.\nEnsuring a reasonable data valuation can encourage them to share their data\nwillingly. This paper presents an end-to-end framework to quantify multi-energy\nload data value by integrating forecasting and decision processes. To address\noptimization problems with integer variables, a two-stage end-to-end model\nsolution is proposed. Moreover, a profit allocation strategy based on\ncontribution to cost savings is investigated to encourage data sharing in MES.\nThe experimental results demonstrate a significant decrease in operation costs,\nsuggesting that the proposed valuation approach more effectively extracts the\ninherent data value than traditional methods. According to the proposed\nincentive mechanism, all sectors can benefit from data sharing by improving\nforecasting accuracy or receiving economic compensation.","publication_date":1700136668,"paper_link":"http://arxiv.org/pdf/2311.09839v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Accurate load forecasting serves as the foundation for the flexible operation of multi-energy systems (MES). Multi-energy loads are tightly coupled and exhibit significant uncertainties. Many works focus on enhancing forecasting accuracy by leveraging cross-sector information. However, data owners may not be motivated to share their data unless it leads to substantial benefits. Ensuring a reasonable data valuation can encourage them to share their data willingly. This paper presents an end-to-end framework to quantify multi-energy load data value by integrating forecasting and decision processes. To address optimization problems with integer variables, a two-stage end-to-end model solution is proposed. Moreover, a profit allocation strategy based on contribution to cost savings is investigated to encourage data sharing in MES. The experimental results demonstrate a significant decrease in operation costs, suggesting that the proposed valuation approach more effectively extracts the inherent data value than traditional methods. According to the proposed incentive mechanism, all sectors can benefit from data sharing by improving forecasting accuracy or receiving economic compensation."}
{"title":"Posterior-Mean Separable Costs of Information Acquisition","authors":["Jeffrey Mensch","Komal Malik"],"raw_abstract":"We analyze a problem of revealed preference given state-dependent stochastic\nchoice data in which the payoff to a decision maker (DM) only depends on their\nbeliefs about posterior means. Often, the DM must also learn about or pay\nattention to the state; in applied work on this subject, a convenient\nassumption is that the costs of such learning are linearly dependent in the\ndistribution over posterior means. We provide testable conditions to identify\nwhether this assumption holds. This allows for the use of information design\ntechniques to solve the DM's problem.","publication_date":1700098105,"paper_link":"http://arxiv.org/pdf/2311.09496v2","categories":["Economics"],"abstract":"We analyze a problem of revealed preference given state-dependent stochastic choice data in which the payoff to a decision maker (DM) only depends on their beliefs about posterior means. Often, the DM must also learn about or pay attention to the state; in applied work on this subject, a convenient assumption is that the costs of such learning are linearly dependent in the distribution over posterior means. We provide testable conditions to identify whether this assumption holds. This allows for the use of information design techniques to solve the DM's problem."}
{"title":"Urban economics of migration in the cities of the northeast region of Brazil","authors":["Denise Cristina Bomtempo"],"raw_abstract":"The focus of this text is to discuss how geographical science can contribute\nto an understanding of international migration in the 21st century. To this\nend, in the introduction we present the central ideas, as well as the internal\nstructure of the text. Then, we present the theoretical and methodological\napproach that guides the research and, in section three, we show the results\nthrough texts and cartograms based on secondary data and empirical information.\nFinally, in the final remarks, we summarize the text with a view to\ncontributing to the proposed debate.","publication_date":1700089642,"paper_link":"http://arxiv.org/pdf/2311.09432v1","categories":["Economics","Quantitative Finance"],"abstract":"The focus of this text is to discuss how geographical science can contribute to an understanding of international migration in the 21st century. To this end, in the introduction we present the central ideas, as well as the internal structure of the text. Then, we present the theoretical and methodological approach that guides the research and, in section three, we show the results through texts and cartograms based on secondary data and empirical information. Finally, in the final remarks, we summarize the text with a view to contributing to the proposed debate."}
{"title":"New configurations of the interface between innovation and urban spatial agglomeration: the localized industrial systems (lis) of the clothing in Fortaleza/Brazil","authors":["Edilson Pereira Junior"],"raw_abstract":"The paper seeks to interpret the interface between innovation, industry and\nurban business agglomeration, by trying to understand how local/regional\nentrepreneurs and social agents use the forces of agglomeration to organize\nproductive activities with a view to achieving greater competitiveness and\nstrategic advantages. There are many territorial manifestations that\nmaterialize from this new reality and the text seeks to propose a reading of\nits characteristics based on what we call \"productive spatial configurations\",\nwhich represent the specific functioning of a certain innovative production\nprocess and its territorial impact. To illustrate this approach, we take as an\nexample a case study that illustrates how productive spatial configurations\nmanifest themselves through the revitalization of an industrial economy that\nincorporates innovative efforts, whether technological, process or\norganizational. This is the localized industrial system (LIS) of clothing and\napparel in Fortaleza, Cear\\'a state, Brazil, which reveals an industrial\nexperience of resistance with significant organizational innovation in the\nproduction and distribution processes of clothing. The main objective of the\nproposal is to organize theoretical and empirical tools that will allow us to\nread the combination of economic, social and political variables in spaces\nwhere collaborative networks of companies, service centers and public\ninstitutions flourish, in the context of various industrial production\nprocesses. This could point to the progress we need to overcome the many false\ncontroversies on the subject.","publication_date":1700089075,"paper_link":"http://arxiv.org/pdf/2311.09429v1","categories":["Economics","Quantitative Finance"],"abstract":"The paper seeks to interpret the interface between innovation, industry and urban business agglomeration, by trying to understand how local/regional entrepreneurs and social agents use the forces of agglomeration to organize productive activities with a view to achieving greater competitiveness and strategic advantages. There are many territorial manifestations that materialize from this new reality and the text seeks to propose a reading of its characteristics based on what we call \"productive spatial configurations\", which represent the specific functioning of a certain innovative production process and its territorial impact. To illustrate this approach, we take as an example a case study that illustrates how productive spatial configurations manifest themselves through the revitalization of an industrial economy that incorporates innovative efforts, whether technological, process or organizational. This is the localized industrial system (LIS) of clothing and apparel in Fortaleza, Cear\\'a state, Brazil, which reveals an industrial experience of resistance with significant organizational innovation in the production and distribution processes of clothing. The main objective of the proposal is to organize theoretical and empirical tools that will allow us to read the combination of economic, social and political variables in spaces where collaborative networks of companies, service centers and public institutions flourish, in the context of various industrial production processes. This could point to the progress we need to overcome the many false controversies on the subject."}
{"title":"Strategic Data Augmentation with CTGAN for Smart Manufacturing: Enhancing Machine Learning Predictions of Paper Breaks in Pulp-and-Paper Production","authors":["Hamed Khosravi","Sarah Farhadpour","Manikanta Grandhi","Ahmed Shoyeb Raihan","Srinjoy Das","Imtiaz Ahmed"],"raw_abstract":"A significant challenge for predictive maintenance in the pulp-and-paper\nindustry is the infrequency of paper breaks during the production process. In\nthis article, operational data is analyzed from a paper manufacturing machine\nin which paper breaks are relatively rare but have a high economic impact.\nUtilizing a dataset comprising 18,398 instances derived from a quality\nassurance protocol, we address the scarcity of break events (124 cases) that\npose a challenge for machine learning predictive models. With the help of\nConditional Generative Adversarial Networks (CTGAN) and Synthetic Minority\nOversampling Technique (SMOTE), we implement a novel data augmentation\nframework. This method ensures that the synthetic data mirrors the distribution\nof the real operational data but also seeks to enhance the performance metrics\nof predictive modeling. Before and after the data augmentation, we evaluate\nthree different machine learning algorithms-Decision Trees (DT), Random Forest\n(RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our\nstudy achieved significant improvements in predictive maintenance performance\nmetrics. The efficacy of CTGAN in addressing data scarcity was evident, with\nthe models' detection of machine breaks (Class 1) improving by over 30% for\nDecision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression.\nWith this methodological advancement, this study contributes to industrial\nquality control and maintenance scheduling by addressing rare event prediction\nin manufacturing processes.","publication_date":1700077635,"paper_link":"http://arxiv.org/pdf/2311.09333v1","categories":["Economics"],"abstract":"A significant challenge for predictive maintenance in the pulp-and-paper industry is the infrequency of paper breaks during the production process. In this article, operational data is analyzed from a paper manufacturing machine in which paper breaks are relatively rare but have a high economic impact. Utilizing a dataset comprising 18,398 instances derived from a quality assurance protocol, we address the scarcity of break events (124 cases) that pose a challenge for machine learning predictive models. With the help of Conditional Generative Adversarial Networks (CTGAN) and Synthetic Minority Oversampling Technique (SMOTE), we implement a novel data augmentation framework. This method ensures that the synthetic data mirrors the distribution of the real operational data but also seeks to enhance the performance metrics of predictive modeling. Before and after the data augmentation, we evaluate three different machine learning algorithms-Decision Trees (DT), Random Forest (RF), and Logistic Regression (LR). Utilizing the CTGAN-enhanced dataset, our study achieved significant improvements in predictive maintenance performance metrics. The efficacy of CTGAN in addressing data scarcity was evident, with the models' detection of machine breaks (Class 1) improving by over 30% for Decision Trees, 20% for Random Forest, and nearly 90% for Logistic Regression. With this methodological advancement, this study contributes to industrial quality control and maintenance scheduling by addressing rare event prediction in manufacturing processes."}
{"title":"Structural Advantages for Integrated Builders in MEV-Boost","authors":["Mallesh Pai","Max Resnick"],"raw_abstract":"Currently, over 90% of Ethereum blocks are built using MEV-Boost, an auction\nthat allows validators to sell their block-building power to builders who\ncompete in an open English auction in each slot. Shortly after the merge, when\nMEV-Boost was in its infancy, most block builders were neutral, meaning they\ndid not trade themselves but rather aggregated transactions from other traders.\nOver time, integrated builders, operated by trading firms, began to overtake\nmany of the neutral builders. Outside of the integrated builder teams, little\nis known about which advantages integration confers beyond latency and how\nlatency advantages distort on-chain trading.\n  This paper explores these poorly understood advantages. We make two\ncontributions. First, we point out that integrated builders are able to bid\ntruthfully in their own bundle merge and then decide how much profit to take\nlater in the final stages of the PBS auction when more information is\navailable, making the auction for them look closer to a second-price auction\nwhile independent searchers are stuck in a first-price auction. Second, we find\nthat latency disadvantages convey a winner's curse on slow bidders when\nunderlying values depend on a stochastic price process that change as bids are\nsubmitted.","publication_date":1700065533,"paper_link":"http://arxiv.org/pdf/2311.09083v1","categories":["Economics"],"abstract":"Currently, over 90% of Ethereum blocks are built using MEV-Boost, an auction that allows validators to sell their block-building power to builders who compete in an open English auction in each slot. Shortly after the merge, when MEV-Boost was in its infancy, most block builders were neutral, meaning they did not trade themselves but rather aggregated transactions from other traders. Over time, integrated builders, operated by trading firms, began to overtake many of the neutral builders. Outside of the integrated builder teams, little is known about which advantages integration confers beyond latency and how latency advantages distort on-chain trading.   This paper explores these poorly understood advantages. We make two contributions. First, we point out that integrated builders are able to bid truthfully in their own bundle merge and then decide how much profit to take later in the final stages of the PBS auction when more information is available, making the auction for them look closer to a second-price auction while independent searchers are stuck in a first-price auction. Second, we find that latency disadvantages convey a winner's curse on slow bidders when underlying values depend on a stochastic price process that change as bids are submitted."}
{"title":"The impact of Electricity Blackouts and poor infrastructure on the livelihood of residents and the local economy of City of Johannesburg, South Africa","authors":["Nkosingizwile Mazwi Mchunu","George Okechukwu Onatu","Trynos Gumbo"],"raw_abstract":"This paper discusses the impact of electricity blackouts and poor\ninfrastructure on the livelihood of residents and the local economy of\nJohannesburg, South Africa. The importance of a stable electricity grid plays a\nvital role in the effective functioning of urban infrastructure and the\neconomy. The importance of electricity in the present-day South Africa has not\nbeen emphasized enough to be prioritized at all levels of government,\nespecially at the local level, as it is where all socio-economic activities\ntake place. The new South Africa needs to redefine the importance of\nelectricity by ensuring that it is accessible, affordable, and produced\nsustainably, and most of all, by ensuring that the energy transition\ninitiatives to green energy take place in a planned manner without causing harm\nto the economy, which might deepen the plight of South Africans. Currently, the\nCity of Johannesburg is a growing spatial entity in both demographic and\nurbanization terms, and growing urban spaces require a stable supply of\nelectricity for the proper functioning of urban systems and the growth of the\nlocal economy. The growth of the city brings about a massive demand for\nelectricity that outstrips the current supply of electricity available on the\nlocal grid. The imbalance in the current supply and growing demand for\nelectricity result in energy blackouts in the city, which have ripple effects\non the economy and livelihoods of the people of Johannesburg. This paper\nexamines the impact of electricity blackouts and poor infrastructure on the\nlivelihood of residents and the local economy of Johannesburg, South Africa.","publication_date":1700053285,"paper_link":"http://arxiv.org/pdf/2311.08929v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper discusses the impact of electricity blackouts and poor infrastructure on the livelihood of residents and the local economy of Johannesburg, South Africa. The importance of a stable electricity grid plays a vital role in the effective functioning of urban infrastructure and the economy. The importance of electricity in the present-day South Africa has not been emphasized enough to be prioritized at all levels of government, especially at the local level, as it is where all socio-economic activities take place. The new South Africa needs to redefine the importance of electricity by ensuring that it is accessible, affordable, and produced sustainably, and most of all, by ensuring that the energy transition initiatives to green energy take place in a planned manner without causing harm to the economy, which might deepen the plight of South Africans. Currently, the City of Johannesburg is a growing spatial entity in both demographic and urbanization terms, and growing urban spaces require a stable supply of electricity for the proper functioning of urban systems and the growth of the local economy. The growth of the city brings about a massive demand for electricity that outstrips the current supply of electricity available on the local grid. The imbalance in the current supply and growing demand for electricity result in energy blackouts in the city, which have ripple effects on the economy and livelihoods of the people of Johannesburg. This paper examines the impact of electricity blackouts and poor infrastructure on the livelihood of residents and the local economy of Johannesburg, South Africa."}
{"title":"Kernel-based independence tests for causal structure learning on functional data","authors":["Felix Laumann","Julius von K\u00fcgelgen","Junhyung Park","Bernhard Sch\u00f6lkopf","Mauricio Barahona"],"raw_abstract":"Measurements of systems taken along a continuous functional dimension, such\nas time or space, are ubiquitous in many fields, from the physical and\nbiological sciences to economics and engineering.Such measurements can be\nviewed as realisations of an underlying smooth process sampled over the\ncontinuum. However, traditional methods for independence testing and causal\nlearning are not directly applicable to such data, as they do not take into\naccount the dependence along the functional dimension. By using specifically\ndesigned kernels, we introduce statistical tests for bivariate, joint, and\nconditional independence for functional variables. Our method not only extends\nthe applicability to functional data of the HSIC and its d-variate version\n(d-HSIC), but also allows us to introduce a test for conditional independence\nby defining a novel statistic for the CPT based on the HSCIC, with optimised\nregularisation strength estimated through an evaluation rejection rate. Our\nempirical results of the size and power of these tests on synthetic functional\ndata show good performance, and we then exemplify their application to several\nconstraint- and regression-based causal structure learning problems, including\nboth synthetic examples and real socio-economic data.","publication_date":1700033008,"paper_link":"http://arxiv.org/pdf/2311.08743v1","categories":["Statistics"],"abstract":"Measurements of systems taken along a continuous functional dimension, such as time or space, are ubiquitous in many fields, from the physical and biological sciences to economics and engineering.Such measurements can be viewed as realisations of an underlying smooth process sampled over the continuum. However, traditional methods for independence testing and causal learning are not directly applicable to such data, as they do not take into account the dependence along the functional dimension. By using specifically designed kernels, we introduce statistical tests for bivariate, joint, and conditional independence for functional variables. Our method not only extends the applicability to functional data of the HSIC and its d-variate version (d-HSIC), but also allows us to introduce a test for conditional independence by defining a novel statistic for the CPT based on the HSCIC, with optimised regularisation strength estimated through an evaluation rejection rate. Our empirical results of the size and power of these tests on synthetic functional data show good performance, and we then exemplify their application to several constraint- and regression-based causal structure learning problems, including both synthetic examples and real socio-economic data."}
{"title":"Joint User Pairing and Beamforming Design of Multi-STAR-RISs-Aided NOMA in the Indoor Environment via Multi-Agent Reinforcement Learning","authors":["Yu Min Park","Yan Kyaw Tun","Choong Seon Hong"],"raw_abstract":"The development of 6G/B5G wireless networks, which have requirements that go\nbeyond current 5G networks, is gaining interest from academia and industry.\nHowever, to increase 6G/B5G network quality, conventional cellular networks\nthat rely on terrestrial base stations are constrained geographically and\neconomically. Meanwhile, NOMA allows multiple users to share the same\nresources, which improves the spectral efficiency of the system and has the\nadvantage of supporting a larger number of users. Additionally, by\nintelligently manipulating the phase and amplitude of both the reflected and\ntransmitted signals, STAR-RISs can achieve improved coverage, increased\nspectral efficiency, and enhanced communication reliability. However, STAR-RISs\nmust simultaneously optimize the amplitude and phase shift corresponding to\nreflection and transmission, which makes the existing terrestrial networks more\ncomplicated and is considered a major challenging issue. Motivated by the\nabove, we study the joint user pairing for NOMA and beamforming design of\nMulti-STAR-RISs in an indoor environment. Then, we formulate the optimization\nproblem with the objective of maximizing the total throughput of MUs by jointly\noptimizing the decoding order, user pairing, active beamforming, and passive\nbeamforming. However, the formulated problem is a MINLP. To address this\nchallenge, we first introduce the decoding order for NOMA networks. Next, we\ndecompose the original problem into two subproblems, namely: 1) MU pairing and\n2) Beamforming optimization under the optimal decoding order. For the first\nsubproblem, we employ correlation-based K-means clustering to solve the user\npairing problem. Then, to jointly deal with beamforming vector optimizations,\nwe propose MAPPO, which can make quick decisions in the given environment owing\nto its low complexity.","publication_date":1700025516,"paper_link":"http://arxiv.org/pdf/2311.08708v2","categories":["Mathematics"],"abstract":"The development of 6G/B5G wireless networks, which have requirements that go beyond current 5G networks, is gaining interest from academia and industry. However, to increase 6G/B5G network quality, conventional cellular networks that rely on terrestrial base stations are constrained geographically and economically. Meanwhile, NOMA allows multiple users to share the same resources, which improves the spectral efficiency of the system and has the advantage of supporting a larger number of users. Additionally, by intelligently manipulating the phase and amplitude of both the reflected and transmitted signals, STAR-RISs can achieve improved coverage, increased spectral efficiency, and enhanced communication reliability. However, STAR-RISs must simultaneously optimize the amplitude and phase shift corresponding to reflection and transmission, which makes the existing terrestrial networks more complicated and is considered a major challenging issue. Motivated by the above, we study the joint user pairing for NOMA and beamforming design of Multi-STAR-RISs in an indoor environment. Then, we formulate the optimization problem with the objective of maximizing the total throughput of MUs by jointly optimizing the decoding order, user pairing, active beamforming, and passive beamforming. However, the formulated problem is a MINLP. To address this challenge, we first introduce the decoding order for NOMA networks. Next, we decompose the original problem into two subproblems, namely: 1) MU pairing and 2) Beamforming optimization under the optimal decoding order. For the first subproblem, we employ correlation-based K-means clustering to solve the user pairing problem. Then, to jointly deal with beamforming vector optimizations, we propose MAPPO, which can make quick decisions in the given environment owing to its low complexity."}
{"title":"The Future of Sustainability in Germany: Areas for Improvement and Innovation","authors":["Mehrnaz Kouhihabibi","Erfan Mohammadi"],"raw_abstract":"This paper reviews the literature on biodegradable waste management in\nGermany, a multifaceted endeavor that reflects its commitment to sustainability\nand environmental responsibility. It examines the processes and benefits of\nseparate collection, recycling, and eco-friendly utilization of biodegradable\nmaterials, which produce valuable byproducts such as compost, digestate, and\nbiogas. These byproducts serve as organic fertilizers, peat substitutes, and\nrenewable energy sources, contributing to ecological preservation and economic\nprudence. The paper also discusses the global implications of biodegradable\nwaste management, such as preventing methane emissions from landfills, a major\nsource of greenhouse gas, and reusing organic matter and essential nutrients.\nMoreover, the paper explores how biodegradable waste management reduces waste\nvolume, facilitates waste sorting and incineration, and sets a global example\nfor addressing climate change and working toward a sustainable future. The\npaper highlights the importance of a comprehensive and holistic approach to\nsustainability that encompasses waste management, renewable energy,\ntransportation, agriculture, waste reduction, and urban development.","publication_date":1700020636,"paper_link":"http://arxiv.org/pdf/2311.08678v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper reviews the literature on biodegradable waste management in Germany, a multifaceted endeavor that reflects its commitment to sustainability and environmental responsibility. It examines the processes and benefits of separate collection, recycling, and eco-friendly utilization of biodegradable materials, which produce valuable byproducts such as compost, digestate, and biogas. These byproducts serve as organic fertilizers, peat substitutes, and renewable energy sources, contributing to ecological preservation and economic prudence. The paper also discusses the global implications of biodegradable waste management, such as preventing methane emissions from landfills, a major source of greenhouse gas, and reusing organic matter and essential nutrients. Moreover, the paper explores how biodegradable waste management reduces waste volume, facilitates waste sorting and incineration, and sets a global example for addressing climate change and working toward a sustainable future. The paper highlights the importance of a comprehensive and holistic approach to sustainability that encompasses waste management, renewable energy, transportation, agriculture, waste reduction, and urban development."}
{"title":"Managing Biotechnology Innovation: Challenges and Opportunities for Startups and Small Companies","authors":["Narges Ramezani","Erfan Mohammadi"],"raw_abstract":"The biotechnology industry presents challenges for startups and small\ncompanies due to high costs and complex regulations but also offers\nopportunities for innovation and growth. Effective strategies for managing\nbiotechnology innovation include partnerships, intellectual property\ndevelopment, digital technologies, customer engagement, and government funding.\nThis paper explores these strategies and highlights the importance of agility\nand niche focus for success in the industry. By adopting these strategies,\nstartups and small companies can compete in this dynamic and rapidly evolving\nfield.","publication_date":1700019263,"paper_link":"http://arxiv.org/pdf/2311.08671v1","categories":["Economics","Quantitative Finance"],"abstract":"The biotechnology industry presents challenges for startups and small companies due to high costs and complex regulations but also offers opportunities for innovation and growth. Effective strategies for managing biotechnology innovation include partnerships, intellectual property development, digital technologies, customer engagement, and government funding. This paper explores these strategies and highlights the importance of agility and niche focus for success in the industry. By adopting these strategies, startups and small companies can compete in this dynamic and rapidly evolving field."}
{"title":"The Use of Symmetry for Models with Variable-size Variables","authors":["Takeshi Fukasawa"],"raw_abstract":"This paper shows the universal representation of symmetric functions with\nmultidimensional variable-size variables, which justifies the use of\napproximation methods aggregating the information of each variable. I then\ndiscuss how the results give insights into economic applications, including\ntwo-step policy function estimation, moment-based Markov equilibrium, and\naggregative games.","publication_date":1700014038,"paper_link":"http://arxiv.org/pdf/2311.08650v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper shows the universal representation of symmetric functions with multidimensional variable-size variables, which justifies the use of approximation methods aggregating the information of each variable. I then discuss how the results give insights into economic applications, including two-step policy function estimation, moment-based Markov equilibrium, and aggregative games."}
{"title":"ColorTrace: Fungible token coloring and attribution","authors":["Ryan Zarick","Bryan Pellegrino","Isaac Zhang","Thomas Kim","Caleb Banister"],"raw_abstract":"We formally define the fungible token coloring problem of attributing\n(coloring) fungible tokens to originating entities (minters), and present, to\nour knowledge, the first practical onchain algorithm to solve it. Tracking\nattribution of colored tokens losslessly using existing approaches such as the\nColored Coins protocol is computationally intractable due to the per-wallet\nstorage requirements growing in proportion to the number of minters. Our first\ncontribution is an elegant solution to the single-chain token coloring problem,\nwhere colored tokens are atomically burned and minted to ensure each wallet\nonly contains tokens of a single color. Our second contribution is an extension\nto this single-chain token coloring algorithm to allow safe and efficient\ncrosschain token transfers. We present ColorTrace, an onchain algorithm to\nachieve globally consistent, economically feasible, fungible token coloring.","publication_date":1700011563,"paper_link":"http://arxiv.org/pdf/2311.08639v1","categories":["Economics"],"abstract":"We formally define the fungible token coloring problem of attributing (coloring) fungible tokens to originating entities (minters), and present, to our knowledge, the first practical onchain algorithm to solve it. Tracking attribution of colored tokens losslessly using existing approaches such as the Colored Coins protocol is computationally intractable due to the per-wallet storage requirements growing in proportion to the number of minters. Our first contribution is an elegant solution to the single-chain token coloring problem, where colored tokens are atomically burned and minted to ensure each wallet only contains tokens of a single color. Our second contribution is an extension to this single-chain token coloring algorithm to allow safe and efficient crosschain token transfers. We present ColorTrace, an onchain algorithm to achieve globally consistent, economically feasible, fungible token coloring."}
{"title":"Crowdsearch","authors":["Hans Gersbach","Akaki Mamageishvili","Fikri Pitsuwan"],"raw_abstract":"A common economic process is crowdsearch, wherein a group of agents is\ninvited to search for a valuable physical or virtual object, e.g. creating and\npatenting an invention, solving an open scientific problem, or identifying\nvulnerabilities in software. We study a binary model of crowdsearch in which\nagents have different abilities to find the object. We characterize the types\nof equilibria and identify which type of crowd maximizes the likelihood of\nfinding the object. Sometimes, however, an unlimited crowd is not sufficient to\nguarantee that the object is found. It even can happen that inviting more\nagents lowers the probability of finding the object. We characterize the\noptimal prize and show that offering only one prize (winner-takes-all)\nmaximizes the probability of finding the object but is not necessarily optimal\nfor the crowdsearch designer.","publication_date":1699995442,"paper_link":"http://arxiv.org/pdf/2311.08532v1","categories":["Economics"],"abstract":"A common economic process is crowdsearch, wherein a group of agents is invited to search for a valuable physical or virtual object, e.g. creating and patenting an invention, solving an open scientific problem, or identifying vulnerabilities in software. We study a binary model of crowdsearch in which agents have different abilities to find the object. We characterize the types of equilibria and identify which type of crowd maximizes the likelihood of finding the object. Sometimes, however, an unlimited crowd is not sufficient to guarantee that the object is found. It even can happen that inviting more agents lowers the probability of finding the object. We characterize the optimal prize and show that offering only one prize (winner-takes-all) maximizes the probability of finding the object but is not necessarily optimal for the crowdsearch designer."}
{"title":"Artificial intelligence and the skill premium","authors":["David E. Bloom","Klaus Prettner","Jamel Saadaoui","Mario Veruete"],"raw_abstract":"What will likely be the effect of the emergence of ChatGPT and other forms of\nartificial intelligence (AI) on the skill premium? To address this question, we\ndevelop a nested constant elasticity of substitution production function that\ndistinguishes between industrial robots and AI. Industrial robots predominantly\nsubstitute for low-skill workers, whereas AI mainly helps to perform the tasks\nof high-skill workers. We show that AI reduces the skill premium as long as it\nis more substitutable for high-skill workers than low-skill workers are for\nhigh-skill workers.","publication_date":1699993015,"paper_link":"http://arxiv.org/pdf/2311.09255v1","categories":["Economics"],"abstract":"What will likely be the effect of the emergence of ChatGPT and other forms of artificial intelligence (AI) on the skill premium? To address this question, we develop a nested constant elasticity of substitution production function that distinguishes between industrial robots and AI. Industrial robots predominantly substitute for low-skill workers, whereas AI mainly helps to perform the tasks of high-skill workers. We show that AI reduces the skill premium as long as it is more substitutable for high-skill workers than low-skill workers are for high-skill workers."}
{"title":"Incompleteness, Independence, and Negative Dominance","authors":["Harvey Lederman"],"raw_abstract":"This paper introduces the axiom of Negative Dominance, stating that if a\nlottery $f$ is strictly preferred to a lottery $g$, then some outcome in the\nsupport of $f$ is strictly preferred to some outcome in the support of $g$. It\nis shown that if preferences are incomplete on a sufficiently rich domain, then\nthis plausible axiom, which holds for complete preferences, is incompatible\nwith an array of otherwise plausible axioms for choice under uncertainty. In\nparticular, in this setting, Negative Dominance conflicts with the standard\nIndependence axiom. A novel theory, which includes Negative Dominance, and\nrejects Independence, is developed and shown to be consistent.","publication_date":1699988517,"paper_link":"http://arxiv.org/pdf/2311.08471v1","categories":["Economics"],"abstract":"This paper introduces the axiom of Negative Dominance, stating that if a lottery __FORMULA__ is strictly preferred to a lottery __FORMULA__, then some outcome in the support of __FORMULA__ is strictly preferred to some outcome in the support of __FORMULA__. It is shown that if preferences are incomplete on a sufficiently rich domain, then this plausible axiom, which holds for complete preferences, is incompatible with an array of otherwise plausible axioms for choice under uncertainty. In particular, in this setting, Negative Dominance conflicts with the standard Independence axiom. A novel theory, which includes Negative Dominance, and rejects Independence, is developed and shown to be consistent."}
{"title":"Consensus and Disagreement: Information Aggregation under (not so) Naive Learning","authors":["Abhijit Banerjee","Olivier Compte"],"raw_abstract":"We explore a model of non-Bayesian information aggregation in networks.\nAgents non-cooperatively choose among Friedkin-Johnsen type aggregation rules\nto maximize payoffs. The DeGroot rule is chosen in equilibrium if and only if\nthere is noiseless information transmission, leading to consensus. With noisy\ntransmission, while some disagreement is inevitable, the optimal choice of rule\namplifies the disagreement: even with little noise, individuals place\nsubstantial weight on their own initial opinion in every period, exacerbating\nthe disagreement. We use this framework to think about equilibrium versus\nsocially efficient choice of rules and its connection to polarization of\nopinions across groups.","publication_date":1699977143,"paper_link":"http://arxiv.org/pdf/2311.08256v1","categories":["Economics","Quantitative Finance"],"abstract":"We explore a model of non-Bayesian information aggregation in networks. Agents non-cooperatively choose among Friedkin-Johnsen type aggregation rules to maximize payoffs. The DeGroot rule is chosen in equilibrium if and only if there is noiseless information transmission, leading to consensus. With noisy transmission, while some disagreement is inevitable, the optimal choice of rule amplifies the disagreement: even with little noise, individuals place substantial weight on their own initial opinion in every period, exacerbating the disagreement. We use this framework to think about equilibrium versus socially efficient choice of rules and its connection to polarization of opinions across groups."}
{"title":"Audit fees in auditor switching","authors":["Sarit Agami"],"raw_abstract":"The auditor work is examining that a company's financial statements\nfaithfully reflect its financial situation. His wage, the audit fees, are not\nfixed among all companies, but can be affected by the financial and structural\ncharacteristics of the company, as well as the characteristics of the firm he\nbelongs to. Another factor that may affect his wage in an auditor switching,\nwhich can be resulted from changes in the company that may influence the fees.\nThis paper examines the effect nature of the auditor switching on his wage, and\nthe factors of the company characteristics and the economy data which determine\nthe wage at switching. A product of the research are tools for predicting and\nevaluating the auditor wage at switching. These tools are important for the\nauditor himself, but also for the company manager to correctly determine the\nwage due to the possibility that the quality of the audit work depends on its\nfees. Two main results are obtained. First, the direction of the wage change in\nthe switching year depends on the economic stability of the economy. Second,\nthe switching effect on the direction and the change size in wage depends on\nthe change size in the company characteristics before and after switching - a\nlarge change versus a stable one. We get that forecasting the change size in\nwage for companies with a larger change is their characteristics is paralleled\nto forecasting a wage increasing. And vice versa, forecasting the change size\nin wage for companies with a stable change in their characteristics is\nparalleled to forecasting a wage decreasing. But, whereas the former can be\nachieved based on the company characteristics and macroeconomics factors, the\npredictably of these characteristics and factors is negligible for the letter.","publication_date":1699976522,"paper_link":"http://arxiv.org/pdf/2311.08250v1","categories":["Quantitative Finance"],"abstract":"The auditor work is examining that a company's financial statements faithfully reflect its financial situation. His wage, the audit fees, are not fixed among all companies, but can be affected by the financial and structural characteristics of the company, as well as the characteristics of the firm he belongs to. Another factor that may affect his wage in an auditor switching, which can be resulted from changes in the company that may influence the fees. This paper examines the effect nature of the auditor switching on his wage, and the factors of the company characteristics and the economy data which determine the wage at switching. A product of the research are tools for predicting and evaluating the auditor wage at switching. These tools are important for the auditor himself, but also for the company manager to correctly determine the wage due to the possibility that the quality of the audit work depends on its fees. Two main results are obtained. First, the direction of the wage change in the switching year depends on the economic stability of the economy. Second, the switching effect on the direction and the change size in wage depends on the change size in the company characteristics before and after switching - a large change versus a stable one. We get that forecasting the change size in wage for companies with a larger change is their characteristics is paralleled to forecasting a wage increasing. And vice versa, forecasting the change size in wage for companies with a stable change in their characteristics is paralleled to forecasting a wage decreasing. But, whereas the former can be achieved based on the company characteristics and macroeconomics factors, the predictably of these characteristics and factors is negligible for the letter."}
{"title":"Frequentist Guarantees of Distributed (Non)-Bayesian Inference","authors":["Bohan Wu","C\u00e9sar A. Uribe"],"raw_abstract":"Motivated by the need to analyze large, decentralized datasets, distributed\nBayesian inference has become a critical research area across multiple fields,\nincluding statistics, electrical engineering, and economics. This paper\nestablishes Frequentist properties, such as posterior consistency, asymptotic\nnormality, and posterior contraction rates, for the distributed (non-)Bayes\nInference problem among agents connected via a communication network. Our\nresults show that, under appropriate assumptions on the communication graph,\ndistributed Bayesian inference retains parametric efficiency while enhancing\nrobustness in uncertainty quantification. We also explore the trade-off between\nstatistical efficiency and communication efficiency by examining how the design\nand size of the communication graph impact the posterior contraction rate.\nFurthermore, We extend our analysis to time-varying graphs and apply our\nresults to exponential family models, distributed logistic regression, and\ndecentralized detection models.","publication_date":1699973446,"paper_link":"http://arxiv.org/pdf/2311.08214v1","categories":["Mathematics","Statistics"],"abstract":"Motivated by the need to analyze large, decentralized datasets, distributed Bayesian inference has become a critical research area across multiple fields, including statistics, electrical engineering, and economics. This paper establishes Frequentist properties, such as posterior consistency, asymptotic normality, and posterior contraction rates, for the distributed (non-)Bayes Inference problem among agents connected via a communication network. Our results show that, under appropriate assumptions on the communication graph, distributed Bayesian inference retains parametric efficiency while enhancing robustness in uncertainty quantification. We also explore the trade-off between statistical efficiency and communication efficiency by examining how the design and size of the communication graph impact the posterior contraction rate. Furthermore, We extend our analysis to time-varying graphs and apply our results to exponential family models, distributed logistic regression, and decentralized detection models."}
{"title":"Smart Skin separation control using distributed-input distributed-output, multi-modal actuators, and machine learning","authors":["Songqi Li"],"raw_abstract":"Efficient flow separation control represents significant economic benefit.\nThis study applies a machine learning algorithm to minimize flow separation in\nSmart Skin, a flow control device that features distributed-input and\ndistributed-output (DIDO). Smart Skin comprises 30 hybrid actuator units, each\nintegrating a height-adjustable vortex generator and a mini-jet actuator. These\nunits are deployed on a backward-facing ramp to reduce flow separation in a\ndistributed manner. To monitor the flow state, distributed pressure taps are\ndeployed around the multi-modal actuators. Parametric studies indicate that the\nmapping between control parameters and separation control performance is\ncomplex. To optimize separation control, a cutting-edge variant of the particle\nswarm optimization (PSO-TPME) is used for the control parameters in the Smart\nSkin. This algorithm is capable of achieving fast optimization in\nhigh-dimensional parameter spaces. The results demonstrate the efficiency of\nPSO-TPME, and the optimized solution significantly outperforms the best result\nfrom the parametric study. These findings represent a promising future of\nmachine learning-based flow control using distributed actuators and sensors.","publication_date":1699964928,"paper_link":"http://arxiv.org/pdf/2311.08116v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"Efficient flow separation control represents significant economic benefit. This study applies a machine learning algorithm to minimize flow separation in Smart Skin, a flow control device that features distributed-input and distributed-output (DIDO). Smart Skin comprises 30 hybrid actuator units, each integrating a height-adjustable vortex generator and a mini-jet actuator. These units are deployed on a backward-facing ramp to reduce flow separation in a distributed manner. To monitor the flow state, distributed pressure taps are deployed around the multi-modal actuators. Parametric studies indicate that the mapping between control parameters and separation control performance is complex. To optimize separation control, a cutting-edge variant of the particle swarm optimization (PSO-TPME) is used for the control parameters in the Smart Skin. This algorithm is capable of achieving fast optimization in high-dimensional parameter spaces. The results demonstrate the efficiency of PSO-TPME, and the optimized solution significantly outperforms the best result from the parametric study. These findings represent a promising future of machine learning-based flow control using distributed actuators and sensors."}
{"title":"Analyze business context data in developing economies using quantum computing","authors":["Ammar Jamshed"],"raw_abstract":"Quantum computing is an advancing area of computing sciences and provides a\nnew base of development for many futuristic technologies discussions on how it\ncan help developing economies will further help developed economies in\ntechnology transfer and economic development initiatives related to Research\nand development within developing countries thus providing a new means of\nforeign direct investment(FDI) and business innovation for the majority of the\nglobe that lacks infrastructure economic resources required for growth in the\ntechnology landscape and cyberinfrastructure for growth in computing\napplications. Discussion of which areas of support quantum computing can help\nwill further assist developing economies in implementing it for growth\nopportunities for local systems and businesses.","publication_date":1699956940,"paper_link":"http://arxiv.org/pdf/2311.08048v1","categories":["Economics"],"abstract":"Quantum computing is an advancing area of computing sciences and provides a new base of development for many futuristic technologies discussions on how it can help developing economies will further help developed economies in technology transfer and economic development initiatives related to Research and development within developing countries thus providing a new means of foreign direct investment(FDI) and business innovation for the majority of the globe that lacks infrastructure economic resources required for growth in the technology landscape and cyberinfrastructure for growth in computing applications. Discussion of which areas of support quantum computing can help will further assist developing economies in implementing it for growth opportunities for local systems and businesses."}
{"title":"Surveying Wikipedians: a dataset of users and contributors' practices on Wikipedia in 8 languages","authors":["Caterina Cruciani","L\u00e9o Joubert","Nicolas Jullien","Laurent Mell","Sasha Piccione","Jeanne Vermeirsche"],"raw_abstract":"The dataset focuses on Wikipedia users and contains information about\ndemographic and socioeconomic characteristics of the respondents and their\nactivity on Wikipedia. The data was collected using a questionnaire available\nonline between June and July 2023. The link to the questionnaire was\ndistributed via a banner published in 8 languages on the Wikipedia page.\nFilling out the questionnaire was voluntary and not incentivised in any way.\nThe survey includes 200 questions about: what people were doing on Wikipedia\nbefore clicking the link to the questionnaire; how they use Wikipedia as\nreaders (``professional'' and ``personal'' uses); their opinion on the quality,\nthe thematic coverage, the importance of the encyclopaedia; the making of\nWikipedia (how they think it is made, if they have ever contributed and how);\ntheir social, sport, artistic and cultural activities, both online and offline;\ntheir socio-economic characteristics including political beliefs, and trust\npropensities. More than 200 000 people opened the questionnaire, 100 332\nstarted to answer, and constitute our dataset, and 10 576 finished it. Among\nother themes identified by future researchers, the dataset can be useful for\nadvancing the research regarding the features of readers vs contributors of\nonline commons, the relationship between trust, information, sources, and the\nuse made of this information.","publication_date":1699947567,"paper_link":"http://arxiv.org/pdf/2311.07964v1","categories":["Economics"],"abstract":"The dataset focuses on Wikipedia users and contains information about demographic and socioeconomic characteristics of the respondents and their activity on Wikipedia. The data was collected using a questionnaire available online between June and July 2023. The link to the questionnaire was distributed via a banner published in 8 languages on the Wikipedia page. Filling out the questionnaire was voluntary and not incentivised in any way. The survey includes 200 questions about: what people were doing on Wikipedia before clicking the link to the questionnaire; how they use Wikipedia as readers (``professional'' and ``personal'' uses); their opinion on the quality, the thematic coverage, the importance of the encyclopaedia; the making of Wikipedia (how they think it is made, if they have ever contributed and how); their social, sport, artistic and cultural activities, both online and offline; their socio-economic characteristics including political beliefs, and trust propensities. More than 200 000 people opened the questionnaire, 100 332 started to answer, and constitute our dataset, and 10 576 finished it. Among other themes identified by future researchers, the dataset can be useful for advancing the research regarding the features of readers vs contributors of online commons, the relationship between trust, information, sources, and the use made of this information."}
{"title":"Dynamic Incentives in Centralized Matching: The Case of Japanese Daycare","authors":["Kan Kuno"],"raw_abstract":"This study investigates the strategic behavior of applicants in the Japanese\ndaycare market, where waitlisted applicants are granted additional priority\npoints in subsequent application rounds. Utilizing data from Tokyo's Bunkyo\nmunicipality, this paper provides evidence of considerable manipulation, with\nparents strategically choosing to be waitlisted to enhance the likelihood of\ntheir child's admission into more selective daycare centers. I extend the\nstatic framework of school choice posited by Agarwal and Somaini (2018) to\nincorporate dynamic incentives and estimate a structural model that allows for\nreapplication if waitlisted. Empirical findings indicate that approximately 30%\nof applicants forgo listing safer options in their initial application, a\nbehavior significantly pronounced among those who stand to benefit from the\nwaitlist prioritization. Counterfactual simulations, conducted under the\nscenario of no additional waitlist priority, predict a 17.7% decrease in the\nnumber of waitlisted applicants and a 1.2% increase in overall welfare. These\nfindings highlight the profound influence of dynamic incentives on applicant\nbehavior and underscore the necessity for reevaluating current priority\nmechanisms.","publication_date":1699940281,"paper_link":"http://arxiv.org/pdf/2311.07920v1","categories":["Economics","Quantitative Finance"],"abstract":"This study investigates the strategic behavior of applicants in the Japanese daycare market, where waitlisted applicants are granted additional priority points in subsequent application rounds. Utilizing data from Tokyo's Bunkyo municipality, this paper provides evidence of considerable manipulation, with parents strategically choosing to be waitlisted to enhance the likelihood of their child's admission into more selective daycare centers. I extend the static framework of school choice posited by Agarwal and Somaini (2018) to incorporate dynamic incentives and estimate a structural model that allows for reapplication if waitlisted. Empirical findings indicate that approximately 30% of applicants forgo listing safer options in their initial application, a behavior significantly pronounced among those who stand to benefit from the waitlist prioritization. Counterfactual simulations, conducted under the scenario of no additional waitlist priority, predict a 17.7% decrease in the number of waitlisted applicants and a 1.2% increase in overall welfare. These findings highlight the profound influence of dynamic incentives on applicant behavior and underscore the necessity for reevaluating current priority mechanisms."}
{"title":"Considering Risk Aversion in Economic Evaluation: A Rank Dependent Approach","authors":["Jacob Smith"],"raw_abstract":"This paper presents a method for incorporating risk aversion into existing\ndecision tree models used in economic evaluations. The method involves applying\na probability weighting function based on rank dependent utility theory to\nreduced lotteries in the decision tree model. This adaptation embodies the fact\nthat different decision makers can observe the same decision tree model\nstructure but come to different conclusions about the optimal treatment. The\nproposed solution to this problem is to compensate risk-averse decision makers\nto use the efficient technology that they are reluctant to adopt.","publication_date":1699937689,"paper_link":"http://arxiv.org/pdf/2311.07905v1","categories":["Economics"],"abstract":"This paper presents a method for incorporating risk aversion into existing decision tree models used in economic evaluations. The method involves applying a probability weighting function based on rank dependent utility theory to reduced lotteries in the decision tree model. This adaptation embodies the fact that different decision makers can observe the same decision tree model structure but come to different conclusions about the optimal treatment. The proposed solution to this problem is to compensate risk-averse decision makers to use the efficient technology that they are reluctant to adopt."}
{"title":"Cooperative AI via Decentralized Commitment Devices","authors":["Xinyuan Sun","Davide Crapis","Matt Stephenson","Barnab\u00e9 Monnot","Thomas Thiery","Jonathan Passerat-Palmbach"],"raw_abstract":"Credible commitment devices have been a popular approach for robust\nmulti-agent coordination. However, existing commitment mechanisms face\nlimitations like privacy, integrity, and susceptibility to mediator or user\nstrategic behavior. It is unclear if the cooperative AI techniques we study are\nrobust to real-world incentives and attack vectors. However, decentralized\ncommitment devices that utilize cryptography have been deployed in the wild,\nand numerous studies have shown their ability to coordinate algorithmic agents\nfacing adversarial opponents with significant economic incentives, currently in\nthe order of several million to billions of dollars. In this paper, we use\nexamples in the decentralization and, in particular, Maximal Extractable Value\n(MEV) (arXiv:1904.05234) literature to illustrate the potential security issues\nin cooperative AI. We call for expanded research into decentralized commitments\nto advance cooperative AI capabilities for secure coordination in open\nenvironments and empirical testing frameworks to evaluate multi-agent\ncoordination ability given real-world commitment constraints.","publication_date":1699921401,"paper_link":"http://arxiv.org/pdf/2311.07815v1","categories":["Economics"],"abstract":"Credible commitment devices have been a popular approach for robust multi-agent coordination. However, existing commitment mechanisms face limitations like privacy, integrity, and susceptibility to mediator or user strategic behavior. It is unclear if the cooperative AI techniques we study are robust to real-world incentives and attack vectors. However, decentralized commitment devices that utilize cryptography have been deployed in the wild, and numerous studies have shown their ability to coordinate algorithmic agents facing adversarial opponents with significant economic incentives, currently in the order of several million to billions of dollars. In this paper, we use examples in the decentralization and, in particular, Maximal Extractable Value (MEV) (arXiv:1904.05234) literature to illustrate the potential security issues in cooperative AI. We call for expanded research into decentralized commitments to advance cooperative AI capabilities for secure coordination in open environments and empirical testing frameworks to evaluate multi-agent coordination ability given real-world commitment constraints."}
{"title":"A Primal-Dual Analysis of Monotone Submodular Maximization","authors":["Deeparnab Chakrabarty","Luc Cote"],"raw_abstract":"In this paper we design a new primal-dual algorithm for the classic discrete\noptimization problem of maximizing a monotone submodular function subject to a\ncardinality constraint achieving the optimal approximation of $(1-1/e)$. This\nproblem and its special case, the maximum $k$-coverage problem, have a wide\nrange of applications in various fields including operations research, machine\nlearning, and economics. While greedy algorithms have been known to achieve\nthis approximation factor, our algorithms also provide a dual certificate which\nupper bounds the optimum value of any instance. This certificate may be used in\npractice to certify much stronger guarantees than the worst-case $(1-1/e)$\napproximation factor.","publication_date":1699919275,"paper_link":"http://arxiv.org/pdf/2311.07808v1","categories":["Economics"],"abstract":"In this paper we design a new primal-dual algorithm for the classic discrete optimization problem of maximizing a monotone submodular function subject to a cardinality constraint achieving the optimal approximation of __FORMULA__. This problem and its special case, the maximum __FORMULA__-coverage problem, have a wide range of applications in various fields including operations research, machine learning, and economics. While greedy algorithms have been known to achieve this approximation factor, our algorithms also provide a dual certificate which upper bounds the optimum value of any instance. This certificate may be used in practice to certify much stronger guarantees than the worst-case __FORMULA__ approximation factor."}
{"title":"Efficient Prior-Free Mechanisms for No-Regret Agents","authors":["Natalie Collina","Aaron Roth","Han Shao"],"raw_abstract":"We study a repeated Principal Agent problem between a long lived Principal\nand Agent pair in a prior free setting. In our setting, the sequence of\nrealized states of nature may be adversarially chosen, the Agent is non-myopic,\nand the Principal aims for a strong form of policy regret. Following Camara,\nHartline, and Johnson, we model the Agent's long-run behavior with behavioral\nassumptions that relax the common prior assumption (for example, that the Agent\nhas no swap regret). Within this framework, we revisit the mechanism proposed\nby Camara et al., which informally uses calibrated forecasts of the unknown\nstates of nature in place of a common prior. We give two main improvements.\nFirst, we give a mechanism that has an exponentially improved dependence (in\nterms of both running time and regret bounds) on the number of distinct states\nof nature. To do this, we show that our mechanism does not require truly\ncalibrated forecasts, but rather forecasts that are unbiased subject to only a\npolynomially sized collection of events -- which can be produced with\npolynomial overhead. Second, in several important special cases -- including\nthe focal linear contracting setting -- we show how to remove strong\n``Alignment'' assumptions (which informally require that near-ties are always\nbroken in favor of the Principal) by specifically deploying ``stable'' policies\nthat do not have any near ties that are payoff relevant to the Principal. Taken\ntogether, our new mechanism makes the compelling framework proposed by Camara\net al. much more powerful, now able to be realized over polynomially sized\nstate spaces, and while requiring only mild assumptions on Agent behavior.","publication_date":1699910022,"paper_link":"http://arxiv.org/pdf/2311.07754v1","categories":["Economics"],"abstract":"We study a repeated Principal Agent problem between a long lived Principal and Agent pair in a prior free setting. In our setting, the sequence of realized states of nature may be adversarially chosen, the Agent is non-myopic, and the Principal aims for a strong form of policy regret. Following Camara, Hartline, and Johnson, we model the Agent's long-run behavior with behavioral assumptions that relax the common prior assumption (for example, that the Agent has no swap regret). Within this framework, we revisit the mechanism proposed by Camara et al., which informally uses calibrated forecasts of the unknown states of nature in place of a common prior. We give two main improvements. First, we give a mechanism that has an exponentially improved dependence (in terms of both running time and regret bounds) on the number of distinct states of nature. To do this, we show that our mechanism does not require truly calibrated forecasts, but rather forecasts that are unbiased subject to only a polynomially sized collection of events -- which can be produced with polynomial overhead. Second, in several important special cases -- including the focal linear contracting setting -- we show how to remove strong ``Alignment'' assumptions (which informally require that near-ties are always broken in favor of the Principal) by specifically deploying ``stable'' policies that do not have any near ties that are payoff relevant to the Principal. Taken together, our new mechanism makes the compelling framework proposed by Camara et al. much more powerful, now able to be realized over polynomially sized state spaces, and while requiring only mild assumptions on Agent behavior."}
{"title":"Assessing the potential impact of environmental land management schemes on emergent infection disease risks","authors":["Christopher J. Banks","Katherine Simpson","Nicholas Hanley","Rowland R. Kao"],"raw_abstract":"Financial incentives are provided by governments to encourage the plantation\nof new woodland to increase habitat, biodiversity, carbon sequestration, and\nother economic benefits for landowners. Whilst these are largely positive\neffects, it is worth considering that greater biodiversity and presence of\nwildlife species in proximity to agricultural holdings may pose a risk of\ndisease transmission between wildlife and livestock. Wildlife transmission and\nthe provision of a reservoir for infectious disease is particularly important\nin the transmission dynamics of bovine tuberculosis.\n  In this paper we develop an economic model for changing land use due to\nforestry subsidies. We use this asses the impact on wild deer populations in\nthe newly created woodland areas and the emergent infectious disease risk\narising from the proximity of new and existing wild deer populations and\nexisting cattle holdings.\n  We consider an area in the South-West of Scotland, having existing woodland,\ndeer populations, and extensive and diverse cattle farm holdings. In this area\nwe find that, with a varying level of subsidy and plausible new woodland\ncreation, the contact risk between areas of wild deer and cattle increases\nbetween 26% and 35% over the contact risk present with zero subsidy.\n  This model provides a foundation for extending to larger regions and for\nexamining potential risk mitigation strategies, for example the targeting of\nsubsidy in low risk areas or provisioning for buffer zones between woodland and\nagricultural holdings.","publication_date":1699908010,"paper_link":"http://arxiv.org/pdf/2311.07735v1","categories":["Quantitative Biology","Economics","Quantitative Finance"],"abstract":"Financial incentives are provided by governments to encourage the plantation of new woodland to increase habitat, biodiversity, carbon sequestration, and other economic benefits for landowners. Whilst these are largely positive effects, it is worth considering that greater biodiversity and presence of wildlife species in proximity to agricultural holdings may pose a risk of disease transmission between wildlife and livestock. Wildlife transmission and the provision of a reservoir for infectious disease is particularly important in the transmission dynamics of bovine tuberculosis.   In this paper we develop an economic model for changing land use due to forestry subsidies. We use this asses the impact on wild deer populations in the newly created woodland areas and the emergent infectious disease risk arising from the proximity of new and existing wild deer populations and existing cattle holdings.   We consider an area in the South-West of Scotland, having existing woodland, deer populations, and extensive and diverse cattle farm holdings. In this area we find that, with a varying level of subsidy and plausible new woodland creation, the contact risk between areas of wild deer and cattle increases between 26% and 35% over the contact risk present with zero subsidy.   This model provides a foundation for extending to larger regions and for examining potential risk mitigation strategies, for example the targeting of subsidy in low risk areas or provisioning for buffer zones between woodland and agricultural holdings."}
{"title":"On Bounding and Approximating Functions of Multiple Expectations using Quasi-Monte Carlo","authors":["Aleksei G. Sorokin","Jagadeeswaran Rathinavel"],"raw_abstract":"Monte Carlo and Quasi-Monte Carlo methods present a convenient approach for\napproximating the expected value of a random variable. Algorithms exist to\nadaptively sample the random variable until a user defined absolute error\ntolerance is satisfied with high probability. This work describes an extension\nof such methods which supports adaptive sampling to satisfy general error\ncriteria for functions of a common array of expectations. Although several\nfunctions involving multiple expectations are being evaluated, only one random\nsequence is required, albeit sometimes of larger dimension than the underlying\nrandomness. These enhanced Monte Carlo and Quasi-Monte Carlo algorithms are\nimplemented in the QMCPy Python package with support for economic and parallel\nfunction evaluation. We exemplify these capabilities on problems from machine\nlearning and global sensitivity analysis.","publication_date":1699901343,"paper_link":"http://arxiv.org/pdf/2311.07555v1","categories":["Mathematics"],"abstract":"Monte Carlo and Quasi-Monte Carlo methods present a convenient approach for approximating the expected value of a random variable. Algorithms exist to adaptively sample the random variable until a user defined absolute error tolerance is satisfied with high probability. This work describes an extension of such methods which supports adaptive sampling to satisfy general error criteria for functions of a common array of expectations. Although several functions involving multiple expectations are being evaluated, only one random sequence is required, albeit sometimes of larger dimension than the underlying randomness. These enhanced Monte Carlo and Quasi-Monte Carlo algorithms are implemented in the QMCPy Python package with support for economic and parallel function evaluation. We exemplify these capabilities on problems from machine learning and global sensitivity analysis."}
{"title":"The Last Decade in Review: Tracing the Evolution of Safety Assurance Cases through a Comprehensive Bibliometric Analysis","authors":["Mithila Sivakumar","Alvine Boaye Belle","Jinjun Shan","Opeyemi Adesina","Song Wang","Marsha Chechik","Marios Fokaefs","Kimya Khakzad Shahandashti","Oluwafemi Odu"],"raw_abstract":"Safety assurance is of paramount importance across various domains, including\nautomotive, aerospace, and nuclear energy, where the reliability and\nacceptability of mission-critical systems are imperative. This assurance is\neffectively realized through the utilization of Safety Assurance Cases. The use\nof safety assurance cases allows for verifying the correctness of the created\nsystems capabilities, preventing system failure. The latter may result in loss\nof life, severe injuries, large-scale environmental damage, property\ndestruction, and major economic loss. Still, the emergence of complex\ntechnologies such as cyber-physical systems (CPSs), characterized by their\nheterogeneity, autonomy, machine learning capabilities, and the uncertainty of\ntheir operational environments poses significant challenges for safety\nassurance activities. Several papers have tried to propose solutions to tackle\nthese challenges, but to the best of our knowledge, no secondary study\ninvestigates the trends, patterns, and relationships characterizing the safety\ncase scientific literature. This makes it difficult to have a holistic view of\nthe safety case landscape and to identify the most promising future research\ndirections. In this paper, we, therefore, rely on state-of-the-art bibliometric\ntools(e.g., VosViewer) to conduct a bibliometric analysis that allows us to\ngenerate valuable insights, identify key authors and venues, and gain a birds\neye view of the current state of research in the safety assurance area. By\nrevealing knowledge gaps and highlighting potential avenues for future\nresearch, our analysis provides an essential foundation for researchers,\ncorporate safety analysts, and regulators seeking to embrace or enhance safety\npractices that align with their specific needs and objectives.","publication_date":1699896863,"paper_link":"http://arxiv.org/pdf/2311.07495v1","categories":["Economics"],"abstract":"Safety assurance is of paramount importance across various domains, including automotive, aerospace, and nuclear energy, where the reliability and acceptability of mission-critical systems are imperative. This assurance is effectively realized through the utilization of Safety Assurance Cases. The use of safety assurance cases allows for verifying the correctness of the created systems capabilities, preventing system failure. The latter may result in loss of life, severe injuries, large-scale environmental damage, property destruction, and major economic loss. Still, the emergence of complex technologies such as cyber-physical systems (CPSs), characterized by their heterogeneity, autonomy, machine learning capabilities, and the uncertainty of their operational environments poses significant challenges for safety assurance activities. Several papers have tried to propose solutions to tackle these challenges, but to the best of our knowledge, no secondary study investigates the trends, patterns, and relationships characterizing the safety case scientific literature. This makes it difficult to have a holistic view of the safety case landscape and to identify the most promising future research directions. In this paper, we, therefore, rely on state-of-the-art bibliometric tools(e.g., VosViewer) to conduct a bibliometric analysis that allows us to generate valuable insights, identify key authors and venues, and gain a birds eye view of the current state of research in the safety assurance area. By revealing knowledge gaps and highlighting potential avenues for future research, our analysis provides an essential foundation for researchers, corporate safety analysts, and regulators seeking to embrace or enhance safety practices that align with their specific needs and objectives."}
{"title":"Connecting the Dots: Graph Neural Network Powered Ensemble and Classification of Medical Images","authors":["Aryan Singh","Pepijn Van de Ven","Ciar\u00e1n Eising","Patrick Denny"],"raw_abstract":"Deep learning models have demonstrated remarkable results for various\ncomputer vision tasks, including the realm of medical imaging. However, their\napplication in the medical domain is limited due to the requirement for large\namounts of training data, which can be both challenging and expensive to\nobtain. To mitigate this, pre-trained models have been fine-tuned on\ndomain-specific data, but such an approach can suffer from inductive biases.\nFurthermore, deep learning models struggle to learn the relationship between\nspatially distant features and their importance, as convolution operations\ntreat all pixels equally. Pioneering a novel solution to this challenge, we\nemploy the Image Foresting Transform to optimally segment images into\nsuperpixels. These superpixels are subsequently transformed into\ngraph-structured data, enabling the proficient extraction of features and\nmodeling of relationships using Graph Neural Networks (GNNs). Our method\nharnesses an ensemble of three distinct GNN architectures to boost its\nrobustness. In our evaluations targeting pneumonia classification, our\nmethodology surpassed prevailing Deep Neural Networks (DNNs) in performance,\nall while drastically cutting down on the parameter count. This not only trims\ndown the expenses tied to data but also accelerates training and minimizes\nbias. Consequently, our proposition offers a sturdy, economically viable, and\nscalable strategy for medical image classification, significantly diminishing\ndependency on extensive training data sets.","publication_date":1699881654,"paper_link":"http://arxiv.org/pdf/2311.07321v1","categories":["Economics"],"abstract":"Deep learning models have demonstrated remarkable results for various computer vision tasks, including the realm of medical imaging. However, their application in the medical domain is limited due to the requirement for large amounts of training data, which can be both challenging and expensive to obtain. To mitigate this, pre-trained models have been fine-tuned on domain-specific data, but such an approach can suffer from inductive biases. Furthermore, deep learning models struggle to learn the relationship between spatially distant features and their importance, as convolution operations treat all pixels equally. Pioneering a novel solution to this challenge, we employ the Image Foresting Transform to optimally segment images into superpixels. These superpixels are subsequently transformed into graph-structured data, enabling the proficient extraction of features and modeling of relationships using Graph Neural Networks (GNNs). Our method harnesses an ensemble of three distinct GNN architectures to boost its robustness. In our evaluations targeting pneumonia classification, our methodology surpassed prevailing Deep Neural Networks (DNNs) in performance, all while drastically cutting down on the parameter count. This not only trims down the expenses tied to data but also accelerates training and minimizes bias. Consequently, our proposition offers a sturdy, economically viable, and scalable strategy for medical image classification, significantly diminishing dependency on extensive training data sets."}
{"title":"Decision-making under risk: when is utility maximization equivalent to risk minimization?","authors":["Francesco Ruscitti","Ram Sewak Dubey","Giorgio Laguzzi"],"raw_abstract":"Motivated by the analysis of a general optimal portfolio selection problem,\nwhich encompasses as special cases an optimal consumption and an optimal\ndebt-arrangement problem, we are concerned with the questions of how a\npersonality trait like risk-perception can be formalized and whether the two\nobjectives of utility-maximization and risk-minimization can be both achieved\nsimultaneously. We address these questions by developing an axiomatic\nfoundation of preferences for which utility-maximization is equivalent to\nminimizing a utility-based shortfall risk measure. Our axiomatization hinges on\na novel axiom in decision theory, namely the risk-perception axiom.","publication_date":1699877301,"paper_link":"http://arxiv.org/pdf/2311.07269v1","categories":["Economics"],"abstract":"Motivated by the analysis of a general optimal portfolio selection problem, which encompasses as special cases an optimal consumption and an optimal debt-arrangement problem, we are concerned with the questions of how a personality trait like risk-perception can be formalized and whether the two objectives of utility-maximization and risk-minimization can be both achieved simultaneously. We address these questions by developing an axiomatic foundation of preferences for which utility-maximization is equivalent to minimizing a utility-based shortfall risk measure. Our axiomatization hinges on a novel axiom in decision theory, namely the risk-perception axiom."}
{"title":"Optimal Estimation of Large-Dimensional Nonlinear Factor Models","authors":["Yingjie Feng"],"raw_abstract":"This paper studies optimal estimation of large-dimensional nonlinear factor\nmodels. The key challenge is that the observed variables are possibly nonlinear\nfunctions of some latent variables where the functional forms are left\nunspecified. A local principal component analysis method is proposed to\nestimate the factor structure and recover information on latent variables and\nlatent functions, which combines $K$-nearest neighbors matching and principal\ncomponent analysis. Large-sample properties are established, including a sharp\nbound on the matching discrepancy of nearest neighbors, sup-norm error bounds\nfor estimated local factors and factor loadings, and the uniform convergence\nrate of the factor structure estimator. Under mild conditions our estimator of\nthe latent factor structure can achieve the optimal rate of uniform convergence\nfor nonparametric regression. The method is illustrated with a Monte Carlo\nexperiment and an empirical application studying the effect of tax cuts on\neconomic growth.","publication_date":1699874700,"paper_link":"http://arxiv.org/pdf/2311.07243v1","categories":["Mathematics","Economics","Statistics"],"abstract":"This paper studies optimal estimation of large-dimensional nonlinear factor models. The key challenge is that the observed variables are possibly nonlinear functions of some latent variables where the functional forms are left unspecified. A local principal component analysis method is proposed to estimate the factor structure and recover information on latent variables and latent functions, which combines __FORMULA__-nearest neighbors matching and principal component analysis. Large-sample properties are established, including a sharp bound on the matching discrepancy of nearest neighbors, sup-norm error bounds for estimated local factors and factor loadings, and the uniform convergence rate of the factor structure estimator. Under mild conditions our estimator of the latent factor structure can achieve the optimal rate of uniform convergence for nonparametric regression. The method is illustrated with a Monte Carlo experiment and an empirical application studying the effect of tax cuts on economic growth."}
{"title":"The Impact of Generative Artificial Intelligence","authors":["Kaichen Zhang","Ohchan Kwon","Hui Xiong"],"raw_abstract":"The rise of generative artificial intelligence (AI) has sparked concerns\nabout its potential influence on unemployment and market depression. This study\naddresses this concern by examining the impact of generative AI on product\nmarkets. To overcome the challenge of causal inference, given the inherent\nlimitations of conducting controlled experiments, this paper identifies an\nunanticipated and sudden leak of a highly proficient image-generative AI as a\nnovel instance of a \"natural experiment\". This AI leak spread rapidly,\nsignificantly reducing the cost of generating anime-style images compared to\nother styles, creating an opportunity for comparative assessment. We collect\nreal-world data from an artwork outsourcing platform. Surprisingly, our results\nshow that while generative AI lowers average prices, it substantially boosts\norder volume and overall revenue. This counterintuitive finding suggests that\ngenerative AI confers benefits upon artists rather than detriments. The study\nfurther offers theoretical economic explanations to elucidate this unexpected\nphenomenon. By furnishing empirical evidence, this paper dispels the notion\nthat generative AI might engender depression, instead underscoring its\npotential to foster market prosperity. These findings carry significant\nimplications for practitioners, policymakers, and the broader AI community.","publication_date":1699849913,"paper_link":"http://arxiv.org/pdf/2311.07071v1","categories":["Economics","Quantitative Finance"],"abstract":"The rise of generative artificial intelligence (AI) has sparked concerns about its potential influence on unemployment and market depression. This study addresses this concern by examining the impact of generative AI on product markets. To overcome the challenge of causal inference, given the inherent limitations of conducting controlled experiments, this paper identifies an unanticipated and sudden leak of a highly proficient image-generative AI as a novel instance of a \"natural experiment\". This AI leak spread rapidly, significantly reducing the cost of generating anime-style images compared to other styles, creating an opportunity for comparative assessment. We collect real-world data from an artwork outsourcing platform. Surprisingly, our results show that while generative AI lowers average prices, it substantially boosts order volume and overall revenue. This counterintuitive finding suggests that generative AI confers benefits upon artists rather than detriments. The study further offers theoretical economic explanations to elucidate this unexpected phenomenon. By furnishing empirical evidence, this paper dispels the notion that generative AI might engender depression, instead underscoring its potential to foster market prosperity. These findings carry significant implications for practitioners, policymakers, and the broader AI community."}
{"title":"From Authority-Respect to Grassroots-Dissent: Degree-Weighted Social Learning and Convergence Speed","authors":["Chen Cheng","Xiao Han","Xin Tong","Yusheng Wu","Yiqing Xing"],"raw_abstract":"Opinions are influenced by neighbors, with varying degrees of emphasis based\non their connections. Some may value more connected neighbors' views due to\nauthority respect, while others might lean towards grassroots perspectives. The\nemergence of ChatGPT could signify a new ``opinion leader'' whose views people\nput a lot of weight on. This study introduces a degree-weighted DeGroot\nlearning model to examine the effects of such belief updates on learning\noutcomes, especially the speed of belief convergence. We find that greater\nrespect for authority doesn't guarantee faster convergence. The influence of\nauthority respect is non-monotonic. The convergence speed, influenced by\nincreased authority-respect or grassroots dissent, hinges on the unity of elite\nand grassroots factions. This research sheds light on the growing skepticism\ntowards public figures and the ensuing dissonance in public debate.","publication_date":1699839845,"paper_link":"http://arxiv.org/pdf/2311.07010v1","categories":["Economics"],"abstract":"Opinions are influenced by neighbors, with varying degrees of emphasis based on their connections. Some may value more connected neighbors' views due to authority respect, while others might lean towards grassroots perspectives. The emergence of ChatGPT could signify a new ``opinion leader'' whose views people put a lot of weight on. This study introduces a degree-weighted DeGroot learning model to examine the effects of such belief updates on learning outcomes, especially the speed of belief convergence. We find that greater respect for authority doesn't guarantee faster convergence. The influence of authority respect is non-monotonic. The convergence speed, influenced by increased authority-respect or grassroots dissent, hinges on the unity of elite and grassroots factions. This research sheds light on the growing skepticism towards public figures and the ensuing dissonance in public debate."}
{"title":"Centralised or Decentralised? Data Analysis of Transaction Network of Hedera Hashgraph","authors":["Lucas Amherd","Sheng-Nan Li","Claudio J. Tessone"],"raw_abstract":"An important virtue of distributed ledger technologies is their acclaimed\nhigher level of decentralisation compared to traditional financial systems.\nEmpirical literature, however, suggests that many systems tend towards\ncentralisation as well. This study expands the current literature by offering a\nfirst-time, data-driven analysis of the degree of decentralisation of the\nplatform Hedera Hashgraph, a public permissioned distributed ledger technology,\nemploying data directly fetched from a network node. The results show a\nconsiderably higher amount of released supply compared to the release schedule\nand a growing number of daily active accounts. Also, Hedera Hashgraph exhibits\na high centralisation of wealth and a shrinking core that acts as an\nintermediary in transactions for the rest of the network. However, the Nakamoto\nindex and Theil index point to recent progress towards a more decentralised\nnetwork.","publication_date":1699800752,"paper_link":"http://arxiv.org/pdf/2311.06865v1","categories":["Economics","Quantitative Finance"],"abstract":"An important virtue of distributed ledger technologies is their acclaimed higher level of decentralisation compared to traditional financial systems. Empirical literature, however, suggests that many systems tend towards centralisation as well. This study expands the current literature by offering a first-time, data-driven analysis of the degree of decentralisation of the platform Hedera Hashgraph, a public permissioned distributed ledger technology, employing data directly fetched from a network node. The results show a considerably higher amount of released supply compared to the release schedule and a growing number of daily active accounts. Also, Hedera Hashgraph exhibits a high centralisation of wealth and a shrinking core that acts as an intermediary in transactions for the rest of the network. However, the Nakamoto index and Theil index point to recent progress towards a more decentralised network."}
{"title":"Quasi-Bayes in Latent Variable Models","authors":["Sid Kankanala"],"raw_abstract":"Latent variable models are widely used to account for unobserved determinants\nof economic behavior. Traditional nonparametric methods to estimate latent\nheterogeneity do not scale well into multidimensional settings. Distributional\nrestrictions alleviate tractability concerns but may impart non-trivial\nmisspecification bias. Motivated by these concerns, this paper introduces a\nquasi-Bayes approach to estimate a large class of multidimensional latent\nvariable models. Our approach to quasi-Bayes is novel in that we center it\naround relating the characteristic function of observables to the distribution\nof unobservables. We propose a computationally attractive class of priors that\nare supported on Gaussian mixtures and derive contraction rates for a variety\nof latent variable models.","publication_date":1699794422,"paper_link":"http://arxiv.org/pdf/2311.06831v1","categories":["Economics","Statistics"],"abstract":"Latent variable models are widely used to account for unobserved determinants of economic behavior. Traditional nonparametric methods to estimate latent heterogeneity do not scale well into multidimensional settings. Distributional restrictions alleviate tractability concerns but may impart non-trivial misspecification bias. Motivated by these concerns, this paper introduces a quasi-Bayes approach to estimate a large class of multidimensional latent variable models. Our approach to quasi-Bayes is novel in that we center it around relating the characteristic function of observables to the distribution of unobservables. We propose a computationally attractive class of priors that are supported on Gaussian mixtures and derive contraction rates for a variety of latent variable models."}
{"title":"A non-invariance result for the spatial AK model","authors":["Cristiano Ricci"],"raw_abstract":"This paper deals with the positivity condition of an infinite-dimensional\nevolutionary equation, associated with a control problem for the optimal\nconsumption over space. We consider a spatial growth model for capital, with\nproduction generating endogenous growth and technology of the form AK. We show\nthat for certain initial data, even in the case of heterogeneous spatial\ndistribution of technology and population, the solution to an auxiliary control\nproblem that is commonly used as a candidate for the original problem is not\nadmissible. In particular, we show that initial conditions that are\nnon-negative, under the auxiliary optimal consumption strategy, may lead to\nnegative capital allocations over time.","publication_date":1699787994,"paper_link":"http://arxiv.org/pdf/2311.06811v1","categories":["Economics"],"abstract":"This paper deals with the positivity condition of an infinite-dimensional evolutionary equation, associated with a control problem for the optimal consumption over space. We consider a spatial growth model for capital, with production generating endogenous growth and technology of the form AK. We show that for certain initial data, even in the case of heterogeneous spatial distribution of technology and population, the solution to an auxiliary control problem that is commonly used as a candidate for the original problem is not admissible. In particular, we show that initial conditions that are non-negative, under the auxiliary optimal consumption strategy, may lead to negative capital allocations over time."}
{"title":"A Strategyproof Mechanism for Ownership Restructuring in Privately Owned Assets","authors":["Gal Danino","Moran Koren","Omer Madmon"],"raw_abstract":"It is unclear how to restructure ownership when an asset is privately held,\nand there is uncertainty about the owners' subjective valuations. When\nownership is divided equally between two owners, a commonly used mechanism is\ncalled a BMBY mechanism. This mechanism works as follows: each owner can\ninitiate a BMBY by naming her price. Once an owner declares a price, the other\nchooses to sell his holdings or buy the shares of the initiator at the given\nprice. This mechanism is simple and tractable; however, it does not elicit\nactual owner valuations, does not guarantee an efficient allocation, and, most\nimportantly, is limited to an equal partnership of two owners. In this paper,\nwe extend this rationale to a multi-owner setting. Our proposed mechanism\nelicits owner valuations truthfully. Additionally, our proposed mechanism\nexhibits several desirable traits: it is easy to implement, budget balanced,\nrobust to collusion (weakly group strategyproof), individually rational, and\nex-post efficient.","publication_date":1699779407,"paper_link":"http://arxiv.org/pdf/2311.06780v1","categories":["Economics"],"abstract":"It is unclear how to restructure ownership when an asset is privately held, and there is uncertainty about the owners' subjective valuations. When ownership is divided equally between two owners, a commonly used mechanism is called a BMBY mechanism. This mechanism works as follows: each owner can initiate a BMBY by naming her price. Once an owner declares a price, the other chooses to sell his holdings or buy the shares of the initiator at the given price. This mechanism is simple and tractable; however, it does not elicit actual owner valuations, does not guarantee an efficient allocation, and, most importantly, is limited to an equal partnership of two owners. In this paper, we extend this rationale to a multi-owner setting. Our proposed mechanism elicits owner valuations truthfully. Additionally, our proposed mechanism exhibits several desirable traits: it is easy to implement, budget balanced, robust to collusion (weakly group strategyproof), individually rational, and ex-post efficient."}
{"title":"Aggregation and Closed-Form Results for Nonhomothetic CES Preferences","authors":["Clement E. Bohr","Mart\u00ed Mestieri","Emre Enes Yavuz"],"raw_abstract":"We provide four novel results for nonhomothetic Constant Elasticity of\nSubstitution preferences (Hanoch, 1975). First, we derive a closed-form\nrepresentation of the expenditure function of nonhomothetic CES under\nrelatively flexible distributional assumptions of demand and price distribution\nparameters. Second, we characterize aggregate demand from heterogeneous\nhouseholds in closed-form, assuming that household total expenditures follow an\nempirically plausible distribution. Third, we leverage these results to study\nthe Euler equation arising from standard intertemporal consumption-saving\nproblems featuring within-period nonhomothetic CES preferences. Finally, we\nshow that nonhomothetic CES expenditure shares arise as the solution of a\ndiscrete-choice logit problem.","publication_date":1699767054,"paper_link":"http://arxiv.org/pdf/2311.06740v1","categories":["Economics","Quantitative Finance"],"abstract":"We provide four novel results for nonhomothetic Constant Elasticity of Substitution preferences (Hanoch, 1975). First, we derive a closed-form representation of the expenditure function of nonhomothetic CES under relatively flexible distributional assumptions of demand and price distribution parameters. Second, we characterize aggregate demand from heterogeneous households in closed-form, assuming that household total expenditures follow an empirically plausible distribution. Third, we leverage these results to study the Euler equation arising from standard intertemporal consumption-saving problems featuring within-period nonhomothetic CES preferences. Finally, we show that nonhomothetic CES expenditure shares arise as the solution of a discrete-choice logit problem."}
{"title":"Sustainable Development Goal (SDG) 8: New Zealand Prospects while Yield Curve Inverts in Central Bank Digital Currency (CBDC) Era","authors":["Qionghua Chu"],"raw_abstract":"In the inverted yield curve environment, I intend to assess the feasibility\nfor New Zealand to fulfill SDG 8, decent work and economic growth, of the\nUnited Nations by 2030. CBDC issuance supports SDG 8, based on Cobb-Douglas\nproduction function, growth accounting relation for Solow, and the Theory of\nAggregate Demand for Keynes. Bright prospects exist for New Zealand.","publication_date":1699758933,"paper_link":"http://arxiv.org/pdf/2311.06718v2","categories":["Economics","Quantitative Finance"],"abstract":"In the inverted yield curve environment, I intend to assess the feasibility for New Zealand to fulfill SDG 8, decent work and economic growth, of the United Nations by 2030. CBDC issuance supports SDG 8, based on Cobb-Douglas production function, growth accounting relation for Solow, and the Theory of Aggregate Demand for Keynes. Bright prospects exist for New Zealand."}
{"title":"Sustainable Development Goals (SDGs): New Zealand Outlook with Central Bank Digital Currency (CBDC) and SDG 8 Realization on the Horizon","authors":["Qionghua Chu"],"raw_abstract":"While the potential issuance of CBDC might enable SDG 8 to be realized in New\nZealand, mutual interactions between SDG 8 and other goals attract interesting\nconsiderations of if both SDG 8 and other goals could be achieved. SDGs are\ngrouped in terms of the common characteristics that will enable them to be\nimpacted by and impact SDG 8 similarly. Besides, other SDGs could mutually\nenable each other to be realized. With comprehensive research and analysis of\nthe mutual interactions, a positive outlook exists for both SDG 8 and other\ngoals to be fulfilled, in consideration of the potential issuance of the CBDC\nas a primary stimulus to achieve decent work and economic growth.","publication_date":1699758771,"paper_link":"http://arxiv.org/pdf/2311.06716v2","categories":["Economics","Quantitative Finance"],"abstract":"While the potential issuance of CBDC might enable SDG 8 to be realized in New Zealand, mutual interactions between SDG 8 and other goals attract interesting considerations of if both SDG 8 and other goals could be achieved. SDGs are grouped in terms of the common characteristics that will enable them to be impacted by and impact SDG 8 similarly. Besides, other SDGs could mutually enable each other to be realized. With comprehensive research and analysis of the mutual interactions, a positive outlook exists for both SDG 8 and other goals to be fulfilled, in consideration of the potential issuance of the CBDC as a primary stimulus to achieve decent work and economic growth."}
{"title":"SpICE: An interpretable method for spatial data","authors":["Natalia da Silva","Ignacio Alvarez-Castro","Leonardo Moreno","Andr\u00e9s Sosa"],"raw_abstract":"Statistical learning methods are widely utilized in tackling complex problems\ndue to their flexibility, good predictive performance and its ability to\ncapture complex relationships among variables. Additionally, recently developed\nautomatic workflows have provided a standardized approach to implementing\nstatistical learning methods across various applications. However these tools\nhighlight a main drawbacks of statistical learning: its lack of interpretation\nin their results. In the past few years an important amount of research has\nbeen focused on methods for interpreting black box models. Having interpretable\nstatistical learning methods is relevant to have a deeper understanding of the\nmodel. In problems were spatial information is relevant, combined interpretable\nmethods with spatial data can help to get better understanding of the problem\nand interpretation of the results.\n  This paper is focused in the individual conditional expectation (ICE-plot), a\nmodel agnostic methods for interpreting statistical learning models and\ncombined them with spatial information. ICE-plot extension is proposed where\nspatial information is used as restriction to define Spatial ICE curves\n(SpICE). Spatial ICE curves are estimated using real data in the context of an\neconomic problem concerning property valuation in Montevideo, Uruguay.\nUnderstanding the key factors that influence property valuation is essential\nfor decision-making, and spatial data plays a relevant role in this regard.","publication_date":1699742716,"paper_link":"http://arxiv.org/pdf/2311.06681v1","categories":["Statistics"],"abstract":"Statistical learning methods are widely utilized in tackling complex problems due to their flexibility, good predictive performance and its ability to capture complex relationships among variables. Additionally, recently developed automatic workflows have provided a standardized approach to implementing statistical learning methods across various applications. However these tools highlight a main drawbacks of statistical learning: its lack of interpretation in their results. In the past few years an important amount of research has been focused on methods for interpreting black box models. Having interpretable statistical learning methods is relevant to have a deeper understanding of the model. In problems were spatial information is relevant, combined interpretable methods with spatial data can help to get better understanding of the problem and interpretation of the results.   This paper is focused in the individual conditional expectation (ICE-plot), a model agnostic methods for interpreting statistical learning models and combined them with spatial information. ICE-plot extension is proposed where spatial information is used as restriction to define Spatial ICE curves (SpICE). Spatial ICE curves are estimated using real data in the context of an economic problem concerning property valuation in Montevideo, Uruguay. Understanding the key factors that influence property valuation is essential for decision-making, and spatial data plays a relevant role in this regard."}
{"title":"Education for a Future in Crisis: Developing a Humanities-Informed STEM Curriculum","authors":["Ethan Lee","Ariel Nicole Hart","Thomas A. Searles","Marc Levis-Fitzgerald","Ram\u00f3n S. Barthelemy","Shanna Shaked","Victoria Marks","Sergio Carbajo"],"raw_abstract":"In the popular imagination, science and technology are often seen as fields\nof knowledge production critical to social progress and a cooperative future.\nThis optimistic portrayal of technological advancement also features\nprominently in internal discourses amongst scientists, industry leaders, and\nSTEM students alike. Yet, an overwhelming body of research, investigation, and\nfirst-person accounts highlight the varying ways modern science, technology,\nand engineering industries contribute to the degradation of our changing\nenvironments and exploit and harm global low-income and marginalized\npopulations. By and large, siloed higher-education STEM curricula provide\ninadequate opportunities for undergraduate and graduate students to critically\nanalyze the historical and epistemological foundations of scientific knowledge\nproduction and even fewer tools to engage with and respond to modern\ncommunity-based cases. Here, we describe the development of a humanities- and\nsocial sciences-informed curriculum designed to address the theory, content,\nand skill-based needs of traditional STEM students considering technoscientific\ncareers. In essence, this course is designed to foster behavior change,\nde-center dominant ways of knowing in the sciences, and bolster self-reflection\nand critical-thinking skills to equip the developing STEM workforce with a more\nnuanced and accurate understanding of the social, political, and economic role\nof science and technology. This curriculum has the potential to empower\nSTEM-educated professionals to contribute to a more promising, inclusive\nfuture. Our framework foregrounds key insights from science and technology\nstudies, Black and Native feminisms, queer theory, and disability studies,\nalongside real-world case studies using critical pedagogies.","publication_date":1699740985,"paper_link":"http://arxiv.org/pdf/2311.06674v1","categories":["Physics"],"abstract":"In the popular imagination, science and technology are often seen as fields of knowledge production critical to social progress and a cooperative future. This optimistic portrayal of technological advancement also features prominently in internal discourses amongst scientists, industry leaders, and STEM students alike. Yet, an overwhelming body of research, investigation, and first-person accounts highlight the varying ways modern science, technology, and engineering industries contribute to the degradation of our changing environments and exploit and harm global low-income and marginalized populations. By and large, siloed higher-education STEM curricula provide inadequate opportunities for undergraduate and graduate students to critically analyze the historical and epistemological foundations of scientific knowledge production and even fewer tools to engage with and respond to modern community-based cases. Here, we describe the development of a humanities- and social sciences-informed curriculum designed to address the theory, content, and skill-based needs of traditional STEM students considering technoscientific careers. In essence, this course is designed to foster behavior change, de-center dominant ways of knowing in the sciences, and bolster self-reflection and critical-thinking skills to equip the developing STEM workforce with a more nuanced and accurate understanding of the social, political, and economic role of science and technology. This curriculum has the potential to empower STEM-educated professionals to contribute to a more promising, inclusive future. Our framework foregrounds key insights from science and technology studies, Black and Native feminisms, queer theory, and disability studies, alongside real-world case studies using critical pedagogies."}
{"title":"Best Complete Approximations of Preference Relations","authors":["Hiroki Nishimura","Efe A. Ok"],"raw_abstract":"We investigate the problem of approximating an incomplete preference relation\n$\\succsim$ on a finite set by a complete preference relation. We aim to obtain\nthis approximation in such a way that the choices on the basis of two\npreferences, one incomplete, the other complete, have the smallest possible\ndiscrepancy in the aggregate. To this end, we use the top-difference metric on\npreferences, and define a best complete approximation of $\\succsim$ as a\ncomplete preference relation nearest to $\\succsim$ relative to this metric. We\nprove that such an approximation must be a maximal completion of $\\succsim$,\nand that it is, in fact, any one completion of $\\succsim$ with the largest\nindex. Finally, we use these results to provide a sufficient condition for the\nbest complete approximation of a preference to be its canonical completion.\nThis leads to closed-form solutions to the best approximation problem in the\ncase of several incomplete preference relations of interest.","publication_date":1699728359,"paper_link":"http://arxiv.org/pdf/2311.06641v1","categories":["Mathematics","Economics"],"abstract":"We investigate the problem of approximating an incomplete preference relation __FORMULA__ on a finite set by a complete preference relation. We aim to obtain this approximation in such a way that the choices on the basis of two preferences, one incomplete, the other complete, have the smallest possible discrepancy in the aggregate. To this end, we use the top-difference metric on preferences, and define a best complete approximation of __FORMULA__ as a complete preference relation nearest to __FORMULA__ relative to this metric. We prove that such an approximation must be a maximal completion of __FORMULA__, and that it is, in fact, any one completion of __FORMULA__ with the largest index. Finally, we use these results to provide a sufficient condition for the best complete approximation of a preference to be its canonical completion. This leads to closed-form solutions to the best approximation problem in the case of several incomplete preference relations of interest."}
{"title":"Optimal resource allocation: Convex quantile regression approach","authors":["Sheng Dai","Natalia Kuosmanen","Timo Kuosmanen","Juuso Liesi\u00f6"],"raw_abstract":"Optimal allocation of resources across sub-units in the context of\ncentralized decision-making systems such as bank branches or supermarket chains\nis a classical application of operations research and management science. In\nthis paper, we develop quantile allocation models to examine how much the\noutput and productivity could potentially increase if the resources were\nefficiently allocated between units. We increase robustness to random noise and\nheteroscedasticity by utilizing the local estimation of multiple production\nfunctions using convex quantile regression. The quantile allocation models then\nrely on the estimated shadow prices instead of detailed data of units and allow\nthe entry and exit of units. Our empirical results on Finland's business sector\nreveal a large potential for productivity gains through better allocation,\nkeeping the current technology and resources fixed.","publication_date":1699715514,"paper_link":"http://arxiv.org/pdf/2311.06590v1","categories":["Quantitative Finance","Economics","Statistics"],"abstract":"Optimal allocation of resources across sub-units in the context of centralized decision-making systems such as bank branches or supermarket chains is a classical application of operations research and management science. In this paper, we develop quantile allocation models to examine how much the output and productivity could potentially increase if the resources were efficiently allocated between units. We increase robustness to random noise and heteroscedasticity by utilizing the local estimation of multiple production functions using convex quantile regression. The quantile allocation models then rely on the estimated shadow prices instead of detailed data of units and allow the entry and exit of units. Our empirical results on Finland's business sector reveal a large potential for productivity gains through better allocation, keeping the current technology and resources fixed."}
{"title":"Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations","authors":["Zengqing Wu","Run Peng","Xu Han","Shuyuan Zheng","Yixin Zhang","Chuan Xiao"],"raw_abstract":"Computer simulations offer a robust toolset for exploring complex systems\nacross various disciplines. A particularly impactful approach within this realm\nis Agent-Based Modeling (ABM), which harnesses the interactions of individual\nagents to emulate intricate system dynamics. ABM's strength lies in its\nbottom-up methodology, illuminating emergent phenomena by modeling the\nbehaviors of individual components of a system. Yet, ABM has its own set of\nchallenges, notably its struggle with modeling natural language instructions\nand common sense in mathematical equations or rules. This paper seeks to\ntranscend these boundaries by integrating Large Language Models (LLMs) like GPT\ninto ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based\nModeling (SABM). Building upon the concept of smart agents -- entities\ncharacterized by their intelligence, adaptability, and computation ability --\nwe explore in the direction of utilizing LLM-powered agents to simulate\nreal-world scenarios with increased nuance and realism. In this comprehensive\nexploration, we elucidate the state of the art of ABM, introduce SABM's\npotential and methodology, and present three case studies (source codes\navailable at https://github.com/Roihn/SABM), demonstrating the SABM methodology\nand validating its effectiveness in modeling real-world systems. Furthermore,\nwe cast a vision towards several aspects of the future of SABM, anticipating a\nbroader horizon for its applications. Through this endeavor, we aspire to\nredefine the boundaries of computer simulations, enabling a more profound\nunderstanding of complex systems.","publication_date":1699642473,"paper_link":"http://arxiv.org/pdf/2311.06330v2","categories":["Economics","Quantitative Finance"],"abstract":"Computer simulations offer a robust toolset for exploring complex systems across various disciplines. A particularly impactful approach within this realm is Agent-Based Modeling (ABM), which harnesses the interactions of individual agents to emulate intricate system dynamics. ABM's strength lies in its bottom-up methodology, illuminating emergent phenomena by modeling the behaviors of individual components of a system. Yet, ABM has its own set of challenges, notably its struggle with modeling natural language instructions and common sense in mathematical equations or rules. This paper seeks to transcend these boundaries by integrating Large Language Models (LLMs) like GPT into ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based Modeling (SABM). Building upon the concept of smart agents -- entities characterized by their intelligence, adaptability, and computation ability -- we explore in the direction of utilizing LLM-powered agents to simulate real-world scenarios with increased nuance and realism. In this comprehensive exploration, we elucidate the state of the art of ABM, introduce SABM's potential and methodology, and present three case studies (source codes available at https://github.com/Roihn/SABM), demonstrating the SABM methodology and validating its effectiveness in modeling real-world systems. Furthermore, we cast a vision towards several aspects of the future of SABM, anticipating a broader horizon for its applications. Through this endeavor, we aspire to redefine the boundaries of computer simulations, enabling a more profound understanding of complex systems."}
{"title":"A Portable Parton-Level Event Generator for the High-Luminosity LHC","authors":["Enrico Bothmann","Taylor Childers","Walter Giele","Stefan H\u00f6che","Joshua Isaacson","Max Knobbe"],"raw_abstract":"Parton-level event generators are one of the most computationally demanding\nparts of the simulation chain for the Large Hadron Collider. The rapid\ndeployment of computing hardware different from the traditional CPU+RAM model\nin data centers around the world mandates a change in event generator design.\nThese changes are required in order to provide economically and ecologically\nsustainable simulations for the high-luminosity era of the LHC. We present the\nfirst complete leading-order parton-level event generation framework capable of\nutilizing most modern hardware. Furthermore, we discuss its performance in the\nstandard candle processes of vector boson and top-quark pair production with up\nto five additional jets.","publication_date":1699637267,"paper_link":"http://arxiv.org/pdf/2311.06198v1","categories":["Physics"],"abstract":"Parton-level event generators are one of the most computationally demanding parts of the simulation chain for the Large Hadron Collider. The rapid deployment of computing hardware different from the traditional CPU+RAM model in data centers around the world mandates a change in event generator design. These changes are required in order to provide economically and ecologically sustainable simulations for the high-luminosity era of the LHC. We present the first complete leading-order parton-level event generation framework capable of utilizing most modern hardware. Furthermore, we discuss its performance in the standard candle processes of vector boson and top-quark pair production with up to five additional jets."}
{"title":"Large-Scale Dynamic Ridesharing with Iterative Assignment","authors":["Akhil Vakayil","Felipe de Souza","Taner Cokyasar","Krishna Murthy Gurumurthy","Jeffrey Larson"],"raw_abstract":"Transportation network companies (TNCs) have become a highly utilized\ntransportation mode over the past years. At their emergence, TNCs were serving\nride requests one by one. However, the economic and environmental benefits of\nridesharing encourages them to dynamically pool multiple ride requests to\nenable people to share vehicles. In a dynamic ridesharing (DRS) system, a fleet\noperator seeks to minimize the overall travel cost while a rider desires to\nexperience a faster (and cheaper) service. While the DRS may provide relatively\ncheaper trips by pooling requests, the service speed is contingent on the\nobjective of the vehicle-to-rider assignments. Moreover, the operator must\nquickly assign a vehicle to requests to prevent customer loss. In this study we\ndevelop an iterative assignment (IA) algorithm with a balanced objective to\nconduct assignments quickly. A greedy algorithm from the literature is also\ntailored to further reduce the computational time. The IA was used to measure\nthe impact on service quality of fleet size; assignment frequency; the weight\ncontrol parameter of the two objectives on vehicle occupancy -- rider wait time\nand vehicle hours traveled. A case study in Austin, TX, reveals that the key\nperformance metrics are the most sensitive to the weight parameter in the\nobjective function.","publication_date":1699633579,"paper_link":"http://arxiv.org/pdf/2311.06160v1","categories":["Mathematics"],"abstract":"Transportation network companies (TNCs) have become a highly utilized transportation mode over the past years. At their emergence, TNCs were serving ride requests one by one. However, the economic and environmental benefits of ridesharing encourages them to dynamically pool multiple ride requests to enable people to share vehicles. In a dynamic ridesharing (DRS) system, a fleet operator seeks to minimize the overall travel cost while a rider desires to experience a faster (and cheaper) service. While the DRS may provide relatively cheaper trips by pooling requests, the service speed is contingent on the objective of the vehicle-to-rider assignments. Moreover, the operator must quickly assign a vehicle to requests to prevent customer loss. In this study we develop an iterative assignment (IA) algorithm with a balanced objective to conduct assignments quickly. A greedy algorithm from the literature is also tailored to further reduce the computational time. The IA was used to measure the impact on service quality of fleet size; assignment frequency; the weight control parameter of the two objectives on vehicle occupancy -- rider wait time and vehicle hours traveled. A case study in Austin, TX, reveals that the key performance metrics are the most sensitive to the weight parameter in the objective function."}
{"title":"Charting multidimensional ideological polarization across demographic groups in the United States","authors":["Jaume Ojer","David C\u00e1rcamo","Romualdo Pastor-Satorras","Michele Starnini"],"raw_abstract":"The causes for the rise in affective polarization are debated, primarily\nrevolving around ideological polarization and partisan sorting hypotheses. The\nformer indicates divergent ideological beliefs on key issues as the main fuel\nfor the increasing political divide. The latter, instead, proposes that the\nalignment of social identity with partisan affiliation intensifies partisan\nrift. We present a novel methodology using data from the American National\nElection Studies (ANES) to discern the contributions of both ideological\npolarization and partisan sorting to affective polarization. We quantify\nmultidimensional polarization by embedding ANES respondents to a variety of\npolitical, social, and economic topics into a two-dimensional ideological\nspace. We identify several demographic attributes (Race, Gender, Age,\nAffluence, and Education) of the ANES respondents to measure their alignment\nwith partisan affiliation. Our results show that income and especially racial\ngroups are strongly sorted into parties, while education is orthogonal to\npartisanship. By analyzing the trajectories of political and demographic groups\nacross time, we observe an increasing partisan divide, due to both parties\nmoving away from the center at different rates. Opinions within groups become\nincreasingly heterogeneous in the last decade, with a share of Democratic\nvoters moving to a novel region of the ideological space. We conclude that the\nobserved growth in partisan divide is due to a combination of partisan sorting\nand increasing ideological polarization, mainly occurring after 2010.","publication_date":1699628244,"paper_link":"http://arxiv.org/pdf/2311.06096v1","categories":["Physics"],"abstract":"The causes for the rise in affective polarization are debated, primarily revolving around ideological polarization and partisan sorting hypotheses. The former indicates divergent ideological beliefs on key issues as the main fuel for the increasing political divide. The latter, instead, proposes that the alignment of social identity with partisan affiliation intensifies partisan rift. We present a novel methodology using data from the American National Election Studies (ANES) to discern the contributions of both ideological polarization and partisan sorting to affective polarization. We quantify multidimensional polarization by embedding ANES respondents to a variety of political, social, and economic topics into a two-dimensional ideological space. We identify several demographic attributes (Race, Gender, Age, Affluence, and Education) of the ANES respondents to measure their alignment with partisan affiliation. Our results show that income and especially racial groups are strongly sorted into parties, while education is orthogonal to partisanship. By analyzing the trajectories of political and demographic groups across time, we observe an increasing partisan divide, due to both parties moving away from the center at different rates. Opinions within groups become increasingly heterogeneous in the last decade, with a share of Democratic voters moving to a novel region of the ideological space. We conclude that the observed growth in partisan divide is due to a combination of partisan sorting and increasing ideological polarization, mainly occurring after 2010."}
{"title":"Patience ensures fairness","authors":["Florian Brandl","Andrew Mackenzie"],"raw_abstract":"We revisit the problem of fairly allocating a sequence of time slots when\nagents may have different levels of patience (Mackenzie and Komornik, 2023).\nFor each number of agents, we provide a lower threshold and an upper threshold\non the level of patience such that (i) if each agent is at least as patient as\nthe lower threshold, then there is a proportional allocation, and (ii) if each\nagent is at least as patient as the upper threshold and moreover has weak\npreference for earlier time slots, then there is an envy-free allocation. In\nboth cases, the proof is constructive.","publication_date":1699627605,"paper_link":"http://arxiv.org/pdf/2311.06092v1","categories":["Economics"],"abstract":"We revisit the problem of fairly allocating a sequence of time slots when agents may have different levels of patience (Mackenzie and Komornik, 2023). For each number of agents, we provide a lower threshold and an upper threshold on the level of patience such that (i) if each agent is at least as patient as the lower threshold, then there is a proportional allocation, and (ii) if each agent is at least as patient as the upper threshold and moreover has weak preference for earlier time slots, then there is an envy-free allocation. In both cases, the proof is constructive."}
{"title":"Empirical Review of Youth-Employment Programs in Ghana","authors":["Monica Lambon-Quayefio","Thomas Yeboah","Nkechi S. Owoo","Marjan Petreski","Catherine Koranchie","Edward Asiedu","Mohammed Zakaria","Ernest Berko","Yaw Nsiah Agyemang"],"raw_abstract":"Ghana-s current youth unemployment rate is 19.7%, and the country faces a\nsignificant youth unemployment problem. While a range of youth-employment\nprograms have been created over the years, no systematic documentation and\nevaluation of the impacts of these public initiatives has been undertaken.\nClarifying which interventions work would guide policy makers in creating\nstrategies and programs to address the youth-employment challenge. By\ncomplementing desk reviews with qualitative data gathered from focus-group\ndiscussions and key informant interviews, we observe that most youth-employment\nprograms implemented in Ghana cover a broad spectrum that includes skills\ntraining, job placement matching, seed capital, and subsidies. Duplication of\ninitiatives, lack of coordination, and few to non-existent impact evaluations\nof programs are the main challenges that plague these programs. For better\ncoordination and effective policy making, a more centralized and coordinated\nsystem is needed for program design and implementation. Along the same lines,\nensuring rigorous evaluation of existing youth-employment programs is necessary\nto provide empirical evidence of the effectiveness and efficiency of these\nprograms.","publication_date":1699622383,"paper_link":"http://arxiv.org/pdf/2311.06048v1","categories":["Economics","Quantitative Finance"],"abstract":"Ghana-s current youth unemployment rate is 19.7%, and the country faces a significant youth unemployment problem. While a range of youth-employment programs have been created over the years, no systematic documentation and evaluation of the impacts of these public initiatives has been undertaken. Clarifying which interventions work would guide policy makers in creating strategies and programs to address the youth-employment challenge. By complementing desk reviews with qualitative data gathered from focus-group discussions and key informant interviews, we observe that most youth-employment programs implemented in Ghana cover a broad spectrum that includes skills training, job placement matching, seed capital, and subsidies. Duplication of initiatives, lack of coordination, and few to non-existent impact evaluations of programs are the main challenges that plague these programs. For better coordination and effective policy making, a more centralized and coordinated system is needed for program design and implementation. Along the same lines, ensuring rigorous evaluation of existing youth-employment programs is necessary to provide empirical evidence of the effectiveness and efficiency of these programs."}
{"title":"Multi-Label Topic Model for Financial Textual Data","authors":["Moritz Scherrmann"],"raw_abstract":"This paper presents a multi-label topic model for financial texts like ad-hoc\nannouncements, 8-K filings, finance related news or annual reports. I train the\nmodel on a new financial multi-label database consisting of 3,044 German ad-hoc\nannouncements that are labeled manually using 20 predefined, economically\nmotivated topics. The best model achieves a macro F1 score of more than 85%.\nTranslating the data results in an English version of the model with similar\nperformance. As application of the model, I investigate differences in stock\nmarket reactions across topics. I find evidence for strong positive or negative\nmarket reactions for some topics, like announcements of new Large Scale\nProjects or Bankruptcy Filings, while I do not observe significant price\neffects for some other topics. Furthermore, in contrast to previous studies,\nthe multi-label structure of the model allows to analyze the effects of\nco-occurring topics on stock market reactions. For many cases, the reaction to\na specific topic depends heavily on the co-occurrence with other topics. For\nexample, if allocated capital from a Seasoned Equity Offering (SEO) is used for\nrestructuring a company in the course of a Bankruptcy Proceeding, the market\nreacts positively on average. However, if that capital is used for covering\nunexpected, additional costs from the development of new drugs, the SEO implies\nnegative reactions on average.","publication_date":1699620967,"paper_link":"http://arxiv.org/pdf/2311.07598v1","categories":["Quantitative Finance"],"abstract":"This paper presents a multi-label topic model for financial texts like ad-hoc announcements, 8-K filings, finance related news or annual reports. I train the model on a new financial multi-label database consisting of 3,044 German ad-hoc announcements that are labeled manually using 20 predefined, economically motivated topics. The best model achieves a macro F1 score of more than 85%. Translating the data results in an English version of the model with similar performance. As application of the model, I investigate differences in stock market reactions across topics. I find evidence for strong positive or negative market reactions for some topics, like announcements of new Large Scale Projects or Bankruptcy Filings, while I do not observe significant price effects for some other topics. Furthermore, in contrast to previous studies, the multi-label structure of the model allows to analyze the effects of co-occurring topics on stock market reactions. For many cases, the reaction to a specific topic depends heavily on the co-occurrence with other topics. For example, if allocated capital from a Seasoned Equity Offering (SEO) is used for restructuring a company in the course of a Bankruptcy Proceeding, the market reacts positively on average. However, if that capital is used for covering unexpected, additional costs from the development of new drugs, the SEO implies negative reactions on average."}
{"title":"A Decision Support System for Liver Diseases Prediction: Integrating Batch Processing, Rule-Based Event Detection and SPARQL Query","authors":["Ritesh Chandra","Sadhana Tiwari","Satyam Rastogi","Sonali Agarwal"],"raw_abstract":"Liver diseases pose a significant global health burden, impacting a\nsubstantial number of individuals and exerting substantial economic and social\nconsequences. Rising liver problems are considered a fatal disease in many\ncountries, such as Egypt, Molda, etc. The objective of this study is to\nconstruct a predictive model for liver illness using Basic Formal Ontology\n(BFO) and detection rules derived from a decision tree algorithm. Based on\nthese rules, events are detected through batch processing using the Apache Jena\nframework. Based on the event detected, queries can be directly processed using\nSPARQL. To make the ontology operational, these Decision Tree (DT) rules are\nconverted into Semantic Web Rule Language (SWRL). Using this SWRL in the\nontology for predicting different types of liver disease with the help of the\nPellet and Drool inference engines in Protege Tools, a total of 615 records are\ntaken from different liver diseases. After inferring the rules, the result can\nbe generated for the patient according to the DT rules, and other\npatient-related details along with different precautionary suggestions can be\nobtained based on these results. Combining query results of batch processing\nand ontology-generated results can give more accurate suggestions for disease\nprevention and detection. This work aims to provide a comprehensive approach\nthat is applicable for liver disease prediction, rich knowledge graph\nrepresentation, and smart querying capabilities. The results show that\ncombining RDF data, SWRL rules, and SPARQL queries for analysing and predicting\nliver disease can help medical professionals to learn more about liver diseases\nand make a Decision Support System (DSS) for health care.","publication_date":1699611669,"paper_link":"http://arxiv.org/pdf/2311.07595v1","categories":["Economics"],"abstract":"Liver diseases pose a significant global health burden, impacting a substantial number of individuals and exerting substantial economic and social consequences. Rising liver problems are considered a fatal disease in many countries, such as Egypt, Molda, etc. The objective of this study is to construct a predictive model for liver illness using Basic Formal Ontology (BFO) and detection rules derived from a decision tree algorithm. Based on these rules, events are detected through batch processing using the Apache Jena framework. Based on the event detected, queries can be directly processed using SPARQL. To make the ontology operational, these Decision Tree (DT) rules are converted into Semantic Web Rule Language (SWRL). Using this SWRL in the ontology for predicting different types of liver disease with the help of the Pellet and Drool inference engines in Protege Tools, a total of 615 records are taken from different liver diseases. After inferring the rules, the result can be generated for the patient according to the DT rules, and other patient-related details along with different precautionary suggestions can be obtained based on these results. Combining query results of batch processing and ontology-generated results can give more accurate suggestions for disease prevention and detection. This work aims to provide a comprehensive approach that is applicable for liver disease prediction, rich knowledge graph representation, and smart querying capabilities. The results show that combining RDF data, SWRL rules, and SPARQL queries for analysing and predicting liver disease can help medical professionals to learn more about liver diseases and make a Decision Support System (DSS) for health care."}
{"title":"ETHOS.FINE: A Framework for Integrated Energy System Assessment","authors":["Theresa Gro\u00df","Kevin Knosala","Maximilian Hoffmann","Noah Pflugradt","Detlef Stolten"],"raw_abstract":"The decarbonization of energy systems worldwide requires a transformation of\ntheir design and operation across all sectors, that is, the residential and\ncommercial, industrial, and transportation sectors. Energy system models are\nfrequently employed for assessing these changes, providing scenarios on\npotential future system design and on how new technologies and a modified\ninfrastructure will meet future energy demands. Thus, they support investment\ndecisions and policy-making. The Python-based Framework for Integrated Energy\nSystem Assessment (ETHOS.FINE) is a software package that provides a toolbox\nfor modeling, analyting and evaluating such energy systems using mathematical\noptimization. ETHOS.FINE is part of the Energy Transformation paTHway\nOptimization Suite (ETHOS) , a collection of modeling tools developed by the\nInstitute of Energy and Climate Research - Techno-Economic System Analysis\n(IEK-3) at Forschungszentrum J\\\"ulich. ETHOS offers a holistic view on energy\nsystems at arbitrary scales providing tools for geospatial renewable potential\nanalyses, time series simulation tools for residential and industrial sector,\ndiscrete choice models for the transportation sector, modeling of global energy\nsupply routes, and local infrastructure assessments, among others. The ETHOS\nmodel suite is, e.g., used for analyzing the energy transition of Germany\n(Stolten et al., 2022).","publication_date":1699605236,"paper_link":"http://arxiv.org/pdf/2311.05930v1","categories":["Mathematics"],"abstract":"The decarbonization of energy systems worldwide requires a transformation of their design and operation across all sectors, that is, the residential and commercial, industrial, and transportation sectors. Energy system models are frequently employed for assessing these changes, providing scenarios on potential future system design and on how new technologies and a modified infrastructure will meet future energy demands. Thus, they support investment decisions and policy-making. The Python-based Framework for Integrated Energy System Assessment (ETHOS.FINE) is a software package that provides a toolbox for modeling, analyting and evaluating such energy systems using mathematical optimization. ETHOS.FINE is part of the Energy Transformation paTHway Optimization Suite (ETHOS) , a collection of modeling tools developed by the Institute of Energy and Climate Research - Techno-Economic System Analysis (IEK-3) at Forschungszentrum J\\\"ulich. ETHOS offers a holistic view on energy systems at arbitrary scales providing tools for geospatial renewable potential analyses, time series simulation tools for residential and industrial sector, discrete choice models for the transportation sector, modeling of global energy supply routes, and local infrastructure assessments, among others. The ETHOS model suite is, e.g., used for analyzing the energy transition of Germany (Stolten et al., 2022)."}
{"title":"Time-Varying Identification of Monetary Policy Shocks","authors":["Annika Camehl","Tomasz Wo\u017aniak"],"raw_abstract":"We propose a new Bayesian heteroskedastic Markov-switching structural vector\nautoregression with data-driven time-varying identification. The model selects\nalternative exclusion restrictions over time and, as a condition for the\nsearch, allows to verify identification through heteroskedasticity within each\nregime. Based on four alternative monetary policy rules, we show that a monthly\nsix-variable system supports time variation in US monetary policy shock\nidentification. In the sample-dominating first regime, systematic monetary\npolicy follows a Taylor rule extended by the term spread and is effective in\ncurbing inflation. In the second regime, occurring after 2000 and gaining more\npersistence after the global financial and COVID crises, the Fed acts according\nto a money-augmented Taylor rule. This regime's unconventional monetary policy\nprovides economic stimulus, features the liquidity effect, and is complemented\nby a pure term spread shock. Absent the specific monetary policy of the second\nregime, inflation would be over one percentage point higher on average after\n2008.","publication_date":1699595581,"paper_link":"http://arxiv.org/pdf/2311.05883v1","categories":["Economics","Statistics"],"abstract":"We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread and is effective in curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, the Fed acts according to a money-augmented Taylor rule. This regime's unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008."}
{"title":"Optimal taxation and the Domar-Musgrave effect","authors":["Brendan K. Beare","Alexis Akira Toda"],"raw_abstract":"This article concerns the optimal choice of flat taxes on labor and capital\nincome, and on consumption, in a tractable economic model. Agents manage a\nportfolio of bonds and physical capital while subject to idiosyncratic\ninvestment risk and random mortality. We identify the tax rates which maximize\nwelfare in stationary equilibrium while preserving tax revenue, finding that a\nvery large increase in welfare can be achieved by only taxing capital income\nand consumption. The optimal rate of capital income taxation is zero if the\nnatural borrowing constraint is strictly binding on entrepreneurs, but may\notherwise be positive and potentially large. The Domar-Musgrave effect, whereby\ncapital income taxation with full offset provisions encourages risky investment\nthrough loss sharing, explains cases where it is optimal to tax capital income.\nIn further analysis we study the dynamic response to the substitution of\nconsumption taxation for labor income taxation. We find that consumption\nimmediately drops before rising rapidly to the new stationary equilibrium,\nwhich is higher on average than initial consumption for workers but lower for\nentrepreneurs.","publication_date":1699580746,"paper_link":"http://arxiv.org/pdf/2311.05822v1","categories":["Economics","Quantitative Finance"],"abstract":"This article concerns the optimal choice of flat taxes on labor and capital income, and on consumption, in a tractable economic model. Agents manage a portfolio of bonds and physical capital while subject to idiosyncratic investment risk and random mortality. We identify the tax rates which maximize welfare in stationary equilibrium while preserving tax revenue, finding that a very large increase in welfare can be achieved by only taxing capital income and consumption. The optimal rate of capital income taxation is zero if the natural borrowing constraint is strictly binding on entrepreneurs, but may otherwise be positive and potentially large. The Domar-Musgrave effect, whereby capital income taxation with full offset provisions encourages risky investment through loss sharing, explains cases where it is optimal to tax capital income. In further analysis we study the dynamic response to the substitution of consumption taxation for labor income taxation. We find that consumption immediately drops before rising rapidly to the new stationary equilibrium, which is higher on average than initial consumption for workers but lower for entrepreneurs."}
{"title":"Entropy Production on Cooperative Opinion Dynamics","authors":["Igor V. G. Oliveira","Chao Wang","Gaogao Dong","Ruijin Du","Carlos E. Fiore","H. Eugene Stanley","Andr\u00e9 L. M. Vilela"],"raw_abstract":"As one of the most widespread social dynamics, cooperative behavior is among\nthe most fascinating collective phenomena. Several animal species, from social\ninsects to human beings, feature social groups altruistically working for a\ncommon benefit. This collaborative conduct pervades the actions and opinions of\nindividuals, yielding strategic decision-making between political, religious,\nethnic, and economic social puzzles. Here, we explore how cooperative behavior\nphenomena impact collective opinion dynamics and entropy generation in social\ngroups. We select a random fraction $f$ of community members as collaborative\nindividuals and model the opinion dynamics using a social temperature parameter\n$q$ that functions as a social anxiety noise. With probability $q$, regular\nindividuals oppose their companions about a social decision, assuming group\ndissent. Collaborative agents experience a reduced effective social noise $\\mu\nq$, where $0 < \\mu < 1$ is the social anxiety noise sensibility parameter that\nenhances social validation. We perform numerical simulations and mean-field\nanalysis and find the system undergoes nonequilibrium order-disorder phase\ntransitions with expressive social entropy production. Our results also\nhighlight the effects of an individual social anxiety attenuation level in\nenhancing group consensus and inducing exuberant collective phenomena in\ncomplex systems.","publication_date":1699575923,"paper_link":"http://arxiv.org/pdf/2311.05803v1","categories":["Physics"],"abstract":"As one of the most widespread social dynamics, cooperative behavior is among the most fascinating collective phenomena. Several animal species, from social insects to human beings, feature social groups altruistically working for a common benefit. This collaborative conduct pervades the actions and opinions of individuals, yielding strategic decision-making between political, religious, ethnic, and economic social puzzles. Here, we explore how cooperative behavior phenomena impact collective opinion dynamics and entropy generation in social groups. We select a random fraction __FORMULA__ of community members as collaborative individuals and model the opinion dynamics using a social temperature parameter __FORMULA__ that functions as a social anxiety noise. With probability __FORMULA__, regular individuals oppose their companions about a social decision, assuming group dissent. Collaborative agents experience a reduced effective social noise __FORMULA__, where __FORMULA__ is the social anxiety noise sensibility parameter that enhances social validation. We perform numerical simulations and mean-field analysis and find the system undergoes nonequilibrium order-disorder phase transitions with expressive social entropy production. Our results also highlight the effects of an individual social anxiety attenuation level in enhancing group consensus and inducing exuberant collective phenomena in complex systems."}
{"title":"Collective Sampling: An Ex Ante Perspective","authors":["Yangfan Zhou"],"raw_abstract":"I study collective dynamic information acquisition. Players determine when to\nend sequential sampling via a collective choice rule. My analysis focuses on\nthe case of two players, but extends to many players. With two players,\ncollective stopping is determined either unilaterally or unanimously. I develop\na methodology to characterize equilibrium outcomes using an ex ante perspective\non posterior distributions. Under unilateral stopping, each player chooses a\nmean-preserving contraction of the other's posterior distribution; under\nunanimous stopping, they choose meanpreserving spreads. Equilibrium outcomes\ncan be determined via concavification. Players learn Pareto inefficiently: too\nlittle under unilateral stopping, while too much under unanimous stopping;\nthese learning inefficiencies are amplified when players' preferences become\nless aligned. I demonstrate the value of my methodological approach in three\napplications: committee search, dynamic persuasion, and competition in\npersuasion.","publication_date":1699566771,"paper_link":"http://arxiv.org/pdf/2311.05758v1","categories":["Economics"],"abstract":"I study collective dynamic information acquisition. Players determine when to end sequential sampling via a collective choice rule. My analysis focuses on the case of two players, but extends to many players. With two players, collective stopping is determined either unilaterally or unanimously. I develop a methodology to characterize equilibrium outcomes using an ex ante perspective on posterior distributions. Under unilateral stopping, each player chooses a mean-preserving contraction of the other's posterior distribution; under unanimous stopping, they choose meanpreserving spreads. Equilibrium outcomes can be determined via concavification. Players learn Pareto inefficiently: too little under unilateral stopping, while too much under unanimous stopping; these learning inefficiencies are amplified when players' preferences become less aligned. I demonstrate the value of my methodological approach in three applications: committee search, dynamic persuasion, and competition in persuasion."}
{"title":"Bridging the Digital Divide: Performance Variation across Socio-Economic Factors in Vision-Language Models","authors":["Joan Nwatu","Oana Ignat","Rada Mihalcea"],"raw_abstract":"Despite the impressive performance of current AI models reported across\nvarious tasks, performance reports often do not include evaluations of how\nthese models perform on the specific groups that will be impacted by these\ntechnologies. Among the minority groups under-represented in AI, data from\nlow-income households are often overlooked in data collection and model\nevaluation. We evaluate the performance of a state-of-the-art vision-language\nmodel (CLIP) on a geo-diverse dataset containing household images associated\nwith different income values (Dollar Street) and show that performance\ninequality exists among households of different income levels. Our results\nindicate that performance for the poorer groups is consistently lower than the\nwealthier groups across various topics and countries. We highlight insights\nthat can help mitigate these issues and propose actionable steps for\neconomic-level inclusive AI development. Code is available at\nhttps://github.com/MichiganNLP/Bridging_the_Digital_Divide.","publication_date":1699564252,"paper_link":"http://arxiv.org/pdf/2311.05746v1","categories":["Economics"],"abstract":"Despite the impressive performance of current AI models reported across various tasks, performance reports often do not include evaluations of how these models perform on the specific groups that will be impacted by these technologies. Among the minority groups under-represented in AI, data from low-income households are often overlooked in data collection and model evaluation. We evaluate the performance of a state-of-the-art vision-language model (CLIP) on a geo-diverse dataset containing household images associated with different income values (Dollar Street) and show that performance inequality exists among households of different income levels. Our results indicate that performance for the poorer groups is consistently lower than the wealthier groups across various topics and countries. We highlight insights that can help mitigate these issues and propose actionable steps for economic-level inclusive AI development. Code is available at https://github.com/MichiganNLP/Bridging_the_Digital_Divide."}
{"title":"Ball Mill Fault Prediction Based on Deep Convolutional Auto-Encoding Network","authors":["Xinkun Ai","Kun Liu","Wei Zheng","Yonggang Fan","Xinwu Wu","Peilong Zhang","LiYe Wang","JanFeng Zhu","Yuan Pan"],"raw_abstract":"Ball mills play a critical role in modern mining operations, making their\nbearing failures a significant concern due to the potential loss of production\nefficiency and economic consequences. This paper presents an anomaly detection\nmethod based on Deep Convolutional Auto-encoding Neural Networks (DCAN) for\naddressing the issue of ball mill bearing fault detection. The proposed\napproach leverages vibration data collected during normal operation for\ntraining, overcoming challenges such as labeling issues and data imbalance\noften encountered in supervised learning methods. DCAN includes the modules of\nconvolutional feature extraction and transposed convolutional feature\nreconstruction, demonstrating exceptional capabilities in signal processing and\nfeature extraction. Additionally, the paper describes the practical deployment\nof the DCAN-based anomaly detection model for bearing fault detection,\nutilizing data from the ball mill bearings of Wuhan Iron & Steel Resources\nGroup and fault data from NASA's bearing vibration dataset. Experimental\nresults validate the DCAN model's reliability in recognizing fault vibration\npatterns. This method holds promise for enhancing bearing fault detection\nefficiency, reducing production interruptions, and lowering maintenance costs.","publication_date":1699552147,"paper_link":"http://arxiv.org/pdf/2311.13571v1","categories":["Economics"],"abstract":"Ball mills play a critical role in modern mining operations, making their bearing failures a significant concern due to the potential loss of production efficiency and economic consequences. This paper presents an anomaly detection method based on Deep Convolutional Auto-encoding Neural Networks (DCAN) for addressing the issue of ball mill bearing fault detection. The proposed approach leverages vibration data collected during normal operation for training, overcoming challenges such as labeling issues and data imbalance often encountered in supervised learning methods. DCAN includes the modules of convolutional feature extraction and transposed convolutional feature reconstruction, demonstrating exceptional capabilities in signal processing and feature extraction. Additionally, the paper describes the practical deployment of the DCAN-based anomaly detection model for bearing fault detection, utilizing data from the ball mill bearings of Wuhan Iron & Steel Resources Group and fault data from NASA's bearing vibration dataset. Experimental results validate the DCAN model's reliability in recognizing fault vibration patterns. This method holds promise for enhancing bearing fault detection efficiency, reducing production interruptions, and lowering maintenance costs."}
{"title":"City formation by dual migration of firms and workers","authors":["Kensuke Ohtake"],"raw_abstract":"This paper studies a mathematical model of city formation by migration of\nfirms and workers. The Core-Periphery model in the new economic geography,\nwhich considers migration of workers driven by real wage inequality among\nregions, is extended to incorporate migration of firms driven by real profit\ninequality among regions. A spatially homogeneous distributions of firms and\nworkers become destabilized and eventually forms several cities, and the number\nof the cities decreases as transport costs become lower.","publication_date":1699530162,"paper_link":"http://arxiv.org/pdf/2311.05292v1","categories":["Mathematics","Economics"],"abstract":"This paper studies a mathematical model of city formation by migration of firms and workers. The Core-Periphery model in the new economic geography, which considers migration of workers driven by real wage inequality among regions, is extended to incorporate migration of firms driven by real profit inequality among regions. A spatially homogeneous distributions of firms and workers become destabilized and eventually forms several cities, and the number of the cities decreases as transport costs become lower."}
{"title":"Towards a Taxonomy of Large Language Model based Business Model Transformations","authors":["Jochen Wulf","Juerg Meierhofer"],"raw_abstract":"Research on the role of Large Language Models (LLMs) in business models and\nservices is limited. Previous studies have utilized econometric models,\ntechnical showcases, and literature reviews. However, this research is\npioneering in its empirical examination of the influence of LLMs at the firm\nlevel. The study introduces a detailed taxonomy that can guide further research\non the criteria for successful LLM-based business model implementation and\ndeepen understanding of LLM-driven business transformations. Existing knowledge\non this subject is sparse and general. This research offers a more detailed\nbusiness model design framework based on LLM-driven transformations. This\ntaxonomy is not only beneficial for academic research but also has practical\nimplications. It can act as a strategic tool for businesses, offering insights\nand best practices. Businesses can lev-erage this taxonomy to make informed\ndecisions about LLM initiatives, ensuring that technology in-vestments align\nwith strategic goals.","publication_date":1699529557,"paper_link":"http://arxiv.org/pdf/2311.05288v1","categories":["Economics","Quantitative Finance"],"abstract":"Research on the role of Large Language Models (LLMs) in business models and services is limited. Previous studies have utilized econometric models, technical showcases, and literature reviews. However, this research is pioneering in its empirical examination of the influence of LLMs at the firm level. The study introduces a detailed taxonomy that can guide further research on the criteria for successful LLM-based business model implementation and deepen understanding of LLM-driven business transformations. Existing knowledge on this subject is sparse and general. This research offers a more detailed business model design framework based on LLM-driven transformations. This taxonomy is not only beneficial for academic research but also has practical implications. It can act as a strategic tool for businesses, offering insights and best practices. Businesses can lev-erage this taxonomy to make informed decisions about LLM initiatives, ensuring that technology in-vestments align with strategic goals."}
{"title":"Workplace diversity and innovation performance: current state of affairs and future directions","authors":["Christian R. \u00d8stergaard","Bram Timmermans"],"raw_abstract":"Over the last 10 years, there has been a growing interest in diversity in\nhuman capital. Fueled by the business case for diversity, there is an\nincreasing interest in understanding how the combination of people with\ndifferent backgrounds fosters the innovation performance of firms. Studies have\nmeasured diversity on a wide range of personal-level characteristics, at\ndifferent levels of the organization, and in particular kinds of settings.\nInnovation performance has been measured using an arsenal of indicators, often\ndrawing on a large range of databases. This paper takes stock of this research,\nidentifying the current state of affairs and proposing future research\ntrajectories in the field of diversity and innovation","publication_date":1699520563,"paper_link":"http://arxiv.org/pdf/2311.05219v1","categories":["Economics","Quantitative Finance"],"abstract":"Over the last 10 years, there has been a growing interest in diversity in human capital. Fueled by the business case for diversity, there is an increasing interest in understanding how the combination of people with different backgrounds fosters the innovation performance of firms. Studies have measured diversity on a wide range of personal-level characteristics, at different levels of the organization, and in particular kinds of settings. Innovation performance has been measured using an arsenal of indicators, often drawing on a large range of databases. This paper takes stock of this research, identifying the current state of affairs and proposing future research trajectories in the field of diversity and innovation"}
{"title":"An efficient Bayesian approach to joint functional principal component analysis for complex sampling designs","authors":["Tui Nolan","Sylvia Richardson","H\u00e9l\u00e8ne Ruffieux"],"raw_abstract":"The analysis of multivariate functional curves has the potential to yield\nimportant scientific discoveries in domains such as healthcare, medicine,\neconomics and social sciences. However it is common for real-world settings to\npresent data that are both sparse and irregularly sampled, and this introduces\nimportant challenges for the current functional data methodology. Here we\npropose a Bayesian hierarchical framework for multivariate functional principal\ncomponent analysis which accommodates the intricacies of such sampling designs\nby flexibly pooling information across subjects and correlated curves. Our\nmodel represents common latent dynamics via shared functional principal\ncomponent scores, thereby effectively borrowing strength across curves while\ncircumventing the computationally challenging task of estimating covariance\nmatrices. These scores also provide a parsimonious representation of the major\nmodes of joint variation of the curves, and constitute interpretable scalar\nsummaries that can be employed in follow-up analyses. We perform inference\nusing a variational message passing algorithm which combines efficiency,\nmodularity and approximate posterior density estimation, enabling the joint\nanalysis of large datasets with parameter uncertainty quantification. We\nconduct detailed simulations to assess the effectiveness of our approach in\nsharing information under complex sampling designs. We also exploit it to\nestimate the molecular disease courses of individual patients with SARS-CoV-2\ninfection and characterise patient heterogeneity in recovery outcomes; this\nstudy reveals key coordinated dynamics across the immune, inflammatory and\nmetabolic systems, which are associated with survival and long-COVID symptoms\nup to one year post disease onset. Our approach is implemented in the R package\nbayesFPCA.","publication_date":1699518449,"paper_link":"http://arxiv.org/pdf/2311.05200v1","categories":["Statistics"],"abstract":"The analysis of multivariate functional curves has the potential to yield important scientific discoveries in domains such as healthcare, medicine, economics and social sciences. However it is common for real-world settings to present data that are both sparse and irregularly sampled, and this introduces important challenges for the current functional data methodology. Here we propose a Bayesian hierarchical framework for multivariate functional principal component analysis which accommodates the intricacies of such sampling designs by flexibly pooling information across subjects and correlated curves. Our model represents common latent dynamics via shared functional principal component scores, thereby effectively borrowing strength across curves while circumventing the computationally challenging task of estimating covariance matrices. These scores also provide a parsimonious representation of the major modes of joint variation of the curves, and constitute interpretable scalar summaries that can be employed in follow-up analyses. We perform inference using a variational message passing algorithm which combines efficiency, modularity and approximate posterior density estimation, enabling the joint analysis of large datasets with parameter uncertainty quantification. We conduct detailed simulations to assess the effectiveness of our approach in sharing information under complex sampling designs. We also exploit it to estimate the molecular disease courses of individual patients with SARS-CoV-2 infection and characterise patient heterogeneity in recovery outcomes; this study reveals key coordinated dynamics across the immune, inflammatory and metabolic systems, which are associated with survival and long-COVID symptoms up to one year post disease onset. Our approach is implemented in the R package bayesFPCA."}
{"title":"$\\textit{Labor Space}$: A Unifying Representation of the Labor Market via Large Language Models","authors":["Seongwoon Kim","Yong-Yeol Ahn","Jaehyuk Park"],"raw_abstract":"The labor market is a complex ecosystem comprising diverse, interconnected\nentities, such as industries, occupations, skills, and firms. Due to the lack\nof a systematic method to map these heterogeneous entities together, each\nentity has been analyzed in isolation or only through pairwise relationships,\ninhibiting comprehensive understanding of the whole ecosystem. Here, we\nintroduce $\\textit{Labor Space}$, a vector-space embedding of heterogeneous\nlabor market entities, derived through applying a large language model with\nfine-tuning. Labor Space exposes the complex relational fabric of various labor\nmarket constituents, facilitating coherent integrative analysis of industries,\noccupations, skills, and firms, while retaining type-specific clustering. We\ndemonstrate its unprecedented analytical capacities, including positioning\nheterogeneous entities on an economic axes, such as\n`Manufacturing--Healthcare'. Furthermore, by allowing vector arithmetic of\nthese entities, Labor Space enables the exploration of complex inter-unit\nrelations, and subsequently the estimation of the ramifications of economic\nshocks on individual units and their ripple effect across the labor market. We\nposit that Labor Space provides policymakers and business leaders with a\ncomprehensive unifying framework for labor market analysis and simulation,\nfostering more nuanced and effective strategic decision-making.","publication_date":1699512070,"paper_link":"http://arxiv.org/pdf/2311.06310v2","categories":["Physics"],"abstract":"The labor market is a complex ecosystem comprising diverse, interconnected entities, such as industries, occupations, skills, and firms. Due to the lack of a systematic method to map these heterogeneous entities together, each entity has been analyzed in isolation or only through pairwise relationships, inhibiting comprehensive understanding of the whole ecosystem. Here, we introduce __FORMULA__, a vector-space embedding of heterogeneous labor market entities, derived through applying a large language model with fine-tuning. Labor Space exposes the complex relational fabric of various labor market constituents, facilitating coherent integrative analysis of industries, occupations, skills, and firms, while retaining type-specific clustering. We demonstrate its unprecedented analytical capacities, including positioning heterogeneous entities on an economic axes, such as `Manufacturing--Healthcare'. Furthermore, by allowing vector arithmetic of these entities, Labor Space enables the exploration of complex inter-unit relations, and subsequently the estimation of the ramifications of economic shocks on individual units and their ripple effect across the labor market. We posit that Labor Space provides policymakers and business leaders with a comprehensive unifying framework for labor market analysis and simulation, fostering more nuanced and effective strategic decision-making."}
{"title":"High-dimensional Newey-Powell Test Via Approximate Message Passing","authors":["Jing Zhou","Hui Zou"],"raw_abstract":"Homoscedastic regression error is a common assumption in many\nhigh-dimensional regression models and theories. Although heteroscedastic error\ncommonly exists in real-world datasets, testing heteroscedasticity remains\nlargely underexplored under high-dimensional settings. We consider the\nheteroscedasticity test proposed in Newey and Powell (1987), whose asymptotic\ntheory has been well-established for the low-dimensional setting. We show that\nthe Newey-Powell test can be developed for high-dimensional data. For\nasymptotic theory, we consider the setting where the number of dimensions grows\nwith the sample size at a linear rate. The asymptotic analysis for the test\nstatistic utilizes the Approximate Message Passing (AMP) algorithm, from which\nwe obtain the limiting distribution of the test. The numerical performance of\nthe test is investigated through an extensive simulation study. As real-data\napplications, we present the analysis based on \"international economic growth\"\ndata (Belloni et al. 2011), which is found to be homoscedastic, and\n\"supermarket\" data (Lan et al., 2016), which is found to be heteroscedastic.","publication_date":1699486947,"paper_link":"http://arxiv.org/pdf/2311.05056v1","categories":["Statistics"],"abstract":"Homoscedastic regression error is a common assumption in many high-dimensional regression models and theories. Although heteroscedastic error commonly exists in real-world datasets, testing heteroscedasticity remains largely underexplored under high-dimensional settings. We consider the heteroscedasticity test proposed in Newey and Powell (1987), whose asymptotic theory has been well-established for the low-dimensional setting. We show that the Newey-Powell test can be developed for high-dimensional data. For asymptotic theory, we consider the setting where the number of dimensions grows with the sample size at a linear rate. The asymptotic analysis for the test statistic utilizes the Approximate Message Passing (AMP) algorithm, from which we obtain the limiting distribution of the test. The numerical performance of the test is investigated through an extensive simulation study. As real-data applications, we present the analysis based on \"international economic growth\" data (Belloni et al. 2011), which is found to be homoscedastic, and \"supermarket\" data (Lan et al., 2016), which is found to be heteroscedastic."}
{"title":"Sustainable Collaborative Strategy in Pharmaceutical Refrigerated Logistics Routing Problem","authors":["Tingting Chen","Feng Chu","Jiantong Zhang","Jiaqing Sun"],"raw_abstract":"The rapid growth of pharmaceutical refrigerated logistics poses\nsustainability challenges, including elevated costs, energy consumption, and\nresource inefficiency. Collaborating multiple depots can enhance logistics\nefficiency when standalone distribution centers have limited transport\nresources, i.e., refrigerated vehicles. However, the sustainable benefits and\nperformance across different strategies remain unexplored. This study fills\nthis research gap by addressing a refrigerated pharmaceutical routing problem.\nWhile many collaborative strategies prioritize economic and environmental\nbenefits, our approach highlights a vital social indicator: maintaining vehicle\nflow equilibrium at each depot during collaboration. This ensures the stability\nof transport resources for all stakeholders, promoting sustainable\ncollaborative logistics. The problem is formulated as a multi-depot vehicle\nrouting problem with time windows (MDVRPTW). Three collaborative strategies\nusing Clustering VRP (CLUVRP) and improved Open VRP (OVRP) are proposed and\ncompared. We develop two approaches to address traditional OVRP limitations in\nensuring vehicle flow equilibrium at each depot. Our models consider perishable\npharmaceuticals and time-dependent travel speeds. Three hybrid heuristics based\non Simulated Annealing and Variable Neighborhood Search (SAVNS) are proposed\nand evaluated for efficacy. Computational experiments and a case study\ndemonstrate distinct sustainable benefits across various strategies, offering\nvaluable insights for decision-makers in the refrigerated logistics market.","publication_date":1699451858,"paper_link":"http://arxiv.org/pdf/2311.04691v1","categories":["Statistics"],"abstract":"The rapid growth of pharmaceutical refrigerated logistics poses sustainability challenges, including elevated costs, energy consumption, and resource inefficiency. Collaborating multiple depots can enhance logistics efficiency when standalone distribution centers have limited transport resources, i.e., refrigerated vehicles. However, the sustainable benefits and performance across different strategies remain unexplored. This study fills this research gap by addressing a refrigerated pharmaceutical routing problem. While many collaborative strategies prioritize economic and environmental benefits, our approach highlights a vital social indicator: maintaining vehicle flow equilibrium at each depot during collaboration. This ensures the stability of transport resources for all stakeholders, promoting sustainable collaborative logistics. The problem is formulated as a multi-depot vehicle routing problem with time windows (MDVRPTW). Three collaborative strategies using Clustering VRP (CLUVRP) and improved Open VRP (OVRP) are proposed and compared. We develop two approaches to address traditional OVRP limitations in ensuring vehicle flow equilibrium at each depot. Our models consider perishable pharmaceuticals and time-dependent travel speeds. Three hybrid heuristics based on Simulated Annealing and Variable Neighborhood Search (SAVNS) are proposed and evaluated for efficacy. Computational experiments and a case study demonstrate distinct sustainable benefits across various strategies, offering valuable insights for decision-makers in the refrigerated logistics market."}
{"title":"Global Vulnerability Assessment of Mobile Telecommunications Infrastructure to Climate Hazards using Crowdsourced Open Data","authors":["Edward J. Oughton","Tom Russell","Jeongjin Oh","Sara Ballan","Jim W. Hall"],"raw_abstract":"The ongoing change in Earth`s climate is causing an increase in the frequency\nand severity of climate-related hazards, for example, from coastal flooding,\nriverine flooding, and tropical cyclones. There is currently an urgent need to\nquantify the potential impacts of these events on infrastructure and users,\nespecially for hitherto neglected infrastructure sectors, such as\ntelecommunications, particularly given our increasing dependence on digital\ntechnologies. In this analysis a global assessment is undertaken, quantifying\nthe number of mobile cells vulnerable to climate hazards using open\ncrowdsourced data equating to 7.6 million 2G, 3G, 4G and 5G assets. For a 0.01%\nannual probability event under a high emissions scenario (RCP8.5), the number\nof affected cells is estimated at 2.26 million for tropical cyclones, equating\nto USD 1.01 billion in direct damage (an increase against the historical\nbaseline of 14% and 44%, respectively). Equally, for coastal flooding the\nnumber of potentially affected cells for an event with a 0.01% annual\nprobability under RCP8.5 is 109.9 thousand, equating to direct damage costs of\nUSD 2.69 billion (an increase against the baseline of 70% and 78%,\nrespectively). The findings demonstrate the need for risk analysts to include\nmobile communications (and telecommunications more broadly) in future critical\nnational infrastructure assessments. Indeed, this paper contributes a proven\nassessment methodology to the literature for use in future research for\nassessing this critical infrastructure sector.","publication_date":1699400869,"paper_link":"http://arxiv.org/pdf/2311.04392v1","categories":["Statistics","Economics","Quantitative Finance"],"abstract":"The ongoing change in Earth`s climate is causing an increase in the frequency and severity of climate-related hazards, for example, from coastal flooding, riverine flooding, and tropical cyclones. There is currently an urgent need to quantify the potential impacts of these events on infrastructure and users, especially for hitherto neglected infrastructure sectors, such as telecommunications, particularly given our increasing dependence on digital technologies. In this analysis a global assessment is undertaken, quantifying the number of mobile cells vulnerable to climate hazards using open crowdsourced data equating to 7.6 million 2G, 3G, 4G and 5G assets. For a 0.01% annual probability event under a high emissions scenario (RCP8.5), the number of affected cells is estimated at 2.26 million for tropical cyclones, equating to USD 1.01 billion in direct damage (an increase against the historical baseline of 14% and 44%, respectively). Equally, for coastal flooding the number of potentially affected cells for an event with a 0.01% annual probability under RCP8.5 is 109.9 thousand, equating to direct damage costs of USD 2.69 billion (an increase against the baseline of 70% and 78%, respectively). The findings demonstrate the need for risk analysts to include mobile communications (and telecommunications more broadly) in future critical national infrastructure assessments. Indeed, this paper contributes a proven assessment methodology to the literature for use in future research for assessing this critical infrastructure sector."}
{"title":"Open RAN xApps Design and Evaluation: Lessons Learnt and Identified Challenges","authors":["Marcin Hoffmann","Salim Janji","Adam Samorzewski","Lukasz Kulacz","Cezary Adamczyk","Marcin Dryja\u0144ski","Pawel Kryszkiewicz","Adrian Kliks","Hanna Bogucka"],"raw_abstract":"Open Radio Access Networks (RAN) offer diverse economic opportunities. A\ntransition to a flexible, modular approach within the disaggregated RAN\nframework is crucial, involving careful planning of RAN architecture and the\ndeployment of specialized software applications. Collaboration across sectors\nis essential for efficiency and reliability, with the open-source community\ndriving innovation. This paper explores challenges for third-party application\ndevelopers in Open RAN. It provides a comparative analysis of solutions,\nfocusing on xApp development and implementation. Challenges arise in two areas:\nthe complexities of xApp development, particularly for advanced use cases like\nbeam management, and issues in low-level software implementation within open\nplatforms. In conclusion, key challenges must promote academia-industry\ncollaboration in Open RAN. This paper shares early lessons from xApp\ndevelopment, guiding the field's evolution.","publication_date":1699397782,"paper_link":"http://arxiv.org/pdf/2311.04380v1","categories":["Economics"],"abstract":"Open Radio Access Networks (RAN) offer diverse economic opportunities. A transition to a flexible, modular approach within the disaggregated RAN framework is crucial, involving careful planning of RAN architecture and the deployment of specialized software applications. Collaboration across sectors is essential for efficiency and reliability, with the open-source community driving innovation. This paper explores challenges for third-party application developers in Open RAN. It provides a comparative analysis of solutions, focusing on xApp development and implementation. Challenges arise in two areas: the complexities of xApp development, particularly for advanced use cases like beam management, and issues in low-level software implementation within open platforms. In conclusion, key challenges must promote academia-industry collaboration in Open RAN. This paper shares early lessons from xApp development, guiding the field's evolution."}
{"title":"Common Knowledge, Regained","authors":["Yannai A. Gonczarowski","Yoram Moses"],"raw_abstract":"Formally, for common knowledge to arise in a dynamic setting, knowledge that\nit has arisen must be simultaneously attained by all players. As a result, new\ncommon knowledge is unattainable in many realistic settings, due to timing\nfrictions. This unintuitive phenomenon, observed by Halpern and Moses (1990),\nwas discussed by Arrow et al. (1987) and by Aumann (1989), was called a paradox\nby Morris (2014), and has evaded satisfactory resolution for four decades. We\nresolve this paradox by proposing a new definition for common knowledge, which\ncoincides with the traditional one in static settings but generalizes it in\ndynamic settings. Under our definition, common knowledge can arise without\nsimultaneity, particularly in canonical examples of the Haplern-Moses paradox.\nWe demonstrate its usefulness by deriving for it an agreement theorem \\`a la\nAumann (1976), and showing that it arises in the setting of Geanakoplos and\nPolemarchakis (1982) with timing frictions added.","publication_date":1699396696,"paper_link":"http://arxiv.org/pdf/2311.04374v1","categories":["Economics"],"abstract":"Formally, for common knowledge to arise in a dynamic setting, knowledge that it has arisen must be simultaneously attained by all players. As a result, new common knowledge is unattainable in many realistic settings, due to timing frictions. This unintuitive phenomenon, observed by Halpern and Moses (1990), was discussed by Arrow et al. (1987) and by Aumann (1989), was called a paradox by Morris (2014), and has evaded satisfactory resolution for four decades. We resolve this paradox by proposing a new definition for common knowledge, which coincides with the traditional one in static settings but generalizes it in dynamic settings. Under our definition, common knowledge can arise without simultaneity, particularly in canonical examples of the Haplern-Moses paradox. We demonstrate its usefulness by deriving for it an agreement theorem \\`a la Aumann (1976), and showing that it arises in the setting of Geanakoplos and Polemarchakis (1982) with timing frictions added."}
{"title":"The Future of Astronomical Data Infrastructure: Meeting Report","authors":["Michael R. Blanton","Janet D. Evans","Dara Norman","William O'Mullane","Adrian Price-Whelan","Luca Rizzi","Alberto Accomazzi","Megan Ansdell","Stephen Bailey","Paul Barrett","Steven Berukoff","Adam Bolton","Julian Borrill","Kelle Cruz","Julianne Dalcanton","Vandana Desai","Gregory P. Dubois-Felsmann","Frossie Economou","Henry Ferguson","Bryan Field","Dan Foreman-Mackey","Jaime Forero-Romero","Niall Gaffney","Kim Gillies","Matthew J. Graham","Steven Gwyn","Joseph Hennawi","Anna L. H. Hughes","Tess Jaffe","Preshanth Jagannathan","Tim Jenness","Mario Juri\u0107","JJ Kavelaars","Kerk Kee","Jeff Kern","Anthony Kremin","Kathleen Labrie","Mark Lacy","Casey Law","Rafael Mart\u00ednez-Galarza","Curtis McCully","Julie McEnery","Bryan Miller","Christopher Moriarty","August Muench","Demitri Muna","Angela Murillo","Gautham Narayan","James D. Neill","Robert Nikutta","Roopesh Ojha","Knut Olsen","John O'Meara","Ben Rusholme","Robert Seaman","Nathaniel Starkman","Martin Still","Felix Stoehr","John D. Swinbank","Peter Teuben","Ignacio Toledo","Erik Tollerud","Matthew D. Turk","James Turner","William Vacca","Joaquin Vieira","Benjamin Weaver","Benjamin Weiner","Jason Weiss","Kyle Westfall","Beth Willman","Lily Zhao"],"raw_abstract":"The astronomical community is grappling with the increasing volume and\ncomplexity of data produced by modern telescopes, due to difficulties in\nreducing, accessing, analyzing, and combining archives of data. To address this\nchallenge, we propose the establishment of a coordinating body, an \"entity,\"\nwith the specific mission of enhancing the interoperability, archiving,\ndistribution, and production of both astronomical data and software. This\nreport is the culmination of a workshop held in February 2023 on the Future of\nAstronomical Data Infrastructure. Attended by 70 scientists and software\nprofessionals from ground-based and space-based missions and archives spanning\nthe entire spectrum of astronomical research, the group deliberated on the\nprevailing state of software and data infrastructure in astronomy, identified\npressing issues, and explored potential solutions. In this report, we describe\nthe ecosystem of astronomical data, its existing flaws, and the many gaps,\nduplication, inconsistencies, barriers to access, drags on productivity, missed\nopportunities, and risks to the long-term integrity of essential data sets. We\nalso highlight the successes and failures in a set of deep dives into several\ndifferent illustrative components of the ecosystem, included as an appendix.","publication_date":1699383601,"paper_link":"http://arxiv.org/pdf/2311.04272v1","categories":["Physics"],"abstract":"The astronomical community is grappling with the increasing volume and complexity of data produced by modern telescopes, due to difficulties in reducing, accessing, analyzing, and combining archives of data. To address this challenge, we propose the establishment of a coordinating body, an \"entity,\" with the specific mission of enhancing the interoperability, archiving, distribution, and production of both astronomical data and software. This report is the culmination of a workshop held in February 2023 on the Future of Astronomical Data Infrastructure. Attended by 70 scientists and software professionals from ground-based and space-based missions and archives spanning the entire spectrum of astronomical research, the group deliberated on the prevailing state of software and data infrastructure in astronomy, identified pressing issues, and explored potential solutions. In this report, we describe the ecosystem of astronomical data, its existing flaws, and the many gaps, duplication, inconsistencies, barriers to access, drags on productivity, missed opportunities, and risks to the long-term integrity of essential data sets. We also highlight the successes and failures in a set of deep dives into several different illustrative components of the ecosystem, included as an appendix."}
{"title":"Models towards Risk Behavior Prediction and Analysis: A Netherlands Case study","authors":["Onaopepo Adekunle","Arno Riedl","Michel Dumontier"],"raw_abstract":"In many countries financial service providers have to elicit their customers\nrisk preferences, when offering products and services. For instance, in the\nNetherlands pension funds will be legally obliged to factor in their clients\nrisk preferences when devising their investment strategies. Therefore,\nassessing and measuring the risk preferences of individuals is critical for the\nanalysis of individuals' behavior and policy prescriptions. In the psychology\nand economics, a number of methods to elicit risk preferences have been\ndeveloped using hypothetical scenarios and economic experiments. These methods\nof eliciting individual risk preferences are usually applied to small samples\nbecause they are expensive and the implementation can be complex and not\nsuitable when large cohorts need to be measured. A large number of supervised\nlearning models ranging from linear regression to support vector machines are\nused to predict risk preference measures using socio-economic register data\nsuch as age, gender, migration background and other demographic variables in\ncombination with data on income, wealth, pension fund contributions, and other\nfinancial data. The employed machine learning models cover a range of\nassumptions and properties as well as a diverse set of regression metrics. The\noptimum model is selected using the metrics and interpretability of the model.\nThe optimal models are lasso regression and gradient boosting machines with\nmean average percentage error of about 30%. This is important as it helps to\nestimate risk attitudes without actually measuring them. It should be noted\nthat with the current accuracy the tested models are not ready for deployment\nfor applications that require high accuracy. However, the results do indicate\nwhich models should be used in situations that do not require the most accurate\npredictions such as augmentation data for pensions' recommendation.","publication_date":1699379240,"paper_link":"http://arxiv.org/pdf/2311.04164v1","categories":["Economics"],"abstract":"In many countries financial service providers have to elicit their customers risk preferences, when offering products and services. For instance, in the Netherlands pension funds will be legally obliged to factor in their clients risk preferences when devising their investment strategies. Therefore, assessing and measuring the risk preferences of individuals is critical for the analysis of individuals' behavior and policy prescriptions. In the psychology and economics, a number of methods to elicit risk preferences have been developed using hypothetical scenarios and economic experiments. These methods of eliciting individual risk preferences are usually applied to small samples because they are expensive and the implementation can be complex and not suitable when large cohorts need to be measured. A large number of supervised learning models ranging from linear regression to support vector machines are used to predict risk preference measures using socio-economic register data such as age, gender, migration background and other demographic variables in combination with data on income, wealth, pension fund contributions, and other financial data. The employed machine learning models cover a range of assumptions and properties as well as a diverse set of regression metrics. The optimum model is selected using the metrics and interpretability of the model. The optimal models are lasso regression and gradient boosting machines with mean average percentage error of about 30%. This is important as it helps to estimate risk attitudes without actually measuring them. It should be noted that with the current accuracy the tested models are not ready for deployment for applications that require high accuracy. However, the results do indicate which models should be used in situations that do not require the most accurate predictions such as augmentation data for pensions' recommendation."}
{"title":"Coarse correlated equilibria in linear quadratic mean field games and application to an emission abatement game","authors":["Luciano Campi","Federico Cannerozzi","Fanny Cartellier"],"raw_abstract":"Coarse correlated equilibria (CCE) are a good alternative to Nash equilibria\n(NE), as they arise more naturally as outcomes of learning algorithms and they\nmay exhibit higher payoffs than NE. CCEs include a device which allows players'\nstrategies to be correlated without any cooperation, only through information\nsent by a mediator. We develop a methodology to concretely compute mean field\nCCEs in a linear-quadratic mean field game framework. We compare their\nperformance to mean field control solutions and mean field NE (usually named\nMFG solutions). Our approach is implemented in the mean field version of an\nemission abatement game between greenhouse gas emitters. In particular, we\nexhibit a simple and tractable class of mean field CCEs which allows to\noutperform very significantly the mean field NE payoff and abatement levels,\nbridging the gap between the mean field NE and the social optimum obtained by\nmean field control.","publication_date":1699378899,"paper_link":"http://arxiv.org/pdf/2311.04162v1","categories":["Mathematics","Economics","Quantitative Finance"],"abstract":"Coarse correlated equilibria (CCE) are a good alternative to Nash equilibria (NE), as they arise more naturally as outcomes of learning algorithms and they may exhibit higher payoffs than NE. CCEs include a device which allows players' strategies to be correlated without any cooperation, only through information sent by a mediator. We develop a methodology to concretely compute mean field CCEs in a linear-quadratic mean field game framework. We compare their performance to mean field control solutions and mean field NE (usually named MFG solutions). Our approach is implemented in the mean field version of an emission abatement game between greenhouse gas emitters. In particular, we exhibit a simple and tractable class of mean field CCEs which allows to outperform very significantly the mean field NE payoff and abatement levels, bridging the gap between the mean field NE and the social optimum obtained by mean field control."}
{"title":"Benchmarking a Neutral-Atom Quantum Computer","authors":["N. Wagner","C. Poole","T. M. Graham","M. Saffman"],"raw_abstract":"In this study, we simulated the algorithmic performance of a small neutral\natom quantum computer and compared its performance when operating with\nall-to-all versus nearest-neighbor connectivity. This comparison was made using\na suite of algorithmic benchmarks developed by the Quantum Economic Development\nConsortium. Circuits were simulated with a noise model consistent with\nexperimental data from Nature 604, 457 (2022). We find that all-to-all\nconnectivity improves simulated circuit fidelity by $10\\%-15\\%$, compared to\nnearest-neighbor connectivity.","publication_date":1699377211,"paper_link":"http://arxiv.org/pdf/2311.04141v1","categories":["Physics"],"abstract":"In this study, we simulated the algorithmic performance of a small neutral atom quantum computer and compared its performance when operating with all-to-all versus nearest-neighbor connectivity. This comparison was made using a suite of algorithmic benchmarks developed by the Quantum Economic Development Consortium. Circuits were simulated with a noise model consistent with experimental data from Nature 604, 457 (2022). We find that all-to-all connectivity improves simulated circuit fidelity by __FORMULA__, compared to nearest-neighbor connectivity."}
{"title":"A Lightweight and Secure PUF-Based Authentication and Key-exchange Protocol for IoT Devices","authors":["Chandranshu Gupta","Gaurav Varshney"],"raw_abstract":"The Internet of Things (IoT) has improved people's lives by seamlessly\nintegrating into many facets of modern life and facilitating information\nsharing across platforms. Device Authentication and Key exchange are major\nchallenges for the IoT. High computational resource requirements for\ncryptographic primitives and message transmission during Authentication make\nthe existing methods like PKI and IBE not suitable for these resource\nconstrained devices. PUF appears to offer a practical and economical security\nmechanism in place of typically sophisticated cryptosystems like PKI and IBE.\nPUF provides an unclonable and tamper sensitive unique signature based on the\nPUF chip by using manufacturing process variability. Therefore, in this study,\nwe use lightweight bitwise XOR, hash function, and PUF to Authenticate IoT\ndevices. Despite several studies employing the PUF to authenticate\ncommunication between IoT devices, to the authors' knowledge, existing\nsolutions require intermediary gateway and internet capabilities by the IoT\ndevice to directly interact with a Server for Authentication and hence, are not\nscalable when the IoT device works on different technologies like BLE, Zigbee,\netc. To address the aforementioned issue, we present a system in which the IoT\ndevice does not require a continuous active internet connection to communicate\nwith the server in order to Authenticate itself. The results of a thorough\nsecurity study are validated against adversarial attacks and PUF modeling\nattacks. For formal security validation, the AVISPA verification tool is also\nused. Performance study recommends this protocol's lightweight characteristics.\nThe proposed protocol's acceptability and defenses against adversarial assaults\nare supported by a prototype developed with ESP32.","publication_date":1699371734,"paper_link":"http://arxiv.org/pdf/2311.04078v1","categories":["Economics"],"abstract":"The Internet of Things (IoT) has improved people's lives by seamlessly integrating into many facets of modern life and facilitating information sharing across platforms. Device Authentication and Key exchange are major challenges for the IoT. High computational resource requirements for cryptographic primitives and message transmission during Authentication make the existing methods like PKI and IBE not suitable for these resource constrained devices. PUF appears to offer a practical and economical security mechanism in place of typically sophisticated cryptosystems like PKI and IBE. PUF provides an unclonable and tamper sensitive unique signature based on the PUF chip by using manufacturing process variability. Therefore, in this study, we use lightweight bitwise XOR, hash function, and PUF to Authenticate IoT devices. Despite several studies employing the PUF to authenticate communication between IoT devices, to the authors' knowledge, existing solutions require intermediary gateway and internet capabilities by the IoT device to directly interact with a Server for Authentication and hence, are not scalable when the IoT device works on different technologies like BLE, Zigbee, etc. To address the aforementioned issue, we present a system in which the IoT device does not require a continuous active internet connection to communicate with the server in order to Authenticate itself. The results of a thorough security study are validated against adversarial attacks and PUF modeling attacks. For formal security validation, the AVISPA verification tool is also used. Performance study recommends this protocol's lightweight characteristics. The proposed protocol's acceptability and defenses against adversarial assaults are supported by a prototype developed with ESP32."}
{"title":"IoT-Based Environmental Control System for Fish Farms with Sensor Integration and Machine Learning Decision Support","authors":["D. Dhinakaran","S. Gopalakrishnan","M. D. Manigandan","T. P. Anish"],"raw_abstract":"In response to the burgeoning global demand for seafood and the challenges of\nmanaging fish farms, we introduce an innovative IoT based environmental control\nsystem that integrates sensor technology and advanced machine learning decision\nsupport. Deploying a network of wireless sensors within the fish farm, we\ncontinuously collect real-time data on crucial environmental parameters,\nincluding water temperature, pH levels, humidity, and fish behavior. This data\nundergoes meticulous preprocessing to ensure its reliability, including\nimputation, outlier detection, feature engineering, and synchronization. At the\nheart of our system are four distinct machine learning algorithms: Random\nForests predict and optimize water temperature and pH levels for the fish,\nfostering their health and growth; Support Vector Machines (SVMs) function as\nan early warning system, promptly detecting diseases and parasites in fish;\nGradient Boosting Machines (GBMs) dynamically fine-tune the feeding schedule\nbased on real-time environmental conditions, promoting resource efficiency and\nfish productivity; Neural Networks manage the operation of critical equipment\nlike water pumps and heaters to maintain the desired environmental conditions\nwithin the farm. These machine learning algorithms collaboratively make\nreal-time decisions to ensure that the fish farm's environmental conditions\nalign with predefined specifications, leading to improved fish health and\nproductivity while simultaneously reducing resource wastage, thereby\ncontributing to increased profitability and sustainability. This research\narticle showcases the power of data-driven decision support in fish farming,\npromising to meet the growing demand for seafood while emphasizing\nenvironmental responsibility and economic viability, thus revolutionizing the\nfuture of fish farming.","publication_date":1699367716,"paper_link":"http://arxiv.org/pdf/2311.04258v1","categories":["Electrical Engineering and Systems Science"],"abstract":"In response to the burgeoning global demand for seafood and the challenges of managing fish farms, we introduce an innovative IoT based environmental control system that integrates sensor technology and advanced machine learning decision support. Deploying a network of wireless sensors within the fish farm, we continuously collect real-time data on crucial environmental parameters, including water temperature, pH levels, humidity, and fish behavior. This data undergoes meticulous preprocessing to ensure its reliability, including imputation, outlier detection, feature engineering, and synchronization. At the heart of our system are four distinct machine learning algorithms: Random Forests predict and optimize water temperature and pH levels for the fish, fostering their health and growth; Support Vector Machines (SVMs) function as an early warning system, promptly detecting diseases and parasites in fish; Gradient Boosting Machines (GBMs) dynamically fine-tune the feeding schedule based on real-time environmental conditions, promoting resource efficiency and fish productivity; Neural Networks manage the operation of critical equipment like water pumps and heaters to maintain the desired environmental conditions within the farm. These machine learning algorithms collaboratively make real-time decisions to ensure that the fish farm's environmental conditions align with predefined specifications, leading to improved fish health and productivity while simultaneously reducing resource wastage, thereby contributing to increased profitability and sustainability. This research article showcases the power of data-driven decision support in fish farming, promising to meet the growing demand for seafood while emphasizing environmental responsibility and economic viability, thus revolutionizing the future of fish farming."}
{"title":"Exploring Dataset-Scale Indicators of Data Quality","authors":["Benjamin Feuer","Chinmay Hegde"],"raw_abstract":"Modern computer vision foundation models are trained on massive amounts of\ndata, incurring large economic and environmental costs. Recent research has\nsuggested that improving data quality can significantly reduce the need for\ndata quantity. But what constitutes data quality in computer vision? We posit\nthat the quality of a given dataset can be decomposed into distinct\nsample-level and dataset-level constituents, and that the former have been more\nextensively studied than the latter. We ablate the effects of two important\ndataset-level constituents: label set design, and class balance. By monitoring\nthese constituents using key indicators we provide, researchers and\npractitioners can better anticipate model performance, measured in terms of its\naccuracy and robustness to distribution shifts.","publication_date":1699366472,"paper_link":"http://arxiv.org/pdf/2311.04016v1","categories":["Economics"],"abstract":"Modern computer vision foundation models are trained on massive amounts of data, incurring large economic and environmental costs. Recent research has suggested that improving data quality can significantly reduce the need for data quantity. But what constitutes data quality in computer vision? We posit that the quality of a given dataset can be decomposed into distinct sample-level and dataset-level constituents, and that the former have been more extensively studied than the latter. We ablate the effects of two important dataset-level constituents: label set design, and class balance. By monitoring these constituents using key indicators we provide, researchers and practitioners can better anticipate model performance, measured in terms of its accuracy and robustness to distribution shifts."}
{"title":"Population dynamics in fresh product markets with no posted prices","authors":["Ali Ellouze","Bastien Fernandez"],"raw_abstract":"We introduce and mathematically study a conceptual model for the dynamics of\nthe buyers population in markets of perishable goods where prices are not\nposted. Buyers behaviours are driven partly by loyalty to previously visited\nmerchants and partly by sensitivity to merchants intrinsic attractiveness.\nMoreover, attractiveness evolve in time depending on the relative volumes of\nbuyers, assuming profit/competitiveness optimisation when\nfavourable/unfavourable. While this negative feedback mechanism is a source of\ninstability that promotes oscillatory behaviour, our analysis identifies those\ncritical features that are responsible for the asymptotic stability of\nstationary states, both in their immediate neighbourhood and globally in phase\nspace. In particular, we show that while full loss of clientele occurs\n(depending on the initial state) in case of a bounded reactivity rate, it\ncannot happen when this rate is unbounded and merchants resilience always\nprevails in this case. Altogether, our analysis provides mathematical insights\ninto the consequences of introducing feedback into buyer-seller interactions\nand their diversified impacts on the long term levels of clientele in the\nmarkets.","publication_date":1699364297,"paper_link":"http://arxiv.org/pdf/2311.03987v1","categories":["Mathematics","Economics","Physics"],"abstract":"We introduce and mathematically study a conceptual model for the dynamics of the buyers population in markets of perishable goods where prices are not posted. Buyers behaviours are driven partly by loyalty to previously visited merchants and partly by sensitivity to merchants intrinsic attractiveness. Moreover, attractiveness evolve in time depending on the relative volumes of buyers, assuming profit/competitiveness optimisation when favourable/unfavourable. While this negative feedback mechanism is a source of instability that promotes oscillatory behaviour, our analysis identifies those critical features that are responsible for the asymptotic stability of stationary states, both in their immediate neighbourhood and globally in phase space. In particular, we show that while full loss of clientele occurs (depending on the initial state) in case of a bounded reactivity rate, it cannot happen when this rate is unbounded and merchants resilience always prevails in this case. Altogether, our analysis provides mathematical insights into the consequences of introducing feedback into buyer-seller interactions and their diversified impacts on the long term levels of clientele in the markets."}
{"title":"Stable partitions for proportional generalized claims problems","authors":["Oihane Gallo","Bettina Klaus"],"raw_abstract":"We consider a set of agents who have claims on an endowment that is not large\nenough to cover all claims. Agents can form coalitions but a minimal coalition\nsize $\\theta$ is required to have positive coalitional funding that is\nproportional to the sum of the claims of its members. We analyze the structure\nof stable partitions when coalition members use well-behaved rules to allocate\ncoalitional endowments, e.g., the well-known constrained equal awards rule\n(CEA) or the constrained equal losses rule (CEL).For continuous, (strictly)\nresource monotonic, and consistent rules, stable partitions with (mostly)\n$\\theta$-size coalitions emerge. For CEA and CEL we provide algorithms to\nconstruct such a stable partition formed by $\\theta$-size coalitions.","publication_date":1699361067,"paper_link":"http://arxiv.org/pdf/2311.03950v1","categories":["Economics"],"abstract":"We consider a set of agents who have claims on an endowment that is not large enough to cover all claims. Agents can form coalitions but a minimal coalition size __FORMULA__ is required to have positive coalitional funding that is proportional to the sum of the claims of its members. We analyze the structure of stable partitions when coalition members use well-behaved rules to allocate coalitional endowments, e.g., the well-known constrained equal awards rule (CEA) or the constrained equal losses rule (CEL).For continuous, (strictly) resource monotonic, and consistent rules, stable partitions with (mostly) __FORMULA__-size coalitions emerge. For CEA and CEL we provide algorithms to construct such a stable partition formed by __FORMULA__-size coalitions."}
{"title":"Multilevel mixtures of latent trait analyzers for clustering multi-layer bipartite networks","authors":["Dalila Failli","Bruno Arpino","Maria Francesca Marino"],"raw_abstract":"Within network data analysis, bipartite networks represent a particular type\nof network where relationships occur between two disjoint sets of nodes,\nformally called sending and receiving nodes. In this context, sending nodes may\nbe organized into layers on the basis of some defined characteristics,\nresulting in a special case of multilayer bipartite network, where each layer\nincludes a specific set of sending nodes. To perform a clustering of sending\nnodes in multi-layer bipartite network, we extend the Mixture of Latent Trait\nAnalyzers (MLTA), also taking into account the influence of concomitant\nvariables on clustering formation and the multi-layer structure of the data. To\nthis aim, a multilevel approach offers a useful methodological tool to properly\naccount for the hierarchical structure of the data and for the unobserved\nsources of heterogeneity at multiple levels. A simulation study is conducted to\ntest the performance of the proposal in terms of parameters' and clustering\nrecovery. Furthermore, the model is applied to the European Social Survey data\n(ESS) to i) perform a clustering of individuals (sending nodes) based on their\ndigital skills (receiving nodes); ii) understand how socio-economic and\ndemographic characteristics influence the individual digitalization level; iii)\naccount for the multilevel structure of the data; iv) obtain a clustering of\ncountries in terms of the base-line attitude to digital technologies of their\nresidents.","publication_date":1699348687,"paper_link":"http://arxiv.org/pdf/2311.03829v1","categories":["Statistics"],"abstract":"Within network data analysis, bipartite networks represent a particular type of network where relationships occur between two disjoint sets of nodes, formally called sending and receiving nodes. In this context, sending nodes may be organized into layers on the basis of some defined characteristics, resulting in a special case of multilayer bipartite network, where each layer includes a specific set of sending nodes. To perform a clustering of sending nodes in multi-layer bipartite network, we extend the Mixture of Latent Trait Analyzers (MLTA), also taking into account the influence of concomitant variables on clustering formation and the multi-layer structure of the data. To this aim, a multilevel approach offers a useful methodological tool to properly account for the hierarchical structure of the data and for the unobserved sources of heterogeneity at multiple levels. A simulation study is conducted to test the performance of the proposal in terms of parameters' and clustering recovery. Furthermore, the model is applied to the European Social Survey data (ESS) to i) perform a clustering of individuals (sending nodes) based on their digital skills (receiving nodes); ii) understand how socio-economic and demographic characteristics influence the individual digitalization level; iii) account for the multilevel structure of the data; iv) obtain a clustering of countries in terms of the base-line attitude to digital technologies of their residents."}
{"title":"Ultimatum game: regret or fairness?","authors":["Lida H. Aleksanyan","Armen E. Allahverdyan","Vardan G. Bardakhchyan"],"raw_abstract":"In the ultimatum game, the challenge is to explain why responders reject\nnon-zero offers thereby defying classical rationality. Fairness and related\nnotions have been the main explanations so far. We explain this rejection\nbehavior via the following principle: if the responder regrets less about\nlosing the offer than the proposer regrets not offering the best option, the\noffer is rejected. This principle qualifies as a rational punishing behavior\nand it replaces the experimentally falsified classical rationality (the subgame\nperfect Nash equilibrium) that leads to accepting any non-zero offer. The\nprinciple is implemented via the transitive regret theory for probabilistic\nlotteries. The expected utility implementation is a limiting case of this. We\nshow that several experimental results normally prescribed to fairness and\nintent-recognition can be given an alternative explanation via rational\npunishment; e.g. the comparison between \"fair\" and \"superfair\", the behavior\nunder raising the stakes etc. Hence we also propose experiments that can\ndistinguish these two scenarios (fairness versus regret-based punishment). They\nassume different utilities for the proposer and responder. We focus on the\nmini-ultimatum version of the game and also show how it can emerge from a more\ngeneral setup of the game.","publication_date":1699347242,"paper_link":"http://arxiv.org/pdf/2311.03814v1","categories":["Economics","Physics"],"abstract":"In the ultimatum game, the challenge is to explain why responders reject non-zero offers thereby defying classical rationality. Fairness and related notions have been the main explanations so far. We explain this rejection behavior via the following principle: if the responder regrets less about losing the offer than the proposer regrets not offering the best option, the offer is rejected. This principle qualifies as a rational punishing behavior and it replaces the experimentally falsified classical rationality (the subgame perfect Nash equilibrium) that leads to accepting any non-zero offer. The principle is implemented via the transitive regret theory for probabilistic lotteries. The expected utility implementation is a limiting case of this. We show that several experimental results normally prescribed to fairness and intent-recognition can be given an alternative explanation via rational punishment; e.g. the comparison between \"fair\" and \"superfair\", the behavior under raising the stakes etc. Hence we also propose experiments that can distinguish these two scenarios (fairness versus regret-based punishment). They assume different utilities for the proposer and responder. We focus on the mini-ultimatum version of the game and also show how it can emerge from a more general setup of the game."}
{"title":"Conversations in Galician: a Large Language Model for an Underrepresented Language","authors":["Eliseo Bao","Anxo P\u00e9rez","Javier Parapar"],"raw_abstract":"The recent proliferation of Large Conversation Language Models has\nhighlighted the economic significance of widespread access to this type of AI\ntechnologies in the current information age. Nevertheless, prevailing models\nhave primarily been trained on corpora consisting of documents written in\npopular languages. The dearth of such cutting-edge tools for low-resource\nlanguages further exacerbates their underrepresentation in the current economic\nlandscape, thereby impacting their native speakers. This paper introduces two\nnovel resources designed to enhance Natural Language Processing (NLP) for the\nGalician language. We present a Galician adaptation of the Alpaca dataset,\ncomprising 52,000 instructions and demonstrations. This dataset proves\ninvaluable for enhancing language models by fine-tuning them to more accurately\nadhere to provided instructions. Additionally, as a demonstration of the\ndataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician,\na language not originally supported by the model, by following the Alpaca\nformat. This work contributes to the research on multilingual models tailored\nfor low-resource settings, a crucial endeavor in ensuring the inclusion of all\nlinguistic communities in the development of Large Language Models. Another\nnoteworthy aspect of this research is the exploration of how knowledge of a\nclosely related language, in this case, Portuguese, can assist in generating\ncoherent text when training resources are scarce. Both the Galician Alpaca\ndataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we\nhave made the source code available to facilitate replication of this\nexperiment and encourage further advancements for underrepresented languages.","publication_date":1699347148,"paper_link":"http://arxiv.org/pdf/2311.03812v1","categories":["Economics"],"abstract":"The recent proliferation of Large Conversation Language Models has highlighted the economic significance of widespread access to this type of AI technologies in the current information age. Nevertheless, prevailing models have primarily been trained on corpora consisting of documents written in popular languages. The dearth of such cutting-edge tools for low-resource languages further exacerbates their underrepresentation in the current economic landscape, thereby impacting their native speakers. This paper introduces two novel resources designed to enhance Natural Language Processing (NLP) for the Galician language. We present a Galician adaptation of the Alpaca dataset, comprising 52,000 instructions and demonstrations. This dataset proves invaluable for enhancing language models by fine-tuning them to more accurately adhere to provided instructions. Additionally, as a demonstration of the dataset utility, we fine-tuned LLaMA-7B to comprehend and respond in Galician, a language not originally supported by the model, by following the Alpaca format. This work contributes to the research on multilingual models tailored for low-resource settings, a crucial endeavor in ensuring the inclusion of all linguistic communities in the development of Large Language Models. Another noteworthy aspect of this research is the exploration of how knowledge of a closely related language, in this case, Portuguese, can assist in generating coherent text when training resources are scarce. Both the Galician Alpaca dataset and Cabuxa-7B are publicly accessible on our Huggingface Hub, and we have made the source code available to facilitate replication of this experiment and encourage further advancements for underrepresented languages."}
{"title":"A finite mixture approach for the analysis of digital skills in Finland, Italy and Bulgaria: the role of socio-economic factors","authors":["Dalila Failli","Bruno Arpino","Maria Francesca Marino"],"raw_abstract":"The digital divide is the gap among population sub-groups in accessing and/or\nusing digital technologies. For instance, older people show a lower propensity\nto have a broadband connection, use the Internet, and adopt new technologies\nthan the younger ones. Motivated by the analysis of the heterogeneity in the\nuse of digital technologies, we build a bipartite network concerning the\npresence of various digital skills in individuals from three different European\ncountries: Finland, Italy, and Bulgaria. Bipartite networks provide a useful\nstructure for representing relationships between two disjoint sets of nodes,\nformally called sending and receiving nodes. The goal is to perform a\nclustering of individuals (sending nodes) based on their digital skills\n(receiving nodes) for each country. In this regard, we employ a Mixture of\nLatent Trait Analyzers (MLTA) accounting for concomitant variables, which\nallows us to (i) cluster individuals according to their individual profile;\n(ii) analyze how socio-economic and demographic characteristics, as well as\nintergenerational ties, influence individual digitalization. Results show that\nthe type of digitalization substantially depends on age, income and level of\neducation, while the presence of children in the household seems to play an\nimportant role in the digitalization process in Italy and Finland only.","publication_date":1699345958,"paper_link":"http://arxiv.org/pdf/2311.03801v1","categories":["Statistics"],"abstract":"The digital divide is the gap among population sub-groups in accessing and/or using digital technologies. For instance, older people show a lower propensity to have a broadband connection, use the Internet, and adopt new technologies than the younger ones. Motivated by the analysis of the heterogeneity in the use of digital technologies, we build a bipartite network concerning the presence of various digital skills in individuals from three different European countries: Finland, Italy, and Bulgaria. Bipartite networks provide a useful structure for representing relationships between two disjoint sets of nodes, formally called sending and receiving nodes. The goal is to perform a clustering of individuals (sending nodes) based on their digital skills (receiving nodes) for each country. In this regard, we employ a Mixture of Latent Trait Analyzers (MLTA) accounting for concomitant variables, which allows us to (i) cluster individuals according to their individual profile; (ii) analyze how socio-economic and demographic characteristics, as well as intergenerational ties, influence individual digitalization. Results show that the type of digitalization substantially depends on age, income and level of education, while the presence of children in the household seems to play an important role in the digitalization process in Italy and Finland only."}
{"title":"Healthcare Security Breaches in the United States: Insights and their Socio-Technical Implications","authors":["Megha M. Moncy","Sadia Afreen","Saptarshi Purkayastha"],"raw_abstract":"This research examines the pivotal role of human behavior in the realm of\nhealthcare data management, situated at the confluence of technological\nadvancements and human conduct. An in-depth analysis of security breaches in\nthe United States from 2009 to the present elucidates the dominance of\nhuman-induced security breaches. While technological weak points are certainly\na concern, our study highlights that a significant proportion of breaches are\nprecipitated by human errors and practices, thus pinpointing a conspicuous\ndeficiency in training, awareness, and organizational architecture. In spite of\nstringent federal mandates, such as the Health Insurance Portability and\nAccountability Act (HIPAA) and the Health Information Technology for Economic\nand Clinical Health (HITECH) Act, breaches persist, emphasizing the\nindispensable role of human factors within this domain. Such oversights not\nonly jeopardize patient data confidentiality but also undermine the\nfoundational trust inherent in the healthcare infrastructure. By probing the\nsocio-technical facets of healthcare security infringements, this article\nadvocates for an integrated, dynamic, and holistic approach to healthcare data\nsecurity. The findings underscore the imperative of augmenting technological\ndefenses while concurrently elevating human conduct and institutional ethos,\nthereby cultivating a robust and impervious healthcare data management\nenvironment.","publication_date":1699323631,"paper_link":"http://arxiv.org/pdf/2311.03664v1","categories":["Economics"],"abstract":"This research examines the pivotal role of human behavior in the realm of healthcare data management, situated at the confluence of technological advancements and human conduct. An in-depth analysis of security breaches in the United States from 2009 to the present elucidates the dominance of human-induced security breaches. While technological weak points are certainly a concern, our study highlights that a significant proportion of breaches are precipitated by human errors and practices, thus pinpointing a conspicuous deficiency in training, awareness, and organizational architecture. In spite of stringent federal mandates, such as the Health Insurance Portability and Accountability Act (HIPAA) and the Health Information Technology for Economic and Clinical Health (HITECH) Act, breaches persist, emphasizing the indispensable role of human factors within this domain. Such oversights not only jeopardize patient data confidentiality but also undermine the foundational trust inherent in the healthcare infrastructure. By probing the socio-technical facets of healthcare security infringements, this article advocates for an integrated, dynamic, and holistic approach to healthcare data security. The findings underscore the imperative of augmenting technological defenses while concurrently elevating human conduct and institutional ethos, thereby cultivating a robust and impervious healthcare data management environment."}
{"title":"Bubble Economics","authors":["Tomohiro Hirano","Alexis Akira Toda"],"raw_abstract":"This article provides a self-contained overview of the theory of rational\nasset price bubbles so that non-experts including PhD students can follow the\nargument. We cover topics from basic definitions, properties, and classical\nresults to frontier research, with an emphasis on bubbles attached to real\nassets such as stocks, housing, and land. The main message is that bubbles\nattached to real assets are fundamentally nonstationary phenomena related to\nunbalanced growth. We present a bare-bones model and draw the important insight\nthat asset pricing implications are markedly different between balanced growth\nand unbalanced growth.","publication_date":1699318496,"paper_link":"http://arxiv.org/pdf/2311.03638v1","categories":["Economics","Quantitative Finance"],"abstract":"This article provides a self-contained overview of the theory of rational asset price bubbles so that non-experts including PhD students can follow the argument. We cover topics from basic definitions, properties, and classical results to frontier research, with an emphasis on bubbles attached to real assets such as stocks, housing, and land. The main message is that bubbles attached to real assets are fundamentally nonstationary phenomena related to unbalanced growth. We present a bare-bones model and draw the important insight that asset pricing implications are markedly different between balanced growth and unbalanced growth."}
{"title":"Brief for the Canada House of Commons Study on the Implications of Artificial Intelligence Technologies for the Canadian Labor Force: Generative Artificial Intelligence Shatters Models of AI and Labor","authors":["Morgan R. Frank"],"raw_abstract":"Exciting advances in generative artificial intelligence (AI) have sparked\nconcern for jobs, education, productivity, and the future of work. As with past\ntechnologies, generative AI may not lead to mass unemployment. But, unlike past\ntechnologies, generative AI is creative, cognitive, and potentially ubiquitous\nwhich makes the usual assumptions of automation predictions ill-suited for\ntoday. Existing projections suggest that generative AI will impact workers in\noccupations that were previously considered immune to automation. As AI's full\nset of capabilities and applications emerge, policy makers should promote\nworkers' career adaptability. This goal requires improved data on job\nseparations and unemployment by locality and job titles in order to identify\nearly-indicators for the workers facing labor disruption. Further, prudent\npolicy should incentivize education programs to accommodate learning with AI as\na tool while preparing students for the demands of the future of work.","publication_date":1699311504,"paper_link":"http://arxiv.org/pdf/2311.03595v1","categories":["Economics","Quantitative Finance"],"abstract":"Exciting advances in generative artificial intelligence (AI) have sparked concern for jobs, education, productivity, and the future of work. As with past technologies, generative AI may not lead to mass unemployment. But, unlike past technologies, generative AI is creative, cognitive, and potentially ubiquitous which makes the usual assumptions of automation predictions ill-suited for today. Existing projections suggest that generative AI will impact workers in occupations that were previously considered immune to automation. As AI's full set of capabilities and applications emerge, policy makers should promote workers' career adaptability. This goal requires improved data on job separations and unemployment by locality and job titles in order to identify early-indicators for the workers facing labor disruption. Further, prudent policy should incentivize education programs to accommodate learning with AI as a tool while preparing students for the demands of the future of work."}
{"title":"A necessary and sufficient condition for the existence of chaotic dynamics in a neoclassical growth model with a pollution effect","authors":["Tomohiro Uchiyama"],"raw_abstract":"In this paper, we study a neoclassical growth model with a (productivity\ninhibiting) pollution effect. In particular, we obtain a necessary and\nsufficient condition for the existence of a topological chaos. We investigate\nhow the condition changes as the strength of the pollution effect changes. This\nis a new application of a recent result characterising the existence of a\ntopological chaos for a unimodal interval map by Deng, Khan, Mitra (2022).","publication_date":1699311297,"paper_link":"http://arxiv.org/pdf/2311.03594v1","categories":["Economics","Quantitative Finance"],"abstract":"In this paper, we study a neoclassical growth model with a (productivity inhibiting) pollution effect. In particular, we obtain a necessary and sufficient condition for the existence of a topological chaos. We investigate how the condition changes as the strength of the pollution effect changes. This is a new application of a recent result characterising the existence of a topological chaos for a unimodal interval map by Deng, Khan, Mitra (2022)."}
{"title":"Model Predictive Control of Diesel Engine Emissions Based on Neural Network Modeling","authors":["Jiadi Zhang","Xiao Li","Ilya Kolmanovsky","Munechika Tsutsumi","Hayato Nakada"],"raw_abstract":"This paper addresses the control of diesel engine nitrogen oxides (NOx) and\nSoot emissions through the application of Model Predictive Control (MPC). The\ndevelopments described in the paper are based on a high-fidelity model of the\nengine airpath and torque response in GT-Power, which is extended with a\nfeedforward neural network (FNN)-based model of engine out (feedgas) emissions\nidentified from experimental engine data to enable the controller co-simulation\nand performance verification. A Recurrent Neural Network (RNN) is then\nidentified for use as a prediction model in the implementation of a nonlinear\neconomic MPC that adjusts intake manifold pressure and EGR rate set-points to\nthe inner loop airpath controller as well as the engine fueling rate. Based on\nGT-Power engine model and FNN emissions model, the closed-loop simulations of\nthe control system and the plant model, over different driving cycles,\ndemonstrate the capability to shape engine out emissions response by adjusting\nweights and constraints in economic MPC formulation.","publication_date":1699307211,"paper_link":"http://arxiv.org/pdf/2311.03555v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper addresses the control of diesel engine nitrogen oxides (NOx) and Soot emissions through the application of Model Predictive Control (MPC). The developments described in the paper are based on a high-fidelity model of the engine airpath and torque response in GT-Power, which is extended with a feedforward neural network (FNN)-based model of engine out (feedgas) emissions identified from experimental engine data to enable the controller co-simulation and performance verification. A Recurrent Neural Network (RNN) is then identified for use as a prediction model in the implementation of a nonlinear economic MPC that adjusts intake manifold pressure and EGR rate set-points to the inner loop airpath controller as well as the engine fueling rate. Based on GT-Power engine model and FNN emissions model, the closed-loop simulations of the control system and the plant model, over different driving cycles, demonstrate the capability to shape engine out emissions response by adjusting weights and constraints in economic MPC formulation."}
{"title":"Modeling and Control of Diesel Engine Emissions using Multi-layer Neural Networks and Economic Model Predictive Control","authors":["Jiadi Zhang","Xiao Li","Mohammad Reza Amini","Ilya Kolmanovsky","Munechika Tsutsumi","Hayato Nakada"],"raw_abstract":"This paper presents the results of developing a multi-layer Neural Network\n(NN) to represent diesel engine emissions and integrating this NN into control\ndesign. Firstly, a NN is trained and validated to simultaneously predict oxides\nof nitrogen (N Ox) and Soot using both transient and steady-state data. Based\non the input-output correlation analysis, inputs to NN with the highest\ninfluence on the emissions are selected while keeping the NN structure simple.\nSecondly, a co-simulation framework is implemented to integrate the NN\nemissions model with a model of a diesel engine airpath system built in\nGT-Power and used to identify a low-order linear parameter-varying (LPV) model\nfor emissions prediction. Finally, an economic supervisory model predictive\ncontroller (MPC) is developed using the LPV emissions model to adjust setpoints\nto an inner-loop airpath tracking MPC. Simulation results are reported\nillustrating the capability of the resulting controller to reduce N Ox, meet\nthe target Soot limit, and track the adjusted intake manifold pressure and\nexhaust gas recirculation (EGR) rate targets.","publication_date":1699306886,"paper_link":"http://arxiv.org/pdf/2311.03552v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper presents the results of developing a multi-layer Neural Network (NN) to represent diesel engine emissions and integrating this NN into control design. Firstly, a NN is trained and validated to simultaneously predict oxides of nitrogen (N Ox) and Soot using both transient and steady-state data. Based on the input-output correlation analysis, inputs to NN with the highest influence on the emissions are selected while keeping the NN structure simple. Secondly, a co-simulation framework is implemented to integrate the NN emissions model with a model of a diesel engine airpath system built in GT-Power and used to identify a low-order linear parameter-varying (LPV) model for emissions prediction. Finally, an economic supervisory model predictive controller (MPC) is developed using the LPV emissions model to adjust setpoints to an inner-loop airpath tracking MPC. Simulation results are reported illustrating the capability of the resulting controller to reduce N Ox, meet the target Soot limit, and track the adjusted intake manifold pressure and exhaust gas recirculation (EGR) rate targets."}
{"title":"Optimizing Climate Policy through C-ROADS and En-ROADS Analysis","authors":["Iveena Mukherjee"],"raw_abstract":"With the onset of climate change and the increasing need for effective\npolicies, a multilateral approach is needed to make an impact on the growing\nthreats facing the environment. Through the use of systematic analysis by way\nof C-ROADS and En-ROADS, numerous scenarios have been simulated to shed light\non the most imperative policy factors to mitigate climate change. Within\nC-ROADS, it was determined that the impacts of the shrinking ice-albedo effect\non global temperatures is significant, however differential sea ice melting\nbetween the poles may not impact human dwellings, as all regions are impacted\nby sea ice melt. Flood risks are also becoming more imminent, specifically in\nhigh population density areas. In terms of afforestation, China is the emerging\nleader, and if other countries follow suit, this can incur substantial\ndividends. Upon conducting a comprehensive analysis of global trends through\nEn-ROADS, intriguing patterns appear between the length of a policy initiative,\nand its effectiveness. Quick policies with gradual increases in taxation proved\nsuccessful. Government intervention was also favorable, however an optimized\nmodel is presented, with moderate subsidization of renewable energy. Through\nthis systematic analysis of assumptions and policy for effective climate change\nmitigation efforts, an optimized, economically-favorable solution arises.","publication_date":1699306258,"paper_link":"http://arxiv.org/pdf/2311.03546v1","categories":["Economics","Quantitative Finance"],"abstract":"With the onset of climate change and the increasing need for effective policies, a multilateral approach is needed to make an impact on the growing threats facing the environment. Through the use of systematic analysis by way of C-ROADS and En-ROADS, numerous scenarios have been simulated to shed light on the most imperative policy factors to mitigate climate change. Within C-ROADS, it was determined that the impacts of the shrinking ice-albedo effect on global temperatures is significant, however differential sea ice melting between the poles may not impact human dwellings, as all regions are impacted by sea ice melt. Flood risks are also becoming more imminent, specifically in high population density areas. In terms of afforestation, China is the emerging leader, and if other countries follow suit, this can incur substantial dividends. Upon conducting a comprehensive analysis of global trends through En-ROADS, intriguing patterns appear between the length of a policy initiative, and its effectiveness. Quick policies with gradual increases in taxation proved successful. Government intervention was also favorable, however an optimized model is presented, with moderate subsidization of renewable energy. Through this systematic analysis of assumptions and policy for effective climate change mitigation efforts, an optimized, economically-favorable solution arises."}
{"title":"Understanding the Impact of Seasonal Climate Change on Canada's Economy by Region and Sector","authors":["Shiyu He","Trang Bui","Yuying Huang","Wenling Zhang","Jie Jian","Samuel W. K. Wong","Tony S. Wirjanto"],"raw_abstract":"To assess the impact of climate change on the Canadian economy, we\ninvestigate and model the relationship between seasonal climate variables and\neconomic growth across provinces and economic sectors. We further provide\nprojections of climate change impacts up to the year 2050, taking into account\nthe diverse climate change patterns and economic conditions across Canada. Our\nresults indicate that rising Fall temperature anomalies have a notable adverse\nimpact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba\nare anticipated to experience the most substantial declines, whereas British\nColumbia and the Maritime provinces will be less impacted. Industry-wide,\nMining is projected to see the greatest benefits, while Agriculture and\nManufacturing are projected to have the most significant downturns. The\ndisparities of climate change effects between provinces and industries\nhighlight the need for governments to tailor their policies accordingly, and\noffer targeted assistance to regions and industries that are particularly\nvulnerable in the face of climate change. Targeted approaches to climate change\nmitigation are likely to be more effective than one-size-fits-all policies for\nthe whole economy.","publication_date":1699301527,"paper_link":"http://arxiv.org/pdf/2311.03497v1","categories":["Statistics"],"abstract":"To assess the impact of climate change on the Canadian economy, we investigate and model the relationship between seasonal climate variables and economic growth across provinces and economic sectors. We further provide projections of climate change impacts up to the year 2050, taking into account the diverse climate change patterns and economic conditions across Canada. Our results indicate that rising Fall temperature anomalies have a notable adverse impact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba are anticipated to experience the most substantial declines, whereas British Columbia and the Maritime provinces will be less impacted. Industry-wide, Mining is projected to see the greatest benefits, while Agriculture and Manufacturing are projected to have the most significant downturns. The disparities of climate change effects between provinces and industries highlight the need for governments to tailor their policies accordingly, and offer targeted assistance to regions and industries that are particularly vulnerable in the face of climate change. Targeted approaches to climate change mitigation are likely to be more effective than one-size-fits-all policies for the whole economy."}
{"title":"FLOGA: A machine learning ready dataset, a benchmark and a novel deep learning model for burnt area mapping with Sentinel-2","authors":["Maria Sdraka","Alkinoos Dimakos","Alexandros Malounis","Zisoula Ntasiou","Konstantinos Karantzalos","Dimitrios Michail","Ioannis Papoutsis"],"raw_abstract":"Over the last decade there has been an increasing frequency and intensity of\nwildfires across the globe, posing significant threats to human and animal\nlives, ecosystems, and socio-economic stability. Therefore urgent action is\nrequired to mitigate their devastating impact and safeguard Earth's natural\nresources. Robust Machine Learning methods combined with the abundance of\nhigh-resolution satellite imagery can provide accurate and timely mappings of\nthe affected area in order to assess the scale of the event, identify the\nimpacted assets and prioritize and allocate resources effectively for the\nproper restoration of the damaged region. In this work, we create and introduce\na machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations\nfor the Greek Area). This dataset is unique as it comprises of satellite\nimagery acquired before and after a wildfire event, it contains information\nfrom Sentinel-2 and MODIS modalities with variable spatial and spectral\nresolution, and contains a large number of events where the corresponding burnt\narea ground truth has been annotated by domain experts. FLOGA covers the wider\nregion of Greece, which is characterized by a Mediterranean landscape and\nclimatic conditions. We use FLOGA to provide a thorough comparison of multiple\nMachine Learning and Deep Learning algorithms for the automatic extraction of\nburnt areas, approached as a change detection task. We also compare the results\nto those obtained using standard specialized spectral indices for burnt area\nmapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our\nbenchmark results demonstrate the efficacy of the proposed technique in the\nautomatic extraction of burnt areas, outperforming all other methods in terms\nof accuracy and robustness. Our dataset and code are publicly available at:\nhttps://github.com/Orion-AI-Lab/FLOGA.","publication_date":1699296125,"paper_link":"http://arxiv.org/pdf/2311.03339v1","categories":["Economics"],"abstract":"Over the last decade there has been an increasing frequency and intensity of wildfires across the globe, posing significant threats to human and animal lives, ecosystems, and socio-economic stability. Therefore urgent action is required to mitigate their devastating impact and safeguard Earth's natural resources. Robust Machine Learning methods combined with the abundance of high-resolution satellite imagery can provide accurate and timely mappings of the affected area in order to assess the scale of the event, identify the impacted assets and prioritize and allocate resources effectively for the proper restoration of the damaged region. In this work, we create and introduce a machine-learning ready dataset we name FLOGA (Forest wiLdfire Observations for the Greek Area). This dataset is unique as it comprises of satellite imagery acquired before and after a wildfire event, it contains information from Sentinel-2 and MODIS modalities with variable spatial and spectral resolution, and contains a large number of events where the corresponding burnt area ground truth has been annotated by domain experts. FLOGA covers the wider region of Greece, which is characterized by a Mediterranean landscape and climatic conditions. We use FLOGA to provide a thorough comparison of multiple Machine Learning and Deep Learning algorithms for the automatic extraction of burnt areas, approached as a change detection task. We also compare the results to those obtained using standard specialized spectral indices for burnt area mapping. Finally, we propose a novel Deep Learning model, namely BAM-CD. Our benchmark results demonstrate the efficacy of the proposed technique in the automatic extraction of burnt areas, outperforming all other methods in terms of accuracy and robustness. Our dataset and code are publicly available at: https://github.com/Orion-AI-Lab/FLOGA."}
{"title":"Faster Run-to-Run Feedforward Control of Electromechanical Switching Devices: a Sensitivity-Based Approach","authors":["Edgar Ramirez-Laboreo","Eduardo Moya-Lasheras","Eloy Serrano-Seco"],"raw_abstract":"Electromechanical switching devices, such as solenoid valves, contactors, and\nrelays, suffer from undesirable phenomena like clicking, mechanical wear, and\ncontact bounce. Despite that, they are still widely used in industry due to\ntheir various economic and technical advantages. This has encouraged the\ndevelopment of controllers aimed at reducing the collisions that occur at the\nend of the switching operations. One of the most successful approaches has been\nthe use of iterative techniques. However, these algorithms typically require a\nlarge number of operations to converge, which is definitely a clear drawback.\nThis paper presents a strategy to improve the convergence rate of such\ncontrollers. Our proposal, which is based on the sensitivity of the control law\nwith respect to the parameters, assumes that the performance of the system is\nmore heavily affected by some parameters than others. Thus, by avoiding\nmovements in the directions that have less impact, the search algorithm is\nexpected to drive the system to near-optimal behaviors using fewer operations.\nResults obtained by simulation show significant improvement in the convergence\nrate of a state-of-the-art run-to-run feedforward controller, which\ndemonstrates the high potential of the proposal.","publication_date":1699292860,"paper_link":"http://arxiv.org/pdf/2311.03300v2","categories":["Electrical Engineering and Systems Science"],"abstract":"Electromechanical switching devices, such as solenoid valves, contactors, and relays, suffer from undesirable phenomena like clicking, mechanical wear, and contact bounce. Despite that, they are still widely used in industry due to their various economic and technical advantages. This has encouraged the development of controllers aimed at reducing the collisions that occur at the end of the switching operations. One of the most successful approaches has been the use of iterative techniques. However, these algorithms typically require a large number of operations to converge, which is definitely a clear drawback. This paper presents a strategy to improve the convergence rate of such controllers. Our proposal, which is based on the sensitivity of the control law with respect to the parameters, assumes that the performance of the system is more heavily affected by some parameters than others. Thus, by avoiding movements in the directions that have less impact, the search algorithm is expected to drive the system to near-optimal behaviors using fewer operations. Results obtained by simulation show significant improvement in the convergence rate of a state-of-the-art run-to-run feedforward controller, which demonstrates the high potential of the proposal."}
{"title":"Some coordination problems are harder than others","authors":["Argyrios Deligkas","Eduard Eiben","Gregory Gutin","Philip R. Neary","Anders Yeo"],"raw_abstract":"In order to coordinate players in a game must first identify a target pattern\nof behaviour. In this paper we investigate the difficulty of identifying\nprominent outcomes in two kinds of binary action coordination problems in\nsocial networks: pure coordination games and anti-coordination games. For both\nenvironments, we determine the computational complexity of finding a strategy\nprofile that (i) maximises welfare, (ii) maximises welfare subject to being an\nequilibrium, and (iii) maximises potential. We show that the complexity of\nthese objectives can vary with the type of coordination problem. Objectives (i)\nand (iii) are tractable problems in pure coordination games, but for\nanti-coordination games are NP-hard. Objective (ii), finding the best Nash\nequilibrium, is NP-hard for both. Our results support the idea that\nenvironments in which actions are strategic complements (e.g., technology\nadoption) facilitate successful coordination more readily than those in which\nactions are strategic substitutes (e.g., public good provision).","publication_date":1699284796,"paper_link":"http://arxiv.org/pdf/2311.03195v2","categories":["Economics"],"abstract":"In order to coordinate players in a game must first identify a target pattern of behaviour. In this paper we investigate the difficulty of identifying prominent outcomes in two kinds of binary action coordination problems in social networks: pure coordination games and anti-coordination games. For both environments, we determine the computational complexity of finding a strategy profile that (i) maximises welfare, (ii) maximises welfare subject to being an equilibrium, and (iii) maximises potential. We show that the complexity of these objectives can vary with the type of coordination problem. Objectives (i) and (iii) are tractable problems in pure coordination games, but for anti-coordination games are NP-hard. Objective (ii), finding the best Nash equilibrium, is NP-hard for both. Our results support the idea that environments in which actions are strategic complements (e.g., technology adoption) facilitate successful coordination more readily than those in which actions are strategic substitutes (e.g., public good provision)."}
{"title":"Artist Area Versus Numerical Cluster","authors":["Patrice Ballester"],"raw_abstract":"Artist area versus numerical cluster. Between land management and production\nof new creative space: the Poblenou 22@ in Barcelona. Poblenou (or the new\nvillage in Catalan) is a district of Barcelona known for over ten years urban\nregeneration based on the concept of the new economics of ICT (Information and\nCommunications Technology) as a source of new attractive image for the location\nof international companies with a creative project called 22@ district. A\ntransition produced a territorial gentrification thwarted by artists,\nintellectuals and local people adopting as the emblem of their action, the\nrescue of a flagship industrial architecture: Can Ricart, home of many artists.\nSince, urban planning has been redesigned in its margins and townscape of the\narea tends to reflect the consensus between politicians, multinational\ncompanies and artists survivors of this high-pressure real estate. An\nassessment of the financial risks is considered by the territorial upheavals\nand the financial sums committed to rebuilding a neighborhood through the\nchimerical model of \"Silicon Valley\". These artists became referents, while\ntrying to fit into the theme of the new creative economy of the territory\nthrough alliances, orders and cooperation shy but regular. The new european\nSoho research yet an identity and a certain lifestyle based on the new balance\nof power highlighted by the artists in their criticism, actions and\ncommunications: cohabitation between old and new residents ; the choice between\ncontemporary architecture and heritage protection; the artist as creator,\nassistant or complicit in the so-called creative class.","publication_date":1699264140,"paper_link":"http://arxiv.org/pdf/2311.03416v1","categories":["Physics"],"abstract":"Artist area versus numerical cluster. Between land management and production of new creative space: the Poblenou 22@ in Barcelona. Poblenou (or the new village in Catalan) is a district of Barcelona known for over ten years urban regeneration based on the concept of the new economics of ICT (Information and Communications Technology) as a source of new attractive image for the location of international companies with a creative project called 22@ district. A transition produced a territorial gentrification thwarted by artists, intellectuals and local people adopting as the emblem of their action, the rescue of a flagship industrial architecture: Can Ricart, home of many artists. Since, urban planning has been redesigned in its margins and townscape of the area tends to reflect the consensus between politicians, multinational companies and artists survivors of this high-pressure real estate. An assessment of the financial risks is considered by the territorial upheavals and the financial sums committed to rebuilding a neighborhood through the chimerical model of \"Silicon Valley\". These artists became referents, while trying to fit into the theme of the new creative economy of the territory through alliances, orders and cooperation shy but regular. The new european Soho research yet an identity and a certain lifestyle based on the new balance of power highlighted by the artists in their criticism, actions and communications: cohabitation between old and new residents ; the choice between contemporary architecture and heritage protection; the artist as creator, assistant or complicit in the so-called creative class."}
{"title":"Imitation Learning based Alternative Multi-Agent Proximal Policy Optimization for Well-Formed Swarm-Oriented Pursuit Avoidance","authors":["Sizhao Li","Yuming Xiang","Rongpeng Li","Zhifeng Zhao","Honggang Zhang"],"raw_abstract":"Multi-Robot System (MRS) has garnered widespread research interest and\nfostered tremendous interesting applications, especially in cooperative control\nfields. Yet little light has been shed on the compound ability of formation,\nmonitoring and defence in decentralized large-scale MRS for pursuit avoidance,\nwhich puts stringent requirements on the capability of coordination and\nadaptability. In this paper, we put forward a decentralized Imitation learning\nbased Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm\nto provide a flexible and communication-economic solution to execute the\npursuit avoidance task in well-formed swarm. In particular, a\npolicy-distillation based MAPPO executor is firstly devised to capably\naccomplish and swiftly switch between multiple formations in a centralized\nmanner. Furthermore, we utilize imitation learning to decentralize the\nformation controller, so as to reduce the communication overheads and enhance\nthe scalability. Afterwards, alternative training is leveraged to compensate\nthe performance loss incurred by decentralization. The simulation results\nvalidate the effectiveness of IA-MAPPO and extensive ablation experiments\nfurther show the performance comparable to a centralized solution with\nsignificant decrease in communication overheads.","publication_date":1699253896,"paper_link":"http://arxiv.org/pdf/2311.02912v1","categories":["Economics"],"abstract":"Multi-Robot System (MRS) has garnered widespread research interest and fostered tremendous interesting applications, especially in cooperative control fields. Yet little light has been shed on the compound ability of formation, monitoring and defence in decentralized large-scale MRS for pursuit avoidance, which puts stringent requirements on the capability of coordination and adaptability. In this paper, we put forward a decentralized Imitation learning based Alternative Multi-Agent Proximal Policy Optimization (IA-MAPPO) algorithm to provide a flexible and communication-economic solution to execute the pursuit avoidance task in well-formed swarm. In particular, a policy-distillation based MAPPO executor is firstly devised to capably accomplish and swiftly switch between multiple formations in a centralized manner. Furthermore, we utilize imitation learning to decentralize the formation controller, so as to reduce the communication overheads and enhance the scalability. Afterwards, alternative training is leveraged to compensate the performance loss incurred by decentralization. The simulation results validate the effectiveness of IA-MAPPO and extensive ablation experiments further show the performance comparable to a centralized solution with significant decrease in communication overheads."}
{"title":"Persuasion and Matching: Optimal Productive Transport","authors":["Anton Kolotilin","Roberto Corrao","Alexander Wolitzky"],"raw_abstract":"We consider general Bayesian persuasion problems where the receiver's utility\nis single-peaked in a one-dimensional action. We show that a signal that pools\nat most two states in each realization is always optimal, and that such\npairwise signals are the only solutions under a non-singularity condition (the\ntwist condition). Our core results provide conditions under which riskier\nprospects induce higher or lower actions, so that the induced action is\nsingle-dipped or single-peaked on each set of nested prospects. We also provide\nconditions for the optimality of either full disclosure or negative assortative\ndisclosure, where all prospects are nested. Methodologically, our results rely\non novel duality and complementary slackness theorems. Our analysis extends to\na general problem of assigning one-dimensional inputs to productive units,\nwhich we call optimal productive transport. This problem covers additional\napplications including club economies (assigning workers to firms, or students\nto schools), robust option pricing (assigning future asset prices to price\ndistributions), and partisan gerrymandering (assigning voters to districts).","publication_date":1699249599,"paper_link":"http://arxiv.org/pdf/2311.02889v1","categories":["Economics"],"abstract":"We consider general Bayesian persuasion problems where the receiver's utility is single-peaked in a one-dimensional action. We show that a signal that pools at most two states in each realization is always optimal, and that such pairwise signals are the only solutions under a non-singularity condition (the twist condition). Our core results provide conditions under which riskier prospects induce higher or lower actions, so that the induced action is single-dipped or single-peaked on each set of nested prospects. We also provide conditions for the optimality of either full disclosure or negative assortative disclosure, where all prospects are nested. Methodologically, our results rely on novel duality and complementary slackness theorems. Our analysis extends to a general problem of assigning one-dimensional inputs to productive units, which we call optimal productive transport. This problem covers additional applications including club economies (assigning workers to firms, or students to schools), robust option pricing (assigning future asset prices to price distributions), and partisan gerrymandering (assigning voters to districts)."}
{"title":"Institutional Screening and the Sustainability of Conditional Cooperation","authors":["Ethan Holdahl","Jiabin Wu"],"raw_abstract":"This paper studies a preference evolution model in which a population of\nagents are matched to play a sequential prisoner's dilemma in an incomplete\ninformation environment. An institution can design an incentive-compatible\nscreening scheme, such as a special zone that requires an entry fee, or a\ncostly label for purchase, to segregate the conditional cooperators from the\nnon-cooperators. We show that institutional intervention of this sort can help\nthe conditional cooperators to prevail when the psychological benefit of\ncooperating for them is sufficiently strong and the membership of the special\nzone or the label is inheritable with a sufficiently high probability.","publication_date":1699233333,"paper_link":"http://arxiv.org/pdf/2311.02813v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper studies a preference evolution model in which a population of agents are matched to play a sequential prisoner's dilemma in an incomplete information environment. An institution can design an incentive-compatible screening scheme, such as a special zone that requires an entry fee, or a costly label for purchase, to segregate the conditional cooperators from the non-cooperators. We show that institutional intervention of this sort can help the conditional cooperators to prevail when the psychological benefit of cooperating for them is sufficiently strong and the membership of the special zone or the label is inheritable with a sufficiently high probability."}
{"title":"Run-to-Run Adaptive Nonlinear Feedforward Control of Electromechanical Switching Devices","authors":["Eduardo Moya-Lasheras","Edgar Ramirez-Laboreo","Eloy Serrano-Seco"],"raw_abstract":"Feedforward control can greatly improve the response time and control\naccuracy of any mechatronic system. However, in order to compensate for the\neffects of modeling errors or disturbances, it is imperative that this type of\ncontrol works in conjunction with some form of feedback. In this paper, we\npresent a new adaptive feedforward control scheme for electromechanical systems\nin which real-time measurements or estimates of the position and its\nderivatives are not technically or economically feasible. This is the case, for\nexample, of commercial electromechanical switching devices such as solenoid\nactuators. Our proposal consists of two blocks: on the one hand, a feedforward\ncontroller based on differential flatness theory; on the other, an iterative\nadaptation law that exploits the repetitive operation of these devices to\nmodify the controller parameters cycle by cycle. As shown, this law can be fed\nwith any available measurement of the system, with the only requirement that it\ncan be processed and converted into an indicator of the performance of any\ngiven operation. Simulated and experimental results show that our proposal is\neffective in dealing with a long-standing control problem in electromechanics:\nthe soft-landing control of electromechanical switching devices.","publication_date":1699216087,"paper_link":"http://arxiv.org/pdf/2311.02756v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Feedforward control can greatly improve the response time and control accuracy of any mechatronic system. However, in order to compensate for the effects of modeling errors or disturbances, it is imperative that this type of control works in conjunction with some form of feedback. In this paper, we present a new adaptive feedforward control scheme for electromechanical systems in which real-time measurements or estimates of the position and its derivatives are not technically or economically feasible. This is the case, for example, of commercial electromechanical switching devices such as solenoid actuators. Our proposal consists of two blocks: on the one hand, a feedforward controller based on differential flatness theory; on the other, an iterative adaptation law that exploits the repetitive operation of these devices to modify the controller parameters cycle by cycle. As shown, this law can be fed with any available measurement of the system, with the only requirement that it can be processed and converted into an indicator of the performance of any given operation. Simulated and experimental results show that our proposal is effective in dealing with a long-standing control problem in electromechanics: the soft-landing control of electromechanical switching devices."}
{"title":"Contract Design With Safety Inspections","authors":["Alireza Fallah","Michael I. Jordan"],"raw_abstract":"We study the role of regulatory inspections in a contract design problem in\nwhich a principal interacts separately with multiple agents. Each agent's\nhidden action includes a dimension that determines whether they undertake an\nextra costly step to adhere to safety protocols. The principal's objective is\nto use payments combined with a limited budget for random inspections to\nincentivize agents towards safety-compliant actions that maximize the\nprincipal's utility. We first focus on the single-agent setting with linear\ncontracts and present an efficient algorithm that characterizes the optimal\nlinear contract, which includes both payment and random inspection. We further\ninvestigate how the optimal contract changes as the inspection cost or the cost\nof adhering to safety protocols vary. Notably, we demonstrate that the agent's\ncompensation increases if either of these costs escalates. However, while the\nprobability of inspection decreases with rising inspection costs, it\ndemonstrates nonmonotonic behavior as a function of the safety action costs.\nLastly, we explore the multi-agent setting, where the principal's challenge is\nto determine the best distribution of inspection budgets among all agents. We\npropose an efficient approach based on dynamic programming to find an\napproximately optimal allocation of inspection budget across contracts. We also\ndesign a random sequential scheme to determine the inspector's assignments,\nensuring each agent is inspected at most once and at the desired probability.\nFinally, we present a case study illustrating that a mere difference in the\ncost of inspection across various agents can drive the principal's decision to\nforego inspecting a significant fraction of them, concentrating its entire\nbudget on those that are less costly to inspect.","publication_date":1699147423,"paper_link":"http://arxiv.org/pdf/2311.02537v1","categories":["Economics"],"abstract":"We study the role of regulatory inspections in a contract design problem in which a principal interacts separately with multiple agents. Each agent's hidden action includes a dimension that determines whether they undertake an extra costly step to adhere to safety protocols. The principal's objective is to use payments combined with a limited budget for random inspections to incentivize agents towards safety-compliant actions that maximize the principal's utility. We first focus on the single-agent setting with linear contracts and present an efficient algorithm that characterizes the optimal linear contract, which includes both payment and random inspection. We further investigate how the optimal contract changes as the inspection cost or the cost of adhering to safety protocols vary. Notably, we demonstrate that the agent's compensation increases if either of these costs escalates. However, while the probability of inspection decreases with rising inspection costs, it demonstrates nonmonotonic behavior as a function of the safety action costs. Lastly, we explore the multi-agent setting, where the principal's challenge is to determine the best distribution of inspection budgets among all agents. We propose an efficient approach based on dynamic programming to find an approximately optimal allocation of inspection budget across contracts. We also design a random sequential scheme to determine the inspector's assignments, ensuring each agent is inspected at most once and at the desired probability. Finally, we present a case study illustrating that a mere difference in the cost of inspection across various agents can drive the principal's decision to forego inspecting a significant fraction of them, concentrating its entire budget on those that are less costly to inspect."}
{"title":"The contribution of US broadband infrastructure subsidy and investment programs to GDP using input-output modeling","authors":["Matthew Sprintson","Edward Oughton"],"raw_abstract":"More than one-fifth of the US population does not subscribe to a fixed\nbroadband service despite being a recognized merit good. For example, less than\n4% of citizens earning more than US \\$70k annually do not have broadband,\ncompared to 26% of those earning below US \\$20k annually. To address this, the\nBiden Administration has undertaken one of the largest broadband investment\nprograms ever via The Bipartisan Infrastructure Law, with the aim of addressing\nthis disparity and expanding broadband connectivity to all citizens. Proponents\nstate this will reduce the US digital divide once-and-for-all. However,\ndetractors say the program leads to unprecedented borrowing at a late stage of\nthe economic cycle, leaving little fiscal headroom. Subsequently, in this\npaper, we examine broadband availability, adoption, and need and then construct\nan input-output model to explore the macroeconomic impacts of broadband\nspending in Gross Domestic Product (GDP) terms. Finally, we quantify\ninter-sectoral macroeconomic supply chain linkages from this investment. The\nresults indicate that federal broadband investment of US \\$42 billion has the\npotential to increase GDP by up to US \\$216 billion, equating to 0.2% of annual\nUS GDP over the next five years, with an estimated Keynesian investment\nmultiplier of 2.89. To our knowledge, we contribute the first economic impact\nassessment of the US Bipartisan Infrastructure Law to the literature.","publication_date":1699111315,"paper_link":"http://arxiv.org/pdf/2311.02431v1","categories":["Economics","Quantitative Finance"],"abstract":"More than one-fifth of the US population does not subscribe to a fixed broadband service despite being a recognized merit good. For example, less than 4% of citizens earning more than US \\__FORMULA__20k annually. To address this, the Biden Administration has undertaken one of the largest broadband investment programs ever via The Bipartisan Infrastructure Law, with the aim of addressing this disparity and expanding broadband connectivity to all citizens. Proponents state this will reduce the US digital divide once-and-for-all. However, detractors say the program leads to unprecedented borrowing at a late stage of the economic cycle, leaving little fiscal headroom. Subsequently, in this paper, we examine broadband availability, adoption, and need and then construct an input-output model to explore the macroeconomic impacts of broadband spending in Gross Domestic Product (GDP) terms. Finally, we quantify inter-sectoral macroeconomic supply chain linkages from this investment. The results indicate that federal broadband investment of US \\__FORMULA__216 billion, equating to 0.2% of annual US GDP over the next five years, with an estimated Keynesian investment multiplier of 2.89. To our knowledge, we contribute the first economic impact assessment of the US Bipartisan Infrastructure Law to the literature."}
{"title":"Beyond the Screen: Safeguarding Mental Health in the Digital Workplace Through Organizational Commitment and Ethical Environment","authors":["Ali Bai","Morteza Vahedian"],"raw_abstract":"This research explores the intricate relationship between organizational\ncommitment and nomophobia, illuminating the mediating influence of the ethical\nenvironment. Utilizing Meyer and Allen's three-component model, the study finds\na significant inverse correlation between organizational commitment and\nnomophobia, highlighting how strong organizational ties can alleviate the\nanxiety of digital disconnection. The ethical environment further emerges as a\nsignificant mediator, indicating its dual role in promoting ethical behavior\nand mitigating nomophobia's psychological effects.\n  The study's theoretical advancement lies in its empirical evidence on the\nseldom-explored nexus between organizational commitment and technology-induced\nstress. By integrating organizational ethics and technological impact, the\nresearch offers a novel perspective on managing digital dependence in the\nworkplace. From a practical standpoint, this study serves as a catalyst for\norganizational leaders to reinforce affective and normative commitment, thereby\nreducing nomophobia. The findings underscore the necessity of ethical\nleadership and comprehensive ethical policies as foundations for employee\nwell-being in the digital age.\n  Conclusively, this study delineates the protective role of organizational\ncommitment and the significance of ethical environments, guiding organizations\nto foster cultures that balance technological efficiency with employee welfare.\nAs a contribution to both academic discourse and practical application, it\nemphasizes the importance of nurturing a supportive and ethically sound\nworkplace in an era of pervasive digital integration.","publication_date":1699109161,"paper_link":"http://arxiv.org/pdf/2311.02422v1","categories":["Economics","Quantitative Finance"],"abstract":"This research explores the intricate relationship between organizational commitment and nomophobia, illuminating the mediating influence of the ethical environment. Utilizing Meyer and Allen's three-component model, the study finds a significant inverse correlation between organizational commitment and nomophobia, highlighting how strong organizational ties can alleviate the anxiety of digital disconnection. The ethical environment further emerges as a significant mediator, indicating its dual role in promoting ethical behavior and mitigating nomophobia's psychological effects.   The study's theoretical advancement lies in its empirical evidence on the seldom-explored nexus between organizational commitment and technology-induced stress. By integrating organizational ethics and technological impact, the research offers a novel perspective on managing digital dependence in the workplace. From a practical standpoint, this study serves as a catalyst for organizational leaders to reinforce affective and normative commitment, thereby reducing nomophobia. The findings underscore the necessity of ethical leadership and comprehensive ethical policies as foundations for employee well-being in the digital age.   Conclusively, this study delineates the protective role of organizational commitment and the significance of ethical environments, guiding organizations to foster cultures that balance technological efficiency with employee welfare. As a contribution to both academic discourse and practical application, it emphasizes the importance of nurturing a supportive and ethically sound workplace in an era of pervasive digital integration."}
{"title":"Efficient Scenario Generation for Chance-constrained Economic Dispatch Considering Ambient Wind Conditions","authors":["Qian Zhang","Apurv Shukla","Le Xie"],"raw_abstract":"Scenario generation is an effective data-driven method for solving\nchance-constrained optimization while ensuring desired risk guarantees with a\nfinite number of samples. Crucial challenges in deploying this technique in the\nreal world arise due to the absence of appropriate risk-tuning models tailored\nfor the desired application. In this paper, we focus on designing efficient\nscenario generation schemes for economic dispatch in power systems. We propose\na novel scenario generation method based on filtering scenarios using ambient\nwind conditions. These filtered scenarios are deployed incrementally in order\nto meet desired risk levels while using minimum resources. In order to study\nthe performance of the proposed scheme, we illustrate the procedure on case\nstudies performed for both 24-bus and 118-bus systems with real-world wind\npower forecasting data. Numerical results suggest that the proposed\nfilter-and-increment scenario generation model leads to a precise and efficient\nsolution for the chance-constrained economic dispatch problem.","publication_date":1699048300,"paper_link":"http://arxiv.org/pdf/2311.02250v1","categories":["Mathematics","Electrical Engineering and Systems Science"],"abstract":"Scenario generation is an effective data-driven method for solving chance-constrained optimization while ensuring desired risk guarantees with a finite number of samples. Crucial challenges in deploying this technique in the real world arise due to the absence of appropriate risk-tuning models tailored for the desired application. In this paper, we focus on designing efficient scenario generation schemes for economic dispatch in power systems. We propose a novel scenario generation method based on filtering scenarios using ambient wind conditions. These filtered scenarios are deployed incrementally in order to meet desired risk levels while using minimum resources. In order to study the performance of the proposed scheme, we illustrate the procedure on case studies performed for both 24-bus and 118-bus systems with real-world wind power forecasting data. Numerical results suggest that the proposed filter-and-increment scenario generation model leads to a precise and efficient solution for the chance-constrained economic dispatch problem."}
{"title":"Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data","authors":["Mubashara Akhtar","Abhilash Shankarampeta","Vivek Gupta","Arpit Patil","Oana Cocarascu","Elena Simperl"],"raw_abstract":"Numbers are crucial for various real-world domains such as finance,\neconomics, and science. Thus, understanding and reasoning with numbers are\nessential skills for language models to solve different tasks. While different\nnumerical benchmarks have been introduced in recent years, they are limited to\nspecific numerical aspects mostly. In this paper, we propose a hierarchical\ntaxonomy for numerical reasoning skills with more than ten reasoning types\nacross four levels: representation, number sense, manipulation, and complex\nreasoning. We conduct a comprehensive evaluation of state-of-the-art models to\nidentify reasoning challenges specific to them. Henceforth, we develop a\ndiverse set of numerical probes employing a semi-automated approach. We focus\non the tabular Natural Language Inference (TNLI) task as a case study and\nmeasure models' performance shifts. Our results show that no model consistently\nexcels across all numerical reasoning types. Among the probed models, FlanT5\n(few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical\nreasoning skills compared to other models. Label-flipping probes indicate that\nmodels often exploit dataset artifacts to predict the correct labels.","publication_date":1699041930,"paper_link":"http://arxiv.org/pdf/2311.02216v1","categories":["Economics"],"abstract":"Numbers are crucial for various real-world domains such as finance, economics, and science. Thus, understanding and reasoning with numbers are essential skills for language models to solve different tasks. While different numerical benchmarks have been introduced in recent years, they are limited to specific numerical aspects mostly. In this paper, we propose a hierarchical taxonomy for numerical reasoning skills with more than ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models to identify reasoning challenges specific to them. Henceforth, we develop a diverse set of numerical probes employing a semi-automated approach. We focus on the tabular Natural Language Inference (TNLI) task as a case study and measure models' performance shifts. Our results show that no model consistently excels across all numerical reasoning types. Among the probed models, FlanT5 (few-/zero-shot) and GPT-3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models. Label-flipping probes indicate that models often exploit dataset artifacts to predict the correct labels."}
{"title":"NSF Integrated Circuit Research, Education and Workforce Development Workshop Final Report","authors":["M. Guthaus","C. Batten","E. Brunvand","P. E. Gaillardon","D. harris","R. Manohar","P. Mazumder","L. Pileggi","J. Stine"],"raw_abstract":"As the pace of progress that has followed Moore's law continues to diminish,\nit is critical that the US support Integrated Circuit (IC or chip) education\nand research to maintain technological innovation. Furthermore, US economic\nindependence, security, and future international standing rely on having\non-shore IC design capabilities. New devices with disparate technologies,\nimproved design software toolchains and methodologies, and technologies to\nintegrate heterogeneous systems will be needed to advance IC design\ncapabilities. This will require rethinking both how we teach design to address\nthe new complexity and how we inspire student interest in a hardware systems\ncareer path. The main recommendation of this workshop is that accessibility is\nthe key issue. To this end, a National Chip Design Center (NCDC) should be\nestablished to further research and education by partnering academics and\nindustry to train our future workforce. This should not be limited to R1\nuniversities, but should also include R2, community college, minority serving\ninstitutions (MSI), and K-12 institutions to have the broadest effect. The NCDC\nshould support the access, development, and maintenance of open design tools,\ntool flows, design kits, design components, and educational materials.\nOpen-source options should be emphasized wherever possible to maximize\naccessibility. The NCDC should also provide access and support for chip\nfabrication, packaging and testing for both research and educational purposes.","publication_date":1699032839,"paper_link":"http://arxiv.org/pdf/2311.02055v1","categories":["Economics"],"abstract":"As the pace of progress that has followed Moore's law continues to diminish, it is critical that the US support Integrated Circuit (IC or chip) education and research to maintain technological innovation. Furthermore, US economic independence, security, and future international standing rely on having on-shore IC design capabilities. New devices with disparate technologies, improved design software toolchains and methodologies, and technologies to integrate heterogeneous systems will be needed to advance IC design capabilities. This will require rethinking both how we teach design to address the new complexity and how we inspire student interest in a hardware systems career path. The main recommendation of this workshop is that accessibility is the key issue. To this end, a National Chip Design Center (NCDC) should be established to further research and education by partnering academics and industry to train our future workforce. This should not be limited to R1 universities, but should also include R2, community college, minority serving institutions (MSI), and K-12 institutions to have the broadest effect. The NCDC should support the access, development, and maintenance of open design tools, tool flows, design kits, design components, and educational materials. Open-source options should be emphasized wherever possible to maximize accessibility. The NCDC should also provide access and support for chip fabrication, packaging and testing for both research and educational purposes."}
{"title":"Beyond a Year of Sanctions in Science","authors":["M. Albrecht","A. Ali","M. Barone","S. Brentjes","M. Bona","J. Ellis","A. Glazov","H. Jung","M. Mangano","G. Neuneck","N. Raicevic","J. Scheffran","M. Spiro","P. van Mechelen","J. Vigen"],"raw_abstract":"While sanctions in political and economic areas are now part of the standard\nrepertoire of Western countries (not always endorsed by UN mandates), sanctions\nin science and culture in general are new. Historically, fundamental research\nas conducted at international research centers such as CERN has long been seen\nas a driver for peace, and the Science4Peace idea has been celebrated for\ndecades. However, much changed with the war against Ukraine, and most Western\nscience organizations put scientific cooperation with Russia and Belarus on\nhold immediately after the start of the war in 2022. In addition, common\npublications and participation in conferences were banned by some institutions,\ngoing against the ideal of free scientific exchange and communication.\n  These and other points were the topics of an international virtual panel\ndiscussion organized by the Science4Peace Forum together with the \"Natural\nScientists Initiative - Responsibility for Peace and Sustainability\" (NatWiss\ne.V.) in Germany and the journal \"Wissenschaft und Frieden\" (W&F) (see the\nFigure). Fellows from the Hamburg Institute for Peace Research and Security\nPolicy (IFSH), scientists collaborating with the large physics research\ninstitutes DESY and CERN, as well as from climate and futures researchers were\nrepresented on the panel.\n  In this Dossier we document the panel discussion, and give additional\nperspectives.\n  The authors of the individual sections present their personal reflections,\nwhich should not be taken as implying that they are endorsed by the\nScience4Peace Forum or any other organizations. It is regrettable that some\ncolleagues who expressed support for this document felt that it would be unwise\nfor them to co-sign it.","publication_date":1699029499,"paper_link":"http://arxiv.org/pdf/2311.02141v1","categories":["Physics"],"abstract":"While sanctions in political and economic areas are now part of the standard repertoire of Western countries (not always endorsed by UN mandates), sanctions in science and culture in general are new. Historically, fundamental research as conducted at international research centers such as CERN has long been seen as a driver for peace, and the Science4Peace idea has been celebrated for decades. However, much changed with the war against Ukraine, and most Western science organizations put scientific cooperation with Russia and Belarus on hold immediately after the start of the war in 2022. In addition, common publications and participation in conferences were banned by some institutions, going against the ideal of free scientific exchange and communication.   These and other points were the topics of an international virtual panel discussion organized by the Science4Peace Forum together with the \"Natural Scientists Initiative - Responsibility for Peace and Sustainability\" (NatWiss e.V.) in Germany and the journal \"Wissenschaft und Frieden\" (W&F) (see the Figure). Fellows from the Hamburg Institute for Peace Research and Security Policy (IFSH), scientists collaborating with the large physics research institutes DESY and CERN, as well as from climate and futures researchers were represented on the panel.   In this Dossier we document the panel discussion, and give additional perspectives.   The authors of the individual sections present their personal reflections, which should not be taken as implying that they are endorsed by the Science4Peace Forum or any other organizations. It is regrettable that some colleagues who expressed support for this document felt that it would be unwise for them to co-sign it."}
{"title":"How to maintain compliance among host country employees who are less anxious after strict government regulations are lifted: An attempt to apply conservation of resources theory to the workplace amid the still-unending COVID-19 pandemic","authors":["Keisuke Kokubun","Yoshiaki Ino","Kazuyoshi Ishimura"],"raw_abstract":"Design/methodology/approach We compared the awareness of 813 people in Wuhan\ncity from January to March 2023 (Wuhan 2023) and 2,973 people in East and South\nChina from February to May 2020 (China 2020) using responses to questionnaires\nconducted at Japanese local subsidiaries during each period. Purpose As the\ncoronavirus pandemic becomes less terrifying than before, there is a trend in\ncountries around the world to abolish strict behavioral restrictions imposed by\ngovernments. How should overseas subsidiaries change the way they manage human\nresources in response to these system changes? To find an answer to this\nquestion, this paper examines what changes occurred in the mindset of employees\nworking at local subsidiaries after the government's strict behavioral\nrestrictions were introduced and lifted during the COVID-19 pandemic. Findings\nThe results showed that the analytical model based on conservation of resources\n(COR) theory can be applied to both China 2020 and Wuhan 2023. However, the\nrelationship between anxiety, fatigue, compliance, turnover intention, and\npsychological and social resources of employees working at local subsidiaries\nchanged after the initiation and removal of government behavioral restrictions\nduring the pandemic, indicating that managers need to adjust their human\nresource management practices in response to these changes. Originality/value\nThis is the first study that compares data after the start of government\nregulations and data after the regulations were lifted. Therefore, this\nresearch proposes a new analytical framework that companies, especially\nforeign-affiliated companies that lack local information, can refer to respond\nappropriately to disasters, which expand damage while changing its nature and\ninfluence while anticipating changes in employee awareness.","publication_date":1699023535,"paper_link":"http://arxiv.org/pdf/2311.01963v1","categories":["Economics","Quantitative Finance"],"abstract":"Design/methodology/approach We compared the awareness of 813 people in Wuhan city from January to March 2023 (Wuhan 2023) and 2,973 people in East and South China from February to May 2020 (China 2020) using responses to questionnaires conducted at Japanese local subsidiaries during each period. Purpose As the coronavirus pandemic becomes less terrifying than before, there is a trend in countries around the world to abolish strict behavioral restrictions imposed by governments. How should overseas subsidiaries change the way they manage human resources in response to these system changes? To find an answer to this question, this paper examines what changes occurred in the mindset of employees working at local subsidiaries after the government's strict behavioral restrictions were introduced and lifted during the COVID-19 pandemic. Findings The results showed that the analytical model based on conservation of resources (COR) theory can be applied to both China 2020 and Wuhan 2023. However, the relationship between anxiety, fatigue, compliance, turnover intention, and psychological and social resources of employees working at local subsidiaries changed after the initiation and removal of government behavioral restrictions during the pandemic, indicating that managers need to adjust their human resource management practices in response to these changes. Originality/value This is the first study that compares data after the start of government regulations and data after the regulations were lifted. Therefore, this research proposes a new analytical framework that companies, especially foreign-affiliated companies that lack local information, can refer to respond appropriately to disasters, which expand damage while changing its nature and influence while anticipating changes in employee awareness."}
{"title":"Resource savings from fault-tolerant circuit design","authors":["Andrew K. Tan","Isaac L. Chuang"],"raw_abstract":"Using fault-tolerant constructions, computations performed with unreliable\ncomponents can simulate their noiseless counterparts though the introduction of\na modest amount of redundancy. Given the modest overhead required to achieve\nfault-tolerance, and the fact that increasing the reliability of basic\ncomponents often comes at a cost, are there situations where fault-tolerance\nmay be more economical? We present a general framework to account for this\noverhead cost in order to effectively compare fault-tolerant to\nnon-fault-tolerant approaches for computation, in the limit of small logical\nerror rates. Using this detailed accounting, we determine explicit boundaries\nat which fault-tolerant designs become more efficient than designs that achieve\ncomparable reliability through direct consumption of resources. We find that\nthe fault-tolerant construction is always preferred in the limit of high\nreliability in cases where the resources required to construct a basic unit\ngrows faster than $\\log(1 / \\epsilon)$ asymptotically for small $\\epsilon$.","publication_date":1699021089,"paper_link":"http://arxiv.org/pdf/2311.02132v1","categories":["Mathematics"],"abstract":"Using fault-tolerant constructions, computations performed with unreliable components can simulate their noiseless counterparts though the introduction of a modest amount of redundancy. Given the modest overhead required to achieve fault-tolerance, and the fact that increasing the reliability of basic components often comes at a cost, are there situations where fault-tolerance may be more economical? We present a general framework to account for this overhead cost in order to effectively compare fault-tolerant to non-fault-tolerant approaches for computation, in the limit of small logical error rates. Using this detailed accounting, we determine explicit boundaries at which fault-tolerant designs become more efficient than designs that achieve comparable reliability through direct consumption of resources. We find that the fault-tolerant construction is always preferred in the limit of high reliability in cases where the resources required to construct a basic unit grows faster than __FORMULA__ asymptotically for small __FORMULA__."}
{"title":"Cascadic Tensor Multigrid Method and Economic Cascadic Tensor Multigrid Method for Image Restoration Problems","authors":["Ziqi Yan","Chenliang Li","Yuhan Chen"],"raw_abstract":"A cascadic tensor multigrid method and an economic cascadic tensor multigrid\nmethod is presented for solving the image restoration models. The methods use\nquadratic interpolation as prolongation operator to provide more accurate\ninitial values for the next fine grid level, and constructs a\npreserving-edge-denoising operator to obtain better edges and remove noise. The\nexperimental results show that the new methods not only improves computational\nefficiency but also achieve better restoration quality.","publication_date":1699020145,"paper_link":"http://arxiv.org/pdf/2311.01924v1","categories":["Mathematics"],"abstract":"A cascadic tensor multigrid method and an economic cascadic tensor multigrid method is presented for solving the image restoration models. The methods use quadratic interpolation as prolongation operator to provide more accurate initial values for the next fine grid level, and constructs a preserving-edge-denoising operator to obtain better edges and remove noise. The experimental results show that the new methods not only improves computational efficiency but also achieve better restoration quality."}
{"title":"Agent-based Modelling of Credit Card Promotions","authors":["Conor B. Hamill","Raad Khraishi","Simona Gherghel","Jerrard Lawrence","Salvatore Mercuri","Ramin Okhrati","Greig A. Cowan"],"raw_abstract":"Interest-free promotions are a prevalent strategy employed by credit card\nlenders to attract new customers, yet the research exploring their effects on\nboth consumers and lenders remains relatively sparse. The process of selecting\nan optimal promotion strategy is intricate, involving the determination of an\ninterest-free period duration and promotion-availability window, all within the\ncontext of competing offers, fluctuating market dynamics, and complex consumer\nbehaviour. In this paper, we introduce an agent-based model that facilitates\nthe exploration of various credit card promotions under diverse market\nscenarios. Our approach, distinct from previous agent-based models,\nconcentrates on optimising promotion strategies and is calibrated using\nbenchmarks from the UK credit card market from 2019 to 2020, with agent\nproperties derived from historical distributions of the UK population from\nroughly the same period. We validate our model against stylised facts and\ntime-series data, thereby demonstrating the value of this technique for\ninvestigating pricing strategies and understanding credit card customer\nbehaviour. Our experiments reveal that, in the absence of competitor\npromotions, lender profit is maximised by an interest-free duration of\napproximately 12 months while market share is maximised by offering the longest\nduration possible. When competitors do not offer promotions, extended promotion\navailability windows yield maximum profit for lenders while also maximising\nmarket share. In the context of concurrent interest-free promotions, we\nidentify that the optimal lender strategy entails offering a more competitive\ninterest-free period and a rapid response to competing promotional offers.\nNotably, a delay of three months in responding to a rival promotion corresponds\nto a 2.4% relative decline in income.","publication_date":1699017681,"paper_link":"http://arxiv.org/pdf/2311.01901v1","categories":["Economics","Quantitative Finance"],"abstract":"Interest-free promotions are a prevalent strategy employed by credit card lenders to attract new customers, yet the research exploring their effects on both consumers and lenders remains relatively sparse. The process of selecting an optimal promotion strategy is intricate, involving the determination of an interest-free period duration and promotion-availability window, all within the context of competing offers, fluctuating market dynamics, and complex consumer behaviour. In this paper, we introduce an agent-based model that facilitates the exploration of various credit card promotions under diverse market scenarios. Our approach, distinct from previous agent-based models, concentrates on optimising promotion strategies and is calibrated using benchmarks from the UK credit card market from 2019 to 2020, with agent properties derived from historical distributions of the UK population from roughly the same period. We validate our model against stylised facts and time-series data, thereby demonstrating the value of this technique for investigating pricing strategies and understanding credit card customer behaviour. Our experiments reveal that, in the absence of competitor promotions, lender profit is maximised by an interest-free duration of approximately 12 months while market share is maximised by offering the longest duration possible. When competitors do not offer promotions, extended promotion availability windows yield maximum profit for lenders while also maximising market share. In the context of concurrent interest-free promotions, we identify that the optimal lender strategy entails offering a more competitive interest-free period and a rapid response to competing promotional offers. Notably, a delay of three months in responding to a rival promotion corresponds to a 2.4% relative decline in income."}
{"title":"Leveraging Mobile Learning Platforms for Flexible Education Delivery: Bridging Educational Gaps in Afghanistan","authors":["Mursal Dawodi","Jawid Ahmad Baktash","Sayed Mohammad Reza Dawodi"],"raw_abstract":"The educational landscape of Afghanistan, besieged by infrastructural\ninadequacies and socio-political tribulations, presents a compelling case for\nthe integration of mobile learning platforms. This article embarks on an\nexploratory voyage into the realms of mobile learning as a potential harbinger\nof educational transformation in Afghanistan. It delineates the pervasive\neducational challenges, underscores the technological innovations powering\nmobile learning platforms, and illuminates the pathways through which mobile\nlearning can transcend the extant barriers to education. Enriched by real-world\ncase studies, the narrative unravels the pragmatic lessons that can be\nharnessed to tailor mobile learning solutions to Afghanistan's unique context.\nThe discussion further traverses the collaborative horizon, elucidating the\nsynergistic interplay among academia, government, the private sector, and\ninternational bodies essential for the successful implementation of mobile\nlearning platforms. The article also furnishes pragmatic recommendations,\nemphasizing the triad of policy formulation, infrastructure enhancement, and\ncapacity building as cornerstone imperatives. The envisioned integration of\nmobile learning platforms augurs a paradigmatic shift towards a more\naccessible, inclusive, and resilient educational framework in Afghanistan, with\nfar-reaching implications for socio-economic development. Through a meticulous\namalgamation of technology, policy, and collaborative endeavors, this article\nposits that Afghanistan stands on the cusp of an educational renaissance, with\nmobile learning platforms serving as a pivotal conduit toward this envisioned\nhorizon.","publication_date":1699010815,"paper_link":"http://arxiv.org/pdf/2311.01850v1","categories":["Economics"],"abstract":"The educational landscape of Afghanistan, besieged by infrastructural inadequacies and socio-political tribulations, presents a compelling case for the integration of mobile learning platforms. This article embarks on an exploratory voyage into the realms of mobile learning as a potential harbinger of educational transformation in Afghanistan. It delineates the pervasive educational challenges, underscores the technological innovations powering mobile learning platforms, and illuminates the pathways through which mobile learning can transcend the extant barriers to education. Enriched by real-world case studies, the narrative unravels the pragmatic lessons that can be harnessed to tailor mobile learning solutions to Afghanistan's unique context. The discussion further traverses the collaborative horizon, elucidating the synergistic interplay among academia, government, the private sector, and international bodies essential for the successful implementation of mobile learning platforms. The article also furnishes pragmatic recommendations, emphasizing the triad of policy formulation, infrastructure enhancement, and capacity building as cornerstone imperatives. The envisioned integration of mobile learning platforms augurs a paradigmatic shift towards a more accessible, inclusive, and resilient educational framework in Afghanistan, with far-reaching implications for socio-economic development. Through a meticulous amalgamation of technology, policy, and collaborative endeavors, this article posits that Afghanistan stands on the cusp of an educational renaissance, with mobile learning platforms serving as a pivotal conduit toward this envisioned horizon."}
{"title":"When fairness is an abstraction: Equity and AI in Swedish compulsory education","authors":["Marie Utterberg Mod\u00e9n","Marisa Ponti","Johan Lundin","Martin Tallvid"],"raw_abstract":"Artificial intelligence experts often question whether AI is fair. They view\nfairness as a property of AI systems rather than of sociopolitical and economic\nsystems. This paper emphasizes the need to be fair in the social, political,\nand economic contexts within which an educational system operates and uses AI.\nTaking Swedish decentralized compulsory education as the context, this paper\nexamines whether and how the use of AI envisaged by national authorities and\nedtech companies exacerbates unfairness. A qualitative content analysis of\nselected Swedish policy documents and edtech reports was conducted using the\nconcept of relevant social groups to understand how different groups view the\nrisks and benefits of AI for fairness. Three groups that view efficiency as a\nkey value of AI are identified, and interpreted as economical, pedagogical and\naccessibility-related. By separating fairness from social justice, this paper\nchallenges the notion of fairness as the formal equality of opportunities.","publication_date":1699008736,"paper_link":"http://arxiv.org/pdf/2311.01838v1","categories":["Economics"],"abstract":"Artificial intelligence experts often question whether AI is fair. They view fairness as a property of AI systems rather than of sociopolitical and economic systems. This paper emphasizes the need to be fair in the social, political, and economic contexts within which an educational system operates and uses AI. Taking Swedish decentralized compulsory education as the context, this paper examines whether and how the use of AI envisaged by national authorities and edtech companies exacerbates unfairness. A qualitative content analysis of selected Swedish policy documents and edtech reports was conducted using the concept of relevant social groups to understand how different groups view the risks and benefits of AI for fairness. Three groups that view efficiency as a key value of AI are identified, and interpreted as economical, pedagogical and accessibility-related. By separating fairness from social justice, this paper challenges the notion of fairness as the formal equality of opportunities."}
{"title":"Labour Absorption In Manufacturing Industry In Indonesia: Anomalous And Regressive Phenomena","authors":["Tongam Sihol Nababan","Elvis Fresly Purba"],"raw_abstract":"The manufacturing industry sector was expected to generate new employment\nopportunities and take on labour. Gradually, however, it emerged as a menace to\nthe sustenance of its workers. According to the findings of this study, 24\nmanufacturing subsectors with ISIC 2 digits in Indonesia exhibited regressive\nand abnormal patterns in the period 2012-2020. This suggests that, to a great\nextent, labour absorption has been limited and, in some cases, even shown a\ndecline. Anomalous occurrences were observed in three subsectors: ISIC 12\n(tobacco products), ISIC 26 (computer, electronic and optical products), and\nISIC 31 (furniture). In contrast, regressive phenomena were present in the\nremaining 21 ISIC subsectors. Furthermore, the manufacturing industry displayed\na negative correlation between employment and efficiency index, demonstrating\nthis anomalous and regressive phenomenon. This implies that as the efficiency\nindex of the manufacturing industry increases, the index of labour absorption\ndecreases","publication_date":1699001880,"paper_link":"http://arxiv.org/pdf/2311.01787v1","categories":["Economics","Quantitative Finance"],"abstract":"The manufacturing industry sector was expected to generate new employment opportunities and take on labour. Gradually, however, it emerged as a menace to the sustenance of its workers. According to the findings of this study, 24 manufacturing subsectors with ISIC 2 digits in Indonesia exhibited regressive and abnormal patterns in the period 2012-2020. This suggests that, to a great extent, labour absorption has been limited and, in some cases, even shown a decline. Anomalous occurrences were observed in three subsectors: ISIC 12 (tobacco products), ISIC 26 (computer, electronic and optical products), and ISIC 31 (furniture). In contrast, regressive phenomena were present in the remaining 21 ISIC subsectors. Furthermore, the manufacturing industry displayed a negative correlation between employment and efficiency index, demonstrating this anomalous and regressive phenomenon. This implies that as the efficiency index of the manufacturing industry increases, the index of labour absorption decreases"}
{"title":"Epidemic Decision-making System Based Federated Reinforcement Learning","authors":["Yangxi Zhou","Junping Du","Zhe Xue","Zhenhui Pan","Weikang Chen"],"raw_abstract":"Epidemic decision-making can effectively help the government to\ncomprehensively consider public security and economic development to respond to\npublic health and safety emergencies. Epidemic decision-making can effectively\nhelp the government to comprehensively consider public security and economic\ndevelopment to respond to public health and safety emergencies. Some studies\nhave shown that intensive learning can effectively help the government to make\nepidemic decision, thus achieving the balance between health security and\neconomic development. Some studies have shown that intensive learning can\neffectively help the government to make epidemic decision, thus achieving the\nbalance between health security and economic development. However, epidemic\ndata often has the characteristics of limited samples and high privacy.\nHowever, epidemic data often has the characteristics of limited samples and\nhigh privacy. This model can combine the epidemic situation data of various\nprovinces for cooperative training to use as an enhanced learning model for\nepidemic situation decision, while protecting the privacy of data. The\nexperiment shows that the enhanced federated learning can obtain more optimized\nperformance and return than the enhanced learning, and the enhanced federated\nlearning can also accelerate the training convergence speed of the training\nmodel. accelerate the training convergence speed of the client. At the same\ntime, through the experimental comparison, A2C is the most suitable\nreinforcement learning model for the epidemic situation decision-making.\nlearning model for the epidemic situation decision-making scenario, followed by\nthe PPO model, and the performance of DDPG is unsatisfactory.","publication_date":1698994661,"paper_link":"http://arxiv.org/pdf/2311.01749v1","categories":["Economics"],"abstract":"Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Epidemic decision-making can effectively help the government to comprehensively consider public security and economic development to respond to public health and safety emergencies. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. Some studies have shown that intensive learning can effectively help the government to make epidemic decision, thus achieving the balance between health security and economic development. However, epidemic data often has the characteristics of limited samples and high privacy. However, epidemic data often has the characteristics of limited samples and high privacy. This model can combine the epidemic situation data of various provinces for cooperative training to use as an enhanced learning model for epidemic situation decision, while protecting the privacy of data. The experiment shows that the enhanced federated learning can obtain more optimized performance and return than the enhanced learning, and the enhanced federated learning can also accelerate the training convergence speed of the training model. accelerate the training convergence speed of the client. At the same time, through the experimental comparison, A2C is the most suitable reinforcement learning model for the epidemic situation decision-making. learning model for the epidemic situation decision-making scenario, followed by the PPO model, and the performance of DDPG is unsatisfactory."}
{"title":"Energy Efficiency Optimization for Subterranean LoRaWAN Using A Reinforcement Learning Approach: A Direct-to-Satellite Scenario","authors":["Kaiqiang Lin","Muhammad Asad Ullah","Hirley Alves","Konstantin Mikhaylov","Tong Hao"],"raw_abstract":"The integration of subterranean LoRaWAN and non-terrestrial networks (NTN)\ndelivers substantial economic and societal benefits in remote agriculture and\ndisaster rescue operations. The LoRa modulation leverages quasi-orthogonal\nspreading factors (SFs) to optimize data rates, airtime, coverage and energy\nconsumption. However, it is still challenging to effectively assign SFs to end\ndevices for minimizing co-SF interference in massive subterranean LoRaWAN NTN.\nTo address this, we investigate a reinforcement learning (RL)-based SFs\nallocation scheme to optimize the system's energy efficiency (EE). To\nefficiently capture the device-to-environment interactions in dense networks,\nwe proposed an SFs allocation technique using the multi-agent dueling double\ndeep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C)\nalgorithms based on an analytical reward mechanism. Our proposed RL-based SFs\nallocation approach evinces better performance compared to four benchmarks in\nthe extreme underground direct-to-satellite scenario. Remarkably, MAD3QN shows\npromising potentials in surpassing MAA2C in terms of convergence rate and EE.","publication_date":1698993236,"paper_link":"http://arxiv.org/pdf/2311.01743v1","categories":["Mathematics"],"abstract":"The integration of subterranean LoRaWAN and non-terrestrial networks (NTN) delivers substantial economic and societal benefits in remote agriculture and disaster rescue operations. The LoRa modulation leverages quasi-orthogonal spreading factors (SFs) to optimize data rates, airtime, coverage and energy consumption. However, it is still challenging to effectively assign SFs to end devices for minimizing co-SF interference in massive subterranean LoRaWAN NTN. To address this, we investigate a reinforcement learning (RL)-based SFs allocation scheme to optimize the system's energy efficiency (EE). To efficiently capture the device-to-environment interactions in dense networks, we proposed an SFs allocation technique using the multi-agent dueling double deep Q-network (MAD3QN) and the multi-agent advantage actor-critic (MAA2C) algorithms based on an analytical reward mechanism. Our proposed RL-based SFs allocation approach evinces better performance compared to four benchmarks in the extreme underground direct-to-satellite scenario. Remarkably, MAD3QN shows promising potentials in surpassing MAA2C in terms of convergence rate and EE."}
{"title":"Responsible Emergent Multi-Agent Behavior","authors":["Niko A. Grupen"],"raw_abstract":"Responsible AI has risen to the forefront of the AI research community. As\nneural network-based learning algorithms continue to permeate real-world\napplications, the field of Responsible AI has played a large role in ensuring\nthat such systems maintain a high-level of human-compatibility. Despite this\nprogress, the state of the art in Responsible AI has ignored one crucial point:\nhuman problems are multi-agent problems. Predominant approaches largely\nconsider the performance of a single AI system in isolation, but human problems\nare, by their very nature, multi-agent. From driving in traffic to negotiating\neconomic policy, human problem-solving involves interaction and the interplay\nof the actions and motives of multiple individuals.\n  This dissertation develops the study of responsible emergent multi-agent\nbehavior, illustrating how researchers and practitioners can better understand\nand shape multi-agent learning with respect to three pillars of Responsible AI:\ninterpretability, fairness, and robustness. First, I investigate multi-agent\ninterpretability, presenting novel techniques for understanding emergent\nmulti-agent behavior at multiple levels of granularity. With respect to\nlow-level interpretability, I examine the extent to which implicit\ncommunication emerges as an aid to coordination in multi-agent populations. I\nintroduce a novel curriculum-driven method for learning high-performing\npolicies in difficult, sparse reward environments and show through a measure of\nposition-based social influence that multi-agent teams that learn sophisticated\ncoordination strategies exchange significantly more information through\nimplicit signals than lesser-coordinated agents. Then, at a high-level, I study\nconcept-based interpretability in the context of multi-agent learning. I\npropose a novel method for learning intrinsically interpretable, concept-based\npolicies and show that it enables...","publication_date":1698961052,"paper_link":"http://arxiv.org/pdf/2311.01609v1","categories":["Economics"],"abstract":"Responsible AI has risen to the forefront of the AI research community. As neural network-based learning algorithms continue to permeate real-world applications, the field of Responsible AI has played a large role in ensuring that such systems maintain a high-level of human-compatibility. Despite this progress, the state of the art in Responsible AI has ignored one crucial point: human problems are multi-agent problems. Predominant approaches largely consider the performance of a single AI system in isolation, but human problems are, by their very nature, multi-agent. From driving in traffic to negotiating economic policy, human problem-solving involves interaction and the interplay of the actions and motives of multiple individuals.   This dissertation develops the study of responsible emergent multi-agent behavior, illustrating how researchers and practitioners can better understand and shape multi-agent learning with respect to three pillars of Responsible AI: interpretability, fairness, and robustness. First, I investigate multi-agent interpretability, presenting novel techniques for understanding emergent multi-agent behavior at multiple levels of granularity. With respect to low-level interpretability, I examine the extent to which implicit communication emerges as an aid to coordination in multi-agent populations. I introduce a novel curriculum-driven method for learning high-performing policies in difficult, sparse reward environments and show through a measure of position-based social influence that multi-agent teams that learn sophisticated coordination strategies exchange significantly more information through implicit signals than lesser-coordinated agents. Then, at a high-level, I study concept-based interpretability in the context of multi-agent learning. I propose a novel method for learning intrinsically interpretable, concept-based policies and show that it enables..."}
{"title":"A Model of Enclosures: Coordination, Conflict, and Efficiency in the Transformation of Land Property Rights","authors":["Matthew J. Baker","Jonathan Conning"],"raw_abstract":"Historians and political economists have long debated the processes that led\nland in frontier regions, managed commons, and a variety of customary\nlandholding regimes to be enclosed and transformed into more exclusive forms of\nprivate property. Using the framework of aggregative games, we examine\nland-holding regimes where access to land is established via possession and\nuse, and then explore the factors that may initiate decentralized privatization\nprocesses. Factors including population density, potential for technology\nimprovement, enclosure costs, shifts in group cohesion and bargaining power, or\nthe policy and institutional environment determine the equilibrium mix of\nproperty regimes. While decentralized processes yield efficient enclosure and\ntechnological transformation in some circumstances, in others, the outcomes\nfall short of second-best. This stems from the interaction of different\nspillover effects, leading to inefficiently low rates of enclosure and\ntechnological transformation in some cases and excessive enclosure in others.\nImplementing policies to strengthen customary governance, compensate displaced\nstakeholders, or subsidize/tax enclosure can realign incentives. However,\naddressing one market failure while overlooking others can worsen outcomes. Our\nanalysis offers a unified framework for evaluating claimed mechanisms and\nprocesses across Neoclassical, neo-institutional, and Marxian interpretations\nof enclosure processes.","publication_date":1698958738,"paper_link":"http://arxiv.org/pdf/2311.01592v1","categories":["Economics","Quantitative Finance"],"abstract":"Historians and political economists have long debated the processes that led land in frontier regions, managed commons, and a variety of customary landholding regimes to be enclosed and transformed into more exclusive forms of private property. Using the framework of aggregative games, we examine land-holding regimes where access to land is established via possession and use, and then explore the factors that may initiate decentralized privatization processes. Factors including population density, potential for technology improvement, enclosure costs, shifts in group cohesion and bargaining power, or the policy and institutional environment determine the equilibrium mix of property regimes. While decentralized processes yield efficient enclosure and technological transformation in some circumstances, in others, the outcomes fall short of second-best. This stems from the interaction of different spillover effects, leading to inefficiently low rates of enclosure and technological transformation in some cases and excessive enclosure in others. Implementing policies to strengthen customary governance, compensate displaced stakeholders, or subsidize/tax enclosure can realign incentives. However, addressing one market failure while overlooking others can worsen outcomes. Our analysis offers a unified framework for evaluating claimed mechanisms and processes across Neoclassical, neo-institutional, and Marxian interpretations of enclosure processes."}
{"title":"Market Concentration Implications of Foundation Models","authors":["Jai Vipra","Anton Korinek"],"raw_abstract":"We analyze the structure of the market for foundation models, i.e., large AI\nmodels such as those that power ChatGPT and that are adaptable to downstream\nuses, and we examine the implications for competition policy and regulation. We\nobserve that the most capable models will have a tendency towards natural\nmonopoly and may have potentially vast markets. This calls for a two-pronged\nregulatory response: (i) Antitrust authorities need to ensure the\ncontestability of the market by tackling strategic behavior, in particular by\nensuring that monopolies do not propagate vertically to downstream uses, and\n(ii) given the diminished potential for market discipline, there is a role for\nregulators to ensure that the most capable models meet sufficient quality\nstandards (including safety, privacy, non-discrimination, reliability and\ninteroperability standards) to maximally contribute to social welfare.\nRegulators should also ensure a level regulatory playing field between AI and\nnon-AI applications in all sectors of the economy. For models that are behind\nthe frontier, we expect competition to be quite intense, implying a more\nlimited role for competition policy, although a role for regulation remains.","publication_date":1698951642,"paper_link":"http://arxiv.org/pdf/2311.01550v1","categories":["Economics","Quantitative Finance"],"abstract":"We analyze the structure of the market for foundation models, i.e., large AI models such as those that power ChatGPT and that are adaptable to downstream uses, and we examine the implications for competition policy and regulation. We observe that the most capable models will have a tendency towards natural monopoly and may have potentially vast markets. This calls for a two-pronged regulatory response: (i) Antitrust authorities need to ensure the contestability of the market by tackling strategic behavior, in particular by ensuring that monopolies do not propagate vertically to downstream uses, and (ii) given the diminished potential for market discipline, there is a role for regulators to ensure that the most capable models meet sufficient quality standards (including safety, privacy, non-discrimination, reliability and interoperability standards) to maximally contribute to social welfare. Regulators should also ensure a level regulatory playing field between AI and non-AI applications in all sectors of the economy. For models that are behind the frontier, we expect competition to be quite intense, implying a more limited role for competition policy, although a role for regulation remains."}
{"title":"Bank Deposits as {\\em Money Quanta}","authors":["Fabio Bagarello","Biagio Bossone"],"raw_abstract":"According to the Accounting View of Money (AVM), the money issued by\ncommercial banks in the form of demand deposits features a hybrid nature, since\ndeposits can be shown to consist of a share of deposits bearing the\ncharacteristics of debt (debt-deposits) and a share of deposits bearing the\ncharacteristics of equity (equity-deposits), in a mix that depends on factors\nthat relate to the issuing banks and the environment where they operate and\ninteract, which may change over time. Following this important finding of the\nAVM, it is only consequential to associate the hybrid nature of bank deposits\nwith the dual nature of the objects which is typical in quantum physics, and to\ninvestigate whether and how the application of quantum analytical methods and\nideas to a form of money showing dualistic features could be used to extract\nvaluable economic information.","publication_date":1698951165,"paper_link":"http://arxiv.org/pdf/2311.01542v1","categories":["Quantitative Finance"],"abstract":"According to the Accounting View of Money (AVM), the money issued by commercial banks in the form of demand deposits features a hybrid nature, since deposits can be shown to consist of a share of deposits bearing the characteristics of debt (debt-deposits) and a share of deposits bearing the characteristics of equity (equity-deposits), in a mix that depends on factors that relate to the issuing banks and the environment where they operate and interact, which may change over time. Following this important finding of the AVM, it is only consequential to associate the hybrid nature of bank deposits with the dual nature of the objects which is typical in quantum physics, and to investigate whether and how the application of quantum analytical methods and ideas to a form of money showing dualistic features could be used to extract valuable economic information."}
{"title":"Economical routes to size-specific assembly of self-closing structures","authors":["Thomas E. Videb\u00e6k","Daichi Hayakawa","Gregory M. Grason","Michael F. Hagan","Seth Fraden","W. Benjamin Rogers"],"raw_abstract":"The push to harness self-assembly to create complex and functional nanoscale\nmaterials has led to the development of a large class of synthetic crystals.\nHowever, many applications require `self-limiting' assembly, which autonomously\nterminates at a well-defined size and geometry. For example, structures such as\ntubules, vesicles, or capsids can be designed to self-close at a particular\nsize, symmetry, and Gaussian curvature. But developing synthetic strategies for\nself-limiting assembly has been challenging, in part because such structures\nare prone to polymorphism that arises from thermal fluctuations of their local\ncurvature. Here, we introduce a strategy to eliminate polymorphism by\nincreasing the assembly complexity. We make triangular subunits using DNA\norigami that have specific, valence-limited interactions and designed\ncurvature, and study their assembly into tubules with targeted width and\nhelicity. By assembling structures with as many as 16 distinct subunit types,\nwe demonstrate that our approach enables assembly with arbitrarily high\ncomplexity. In the limit of single-subunit assembly, we find that thermal\nfluctuations allow the system to access nearby, off-target states with varying\nwidths and helicities. By increasing the number of distinct components, we\nreduce the density of off-target states, increasing the selectivity of a\nuser-specified target structure to nearly 100%. We further show that by\nreducing the design constraints by targeting either the pitch or the width of\ntubules, fewer subunits are needed to reach complete selectivity. Combining\nexperiments with theory, our results reveal an economical limit, which\ndetermines the minimum number of subunit species that are required to create\narbitrary assembly sizes with full selectivity. In the future, this approach\ncould be extended to more complex self-limited structures, such as shells or\ntriply periodic surfaces.","publication_date":1698943330,"paper_link":"http://arxiv.org/pdf/2311.01383v1","categories":["Physics"],"abstract":"The push to harness self-assembly to create complex and functional nanoscale materials has led to the development of a large class of synthetic crystals. However, many applications require `self-limiting' assembly, which autonomously terminates at a well-defined size and geometry. For example, structures such as tubules, vesicles, or capsids can be designed to self-close at a particular size, symmetry, and Gaussian curvature. But developing synthetic strategies for self-limiting assembly has been challenging, in part because such structures are prone to polymorphism that arises from thermal fluctuations of their local curvature. Here, we introduce a strategy to eliminate polymorphism by increasing the assembly complexity. We make triangular subunits using DNA origami that have specific, valence-limited interactions and designed curvature, and study their assembly into tubules with targeted width and helicity. By assembling structures with as many as 16 distinct subunit types, we demonstrate that our approach enables assembly with arbitrarily high complexity. In the limit of single-subunit assembly, we find that thermal fluctuations allow the system to access nearby, off-target states with varying widths and helicities. By increasing the number of distinct components, we reduce the density of off-target states, increasing the selectivity of a user-specified target structure to nearly 100%. We further show that by reducing the design constraints by targeting either the pitch or the width of tubules, fewer subunits are needed to reach complete selectivity. Combining experiments with theory, our results reveal an economical limit, which determines the minimum number of subunit species that are required to create arbitrary assembly sizes with full selectivity. In the future, this approach could be extended to more complex self-limited structures, such as shells or triply periodic surfaces."}
{"title":"A connected and automated vehicle readiness framework to support road authorities for C-ITS services","authors":["Bahman Madadi","Ary P. Silvano","Kevin McPherson","John McCarthy","Risto \u00d6\u00f6rni","Gon\u00e7alo Homem de Almeida Correiaa"],"raw_abstract":"Connected and Automated Vehicles (CAVs) can have a profound influence on\ntransport systems. However, most levels of automation and connectivity require\nsupport from the road infrastructure. Additional support such as Cooperative\nIntelligent Transport Systems (C-ITS) services can facilitate safe and\nefficient traffic, and alleviate the environmental impacts of road surface\nvehicles. However, due to the rapidly evolving technology, C-ITS service\ndeployment requirements are not always clear. Furthermore, the costs and\nbenefits of infrastructure investments are subject to tremendous uncertainty.\nThis study articulates the requirements using a structured approach to propose\na CAV-Readiness Framework (CRF). The main purpose of the CRF is allowing road\nauthorities to assess their physical and digital infrastructure readiness,\ndefine requirements for C-ITS services, and identify future development paths\nto reach higher levels of readiness to support CAVs by enabling C-ITS services.\nThe CRF is intended to guide and support road authorities' investment decisions\non infrastructure.","publication_date":1698935405,"paper_link":"http://arxiv.org/pdf/2311.01268v1","categories":["Economics","Quantitative Finance"],"abstract":"Connected and Automated Vehicles (CAVs) can have a profound influence on transport systems. However, most levels of automation and connectivity require support from the road infrastructure. Additional support such as Cooperative Intelligent Transport Systems (C-ITS) services can facilitate safe and efficient traffic, and alleviate the environmental impacts of road surface vehicles. However, due to the rapidly evolving technology, C-ITS service deployment requirements are not always clear. Furthermore, the costs and benefits of infrastructure investments are subject to tremendous uncertainty. This study articulates the requirements using a structured approach to propose a CAV-Readiness Framework (CRF). The main purpose of the CRF is allowing road authorities to assess their physical and digital infrastructure readiness, define requirements for C-ITS services, and identify future development paths to reach higher levels of readiness to support CAVs by enabling C-ITS services. The CRF is intended to guide and support road authorities' investment decisions on infrastructure."}
{"title":"How Does China's Household Portfolio Selection Vary with Financial Inclusion?","authors":["Yong Bian","Xiqian Wang","Qin Zhang"],"raw_abstract":"Portfolio underdiversification is one of the most costly losses accumulated\nover a household's life cycle. We provide new evidence on the impact of\nfinancial inclusion services on households' portfolio choice and investment\nefficiency using 2015, 2017, and 2019 survey data for Chinese households. We\nhypothesize that higher financial inclusion penetration encourages households\nto participate in the financial market, leading to better portfolio\ndiversification and investment efficiency. The results of the baseline model\nare consistent with our proposed hypothesis that higher accessibility to\nfinancial inclusion encourages households to invest in risky assets and\nincreases investment efficiency. We further estimate a dynamic double machine\nlearning model to quantitatively investigate the non-linear causal effects and\ntrack the dynamic change of those effects over time. We observe that the\nmarginal effect increases over time, and those effects are more pronounced\namong low-asset, less-educated households and those located in non-rural areas,\nexcept for investment efficiency for high-asset households.","publication_date":1698930008,"paper_link":"http://arxiv.org/pdf/2311.01206v1","categories":["Economics","Quantitative Finance"],"abstract":"Portfolio underdiversification is one of the most costly losses accumulated over a household's life cycle. We provide new evidence on the impact of financial inclusion services on households' portfolio choice and investment efficiency using 2015, 2017, and 2019 survey data for Chinese households. We hypothesize that higher financial inclusion penetration encourages households to participate in the financial market, leading to better portfolio diversification and investment efficiency. The results of the baseline model are consistent with our proposed hypothesis that higher accessibility to financial inclusion encourages households to invest in risky assets and increases investment efficiency. We further estimate a dynamic double machine learning model to quantitatively investigate the non-linear causal effects and track the dynamic change of those effects over time. We observe that the marginal effect increases over time, and those effects are more pronounced among low-asset, less-educated households and those located in non-rural areas, except for investment efficiency for high-asset households."}
{"title":"Several Consequences of Optimality","authors":["Dibakar Das"],"raw_abstract":"Rationality is frequently associated with making the best possible decisions.\nIt's widely acknowledged that humans, as rational beings, have limitations in\ntheir decision-making capabilities. Nevertheless, recent advancements in\nfields, such as, computing, science and technology, combined with the\navailability of vast amounts of data, have sparked optimism that these\ndevelopments could potentially expand the boundaries of human bounded\nrationality through the augmentation of machine intelligence. In this paper,\nfindings from a computational model demonstrated that when an increasing number\nof agents independently strive to achieve global optimality, facilitated by\nimproved computing power, etc., they indirectly accelerated the occurrence of\nthe \"tragedy of the commons\" by depleting shared resources at a faster rate.\nFurther, as agents achieve optimality, there is a drop in information entropy\namong the solutions of the agents. Also, clear economic divide emerges among\nagents. Considering, two groups, one as producer and the other (the group\nagents searching for optimality) as consumer of the highest consumed resource,\nthe consumers seem to gain more than the producers. Thus, bounded rationality\ncould be seen as boon to sustainability.","publication_date":1698924831,"paper_link":"http://arxiv.org/pdf/2311.01156v1","categories":["Economics"],"abstract":"Rationality is frequently associated with making the best possible decisions. It's widely acknowledged that humans, as rational beings, have limitations in their decision-making capabilities. Nevertheless, recent advancements in fields, such as, computing, science and technology, combined with the availability of vast amounts of data, have sparked optimism that these developments could potentially expand the boundaries of human bounded rationality through the augmentation of machine intelligence. In this paper, findings from a computational model demonstrated that when an increasing number of agents independently strive to achieve global optimality, facilitated by improved computing power, etc., they indirectly accelerated the occurrence of the \"tragedy of the commons\" by depleting shared resources at a faster rate. Further, as agents achieve optimality, there is a drop in information entropy among the solutions of the agents. Also, clear economic divide emerges among agents. Considering, two groups, one as producer and the other (the group agents searching for optimality) as consumer of the highest consumed resource, the consumers seem to gain more than the producers. Thus, bounded rationality could be seen as boon to sustainability."}
{"title":"Bi-Preference Learning Heterogeneous Hypergraph Networks for Session-based Recommendation","authors":["Xiaokun Zhang","Bo Xu","Fenglong Ma","Chenliang Li","Yuan Lin","Hongfei Lin"],"raw_abstract":"Session-based recommendation intends to predict next purchased items based on\nanonymous behavior sequences. Numerous economic studies have revealed that item\nprice is a key factor influencing user purchase decisions. Unfortunately,\nexisting methods for session-based recommendation only aim at capturing user\ninterest preference, while ignoring user price preference. Actually, there are\nprimarily two challenges preventing us from accessing price preference.\nFirstly, the price preference is highly associated to various item features\n(i.e., category and brand), which asks us to mine price preference from\nheterogeneous information. Secondly, price preference and interest preference\nare interdependent and collectively determine user choice, necessitating that\nwe jointly consider both price and interest preference for intent modeling. To\nhandle above challenges, we propose a novel approach Bi-Preference Learning\nHeterogeneous Hypergraph Networks (BiPNet) for session-based recommendation.\nSpecifically, the customized heterogeneous hypergraph networks with a\ntriple-level convolution are devised to capture user price and interest\npreference from heterogeneous features of items. Besides, we develop a\nBi-Preference Learning schema to explore mutual relations between price and\ninterest preference and collectively learn these two preferences under the\nmulti-task learning architecture. Extensive experiments on multiple public\ndatasets confirm the superiority of BiPNet over competitive baselines.\nAdditional research also supports the notion that the price is crucial for the\ntask.","publication_date":1698920188,"paper_link":"http://arxiv.org/pdf/2311.01125v1","categories":["Economics"],"abstract":"Session-based recommendation intends to predict next purchased items based on anonymous behavior sequences. Numerous economic studies have revealed that item price is a key factor influencing user purchase decisions. Unfortunately, existing methods for session-based recommendation only aim at capturing user interest preference, while ignoring user price preference. Actually, there are primarily two challenges preventing us from accessing price preference. Firstly, the price preference is highly associated to various item features (i.e., category and brand), which asks us to mine price preference from heterogeneous information. Secondly, price preference and interest preference are interdependent and collectively determine user choice, necessitating that we jointly consider both price and interest preference for intent modeling. To handle above challenges, we propose a novel approach Bi-Preference Learning Heterogeneous Hypergraph Networks (BiPNet) for session-based recommendation. Specifically, the customized heterogeneous hypergraph networks with a triple-level convolution are devised to capture user price and interest preference from heterogeneous features of items. Besides, we develop a Bi-Preference Learning schema to explore mutual relations between price and interest preference and collectively learn these two preferences under the multi-task learning architecture. Extensive experiments on multiple public datasets confirm the superiority of BiPNet over competitive baselines. Additional research also supports the notion that the price is crucial for the task."}
{"title":"From Doubt to Devotion: Trials and Learning-Based Pricing","authors":["Tan Gan","Nicholas Wu"],"raw_abstract":"An informed seller designs a dynamic mechanism to sell an experience good.\nThe seller has partial information about the product match, which affects the\nbuyer's private consumption experience. We characterize equilibrium mechanisms\nof this dynamic informed principal problem. The belief gap between the informed\nseller and the uninformed buyer, coupled with the buyer's learning, gives rise\nto mechanisms that provide the skeptical buyer with limited access to the\nproduct and an option to upgrade if the buyer is swayed by a good experience.\nDepending on the seller's screening technology, this takes the form of\nfree/discounted trials or tiered pricing, which are prevalent in digital\nmarkets. In contrast to static environments, having consumer data can reduce\nsellers' revenue in equilibrium, as they fine-tune the dynamic design with\ntheir data forecasting the buyer's learning process.","publication_date":1698871934,"paper_link":"http://arxiv.org/pdf/2311.00846v1","categories":["Economics"],"abstract":"An informed seller designs a dynamic mechanism to sell an experience good. The seller has partial information about the product match, which affects the buyer's private consumption experience. We characterize equilibrium mechanisms of this dynamic informed principal problem. The belief gap between the informed seller and the uninformed buyer, coupled with the buyer's learning, gives rise to mechanisms that provide the skeptical buyer with limited access to the product and an option to upgrade if the buyer is swayed by a good experience. Depending on the seller's screening technology, this takes the form of free/discounted trials or tiered pricing, which are prevalent in digital markets. In contrast to static environments, having consumer data can reduce sellers' revenue in equilibrium, as they fine-tune the dynamic design with their data forecasting the buyer's learning process."}
{"title":"Graph-Based Optimization for Technology Pathway Analysis: A Case Study in Decarbonization of University Campuses","authors":["Blake Lopez","Jiaze Ma","Victor M. Zavala"],"raw_abstract":"Industrial sectors such as urban centers, chemical companies, manufacturing\nfacilities, and microgrids are actively exploring strategies to help reduce\ntheir carbon footprint. For instance, university campuses are complex urban\ndistricts (involving collections of buildings and utility systems) that are\nseeking to reduce carbon footprints that originate from diverse activities\n(e.g., transportation operations and production of heating, cooling, and power\nutilities). This work presents an optimization framework to identify technology\npathways that enable decarbonization of complex industrial sectors. The\nframework uses a graph abstraction that compactly captures interdependencies\nbetween diverse products and technologies as well as diverse externalities\n(e.g., market, policy, and carbon prices). Duality analysis reveals that the\nformulation can be interpreted as an economy, market, or value chain that uses\ntechnologies to generate economic value (wealth) by transforming basic products\ninto higher value products. This interpretation also reveals that the\nformulation identifies pathways that maximize the profit of stakeholders, helps\nreveal the inherent value (prices) of intermediate products, and helps analyze\nthe impact of externalities and technology specifications on product values.\nOur developments are illustrated via a case study involving a prototypical\nuniversity campus that seeks to identify pathways that reduce its carbon\nfootprint (e.g., via electrification and deployment of hydrogen technologies).\nWe use the framework to determine carbon tax values, technology specifications,\nand investment budgets that activate different technology pathways and that\nachieve different levels of decarbonization.","publication_date":1698868117,"paper_link":"http://arxiv.org/pdf/2311.00809v1","categories":["Mathematics","Physics"],"abstract":"Industrial sectors such as urban centers, chemical companies, manufacturing facilities, and microgrids are actively exploring strategies to help reduce their carbon footprint. For instance, university campuses are complex urban districts (involving collections of buildings and utility systems) that are seeking to reduce carbon footprints that originate from diverse activities (e.g., transportation operations and production of heating, cooling, and power utilities). This work presents an optimization framework to identify technology pathways that enable decarbonization of complex industrial sectors. The framework uses a graph abstraction that compactly captures interdependencies between diverse products and technologies as well as diverse externalities (e.g., market, policy, and carbon prices). Duality analysis reveals that the formulation can be interpreted as an economy, market, or value chain that uses technologies to generate economic value (wealth) by transforming basic products into higher value products. This interpretation also reveals that the formulation identifies pathways that maximize the profit of stakeholders, helps reveal the inherent value (prices) of intermediate products, and helps analyze the impact of externalities and technology specifications on product values. Our developments are illustrated via a case study involving a prototypical university campus that seeks to identify pathways that reduce its carbon footprint (e.g., via electrification and deployment of hydrogen technologies). We use the framework to determine carbon tax values, technology specifications, and investment budgets that activate different technology pathways and that achieve different levels of decarbonization."}
{"title":"What is a Labor Market? Classifying Workers and Jobs Using Network Theory","authors":["Jamie Fogel","Bernardo Modenesi"],"raw_abstract":"This paper develops a new data-driven approach to characterizing latent\nworker skill and job task heterogeneity by applying an empirical tool from\nnetwork theory to large-scale Brazilian administrative data on worker--job\nmatching. We microfound this tool using a standard equilibrium model of workers\nmatching with jobs according to comparative advantage. Our classifications\nidentify important dimensions of worker and job heterogeneity that standard\nclassifications based on occupations and sectors miss. The equilibrium model\nbased on our classifications more accurately predicts wage changes in response\nto the 2016 Olympics than a model based on occupations and sectors.\nAdditionally, for a large simulated shock to demand for workers, we show that\nreduced form estimates of the effects of labor market shock exposure on\nworkers' earnings are nearly 4 times larger when workers and jobs are\nclassified using our classifications as opposed to occupations and sectors.","publication_date":1698864297,"paper_link":"http://arxiv.org/pdf/2311.00777v1","categories":["Economics","Quantitative Finance"],"abstract":"This paper develops a new data-driven approach to characterizing latent worker skill and job task heterogeneity by applying an empirical tool from network theory to large-scale Brazilian administrative data on worker--job matching. We microfound this tool using a standard equilibrium model of workers matching with jobs according to comparative advantage. Our classifications identify important dimensions of worker and job heterogeneity that standard classifications based on occupations and sectors miss. The equilibrium model based on our classifications more accurately predicts wage changes in response to the 2016 Olympics than a model based on occupations and sectors. Additionally, for a large simulated shock to demand for workers, we show that reduced form estimates of the effects of labor market shock exposure on workers' earnings are nearly 4 times larger when workers and jobs are classified using our classifications as opposed to occupations and sectors."}
{"title":"A Systematic Review of Approximability Results for Traveling Salesman Problems leveraging the TSP-T3CO Definition Scheme","authors":["Sophia Saller","Jana Koehler","Andreas Karrenbauer"],"raw_abstract":"The traveling salesman (or salesperson) problem, short TSP, is a problem of\nstrong interest to many researchers from mathematics, economics, and computer\nscience. Manifold TSP variants occur in nearly every scientific field and\napplication domain: engineering, physics, biology, life sciences, and\nmanufacturing just to name a few. Several thousand papers are published on\ntheoretical research or application-oriented results each year. This paper\nprovides the first systematic survey on the best currently known\napproximability and inapproximability results for well-known TSP variants such\nas the \"standard\" TSP, Path TSP, Bottleneck TSP, Maximum Scatter TSP,\nGeneralized TSP, Clustered TSP, Traveling Purchaser Problem, Profitable Tour\nProblem, Quota TSP, Prize-Collecting TSP, Orienteering Problem, Time-dependent\nTSP, TSP with Time Windows, and the Orienteering Problem with Time Windows. The\nfoundation of our survey is the definition scheme T3CO, which we propose as a\nuniform, easy-to-use and extensible means for the formal and precise definition\nof TSP variants. Applying T3CO to formally define the variant studied by a\npaper reveals subtle differences within the same named variant and also brings\nout the differences between the variants more clearly. We achieve the first\ncomprehensive, concise, and compact representation of approximability results\nby using T3CO definitions. This makes it easier to understand the\napproximability landscape and the assumptions under which certain results hold.\nOpen gaps become more evident and results can be compared more easily.","publication_date":1698853997,"paper_link":"http://arxiv.org/pdf/2311.00604v1","categories":["Economics"],"abstract":"The traveling salesman (or salesperson) problem, short TSP, is a problem of strong interest to many researchers from mathematics, economics, and computer science. Manifold TSP variants occur in nearly every scientific field and application domain: engineering, physics, biology, life sciences, and manufacturing just to name a few. Several thousand papers are published on theoretical research or application-oriented results each year. This paper provides the first systematic survey on the best currently known approximability and inapproximability results for well-known TSP variants such as the \"standard\" TSP, Path TSP, Bottleneck TSP, Maximum Scatter TSP, Generalized TSP, Clustered TSP, Traveling Purchaser Problem, Profitable Tour Problem, Quota TSP, Prize-Collecting TSP, Orienteering Problem, Time-dependent TSP, TSP with Time Windows, and the Orienteering Problem with Time Windows. The foundation of our survey is the definition scheme T3CO, which we propose as a uniform, easy-to-use and extensible means for the formal and precise definition of TSP variants. Applying T3CO to formally define the variant studied by a paper reveals subtle differences within the same named variant and also brings out the differences between the variants more clearly. We achieve the first comprehensive, concise, and compact representation of approximability results by using T3CO definitions. This makes it easier to understand the approximability landscape and the assumptions under which certain results hold. Open gaps become more evident and results can be compared more easily."}
{"title":"Crop Disease Classification using Support Vector Machines with Green Chromatic Coordinate (GCC) and Attention based feature extraction for IoT based Smart Agricultural Applications","authors":["Shashwat Jha","Vishvaditya Luhach","Gauri Shanker Gupta","Beependra Singh"],"raw_abstract":"Crops hold paramount significance as they serve as the primary provider of\nenergy, nutrition, and medicinal benefits for the human population. Plant\ndiseases, however, can negatively affect leaves during agricultural\ncultivation, resulting in significant losses in crop output and economic value.\nTherefore, it is crucial for farmers to identify crop diseases. However, this\nmethod frequently necessitates hard work, a lot of planning, and in-depth\nfamiliarity with plant pathogens. Given these numerous obstacles, it is\nessential to provide solutions that can easily interface with mobile and IoT\ndevices so that our farmers can guarantee the best possible crop development.\nVarious machine learning (ML) as well as deep learning (DL) algorithms have\nbeen created & studied for the identification of plant disease detection,\nyielding substantial and promising results. This article presents a novel\nclassification method that builds on prior work by utilising attention-based\nfeature extraction, RGB channel-based chromatic analysis, Support Vector\nMachines (SVM) for improved performance, and the ability to integrate with\nmobile applications and IoT devices after quantization of information. Several\ndisease classification algorithms were compared with the suggested model, and\nit was discovered that, in terms of accuracy, Vision Transformer-based feature\nextraction and additional Green Chromatic Coordinate feature with SVM\nclassification achieved an accuracy of (GCCViT-SVM) - 99.69%, whereas after\nquantization for IoT device integration achieved an accuracy of - 97.41% while\nalmost reducing 4x in size. Our findings have profound implications because\nthey have the potential to transform how farmers identify crop illnesses with\nprecise and fast information, thereby preserving agricultural output and\nensuring food security.","publication_date":1698835489,"paper_link":"http://arxiv.org/pdf/2311.00429v2","categories":["Electrical Engineering and Systems Science"],"abstract":"Crops hold paramount significance as they serve as the primary provider of energy, nutrition, and medicinal benefits for the human population. Plant diseases, however, can negatively affect leaves during agricultural cultivation, resulting in significant losses in crop output and economic value. Therefore, it is crucial for farmers to identify crop diseases. However, this method frequently necessitates hard work, a lot of planning, and in-depth familiarity with plant pathogens. Given these numerous obstacles, it is essential to provide solutions that can easily interface with mobile and IoT devices so that our farmers can guarantee the best possible crop development. Various machine learning (ML) as well as deep learning (DL) algorithms have been created & studied for the identification of plant disease detection, yielding substantial and promising results. This article presents a novel classification method that builds on prior work by utilising attention-based feature extraction, RGB channel-based chromatic analysis, Support Vector Machines (SVM) for improved performance, and the ability to integrate with mobile applications and IoT devices after quantization of information. Several disease classification algorithms were compared with the suggested model, and it was discovered that, in terms of accuracy, Vision Transformer-based feature extraction and additional Green Chromatic Coordinate feature with SVM classification achieved an accuracy of (GCCViT-SVM) - 99.69%, whereas after quantization for IoT device integration achieved an accuracy of - 97.41% while almost reducing 4x in size. Our findings have profound implications because they have the potential to transform how farmers identify crop illnesses with precise and fast information, thereby preserving agricultural output and ensuring food security."}
{"title":"A cost-benefit source-receptor framework for implementation of Blue-Green flood risk management","authors":["Christos Iliadis","Vassilis Glenis","Chris Kilsby"],"raw_abstract":"As floods are a major and growing source of risk in urban areas, there is a\nnecessity to improve flood risk management frameworks and civil protection\nthrough planning interventions that modify surface flow pathways and introduce\nstorage. Despite the complexity of densely urbanised areas, modern flood models\ncan represent urban features and flow characteristics to help researchers,\nlocal authorities, and insurance companies to develop and improve efficient\nflood risk frameworks to achieve resilience in cities. A cost-benefit driven\nsource-receptor flood risk framework is developed in this study to identify (1)\nlocations contributing to surface flooding (sources), (2) buildings and\nlocations at high flood risk (receptors), (3) the cost-benefit nexus between\nthe source and the receptor, and finally (4) ways to mitigate flooding at the\nreceptor by adding Blue-Green Infrastructure (BGI) in critical locations. The\nanalysis is based on five steps to identify the source and the receptor in a\nstudy area based on the flood exposure of buildings, damages arising from\nflooding and available green spaces with the best potential to add sustainable\nand resilient solutions to reduce flooding. The framework was developed using\nthe detailed hydrodynamic model CityCAT in a case study of the city centre of\nNewcastle upon Tyne, UK. The novelty of this analysis is that firstly, multiple\nstorm magnitudes (i.e. small and large floods) are used combined with a method\nto locate the areas and the buildings at flood risk and a prioritized set of\nbest places to add interventions upstream and downstream. Secondly, planning\ndecisions are informed by considering the benefit from reduced damages to\nproperties and the cost to construct resilient BGI options rather than a\nrestricted hydraulic analysis considering only flood depths and storages in\nisolation from real-world economics.","publication_date":1698834381,"paper_link":"http://arxiv.org/pdf/2311.00420v1","categories":["Economics"],"abstract":"As floods are a major and growing source of risk in urban areas, there is a necessity to improve flood risk management frameworks and civil protection through planning interventions that modify surface flow pathways and introduce storage. Despite the complexity of densely urbanised areas, modern flood models can represent urban features and flow characteristics to help researchers, local authorities, and insurance companies to develop and improve efficient flood risk frameworks to achieve resilience in cities. A cost-benefit driven source-receptor flood risk framework is developed in this study to identify (1) locations contributing to surface flooding (sources), (2) buildings and locations at high flood risk (receptors), (3) the cost-benefit nexus between the source and the receptor, and finally (4) ways to mitigate flooding at the receptor by adding Blue-Green Infrastructure (BGI) in critical locations. The analysis is based on five steps to identify the source and the receptor in a study area based on the flood exposure of buildings, damages arising from flooding and available green spaces with the best potential to add sustainable and resilient solutions to reduce flooding. The framework was developed using the detailed hydrodynamic model CityCAT in a case study of the city centre of Newcastle upon Tyne, UK. The novelty of this analysis is that firstly, multiple storm magnitudes (i.e. small and large floods) are used combined with a method to locate the areas and the buildings at flood risk and a prioritized set of best places to add interventions upstream and downstream. Secondly, planning decisions are informed by considering the benefit from reduced damages to properties and the cost to construct resilient BGI options rather than a restricted hydraulic analysis considering only flood depths and storages in isolation from real-world economics."}
{"title":"OpenForest: A data catalogue for machine learning in forest monitoring","authors":["Arthur Ouaknine","Teja Kattenborn","Etienne Lalibert\u00e9","David Rolnick"],"raw_abstract":"Forests play a crucial role in Earth's system processes and provide a suite\nof social and economic ecosystem services, but are significantly impacted by\nhuman activities, leading to a pronounced disruption of the equilibrium within\necosystems. Advancing forest monitoring worldwide offers advantages in\nmitigating human impacts and enhancing our comprehension of forest composition,\nalongside the effects of climate change. While statistical modeling has\ntraditionally found applications in forest biology, recent strides in machine\nlearning and computer vision have reached important milestones using remote\nsensing data, such as tree species identification, tree crown segmentation and\nforest biomass assessments. For this, the significance of open access data\nremains essential in enhancing such data-driven algorithms and methodologies.\nHere, we provide a comprehensive and extensive overview of 86 open access\nforest datasets across spatial scales, encompassing inventories, ground-based,\naerial-based, satellite-based recordings, and country or world maps. These\ndatasets are grouped in OpenForest, a dynamic catalogue open to contributions\nthat strives to reference all available open access forest datasets. Moreover,\nin the context of these datasets, we aim to inspire research in machine\nlearning applied to forest biology by establishing connections between\ncontemporary topics, perspectives and challenges inherent in both domains. We\nhope to encourage collaborations among scientists, fostering the sharing and\nexploration of diverse datasets through the application of machine learning\nmethods for large-scale forest monitoring. OpenForest is available at\nhttps://github.com/RolnickLab/OpenForest .","publication_date":1698811160,"paper_link":"http://arxiv.org/pdf/2311.00277v2","categories":["Economics"],"abstract":"Forests play a crucial role in Earth's system processes and provide a suite of social and economic ecosystem services, but are significantly impacted by human activities, leading to a pronounced disruption of the equilibrium within ecosystems. Advancing forest monitoring worldwide offers advantages in mitigating human impacts and enhancing our comprehension of forest composition, alongside the effects of climate change. While statistical modeling has traditionally found applications in forest biology, recent strides in machine learning and computer vision have reached important milestones using remote sensing data, such as tree species identification, tree crown segmentation and forest biomass assessments. For this, the significance of open access data remains essential in enhancing such data-driven algorithms and methodologies. Here, we provide a comprehensive and extensive overview of 86 open access forest datasets across spatial scales, encompassing inventories, ground-based, aerial-based, satellite-based recordings, and country or world maps. These datasets are grouped in OpenForest, a dynamic catalogue open to contributions that strives to reference all available open access forest datasets. Moreover, in the context of these datasets, we aim to inspire research in machine learning applied to forest biology by establishing connections between contemporary topics, perspectives and challenges inherent in both domains. We hope to encourage collaborations among scientists, fostering the sharing and exploration of diverse datasets through the application of machine learning methods for large-scale forest monitoring. OpenForest is available at https://github.com/RolnickLab/OpenForest ."}
{"title":"EdgeDis: Enabling Fast, Economical, and Reliable Data Dissemination for Mobile Edge Computing","authors":["Bo Li","Qiang He","Feifei Chen","Lingjuan Lyu","Athman Bouguettaya","Yun Yang"],"raw_abstract":"Mobile edge computing (MEC) enables web data caching in close geographic\nproximity to end users. Popular data can be cached on edge servers located less\nthan hundreds of meters away from end users. This ensures bounded latency\nguarantees for various latency-sensitive web applications. However,\ntransmitting a large volume of data out of the cloud onto many\ngeographically-distributed web servers individually can be expensive. In\naddition, web content dissemination may be interrupted by various intentional\nand accidental events in the volatile MEC environment, which undermines\ndissemination efficiency and subsequently incurs extra transmission costs. To\ntackle the above challenges, we present a novel scheme named EdgeDis that\ncoordinates data dissemination by distributed consensus among those servers. We\nanalyze EdgeDis's validity theoretically and evaluate its performance\nexperimentally. Results demonstrate that compared with baseline and\nstate-of-the-art schemes, EdgeDis: 1) is 5.97x - 7.52x faster; 2) reduces\ndissemination costs by 48.21% to 91.87%; and 3) reduces performance loss caused\nby dissemination failures by up to 97.30% in time and 96.35% in costs.","publication_date":1698810103,"paper_link":"http://arxiv.org/pdf/2311.00271v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Mobile edge computing (MEC) enables web data caching in close geographic proximity to end users. Popular data can be cached on edge servers located less than hundreds of meters away from end users. This ensures bounded latency guarantees for various latency-sensitive web applications. However, transmitting a large volume of data out of the cloud onto many geographically-distributed web servers individually can be expensive. In addition, web content dissemination may be interrupted by various intentional and accidental events in the volatile MEC environment, which undermines dissemination efficiency and subsequently incurs extra transmission costs. To tackle the above challenges, we present a novel scheme named EdgeDis that coordinates data dissemination by distributed consensus among those servers. We analyze EdgeDis's validity theoretically and evaluate its performance experimentally. Results demonstrate that compared with baseline and state-of-the-art schemes, EdgeDis: 1) is 5.97x - 7.52x faster; 2) reduces dissemination costs by 48.21% to 91.87%; and 3) reduces performance loss caused by dissemination failures by up to 97.30% in time and 96.35% in costs."}
{"title":"Coalitional Manipulations and Immunity of the Shapley Value","authors":["Christian Basteck","Frank Huettner"],"raw_abstract":"We consider manipulations in the context of coalitional games, where a\ncoalition aims to increase the total payoff of its members. An allocation rule\nis immune to coalitional manipulation if no coalition can benefit from internal\nreallocation of worth on the level of its subcoalitions\n(reallocation-proofness), and if no coalition benefits from a lower worth while\nall else remains the same (weak coalitional monotonicity). Replacing additivity\nin Shapley's original characterization by these requirements yields a new\nfoundation of the Shapley value, i.e., it is the unique efficient and symmetric\nallocation rule that awards nothing to a null player and is immune to\ncoalitional manipulations. We further find that for efficient allocation rules,\nreallocation-proofness is equivalent to constrained marginality, a weaker\nvariant of Young's marginality axiom. Our second characterization improves upon\nYoung's characterization by weakening the independence requirement intrinsic to\nmarginality.","publication_date":1698756211,"paper_link":"http://arxiv.org/pdf/2310.20415v1","categories":["Economics","Statistics"],"abstract":"We consider manipulations in the context of coalitional games, where a coalition aims to increase the total payoff of its members. An allocation rule is immune to coalitional manipulation if no coalition can benefit from internal reallocation of worth on the level of its subcoalitions (reallocation-proofness), and if no coalition benefits from a lower worth while all else remains the same (weak coalitional monotonicity). Replacing additivity in Shapley's original characterization by these requirements yields a new foundation of the Shapley value, i.e., it is the unique efficient and symmetric allocation rule that awards nothing to a null player and is immune to coalitional manipulations. We further find that for efficient allocation rules, reallocation-proofness is equivalent to constrained marginality, a weaker variant of Young's marginality axiom. Our second characterization improves upon Young's characterization by weakening the independence requirement intrinsic to marginality."}
{"title":"Optimal Monotone Mean-Variance Problem in a Catastrophe Insurance Model","authors":["Bohan Li","Junyi Guo","Xiaoqing Liang"],"raw_abstract":"This paper explores an optimal investment and reinsurance problem involving\nboth ordinary and catastrophe insurance businesses. The catastrophic events are\nmodeled as following a compound Poisson process, impacting the ordinary\ninsurance business. The claim intensity for the ordinary insurance business is\ndescribed using a Cox process with a shot-noise intensity, the jump of which is\nproportional to the size of the catastrophe event. This intensity increases\nwhen a catastrophe occurs and then decays over time. The insurer's objective is\nto maximize their terminal wealth under the Monotone Mean-Variance (MMV)\ncriterion. In contrast to the classical Mean-Variance (MV) criterion, the MMV\ncriterion is monotonic across its entire domain, aligning better with\nfundamental economic principles. We first formulate the original MMV\noptimization problem as an auxiliary zero-sum game. Through solving the\nHamilton-Jacobi-Bellman-Isaacs (HJBI) equation, explicit forms of the value\nfunction and optimal strategies are obtained. Additionally, we provides the\nefficient frontier within the MMV criterion. Several numerical examples are\npresented to demonstrate the practical implications of the results.","publication_date":1698727920,"paper_link":"http://arxiv.org/pdf/2310.20173v1","categories":["Mathematics"],"abstract":"This paper explores an optimal investment and reinsurance problem involving both ordinary and catastrophe insurance businesses. The catastrophic events are modeled as following a compound Poisson process, impacting the ordinary insurance business. The claim intensity for the ordinary insurance business is described using a Cox process with a shot-noise intensity, the jump of which is proportional to the size of the catastrophe event. This intensity increases when a catastrophe occurs and then decays over time. The insurer's objective is to maximize their terminal wealth under the Monotone Mean-Variance (MMV) criterion. In contrast to the classical Mean-Variance (MV) criterion, the MMV criterion is monotonic across its entire domain, aligning better with fundamental economic principles. We first formulate the original MMV optimization problem as an auxiliary zero-sum game. Through solving the Hamilton-Jacobi-Bellman-Isaacs (HJBI) equation, explicit forms of the value function and optimal strategies are obtained. Additionally, we provides the efficient frontier within the MMV criterion. Several numerical examples are presented to demonstrate the practical implications of the results."}
{"title":"Charging Autonomous Electric Vehicle Fleet for Mobility-on-Demand Services: Plug in or Swap out?","authors":["Jing Gao","Sen Li"],"raw_abstract":"This paper compares two prevalent charging strategies for electric vehicles,\nplug-in charging and battery swapping, to investigate which charging strategy\nis superior for electric autonomous mobility-on-demand (AMoD) systems. To this\nend, we use a queueing-theoretic model to characterize the vehicle waiting time\nat charging stations and battery swapping stations, respectively. The model is\nintegrated into an economic analysis of the electric AMoD system operated by a\ntransportation network company (TNC), where the incentives of passengers, the\ncharging/operating shift of TNC vehicles, the operational decisions of the\nplatform, and the planning decisions of the government are captured. Overall, a\nbi-level optimization framework is proposed for charging infrastructure\nplanning of the electric AMoD system. Based on the proposed framework, we\ncompare the socio-economic performance of plug-in charging and battery\nswapping, and investigate how this comparison depends on the evolving charging\ntechnologies (such as charging speed, battery capacity, and infrastructure\ncost). At the planning level, we find that when choosing plug-in charging,\nincreased charging speed leads to a transformation of infrastructure from\nsparsely distributed large stations to densely distributed small stations,\nwhile enlarged battery capacity transforms the infrastructure from densely\ndistributed small stations to sparsely distributed large stations. On the other\nhand, when choosing battery swapping, both increased charging speed and\nenlarged battery capacity will lead to a smaller number of battery swapping\nstations. At the operational level, we find that improved charging speed leads\nto increased TNC profit when choosing plug-in charging, whereas improved\ncharging speed may lead to smaller TNC profit under battery swapping. The above\ninsights are validated through realistic numerical studies.","publication_date":1698720540,"paper_link":"http://arxiv.org/pdf/2310.20130v1","categories":["Mathematics"],"abstract":"This paper compares two prevalent charging strategies for electric vehicles, plug-in charging and battery swapping, to investigate which charging strategy is superior for electric autonomous mobility-on-demand (AMoD) systems. To this end, we use a queueing-theoretic model to characterize the vehicle waiting time at charging stations and battery swapping stations, respectively. The model is integrated into an economic analysis of the electric AMoD system operated by a transportation network company (TNC), where the incentives of passengers, the charging/operating shift of TNC vehicles, the operational decisions of the platform, and the planning decisions of the government are captured. Overall, a bi-level optimization framework is proposed for charging infrastructure planning of the electric AMoD system. Based on the proposed framework, we compare the socio-economic performance of plug-in charging and battery swapping, and investigate how this comparison depends on the evolving charging technologies (such as charging speed, battery capacity, and infrastructure cost). At the planning level, we find that when choosing plug-in charging, increased charging speed leads to a transformation of infrastructure from sparsely distributed large stations to densely distributed small stations, while enlarged battery capacity transforms the infrastructure from densely distributed small stations to sparsely distributed large stations. On the other hand, when choosing battery swapping, both increased charging speed and enlarged battery capacity will lead to a smaller number of battery swapping stations. At the operational level, we find that improved charging speed leads to increased TNC profit when choosing plug-in charging, whereas improved charging speed may lead to smaller TNC profit under battery swapping. The above insights are validated through realistic numerical studies."}
{"title":"Data Market Design through Deep Learning","authors":["Sai Srivatsa Ravindranath","Yanchen Jiang","David C. Parkes"],"raw_abstract":"The $\\textit{data market design}$ problem is a problem in economic theory to\nfind a set of signaling schemes (statistical experiments) to maximize expected\nrevenue to the information seller, where each experiment reveals some of the\ninformation known to a seller and has a corresponding price [Bergemann et al.,\n2018]. Each buyer has their own decision to make in a world environment, and\ntheir subjective expected value for the information associated with a\nparticular experiment comes from the improvement in this decision and depends\non their prior and value for different outcomes. In a setting with multiple\nbuyers, a buyer's expected value for an experiment may also depend on the\ninformation sold to others [Bonatti et al., 2022]. We introduce the application\nof deep learning for the design of revenue-optimal data markets, looking to\nexpand the frontiers of what can be understood and achieved. Relative to\nearlier work on deep learning for auction design [D\\\"utting et al., 2023], we\nmust learn signaling schemes rather than allocation rules and handle\n$\\textit{obedience constraints}$ $-$ these arising from modeling the downstream\nactions of buyers $-$ in addition to incentive constraints on bids. Our\nexperiments demonstrate that this new deep learning framework can almost\nprecisely replicate all known solutions from theory, expand to more complex\nsettings, and be used to establish the optimality of new designs for data\nmarkets and make conjectures in regard to the structure of optimal designs.","publication_date":1698711669,"paper_link":"http://arxiv.org/pdf/2310.20096v1","categories":["Economics"],"abstract":"The __FORMULA__ problem is a problem in economic theory to find a set of signaling schemes (statistical experiments) to maximize expected revenue to the information seller, where each experiment reveals some of the information known to a seller and has a corresponding price [Bergemann et al., 2018]. Each buyer has their own decision to make in a world environment, and their subjective expected value for the information associated with a particular experiment comes from the improvement in this decision and depends on their prior and value for different outcomes. In a setting with multiple buyers, a buyer's expected value for an experiment may also depend on the information sold to others [Bonatti et al., 2022]. We introduce the application of deep learning for the design of revenue-optimal data markets, looking to expand the frontiers of what can be understood and achieved. Relative to earlier work on deep learning for auction design [D\\\"utting et al., 2023], we must learn signaling schemes rather than allocation rules and handle __FORMULA__ __FORMULA__ these arising from modeling the downstream actions of buyers __FORMULA__ in addition to incentive constraints on bids. Our experiments demonstrate that this new deep learning framework can almost precisely replicate all known solutions from theory, expand to more complex settings, and be used to establish the optimality of new designs for data markets and make conjectures in regard to the structure of optimal designs."}
{"title":"Hybridizing Physics and Neural ODEs for Predicting Plasma Inductance Dynamics in Tokamak Fusion Reactors","authors":["Allen M. Wang","Darren T. Garnier","Cristina Rea"],"raw_abstract":"While fusion reactors known as tokamaks hold promise as a firm energy source,\nadvances in plasma control, and handling of events where control of plasmas is\nlost, are needed for them to be economical. A significant bottleneck towards\napplying more advanced control algorithms is the need for better plasma\nsimulation, where both physics-based and data-driven approaches currently fall\nshort. The former is bottle-necked by both computational cost and the\ndifficulty of modelling plasmas, and the latter is bottle-necked by the\nrelative paucity of data. To address this issue, this work applies the neural\nordinary differential equations (ODE) framework to the problem of predicting a\nsubset of plasma dynamics, namely the coupled plasma current and internal\ninductance dynamics. As the neural ODE framework allows for the natural\ninclusion of physics-based inductive biases, we train both physics-based and\nneural network models on data from the Alcator C-Mod fusion reactor and find\nthat a model that combines physics-based equations with a neural ODE performs\nbetter than both existing physics-motivated ODEs and a pure neural ODE model.","publication_date":1698708354,"paper_link":"http://arxiv.org/pdf/2310.20079v1","categories":["Physics"],"abstract":"While fusion reactors known as tokamaks hold promise as a firm energy source, advances in plasma control, and handling of events where control of plasmas is lost, are needed for them to be economical. A significant bottleneck towards applying more advanced control algorithms is the need for better plasma simulation, where both physics-based and data-driven approaches currently fall short. The former is bottle-necked by both computational cost and the difficulty of modelling plasmas, and the latter is bottle-necked by the relative paucity of data. To address this issue, this work applies the neural ordinary differential equations (ODE) framework to the problem of predicting a subset of plasma dynamics, namely the coupled plasma current and internal inductance dynamics. As the neural ODE framework allows for the natural inclusion of physics-based inductive biases, we train both physics-based and neural network models on data from the Alcator C-Mod fusion reactor and find that a model that combines physics-based equations with a neural ODE performs better than both existing physics-motivated ODEs and a pure neural ODE model."}
{"title":"From the Top Down: Does Corruption Affect Performance?","authors":["Maurizio La Rocca","Tiziana La Rocca","Francesco Fasano","Javier Sanchez-Vidal"],"raw_abstract":"Corruption, fraud, and unethical activities have emerged as significant\nobstacles to global economic, political, and social progress. Although many\nempirical studies have focused on country-level corruption metrics, this study\nis the first to utilize a substantial international dataset to assess the\neffects of illicit and unethical managerial practices on firm performance.\nEmploying cross-sectional data, this research examines the influence of\ncorruption on corporate outcomes. Our definition of corruption evaluates the\ndegree to which managers engage in mismanagement, misconduct, or corrupt\nactivities. The repercussions for corporate governance, especially concerning\nthe process of appointing managers, are both crucial and strategic.","publication_date":1698701088,"paper_link":"http://arxiv.org/pdf/2310.20028v1","categories":["Quantitative Finance"],"abstract":"Corruption, fraud, and unethical activities have emerged as significant obstacles to global economic, political, and social progress. Although many empirical studies have focused on country-level corruption metrics, this study is the first to utilize a substantial international dataset to assess the effects of illicit and unethical managerial practices on firm performance. Employing cross-sectional data, this research examines the influence of corruption on corporate outcomes. Our definition of corruption evaluates the degree to which managers engage in mismanagement, misconduct, or corrupt activities. The repercussions for corporate governance, especially concerning the process of appointing managers, are both crucial and strategic."}
{"title":"Robust Estimation of Realized Correlation: New Insight about Intraday Fluctuations in Market Betas","authors":["Peter Reinhard Hansen","Yiyao Luo"],"raw_abstract":"Time-varying volatility is an inherent feature of most economic time-series,\nwhich causes standard correlation estimators to be inconsistent. The quadrant\ncorrelation estimator is consistent but very inefficient. We propose a novel\nsubsampled quadrant estimator that improves efficiency while preserving\nconsistency and robustness. This estimator is particularly well-suited for\nhigh-frequency financial data and we apply it to a large panel of US stocks.\nOur empirical analysis sheds new light on intra-day fluctuations in market\nbetas by decomposing them into time-varying correlations and relative\nvolatility changes. Our results show that intraday variation in betas is\nprimarily driven by intraday variation in correlations.","publication_date":1698697271,"paper_link":"http://arxiv.org/pdf/2310.19992v1","categories":["Statistics","Economics","Quantitative Finance"],"abstract":"Time-varying volatility is an inherent feature of most economic time-series, which causes standard correlation estimators to be inconsistent. The quadrant correlation estimator is consistent but very inefficient. We propose a novel subsampled quadrant estimator that improves efficiency while preserving consistency and robustness. This estimator is particularly well-suited for high-frequency financial data and we apply it to a large panel of US stocks. Our empirical analysis sheds new light on intra-day fluctuations in market betas by decomposing them into time-varying correlations and relative volatility changes. Our results show that intraday variation in betas is primarily driven by intraday variation in correlations."}
{"title":"Kinetic Exchange Models of Income and Wealth Distribution: Self Organization and Poverty Level","authors":["Sanjukta Paul","Bikas K. Chakrabarti"],"raw_abstract":"In this invited book chapter, we draw the reader to a brief review of the\ndifferent Kinetic Exchange Models (KEMs) that have gradually developed for\nmarkets and how they can be employed to quantitatively study inequalities (the\nGini Index and the Kolkata Index) that pervade real economies. Since a\nmany-body economical market can be studied using tested laws of physics, these\nmodels have the freedom to incorporate provisions like inclusion of individual\nsaving behaviors while trading and rendering these saving behaviors to be\ntime-independent and time-dependent respectively. This is to observe when and\nhow the well known exponential distribution for no-saving gives rise to a\ndistribution with a most probable income. A review of the earlier cases along\nwith their implementation with a bias, where the population in the low-income\nbracket of the society is favored by selecting one of the traders in the\ntrading process to be the poorest of all, follows. The biased selection of\nagents ultimately giving rise to self-organizing features in the money\ndistribution has also been reviewed in detail, using the behavior of the\nSelf-Organized Poverty Level. In the end, we elucidate the recent endeavors of\nthe KEMs in finding answers to the growing condensation of wealth in the hands\nof a countable few rich people and providing probable solutions that can curb\nfurther expansion of oligarchic societies.","publication_date":1698685293,"paper_link":"http://arxiv.org/pdf/2310.19738v1","categories":["Physics"],"abstract":"In this invited book chapter, we draw the reader to a brief review of the different Kinetic Exchange Models (KEMs) that have gradually developed for markets and how they can be employed to quantitatively study inequalities (the Gini Index and the Kolkata Index) that pervade real economies. Since a many-body economical market can be studied using tested laws of physics, these models have the freedom to incorporate provisions like inclusion of individual saving behaviors while trading and rendering these saving behaviors to be time-independent and time-dependent respectively. This is to observe when and how the well known exponential distribution for no-saving gives rise to a distribution with a most probable income. A review of the earlier cases along with their implementation with a bias, where the population in the low-income bracket of the society is favored by selecting one of the traders in the trading process to be the poorest of all, follows. The biased selection of agents ultimately giving rise to self-organizing features in the money distribution has also been reviewed in detail, using the behavior of the Self-Organized Poverty Level. In the end, we elucidate the recent endeavors of the KEMs in finding answers to the growing condensation of wealth in the hands of a countable few rich people and providing probable solutions that can curb further expansion of oligarchic societies."}
{"title":"Self-assembled physical unclonable function labels based on plasmonic coupling","authors":["Mihir Dass","Lena Raab","Christoph Pauer","Christoph Sikeler","Larissa Heinze","Joe Tavacoli","Irina V. Martynenko","Ulrich R\u00fchrmair","Gregor Posnjak","Tim Liedl"],"raw_abstract":"Counterfeiting threatens human health, social equity, national security and\nglobal and local economies. Hardware-based cryptography that exploits physical\nunclonable functions (PUFs) provides the means for secure identification and\nauthentication of products. While optical PUFs are among the hardest to\nreplicate, they suffer from low encoding capacity and often complex and\nexpensive read-out. Here we report PUF labels with nanoscale features and\noptical responses that arise from the guided self-assembly of plasmonic\nnanoparticles. Nanosphere lithography combined with DNA origami placement are\nused to create tightly packed randomised nanoparticle assemblies. Nanoscale\nvariations within these assemblies define the scattering color of the\nindividual spots that are arranged in a hexagonal lattice with spacing down to\nthe optical resolution limit. Due to the nanoscale dimensions, the intrinsic\nrandomness of the particle assemblies and their resulting optical responses,\nour PUFs are virtually impossible to replicate while they can be read-out with\neconomical 3D-printed hardware.","publication_date":1698677067,"paper_link":"http://arxiv.org/pdf/2310.19587v2","categories":["Physics"],"abstract":"Counterfeiting threatens human health, social equity, national security and global and local economies. Hardware-based cryptography that exploits physical unclonable functions (PUFs) provides the means for secure identification and authentication of products. While optical PUFs are among the hardest to replicate, they suffer from low encoding capacity and often complex and expensive read-out. Here we report PUF labels with nanoscale features and optical responses that arise from the guided self-assembly of plasmonic nanoparticles. Nanosphere lithography combined with DNA origami placement are used to create tightly packed randomised nanoparticle assemblies. Nanoscale variations within these assemblies define the scattering color of the individual spots that are arranged in a hexagonal lattice with spacing down to the optical resolution limit. Due to the nanoscale dimensions, the intrinsic randomness of the particle assemblies and their resulting optical responses, our PUFs are virtually impossible to replicate while they can be read-out with economical 3D-printed hardware."}
{"title":"Green ammonia supply chain and associated market structure: an analysis based on transaction cost economics","authors":["Hanxin Zhao"],"raw_abstract":"Green ammonia is poised to be a key part in the hydrogen economy. This paper\ndiscusses green ammonia supply chains from a higher-level industry perspective\nwith a focus on market structures. The architecture of upstream and downstream\nsupply chains are explored. Potential ways to accelerate market emergence are\ndiscussed. Market structure is explored based on transaction cost economics and\nlessons from the oil and gas industry. Three market structure prototypes are\ndeveloped for different phases. In the infancy, a highly vertically integrated\nstructure is proposed to reduce risks and ensure capital recovery. A\nrestructuring towards a disintegrated structure is necessary in the next stage\nto improve the efficiency. In the late stage, a competitive structure\ncharacterized by a separation between asset ownership and production activities\nand further development of short-term and spot markets is proposed towards a\nmarket-driven industry. Finally, a multi-linear regression model is developed\nto evaluate the developed structures using a case in the gas industry. Results\nindicate that high asset specificity and uncertainty and low frequency lead to\na more disintegrated market structure, and vice versa, thus supporting the\nstructures designed. We assume the findings and results contribute to\ndeveloping green ammonia supply chains and the hydrogen economy.","publication_date":1698669593,"paper_link":"http://arxiv.org/pdf/2310.19498v1","categories":["Economics","Quantitative Finance"],"abstract":"Green ammonia is poised to be a key part in the hydrogen economy. This paper discusses green ammonia supply chains from a higher-level industry perspective with a focus on market structures. The architecture of upstream and downstream supply chains are explored. Potential ways to accelerate market emergence are discussed. Market structure is explored based on transaction cost economics and lessons from the oil and gas industry. Three market structure prototypes are developed for different phases. In the infancy, a highly vertically integrated structure is proposed to reduce risks and ensure capital recovery. A restructuring towards a disintegrated structure is necessary in the next stage to improve the efficiency. In the late stage, a competitive structure characterized by a separation between asset ownership and production activities and further development of short-term and spot markets is proposed towards a market-driven industry. Finally, a multi-linear regression model is developed to evaluate the developed structures using a case in the gas industry. Results indicate that high asset specificity and uncertainty and low frequency lead to a more disintegrated market structure, and vice versa, thus supporting the structures designed. We assume the findings and results contribute to developing green ammonia supply chains and the hydrogen economy."}
{"title":"Multilateral matching with scale economies","authors":["Chao Huang"],"raw_abstract":"This paper studies multilateral matching in which any set of agents can\nnegotiate contracts. We assume scale economies in the sense that an agent\nsubstitutes some contracts with some new contracts only if the newly signed\ncontracts involve a weakly larger set of partners. We show that a weakly\nsetwise stable outcome exists in a market with scale economies and a setwise\nstable outcome exists under a stronger scale economies condition. Our\nconditions apply to environments in which more partners bring advantages, and\nallow agents to bargain over contracts signed by them.","publication_date":1698667773,"paper_link":"http://arxiv.org/pdf/2310.19479v1","categories":["Economics"],"abstract":"This paper studies multilateral matching in which any set of agents can negotiate contracts. We assume scale economies in the sense that an agent substitutes some contracts with some new contracts only if the newly signed contracts involve a weakly larger set of partners. We show that a weakly setwise stable outcome exists in a market with scale economies and a setwise stable outcome exists under a stronger scale economies condition. Our conditions apply to environments in which more partners bring advantages, and allow agents to bargain over contracts signed by them."}
{"title":"Regret-Minimization Algorithms for Multi-Agent Cooperative Learning Systems","authors":["Jialin Yi"],"raw_abstract":"A Multi-Agent Cooperative Learning (MACL) system is an artificial\nintelligence (AI) system where multiple learning agents work together to\ncomplete a common task. Recent empirical success of MACL systems in various\ndomains (e.g. traffic control, cloud computing, robotics) has sparked active\nresearch into the design and analysis of MACL systems for sequential decision\nmaking problems. One important metric of the learning algorithm for decision\nmaking problems is its regret, i.e. the difference between the highest\nachievable reward and the actual reward that the algorithm gains. The design\nand development of a MACL system with low-regret learning algorithms can create\nhuge economic values. In this thesis, I analyze MACL systems for different\nsequential decision making problems. Concretely, the Chapter 3 and 4\ninvestigate the cooperative multi-agent multi-armed bandit problems, with\nfull-information or bandit feedback, in which multiple learning agents can\nexchange their information through a communication network and the agents can\nonly observe the rewards of the actions they choose. Chapter 5 considers the\ncommunication-regret trade-off for online convex optimization in the\ndistributed setting. Chapter 6 discusses how to form high-productive teams for\nagents based on their unknown but fixed types using adaptive incremental\nmatchings. For the above problems, I present the regret lower bounds for\nfeasible learning algorithms and provide the efficient algorithms to achieve\nthis bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the\nregret depends on the connectivity of the communication network and the\ncommunication delay, thus giving useful guidance on design of the communication\nprotocol in MACL systems","publication_date":1698666631,"paper_link":"http://arxiv.org/pdf/2310.19468v1","categories":["Statistics"],"abstract":"A Multi-Agent Cooperative Learning (MACL) system is an artificial intelligence (AI) system where multiple learning agents work together to complete a common task. Recent empirical success of MACL systems in various domains (e.g. traffic control, cloud computing, robotics) has sparked active research into the design and analysis of MACL systems for sequential decision making problems. One important metric of the learning algorithm for decision making problems is its regret, i.e. the difference between the highest achievable reward and the actual reward that the algorithm gains. The design and development of a MACL system with low-regret learning algorithms can create huge economic values. In this thesis, I analyze MACL systems for different sequential decision making problems. Concretely, the Chapter 3 and 4 investigate the cooperative multi-agent multi-armed bandit problems, with full-information or bandit feedback, in which multiple learning agents can exchange their information through a communication network and the agents can only observe the rewards of the actions they choose. Chapter 5 considers the communication-regret trade-off for online convex optimization in the distributed setting. Chapter 6 discusses how to form high-productive teams for agents based on their unknown but fixed types using adaptive incremental matchings. For the above problems, I present the regret lower bounds for feasible learning algorithms and provide the efficient algorithms to achieve this bound. The regret bounds I present in Chapter 3, 4 and 5 quantify how the regret depends on the connectivity of the communication network and the communication delay, thus giving useful guidance on design of the communication protocol in MACL systems"}
{"title":"Enhancing Time Series Aggregation For Power System Optimization Models: Incorporating Network and Ramping Constraints","authors":["David Cardona-Vasquez","Thomas Klatzer","Sonja Wogrin"],"raw_abstract":"Power system optimization models are large mathematical models used by\nresearchers and policymakers that pose tractability issues when representing\nreal-world systems. Several aggregation techniques have been proposed to\naddress these computational challenges and it remains a relevant topic in power\nsystems research. In this paper, we extend a recently developed Basis-Oriented\ntime series aggregation approach used for power system optimization models that\naggregates time steps within their Simplex basis. This has proven to be an\nexact aggregation for simple economic dispatch problems. We extend this\nmethodology to include network and ramping constraints; for the latter (and to\nhandle temporal linking), we develop a heuristic algorithm that finds an exact\npartition of the input data, which is then aggregated. Our numerical results,\nfor a simple 3-Bus system, indicate that: with network constraints only, we can\nachieve a computational reduction by a factor of 1747 (measured in the number\nof variables of the optimization model), and of 12 with ramping constraints.\nMoreover, our findings indicate that with temporal linking constraints,\naggregations of variable length must be employed to obtain an exact result (the\nsame objective function value in the aggregated model) while maintaining the\ncomputational tractability, this implies that the duration of the aggregations\ndoes not necessarily correspond to commonly used lengths like days or weeks.\nFinally, our results support previous research concerning the importance of\nextreme periods on model results.","publication_date":1698657713,"paper_link":"http://arxiv.org/pdf/2310.19369v1","categories":["Mathematics"],"abstract":"Power system optimization models are large mathematical models used by researchers and policymakers that pose tractability issues when representing real-world systems. Several aggregation techniques have been proposed to address these computational challenges and it remains a relevant topic in power systems research. In this paper, we extend a recently developed Basis-Oriented time series aggregation approach used for power system optimization models that aggregates time steps within their Simplex basis. This has proven to be an exact aggregation for simple economic dispatch problems. We extend this methodology to include network and ramping constraints; for the latter (and to handle temporal linking), we develop a heuristic algorithm that finds an exact partition of the input data, which is then aggregated. Our numerical results, for a simple 3-Bus system, indicate that: with network constraints only, we can achieve a computational reduction by a factor of 1747 (measured in the number of variables of the optimization model), and of 12 with ramping constraints. Moreover, our findings indicate that with temporal linking constraints, aggregations of variable length must be employed to obtain an exact result (the same objective function value in the aggregated model) while maintaining the computational tractability, this implies that the duration of the aggregations does not necessarily correspond to commonly used lengths like days or weeks. Finally, our results support previous research concerning the importance of extreme periods on model results."}
{"title":"Modeling the Telemarketing Process using Genetic Algorithms and Extreme Boosting: Feature Selection and Cost-Sensitive Analytical Approach","authors":["Nazeeh Ghatasheh","Ismail Altaharwa","Khaled Aldebei"],"raw_abstract":"Currently, almost all direct marketing activities take place virtually rather\nthan in person, weakening interpersonal skills at an alarming pace.\nFurthermore, businesses have been striving to sense and foster the tendency of\ntheir clients to accept a marketing offer. The digital transformation and the\nincreased virtual presence forced firms to seek novel marketing research\napproaches. This research aims at leveraging the power of telemarketing data in\nmodeling the willingness of clients to make a term deposit and finding the most\nsignificant characteristics of the clients. Real-world data from a Portuguese\nbank and national socio-economic metrics are used to model the telemarketing\ndecision-making process. This research makes two key contributions. First,\npropose a novel genetic algorithm-based classifier to select the best\ndiscriminating features and tune classifier parameters simultaneously. Second,\nbuild an explainable prediction model. The best-generated classification models\nwere intensively validated using 50 times repeated 10-fold stratified\ncross-validation and the selected features have been analyzed. The models\nsignificantly outperform the related works in terms of class of interest\naccuracy, they attained an average of 89.07\\% and 0.059 in terms of geometric\nmean and type I error respectively. The model is expected to maximize the\npotential profit margin at the least possible cost and provide more insights to\nsupport marketing decision-making.","publication_date":1698655615,"paper_link":"http://arxiv.org/pdf/2310.19843v1","categories":["Economics"],"abstract":"Currently, almost all direct marketing activities take place virtually rather than in person, weakening interpersonal skills at an alarming pace. Furthermore, businesses have been striving to sense and foster the tendency of their clients to accept a marketing offer. The digital transformation and the increased virtual presence forced firms to seek novel marketing research approaches. This research aims at leveraging the power of telemarketing data in modeling the willingness of clients to make a term deposit and finding the most significant characteristics of the clients. Real-world data from a Portuguese bank and national socio-economic metrics are used to model the telemarketing decision-making process. This research makes two key contributions. First, propose a novel genetic algorithm-based classifier to select the best discriminating features and tune classifier parameters simultaneously. Second, build an explainable prediction model. The best-generated classification models were intensively validated using 50 times repeated 10-fold stratified cross-validation and the selected features have been analyzed. The models significantly outperform the related works in terms of class of interest accuracy, they attained an average of 89.07\\% and 0.059 in terms of geometric mean and type I error respectively. The model is expected to maximize the potential profit margin at the least possible cost and provide more insights to support marketing decision-making."}
{"title":"The minimax property in infinite two-person win-lose games","authors":["Ron Holzman"],"raw_abstract":"We explore a version of the minimax theorem for two-person win-lose games\nwith infinitely many pure strategies. In the countable case, we give a\ncombinatorial condition on the game which implies the minimax property. In the\ngeneral case, we prove that a game satisfies the minimax property along with\nall its subgames if and only if none of its subgames is isomorphic to the\n\"larger number game.\" This generalizes a recent theorem of Hanneke, Livni and\nMoran. We also propose several applications of our results outside of game\ntheory.","publication_date":1698650512,"paper_link":"http://arxiv.org/pdf/2310.19314v1","categories":["Mathematics","Economics"],"abstract":"We explore a version of the minimax theorem for two-person win-lose games with infinitely many pure strategies. In the countable case, we give a combinatorial condition on the game which implies the minimax property. In the general case, we prove that a game satisfies the minimax property along with all its subgames if and only if none of its subgames is isomorphic to the \"larger number game.\" This generalizes a recent theorem of Hanneke, Livni and Moran. We also propose several applications of our results outside of game theory."}
{"title":"Cram\u00e9r-Rao Bounds for the Simultaneous Estimation of Power System Electromechanical Modes and Forced Oscillations","authors":["Luke Dosiek"],"raw_abstract":"In this paper, the Cram\\'{e}r-Rao Bounds (CRB) for the simultaneous\nestimation of power system electromechanical modes and forced oscillations (FO)\nare derived. Two cases are considered; in the first case only the steady-state\nresponse to the FO is present in the measured system output used by estimation\nalgorithms. In the second, the startup transient of the FO is present in\naddition to the steady-state response. The CRBs are analyzed numerically to\nexplore sensitivities to FO frequency, signal-to-noise ratio (SNR) and\nobservation window length. It is demonstrated that 1) the CRB of FO parameters\nis not affected by the presence of the transient response, 2) the CRB of the\nsystem modes is not affected by the presence of an FO in steady-state and 3)\nthe CRB of the system modes can be drastically reduced by the presence of a FO\nstartup transient.","publication_date":1700679535,"paper_link":"http://arxiv.org/pdf/2311.13598v1","categories":["Electrical Engineering and Systems Science"],"abstract":"In this paper, the Cram\\'{e}r-Rao Bounds (CRB) for the simultaneous estimation of power system electromechanical modes and forced oscillations (FO) are derived. Two cases are considered; in the first case only the steady-state response to the FO is present in the measured system output used by estimation algorithms. In the second, the startup transient of the FO is present in addition to the steady-state response. The CRBs are analyzed numerically to explore sensitivities to FO frequency, signal-to-noise ratio (SNR) and observation window length. It is demonstrated that 1) the CRB of FO parameters is not affected by the presence of the transient response, 2) the CRB of the system modes is not affected by the presence of an FO in steady-state and 3) the CRB of the system modes can be drastically reduced by the presence of a FO startup transient."}
{"title":"Quadratic residues and domino tilings","authors":["Yuhi Kamio","Junnosuke Koizumi","Toshihiko Nakazawa"],"raw_abstract":"The formula for the number of domino tilings due to Kasteleyn and\nTemperley-Fisher is strikingly similar to Eisenstein's formula for the Legendre\nsymbol. We study the connection between these two concepts and prove a formula\nwhich expresses the Jacobi symbol in terms of domino tilings.","publication_date":1700679532,"paper_link":"http://arxiv.org/pdf/2311.13597v1","categories":["Mathematics"],"abstract":"The formula for the number of domino tilings due to Kasteleyn and Temperley-Fisher is strikingly similar to Eisenstein's formula for the Legendre symbol. We study the connection between these two concepts and prove a formula which expresses the Jacobi symbol in terms of domino tilings."}
{"title":"Partial Resolutions of Affine Symplectic Singularities","authors":["Alberto San Miguel Malaney"],"raw_abstract":"We explore the relationship between the Poisson deformation theory,\nbirational geometry, and Springer theory of crepant partial resolutions of\nconical affine symplectic singularities. We show that given any crepant partial\nresolution $X'$, the Poisson deformation functor of $X'$ is prorepresentable\nand unobstructed. Additionally, we define a version of the Namikawa Weyl group\n$W_{X'}$ for partial resolutions, such that $W_{X'}$ is a parabolic subgroup of\nthe Namikawa Weyl group $W_X$ determined by the birational geometry of $X'$, in\nparticular by a face of a relative movable cone. If $Y$ is a Q-factorial\nterminalization which covers $X'$, we show there is a natural morphism from\nPoisson deformations of $Y$ to those of $X'$, and that this morphism is a\nGalois covering with Galois group $W_{X'}$, building on work of Namikawa.\nFinally, we put these partial resolutions and their universal deformations into\nthe context of work of McGerty and Nevins, obtaining some preliminary results\nconcerning their Springer theory.","publication_date":1700679304,"paper_link":"http://arxiv.org/pdf/2311.13593v1","categories":["Mathematics"],"abstract":"We explore the relationship between the Poisson deformation theory, birational geometry, and Springer theory of crepant partial resolutions of conical affine symplectic singularities. We show that given any crepant partial resolution __FORMULA__, the Poisson deformation functor of __FORMULA__ is prorepresentable and unobstructed. Additionally, we define a version of the Namikawa Weyl group __FORMULA__ for partial resolutions, such that __FORMULA__ is a parabolic subgroup of the Namikawa Weyl group __FORMULA__ determined by the birational geometry of __FORMULA__, in particular by a face of a relative movable cone. If __FORMULA__ is a Q-factorial terminalization which covers __FORMULA__, we show there is a natural morphism from Poisson deformations of __FORMULA__ to those of __FORMULA__, and that this morphism is a Galois covering with Galois group __FORMULA__, building on work of Namikawa. Finally, we put these partial resolutions and their universal deformations into the context of work of McGerty and Nevins, obtaining some preliminary results concerning their Springer theory."}
{"title":"High-efficiency high-NA metalens designed by maximizing the efficiency limit","authors":["Shiyu Li","Ho-Chun Lin","Chia Wei Hsu"],"raw_abstract":"Theoretical bounds are commonly used to assess the limitations of photonic\ndesign. Here we employ a more active role for theoretical bounds by using them\nto identify optimal system parameters that maximize the upper limit of\nefficiency. As an example, we consider wide-field-of-view\nhigh-numerical-aperture metalenses, which can be used for high-resolution\nimaging in microscopy and endoscopy, but no existing design has achieved a high\nefficiency. By first maximizing the efficiency limit and then performing\ninverse design, we come up with a high-numerical-aperture (NA = 0.9) metalens\ndesign with 98% transmission efficiency and 92% Strehl ratio across all\nincident angles within a 60-deg field of view, reaching the maximized bound.\nThis maximizing-efficiency-limit approach applies to any multi-channel system\nand can help a wide range of optical devices reach their highest possible\nperformance.","publication_date":1700679204,"paper_link":"http://arxiv.org/pdf/2311.13592v1","categories":["Physics"],"abstract":"Theoretical bounds are commonly used to assess the limitations of photonic design. Here we employ a more active role for theoretical bounds by using them to identify optimal system parameters that maximize the upper limit of efficiency. As an example, we consider wide-field-of-view high-numerical-aperture metalenses, which can be used for high-resolution imaging in microscopy and endoscopy, but no existing design has achieved a high efficiency. By first maximizing the efficiency limit and then performing inverse design, we come up with a high-numerical-aperture (NA = 0.9) metalens design with 98% transmission efficiency and 92% Strehl ratio across all incident angles within a 60-deg field of view, reaching the maximized bound. This maximizing-efficiency-limit approach applies to any multi-channel system and can help a wide range of optical devices reach their highest possible performance."}
{"title":"Risk-sensitive Markov Decision Process and Learning under General Utility Functions","authors":["Zhengqi Wu","Renyuan Xu"],"raw_abstract":"Reinforcement Learning (RL) has gained substantial attention across diverse\napplication domains and theoretical investigations. Existing literature on RL\ntheory largely focuses on risk-neutral settings where the decision-maker learns\nto maximize the expected cumulative reward. However, in practical scenarios\nsuch as portfolio management and e-commerce recommendations, decision-makers\noften persist in heterogeneous risk preferences subject to outcome\nuncertainties, which can not be well-captured by the risk-neural framework.\nIncorporating these preferences can be approached through utility theory, yet\nthe development of risk-sensitive RL under general utility functions remains an\nopen question for theoretical exploration.\n  In this paper, we consider a scenario where the decision-maker seeks to\noptimize a general utility function of the cumulative reward in the framework\nof a Markov decision process (MDP). To facilitate the Dynamic Programming\nPrinciple and Bellman equation, we enlarge the state space with an additional\ndimension that accounts for the cumulative reward. We propose a discretized\napproximation scheme to the MDP under enlarged state space, which is tractable\nand key for algorithmic design. We then propose a modified value iteration\nalgorithm that employs an epsilon-covering over the space of cumulative reward.\nWhen a simulator is accessible, our algorithm efficiently learns a near-optimal\npolicy with guaranteed sample complexity. In the absence of a simulator, our\nalgorithm, designed with an upper-confidence-bound exploration approach,\nidentifies a near-optimal policy while ensuring a guaranteed regret bound. For\nboth algorithms, we match the theoretical lower bounds for the risk-neutral\nsetting.","publication_date":1700679006,"paper_link":"http://arxiv.org/pdf/2311.13589v1","categories":["Mathematics"],"abstract":"Reinforcement Learning (RL) has gained substantial attention across diverse application domains and theoretical investigations. Existing literature on RL theory largely focuses on risk-neutral settings where the decision-maker learns to maximize the expected cumulative reward. However, in practical scenarios such as portfolio management and e-commerce recommendations, decision-makers often persist in heterogeneous risk preferences subject to outcome uncertainties, which can not be well-captured by the risk-neural framework. Incorporating these preferences can be approached through utility theory, yet the development of risk-sensitive RL under general utility functions remains an open question for theoretical exploration.   In this paper, we consider a scenario where the decision-maker seeks to optimize a general utility function of the cumulative reward in the framework of a Markov decision process (MDP). To facilitate the Dynamic Programming Principle and Bellman equation, we enlarge the state space with an additional dimension that accounts for the cumulative reward. We propose a discretized approximation scheme to the MDP under enlarged state space, which is tractable and key for algorithmic design. We then propose a modified value iteration algorithm that employs an epsilon-covering over the space of cumulative reward. When a simulator is accessible, our algorithm efficiently learns a near-optimal policy with guaranteed sample complexity. In the absence of a simulator, our algorithm, designed with an upper-confidence-bound exploration approach, identifies a near-optimal policy while ensuring a guaranteed regret bound. For both algorithms, we match the theoretical lower bounds for the risk-neutral setting."}
{"title":"Summary Reports Optimization in the Privacy Sandbox Attribution Reporting API","authors":["Hidayet Aksu","Badih Ghazi","Pritish Kamath","Ravi Kumar","Pasin Manurangsi","Adam Sealfon","Avinash V Varadarajan"],"raw_abstract":"The Privacy Sandbox Attribution Reporting API has been recently deployed by\nGoogle Chrome to support the basic advertising functionality of attribution\nreporting (aka conversion measurement) after deprecation of third-party\ncookies. The API implements a collection of privacy-enhancing guardrails\nincluding contribution bounding and noise injection. It also offers flexibility\nfor the analyst to allocate the contribution budget.\n  In this work, we present methods for optimizing the allocation of the\ncontribution budget for summary reports from the Attribution Reporting API. We\nevaluate them on real-world datasets as well as on a synthetic data model that\nwe find to accurately capture real-world conversion data. Our results\ndemonstrate that optimizing the parameters that can be set by the analyst can\nsignificantly improve the utility achieved by querying the API while satisfying\nthe same privacy bounds.","publication_date":1700678720,"paper_link":"http://arxiv.org/pdf/2311.13586v1","categories":["Electrical Engineering and Systems Science"],"abstract":"The Privacy Sandbox Attribution Reporting API has been recently deployed by Google Chrome to support the basic advertising functionality of attribution reporting (aka conversion measurement) after deprecation of third-party cookies. The API implements a collection of privacy-enhancing guardrails including contribution bounding and noise injection. It also offers flexibility for the analyst to allocate the contribution budget.   In this work, we present methods for optimizing the allocation of the contribution budget for summary reports from the Attribution Reporting API. We evaluate them on real-world datasets as well as on a synthetic data model that we find to accurately capture real-world conversion data. Our results demonstrate that optimizing the parameters that can be set by the analyst can significantly improve the utility achieved by querying the API while satisfying the same privacy bounds."}
{"title":"On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates","authors":["Stefano Bruno","Ying Zhang","Dong-Young Lim","\u00d6mer Deniz Akyildiz","Sotirios Sabanis"],"raw_abstract":"We provide full theoretical guarantees for the convergence behaviour of\ndiffusion-based generative models under the assumption of strongly logconcave\ndata distributions while our approximating class of functions used for score\nestimation is made of Lipschitz continuous functions. We demonstrate via a\nmotivating example, sampling from a Gaussian distribution with unknown mean,\nthe powerfulness of our approach. In this case, explicit estimates are provided\nfor the associated optimization problem, i.e. score approximation, while these\nare combined with the corresponding sampling estimates. As a result, we obtain\nthe best known upper bound estimates in terms of key quantities of interest,\nsuch as the dimension and rates of convergence, for the Wasserstein-2 distance\nbetween the data distribution (Gaussian with unknown mean) and our sampling\nalgorithm.\n  Beyond the motivating example and in order to allow for the use of a diverse\nrange of stochastic optimizers, we present our results using an $L^2$-accurate\nscore estimation assumption, which crucially is formed under an expectation\nwith respect to the stochastic optimizer and our novel auxiliary process that\nuses only known information. This approach yields the best known convergence\nrate for our sampling algorithm.","publication_date":1700678445,"paper_link":"http://arxiv.org/pdf/2311.13584v1","categories":["Mathematics","Statistics"],"abstract":"We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly logconcave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range of stochastic optimizers, we present our results using an __FORMULA__-accurate score estimation assumption, which crucially is formed under an expectation with respect to the stochastic optimizer and our novel auxiliary process that uses only known information. This approach yields the best known convergence rate for our sampling algorithm."}
{"title":"Some Upper Bounds on Ramsey Numbers Involving $C_4$","authors":["Luis Boza","Stanis\u0142aw Radziszowski"],"raw_abstract":"We obtain some new upper bounds on the Ramsey numbers of the form\n$R(\\underbrace{C_4,\\ldots,C_4}_m,G_1,\\ldots,G_n)$, where $m\\ge 1$ and\n$G_1,\\ldots,G_n$ are arbitrary graphs. We focus on the cases of $G_i$'s being\ncomplete, star $K_{1,k}$ or book graphs $B_k$, where $B_k=K_2+kK_1$. If $k\\ge\n2$, then our main upper bound theorem implies that $$R(C_4,B_k) \\le\nR(C_4,K_{1,k})+\\left\\lceil\\sqrt{R(C_4,K_{1,k})}\\right\\rceil+1.$$\n  Our techniques are used to obtain new upper bounds in several concrete cases,\nincluding: $R(C_4,K_{11})\\leq 43$, $R(C_4,K_{12})\\leq 51$, $R(C_4,K_3,K_4)\\leq\n29$, $R(C_4, K_4,K_4)\\leq 66$, $R(C_4,K_3,K_3,K_3)\\leq 57$,\n$R(C_4,C_4,K_3,K_4)\\leq 75$, and $R(C_4,C_4,K_4,K_4)\\leq 177$, and also\n$R(C_4,B_{17})\\leq 28$.","publication_date":1700678262,"paper_link":"http://arxiv.org/pdf/2311.13582v1","categories":["Mathematics"],"abstract":"We obtain some new upper bounds on the Ramsey numbers of the form __FORMULA__, where __FORMULA__ and __FORMULA__ are arbitrary graphs. We focus on the cases of __FORMULA__'s being complete, star __FORMULA__ or book graphs __FORMULA__, where __FORMULA__. If __FORMULA__, then our main upper bound theorem implies that $__FORMULA____FORMULA__R(C_4,K_{11})\\leq 43__FORMULA__R(C_4,K_{12})\\leq 51__FORMULA__R(C_4,K_3,K_4)\\leq 29__FORMULA__R(C_4, K_4,K_4)\\leq 66__FORMULA__R(C_4,K_3,K_3,K_3)\\leq 57__FORMULA__R(C_4,C_4,K_3,K_4)\\leq 75__FORMULA__R(C_4,C_4,K_4,K_4)\\leq 177__FORMULA__R(C_4,B_{17})\\leq 28$."}
{"title":"$\u03c3$-PCA: a unified neural model for linear and nonlinear principal component analysis","authors":["Fahdi Kanavati","Lucy Katsnith","Masayuki Tsuneki"],"raw_abstract":"Linear principal component analysis (PCA), nonlinear PCA, and linear\nindependent component analysis (ICA) -- those are three methods with\nsingle-layer autoencoder formulations for learning linear transformations from\ndata. Linear PCA learns orthogonal transformations (rotations) that orient axes\nto maximise variance, but it suffers from a subspace rotational indeterminacy:\nit fails to find a unique rotation for axes that share the same variance. Both\nnonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational\nto permutational by maximising statistical independence under the assumption of\nunit variance. The main difference between them is that nonlinear PCA only\nlearns rotations while linear ICA learns not just rotations but any linear\ntransformation with unit variance. The relationship between all three can be\nunderstood by the singular value decomposition of the linear ICA transformation\ninto a sequence of rotation, scale, rotation. Linear PCA learns the first\nrotation; nonlinear PCA learns the second. The scale is simply the inverse of\nthe standard deviations. The problem is that, in contrast to linear PCA,\nconventional nonlinear PCA cannot be used directly on the data to learn the\nfirst rotation, the first being special as it reduces dimensionality and orders\nby variances. In this paper, we have identified the cause, and as a solution we\npropose $\\sigma$-PCA: a unified neural model for linear and nonlinear PCA as\nsingle-layer autoencoders. One of its key ingredients: modelling not just the\nrotation but also the scale -- the variances. This model bridges the disparity\nbetween linear and nonlinear PCA. And so, like linear PCA, it can learn a\nsemi-orthogonal transformation that reduces dimensionality and orders by\nvariances, but, unlike linear PCA, it does not suffer from rotational\nindeterminacy.","publication_date":1700678089,"paper_link":"http://arxiv.org/pdf/2311.13580v1","categories":["Statistics"],"abstract":"Linear principal component analysis (PCA), nonlinear PCA, and linear independent component analysis (ICA) -- those are three methods with single-layer autoencoder formulations for learning linear transformations from data. Linear PCA learns orthogonal transformations (rotations) that orient axes to maximise variance, but it suffers from a subspace rotational indeterminacy: it fails to find a unique rotation for axes that share the same variance. Both nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational to permutational by maximising statistical independence under the assumption of unit variance. The main difference between them is that nonlinear PCA only learns rotations while linear ICA learns not just rotations but any linear transformation with unit variance. The relationship between all three can be understood by the singular value decomposition of the linear ICA transformation into a sequence of rotation, scale, rotation. Linear PCA learns the first rotation; nonlinear PCA learns the second. The scale is simply the inverse of the standard deviations. The problem is that, in contrast to linear PCA, conventional nonlinear PCA cannot be used directly on the data to learn the first rotation, the first being special as it reduces dimensionality and orders by variances. In this paper, we have identified the cause, and as a solution we propose __FORMULA__-PCA: a unified neural model for linear and nonlinear PCA as single-layer autoencoders. One of its key ingredients: modelling not just the rotation but also the scale -- the variances. This model bridges the disparity between linear and nonlinear PCA. And so, like linear PCA, it can learn a semi-orthogonal transformation that reduces dimensionality and orders by variances, but, unlike linear PCA, it does not suffer from rotational indeterminacy."}
{"title":"Large-Sample Properties of the Synthetic Control Method under Selection on Unobservables","authors":["Dmitry Arkhangelsky","David Hirshberg"],"raw_abstract":"We analyze the properties of the synthetic control (SC) method in settings\nwith a large number of units. We assume that the selection into treatment is\nbased on unobserved permanent heterogeneity and pretreatment information, thus\nallowing for both strictly and sequentially exogenous assignment processes.\nExploiting duality, we interpret the solution of the SC optimization problem as\nan estimator for the underlying treatment probabilities. We use this to derive\nthe asymptotic representation for the SC method and characterize sufficient\nconditions for its asymptotic normality. We show that the critical property\nthat determines the behavior of the SC method is the ability of input features\nto approximate the unobserved heterogeneity. Our results imply that the SC\nmethod delivers asymptotically normal estimators for a large class of linear\npanel data models as long as the number of pretreatment periods is large,\nmaking it a natural alternative to conventional methods built on the\nDifference-in-Differences.","publication_date":1700677843,"paper_link":"http://arxiv.org/pdf/2311.13575v1","categories":["Economics"],"abstract":"We analyze the properties of the synthetic control (SC) method in settings with a large number of units. We assume that the selection into treatment is based on unobserved permanent heterogeneity and pretreatment information, thus allowing for both strictly and sequentially exogenous assignment processes. Exploiting duality, we interpret the solution of the SC optimization problem as an estimator for the underlying treatment probabilities. We use this to derive the asymptotic representation for the SC method and characterize sufficient conditions for its asymptotic normality. We show that the critical property that determines the behavior of the SC method is the ability of input features to approximate the unobserved heterogeneity. Our results imply that the SC method delivers asymptotically normal estimators for a large class of linear panel data models as long as the number of pretreatment periods is large, making it a natural alternative to conventional methods built on the Difference-in-Differences."}
{"title":"High order universal portfolios","authors":["Gabriel Turinici"],"raw_abstract":"The Cover universal portfolio (UP from now on) has many interesting\ntheoretical and numerical properties and was investigated for a long time.\nBuilding on it, we explore what happens when we add this UP to the market as a\nnew synthetic asset and construct by recurrence higher order UPs. We\ninvestigate some important theoretical properties of the high order UPs and\nshow in particular that they are indeed different from the Cover UP and are\ncapable to break the time permutation invariance. Numerical experiences on a\nbenchmark from the literature show that in all cases high order UPs improve\nCover's UP performances.","publication_date":1700677345,"paper_link":"http://arxiv.org/pdf/2311.13564v1","categories":["Mathematics","Quantitative Finance"],"abstract":"The Cover universal portfolio (UP from now on) has many interesting theoretical and numerical properties and was investigated for a long time. Building on it, we explore what happens when we add this UP to the market as a new synthetic asset and construct by recurrence higher order UPs. We investigate some important theoretical properties of the high order UPs and show in particular that they are indeed different from the Cover UP and are capable to break the time permutation invariance. Numerical experiences on a benchmark from the literature show that in all cases high order UPs improve Cover's UP performances."}
{"title":"Detection of the relativistic Shapiro delay in a highly inclined millisecond pulsar binary PSR J1012$-$4235","authors":["T. Gautam","P. C. C. Freire","J. Wu","V. Venkatraman Krishnan","M. Kramer","E. D. Barr","M. Bailes","A. D. Cameron"],"raw_abstract":"PSR J1012$-$4235 is a 3.1ms pulsar in a wide binary (37.9 days) with a white\ndwarf companion. We detect, for the first time, a strong relativistic Shapiro\ndelay signature in PSR J1012$-$4235. Our detection is the result of a timing\nanalysis of data spanning 13 years and collected with the Green Bank, Parkes,\nand MeerKAT Radio Telescopes and the Fermi $\\gamma$-ray space telescope. We\nmeasured the orthometric parameters for Shapiro delay and obtained a 22$\\sigma$\ndetection of the $h_{\\rm 3}$ parameter of 1.222(54) $\\mu$s and a 200$\\sigma$\ndetection of $\\varsigma$ of 0.9646(49). With the assumption of general\nrelativity, these measurements constrain the pulsar mass ($M_{\\rm\np}=1.44^{+0.13}_{-0.12}$M$_{\\odot}$), the mass of the white dwarf companion\n($M_{\\rm c} = 0.270^{+0.016}_{-0.015}$M$_{\\odot}$ ), and the orbital\ninclination ($i=88.06^{+0.28}_{-0.25} \\deg$). Including the early $\\gamma$-ray\ndata in our timing analysis facilitated a precise measurement of the proper\nmotion of the system of 6.58(5) mas yr$^{-1}$. We also show that the system has\nunusually small kinematic corrections to the measurement of the orbital period\nderivative, and therefore has the potential to yield stringent constraints on\nthe variation of the gravitational constant in the future.","publication_date":1700677012,"paper_link":"http://arxiv.org/pdf/2311.13563v1","categories":["Physics"],"abstract":"PSR J1012__FORMULA__4235 is a 3.1ms pulsar in a wide binary (37.9 days) with a white dwarf companion. We detect, for the first time, a strong relativistic Shapiro delay signature in PSR J1012__FORMULA__4235. Our detection is the result of a timing analysis of data spanning 13 years and collected with the Green Bank, Parkes, and MeerKAT Radio Telescopes and the Fermi __FORMULA__-ray space telescope. We measured the orthometric parameters for Shapiro delay and obtained a 22__FORMULA__ detection of the __FORMULA__ parameter of 1.222(54) __FORMULA__s and a 200__FORMULA__ detection of __FORMULA__ of 0.9646(49). With the assumption of general relativity, these measurements constrain the pulsar mass (__FORMULA__M__FORMULA__), the mass of the white dwarf companion (__FORMULA__M__FORMULA__ ), and the orbital inclination (__FORMULA__). Including the early __FORMULA__-ray data in our timing analysis facilitated a precise measurement of the proper motion of the system of 6.58(5) mas yr__FORMULA__. We also show that the system has unusually small kinematic corrections to the measurement of the orbital period derivative, and therefore has the potential to yield stringent constraints on the variation of the gravitational constant in the future."}
{"title":"Driven-Dissipative Bose-Einstein Condensation and the Upper Critical Dimension","authors":["Yikang Zhang","Thomas Barthel"],"raw_abstract":"Driving and dissipation can stabilize Bose-Einstein condensates. Using\nKeldysh field theory, we analyze this phenomenon for Markovian systems that can\ncomprise on-site two-particle driving, on-site single-particle and two-particle\nloss, as well as edge-correlated pumping. Above the upper critical dimension,\nmean-field theory shows that pumping and two-particle driving induce\ncondensation right at the boundary between the stable and unstable regions of\nthe non-interacting theory. With nonzero two-particle driving, the condensate\nis gapped. This picture is consistent with the recent observation that, without\nsymmetry constraints beyond invariance under single-particle basis\ntransformations, all gapped quadratic bosonic Liouvillians belong to the same\nphase. For systems below the upper critical dimension, the edge-correlated\npumping penalizes high-momentum fluctuations, rendering the theory\nrenormalizable. We perform the one-loop renormalization group analysis, finding\na condensation transition inside the unstable region of the non-interacting\ntheory. Interestingly, its critical behavior is determined by a\nWilson-Fisher-like fixed point with universal correlation-length exponent\n$\\nu=0.6$ in three dimensions.","publication_date":1700676768,"paper_link":"http://arxiv.org/pdf/2311.13561v1","categories":["Physics"],"abstract":"Driving and dissipation can stabilize Bose-Einstein condensates. Using Keldysh field theory, we analyze this phenomenon for Markovian systems that can comprise on-site two-particle driving, on-site single-particle and two-particle loss, as well as edge-correlated pumping. Above the upper critical dimension, mean-field theory shows that pumping and two-particle driving induce condensation right at the boundary between the stable and unstable regions of the non-interacting theory. With nonzero two-particle driving, the condensate is gapped. This picture is consistent with the recent observation that, without symmetry constraints beyond invariance under single-particle basis transformations, all gapped quadratic bosonic Liouvillians belong to the same phase. For systems below the upper critical dimension, the edge-correlated pumping penalizes high-momentum fluctuations, rendering the theory renormalizable. We perform the one-loop renormalization group analysis, finding a condensation transition inside the unstable region of the non-interacting theory. Interestingly, its critical behavior is determined by a Wilson-Fisher-like fixed point with universal correlation-length exponent __FORMULA__ in three dimensions."}
{"title":"Inverse energy cascade in ocean macroscopic turbulence: Kolmogorov self-similarity in surface drifter observations and Richardson-Obhukov constant","authors":["J. Dr\u00e4ger-Dietel","A. Griesel"],"raw_abstract":"We combine two point velocity and position data from surface drifter\nobservations in the Benguela upwelling region off the coast of Namibia. The\ncompensated third order longitudinal velocity structure function\n$\\left\\langle{\\Delta u_{\\ell}^{\\rm 3}}\\right\\rangle/s$ shows a positive plateau\nfor inertial separations $s$ roughly between $9~\\rm{km}$ and $120~\\rm{km}$\nrevealing an inverse energy cascade with energy transfer rate\n$\\varepsilon\\simeq 1.2 \\pm 0.1 \\cdot 10^{-7} m^3/s^2$. Deviations from\nGaussianity of the corresponding probability distribution $P(\\Delta u_{\\ell}\n|s)$ of two-point velocity increments $\\Delta u_{\\ell}$ for given pair\nseparation $s$ show up in the n$^{th}$ antisymetric structure functions\n$S_{-}^{(n)}(r)=\\int u^n(P(u)-P(-u)d u$, which scale in agreement with\nKolmogorov's prediction, $S_{-}^{(n)}(r)\\sim r^{(n/3)}$, for $n=2,4,6$. The\ncombination of $\\varepsilon$ with Richardson dispersion\n  $\\left\\langle s^2(t)\\right\\rangle=g\\varepsilon t^3$, where $\\left\\langle\ns^2(t)\\right\\rangle$ is mean squared pair separation at time $ t$, reveals a\nRichardson-Obhukov constant of $g\\simeq 0.11\\pm 0.03$.","publication_date":1700676655,"paper_link":"http://arxiv.org/pdf/2311.13560v1","categories":["Physics"],"abstract":"We combine two point velocity and position data from surface drifter observations in the Benguela upwelling region off the coast of Namibia. The compensated third order longitudinal velocity structure function __FORMULA__ shows a positive plateau for inertial separations __FORMULA__ roughly between __FORMULA__ and __FORMULA__ revealing an inverse energy cascade with energy transfer rate __FORMULA__. Deviations from Gaussianity of the corresponding probability distribution __FORMULA__ of two-point velocity increments __FORMULA__ for given pair separation __FORMULA__ show up in the n__FORMULA__ antisymetric structure functions __FORMULA__, which scale in agreement with Kolmogorov's prediction, __FORMULA__, for __FORMULA__. The combination of __FORMULA__ with Richardson dispersion   __FORMULA__, where __FORMULA__ is mean squared pair separation at time __FORMULA__, reveals a Richardson-Obhukov constant of __FORMULA__."}
{"title":"Universally Optimal Multivariate Crossover Designs","authors":["Shubham Niphadkar","Siuli Mukhopadhyay"],"raw_abstract":"In this article, universally optimal multivariate crossover designs are\nstudied. The multiple response crossover design is motivated by a $3 \\times 3$\ncrossover setup, where the effect of $3$ doses of an oral drug are studied on\ngene expressions related to mucosal inflammation. Subjects are assigned to\nthree treatment sequences and response measurements on 5 different gene\nexpressions are taken from each subject in each of the $3$ time periods. To\nmodel multiple or $g$ responses, where $g>1$, in a crossover setup, a\nmultivariate fixed effect model with both direct and carryover treatment\neffects is considered. It is assumed that there are non zero within response\ncorrelations, while between response correlations are taken to be zero. The\ninformation matrix corresponding to the direct effects is obtained and some\nresults are studied. The information matrix in the multivariate case is shown\nto differ from the univariate case, particularly in the completely symmetric\nproperty. For the $g>1$ case, with $t$ treatments and $p$ periods, for $p=t\n\\geq 3$, the design represented by a Type $I$ orthogonal array of strength $2$\nis proved to be universally optimal over the class of binary designs, for the\ndirect treatment effects.","publication_date":1700675871,"paper_link":"http://arxiv.org/pdf/2311.13556v1","categories":["Statistics"],"abstract":"In this article, universally optimal multivariate crossover designs are studied. The multiple response crossover design is motivated by a __FORMULA__ crossover setup, where the effect of __FORMULA__ doses of an oral drug are studied on gene expressions related to mucosal inflammation. Subjects are assigned to three treatment sequences and response measurements on 5 different gene expressions are taken from each subject in each of the __FORMULA__ time periods. To model multiple or __FORMULA__ responses, where __FORMULA__, in a crossover setup, a multivariate fixed effect model with both direct and carryover treatment effects is considered. It is assumed that there are non zero within response correlations, while between response correlations are taken to be zero. The information matrix corresponding to the direct effects is obtained and some results are studied. The information matrix in the multivariate case is shown to differ from the univariate case, particularly in the completely symmetric property. For the __FORMULA__ case, with __FORMULA__ treatments and __FORMULA__ periods, for __FORMULA__, the design represented by a Type __FORMULA__ orthogonal array of strength __FORMULA__ is proved to be universally optimal over the class of binary designs, for the direct treatment effects."}
{"title":"A discrete mean value of the Riemann zeta function","authors":["K\u00fcbra Benli","Ertan Elma","Nathan Ng"],"raw_abstract":"In this work, we estimate the sum \\begin{align*}\n  \\sum_{0 < \\Im(\\rho) \\leq T} \\zeta(\\rho+\\alpha)X(\\rho) Y(1\\!-\\! \\rho)\n\\end{align*}\n  over the nontirival zeros $\\rho$ of the Riemann zeta funtion where $\\alpha$\nis a complex number with $\\alpha\\ll 1/\\log T$ and $X(\\cdot)$ and $Y(\\cdot)$ are\nsome Dirichlet polynomials. Moreover, we estimate the discrete mean value above\nfor higher derivatives where $\\zeta(\\rho+\\alpha)$ is replaced by\n$\\zeta^{(m)}(\\rho)$ for all $m\\in\\mathbb{N}$. The formulae we obtain generalize\na number of previous results in the literature. As an application, assuming the\nRiemann Hypothesis we obtain the lower bound \\begin{align*}\n  \\sum_{0 < \\Im(\\rho) < T} | \\zeta^{(m)}(\\rho)|^{2k} \\gg T(\\log T)^{k^2+2km+1}\n\\quad \\quad (k,m\\in\\mathbb{N})\n  \\end{align*}\n  which was previously known under the Generalized Riemann Hypothesis, in the\ncase $m=1$.","publication_date":1700675723,"paper_link":"http://arxiv.org/pdf/2311.13554v1","categories":["Mathematics"],"abstract":"In this work, we estimate the sum align*   \\sum_{0 < \\Im(\\rho) \\leq T} \\zeta(\\rho+\\alpha)X(\\rho) Y(1\\!-\\! \\rho) align*   over the nontirival zeros __FORMULA__ of the Riemann zeta funtion where __FORMULA__ is a complex number with __FORMULA__ and __FORMULA__ and __FORMULA__ are some Dirichlet polynomials. Moreover, we estimate the discrete mean value above for higher derivatives where __FORMULA__ is replaced by __FORMULA__ for all __FORMULA__. The formulae we obtain generalize a number of previous results in the literature. As an application, assuming the Riemann Hypothesis we obtain the lower bound align*   \\sum_{0 < \\Im(\\rho) < T} | \\zeta^{(m)}(\\rho)|^{2k} \\gg T(\\log T)^{k^2+2km+1} \\quad \\quad (k,m\\inN)   align*   which was previously known under the Generalized Riemann Hypothesis, in the case __FORMULA__."}
{"title":"Asymptotics of Redistricting the $n\\times n$ grid","authors":["Christopher Donnay","Matthew Kahle"],"raw_abstract":"How many ways are there to redistrict the $n\\times n$ grid into $n$ districts\nof equal size? How many of these redistricting plans are compact? In this\narticle we give asymptotic bounds on the number of plans, showing there are\nexponentially many in $n^2$. We then use the lower bound to show that most\nplans are as least compact as possible.","publication_date":1700675240,"paper_link":"http://arxiv.org/pdf/2311.13550v1","categories":["Mathematics"],"abstract":"How many ways are there to redistrict the __FORMULA__ grid into __FORMULA__ districts of equal size? How many of these redistricting plans are compact? In this article we give asymptotic bounds on the number of plans, showing there are exponentially many in __FORMULA__. We then use the lower bound to show that most plans are as least compact as possible."}
{"title":"Superfluid vortex dynamics in an elliptical boundary","authors":["Matteo Caldara","Andrea Richaud","Pietro Massignan","Alexander L. Fetter"],"raw_abstract":"Recent advances in cold atom platforms, providing experimental accessibility\nto real-time dynamics, have renewed interest in the motion of superfluid\nvortices in two-dimensional domains. Motivated by this development, we study\nthe dynamics of a vortex in a two-dimensional incompressible superfluid inside\nan elliptical boundary. Employing the Joukowsky conformal map from a circle to\nan ellipse, we derive an analytical expression for the complex potential\ndescribing the hydrodynamic flow around the vortex. We integrate the resulting\nequations of motion, finding that the vortex moves along a nearly (but not\nexactly) elliptical trajectory. In addition, we obtain a simple closed\nexpression for the vortex self-energy, which serves as the Hamiltonian of the\nsystem.","publication_date":1700674752,"paper_link":"http://arxiv.org/pdf/2311.13545v1","categories":["Physics"],"abstract":"Recent advances in cold atom platforms, providing experimental accessibility to real-time dynamics, have renewed interest in the motion of superfluid vortices in two-dimensional domains. Motivated by this development, we study the dynamics of a vortex in a two-dimensional incompressible superfluid inside an elliptical boundary. Employing the Joukowsky conformal map from a circle to an ellipse, we derive an analytical expression for the complex potential describing the hydrodynamic flow around the vortex. We integrate the resulting equations of motion, finding that the vortex moves along a nearly (but not exactly) elliptical trajectory. In addition, we obtain a simple closed expression for the vortex self-energy, which serves as the Hamiltonian of the system."}
{"title":"Linear Log-Normal Attention with Unbiased Concentration","authors":["Yury Nahshan","Joseph Kampeas","Emir Haleva"],"raw_abstract":"Transformer models have achieved remarkable results in a wide range of\napplications. However, their scalability is hampered by the quadratic time and\nmemory complexity of the self-attention mechanism concerning the sequence\nlength. This limitation poses a substantial obstacle when dealing with long\ndocuments or high-resolution images. In this work, we study the self-attention\nmechanism by analyzing the distribution of the attention matrix and its\nconcentration ability. Furthermore, we propose instruments to measure these\nquantities and introduce a novel self-attention mechanism, Linear Log-Normal\nAttention, designed to emulate the distribution and concentration behavior of\nthe original self-attention. Our experimental results on popular natural\nlanguage benchmarks reveal that our proposed Linear Log-Normal Attention\noutperforms other linearized attention alternatives, offering a promising\navenue for enhancing the scalability of transformer models. Our code is\navailable in supplementary materials.","publication_date":1700674241,"paper_link":"http://arxiv.org/pdf/2311.13541v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary materials."}
{"title":"Belted sum decompositions of fully augmented links","authors":["Porter Morgan","Brian Ransom","Dean Spyropoulos","Cameron Ziegler","Rolland Trapp"],"raw_abstract":"Given two orientable, cusped hyperbolic 3-manifolds containing certain\nthrice-punctured spheres, Adams gave a diagrammatic definition for a third such\nmanifold, their belted sum. Fully augmented links, or FALs, are hyperbolic\nlinks constructed by augmenting a link diagram. This work considers belted sum\ndecompositions in which all manifolds involved are FAL complements. To do so,\nwe provide explicit classifications of thrice punctured spheres in FAL\ncomplements, making them easily recognizable. These classifications are used to\ncharacterize belted sum prime FALs geometrically, combinatorially and\ndiagrammatically. Finally we prove that, in the context of belted sums, every\nFAL complement canonically decomposes into FALs which are either prime or\ntwo-fold covers of the Whitehead link.","publication_date":1700674208,"paper_link":"http://arxiv.org/pdf/2311.13540v1","categories":["Mathematics"],"abstract":"Given two orientable, cusped hyperbolic 3-manifolds containing certain thrice-punctured spheres, Adams gave a diagrammatic definition for a third such manifold, their belted sum. Fully augmented links, or FALs, are hyperbolic links constructed by augmenting a link diagram. This work considers belted sum decompositions in which all manifolds involved are FAL complements. To do so, we provide explicit classifications of thrice punctured spheres in FAL complements, making them easily recognizable. These classifications are used to characterize belted sum prime FALs geometrically, combinatorially and diagrammatically. Finally we prove that, in the context of belted sums, every FAL complement canonically decomposes into FALs which are either prime or two-fold covers of the Whitehead link."}
{"title":"Speak Like a Native: Prompting Large Language Models in a Native Style","authors":["Zhicheng Yang","Yiwei Wang","Yinya Huang","Jing Xiong","Xiaodan Liang","Jing Tang"],"raw_abstract":"Existing work has found that the prompt engineering heavily influences the\nperformance of large language models (LLMs). Chain-of-thought (CoT), as a\npopular prompt engineering technique, prompted LLMs using in-context examples\nwith reasoning steps. In current studies, the few-shot examples of CoT are\ngenerally handcrafted by humans. However, how the text style of in-context\nexamples influence the outputs of LLMs still remains under-explored. This paper\npresents a novel and effective approach, named \\textbf{AlignCoT}, to improve\nthe reasoning capability of LLMs by aligning the in-context examples with the\nnative style of LLMs. ``Native'' refers to the inherent characteristic style of\nLLMs which can be probed by original zero-shot scenarios. AlignCoT is\northogonal to other prompt engineering methods, making it easy to combine with\nstate-of-the-art techniques to further improve the LLMs' performance. We\nconduct extensive and comprehensive experiments on several benchmarks. The\nempirical results demonstrate that our AlignCoTsignificantly improves\nperformance over the carefully handcrafted in-context examples. For instance,\nwith GPT-3.5-turbo, we observed a +2.5\\% improvement on GSM8K. Furthermore, our\nAlignCoT consistently improve the performance when combined with other\nstate-of-the-art prompt engineering methods. The source code and dataset will\nbe available at\n\\href{https://github.com/yangzhch6/AlignCoT}{https://github.com/yangzhch6/AlignCoT}.","publication_date":1700673861,"paper_link":"http://arxiv.org/pdf/2311.13538v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Existing work has found that the prompt engineering heavily influences the performance of large language models (LLMs). Chain-of-thought (CoT), as a popular prompt engineering technique, prompted LLMs using in-context examples with reasoning steps. In current studies, the few-shot examples of CoT are generally handcrafted by humans. However, how the text style of in-context examples influence the outputs of LLMs still remains under-explored. This paper presents a novel and effective approach, named AlignCoT, to improve the reasoning capability of LLMs by aligning the in-context examples with the native style of LLMs. ``Native'' refers to the inherent characteristic style of LLMs which can be probed by original zero-shot scenarios. AlignCoT is orthogonal to other prompt engineering methods, making it easy to combine with state-of-the-art techniques to further improve the LLMs' performance. We conduct extensive and comprehensive experiments on several benchmarks. The empirical results demonstrate that our AlignCoTsignificantly improves performance over the carefully handcrafted in-context examples. For instance, with GPT-3.5-turbo, we observed a +2.5\\% improvement on GSM8K. Furthermore, our AlignCoT consistently improve the performance when combined with other state-of-the-art prompt engineering methods. The source code and dataset will be available at https://github.com/yangzhch6/AlignCoT{https://github.com/yangzhch6/AlignCoT}."}
{"title":"Volumetric 3D Point Cloud Attribute Compression: Learned polynomial bilateral filter for prediction","authors":["Tam Thuc Do","Philip A. Chou","Gene Cheung"],"raw_abstract":"We extend a previous study on 3D point cloud attribute compression scheme\nthat uses a volumetric approach: given a target volumetric attribute function\n$f : \\mathbb{R}^3 \\mapsto \\mathbb{R}$, we quantize and encode parameters\n$\\theta$ that characterize $f$ at the encoder, for reconstruction\n$f_{\\hat{\\theta}}(\\mathbf(x))$ at known 3D points $\\mathbf(x)$ at the decoder.\nSpecifically, parameters $\\theta$ are quantized coefficients of B-spline basis\nvectors $\\mathbf{\\Phi}_l$ (for order $p \\geq 2$) that span the function space\n$\\mathcal{F}_l^{(p)}$ at a particular resolution $l$, which are coded from\ncoarse to fine resolutions for scalability. In this work, we focus on the\nprediction of finer-grained coefficients given coarser-grained ones by learning\nparameters of a polynomial bilateral filter (PBF) from data. PBF is a\npseudo-linear filter that is signal-dependent with a graph spectral\ninterpretation common in the graph signal processing (GSP) field. We\ndemonstrate PBF's predictive performance over a linear predictor inspired by\nMPEG standardization over a wide range of point cloud datasets.","publication_date":1700673204,"paper_link":"http://arxiv.org/pdf/2311.13533v1","categories":["Electrical Engineering and Systems Science"],"abstract":"We extend a previous study on 3D point cloud attribute compression scheme that uses a volumetric approach: given a target volumetric attribute function __FORMULA__, we quantize and encode parameters __FORMULA__ that characterize __FORMULA__ at the encoder, for reconstruction __FORMULA__ at known 3D points __FORMULA__ at the decoder. Specifically, parameters __FORMULA__ are quantized coefficients of B-spline basis vectors __FORMULA__ (for order __FORMULA__) that span the function space __FORMULA__ at a particular resolution __FORMULA__, which are coded from coarse to fine resolutions for scalability. In this work, we focus on the prediction of finer-grained coefficients given coarser-grained ones by learning parameters of a polynomial bilateral filter (PBF) from data. PBF is a pseudo-linear filter that is signal-dependent with a graph spectral interpretation common in the graph signal processing (GSP) field. We demonstrate PBF's predictive performance over a linear predictor inspired by MPEG standardization over a wide range of point cloud datasets."}
{"title":"$\u03b2$ Pictoris b through the eyes of the upgraded CRIRES+","authors":["Rico Landman","Tomas Stolker","Ignas Snellen","Jean Costes","Sam de Regt","Yapeng Zhang","Siddharth Gandhi","Paul Molli\u00e8re","Aurora Kesseli","Arthur Vigan","Alejandro S\u00e1nchez-L\u00f3pez"],"raw_abstract":"Context: High-resolution spectrographs fed by adaptive optics (AO) provide a\nunique opportunity to characterize directly imaged exoplanets. Observations\nwith such instruments allow us to probe the atmospheric composition, spin\nrotation, and radial velocity of the planet, thereby helping to reveal\ninformation on its formation and migration history. The recent upgrade of the\nCryogenic High-Resolution Infrared Echelle Spectrograph (CRIRES+) at the VLT\nmakes it a highly suitable instrument for characterizing directly imaged\nexoplanets.\n  Aims: In this work, we report on observations of $\\beta$ Pictoris b with\nCRIRES+ and use them to constrain the planets atmospheric properties and update\nthe estimation of its spin rotation.\n  Methods: The data were reduced using the open-source \\textit{pycrires}\npackage. We subsequently forward-modeled the stellar, planetary, and systematic\ncontribution to the data to detect molecules in the planet's atmosphere. We\nalso used atmospheric retrievals to provide new constraints on its atmosphere.\n  Results: We confidently detected water and carbon monoxide in the atmosphere\nof $\\beta$ Pictoris b and retrieved a slightly sub-solar carbon-to-oxygen\nratio, which is in agreement with previous results. The interpretation is\nhampered by our limited knowledge of the C/O ratio of the host star. We also\nobtained a much improved constraint on its spin rotation of $19.9 \\pm 1.0$\nkm/s, which gives a rotation period of $8.7 \\pm 0.8$ hours, assuming no\nobliquity. We find that there is a degeneracy between the metallicity and\nclouds, but this has minimal impact on the retrieved C/O, $v\\sin{i}$, and\nradial velocity. Our results show that CRIRES+ is performing well and stands as\na highly useful instrument for characterizing directly imaged planets.","publication_date":1700672551,"paper_link":"http://arxiv.org/pdf/2311.13527v1","categories":["Physics"],"abstract":"Context: High-resolution spectrographs fed by adaptive optics (AO) provide a unique opportunity to characterize directly imaged exoplanets. Observations with such instruments allow us to probe the atmospheric composition, spin rotation, and radial velocity of the planet, thereby helping to reveal information on its formation and migration history. The recent upgrade of the Cryogenic High-Resolution Infrared Echelle Spectrograph (CRIRES+) at the VLT makes it a highly suitable instrument for characterizing directly imaged exoplanets.   Aims: In this work, we report on observations of __FORMULA__ Pictoris b with CRIRES+ and use them to constrain the planets atmospheric properties and update the estimation of its spin rotation.   Methods: The data were reduced using the open-source pycrires package. We subsequently forward-modeled the stellar, planetary, and systematic contribution to the data to detect molecules in the planet's atmosphere. We also used atmospheric retrievals to provide new constraints on its atmosphere.   Results: We confidently detected water and carbon monoxide in the atmosphere of __FORMULA__ Pictoris b and retrieved a slightly sub-solar carbon-to-oxygen ratio, which is in agreement with previous results. The interpretation is hampered by our limited knowledge of the C/O ratio of the host star. We also obtained a much improved constraint on its spin rotation of __FORMULA__ km/s, which gives a rotation period of __FORMULA__ hours, assuming no obliquity. We find that there is a degeneracy between the metallicity and clouds, but this has minimal impact on the retrieved C/O, __FORMULA__, and radial velocity. Our results show that CRIRES+ is performing well and stands as a highly useful instrument for characterizing directly imaged planets."}
{"title":"On the Galois structure of units in totally real $p$-rational number fields","authors":["Zakariae Bouazzaoui","Donghyeok Lim"],"raw_abstract":"The theory of factor-equivalence of integral lattices gives a far reaching\nrelationship between the Galois module structure of units of the ring of\nintegers of a number field and its arithmetic. For a number field $K$ that is\nGalois over $\\mathbb{Q}$ or an imaginary quadratic field, we prove a necessary\nand sufficient condition on the quotients of class numbers of subfields of $K$,\nfor the quotient $E_{K}$ of the group of units of the ring of integers of $K$\nby the subgroup of roots of unity to be factor equivalent to the standard\ncyclic Galois module. By using strong arithmetic properties of totally real\n$p$-rational number fields, we prove that the non-abelian $p$-rational\n$p$-extensions of $\\mathbb{Q}$ do not have Minkowski units, which extends a\nresult of Burns to non-abelian number fields. We also study the relative Galois\nmodule structure of $E_{L}$ for varying Galois extensions $L/F$ of totally real\n$p$-rational number fields whose Galois groups are isomorphic to a fixed finite\ngroup $G$. In that case, we prove that there is a finite set $\\Omega$ of\n$\\mathbb{Z}_p[G]$-lattices such that for every $L$, $\\mathbb{Z}_{p}\n\\otimes_{\\mathbb{Z}} E_{L}$ is factor equivalent to $\\mathbb{Z}_{p}[G]^{n}\n\\oplus X$ as $\\mathbb{Z}_p[G]$-lattices for some $X \\in \\Omega$ and an integer\n$n \\geq 0$.","publication_date":1700672351,"paper_link":"http://arxiv.org/pdf/2311.13525v1","categories":["Mathematics"],"abstract":"The theory of factor-equivalence of integral lattices gives a far reaching relationship between the Galois module structure of units of the ring of integers of a number field and its arithmetic. For a number field __FORMULA__ that is Galois over __FORMULA__ or an imaginary quadratic field, we prove a necessary and sufficient condition on the quotients of class numbers of subfields of __FORMULA__, for the quotient __FORMULA__ of the group of units of the ring of integers of __FORMULA__ by the subgroup of roots of unity to be factor equivalent to the standard cyclic Galois module. By using strong arithmetic properties of totally real __FORMULA__-rational number fields, we prove that the non-abelian __FORMULA__-rational __FORMULA__-extensions of __FORMULA__ do not have Minkowski units, which extends a result of Burns to non-abelian number fields. We also study the relative Galois module structure of __FORMULA__ for varying Galois extensions __FORMULA__ of totally real __FORMULA__-rational number fields whose Galois groups are isomorphic to a fixed finite group __FORMULA__. In that case, we prove that there is a finite set __FORMULA__ of __FORMULA__-lattices such that for every __FORMULA__, __FORMULA__ is factor equivalent to __FORMULA__ as __FORMULA__-lattices for some __FORMULA__ and an integer __FORMULA__."}
{"title":"Flag transitive geometries with trialities and no dualities coming from Suzuki groups","authors":["Dimitri Leemans","Klara Stokes","Philippe Tranchida"],"raw_abstract":"Recently, Leemans and Stokes constructed an infinite family of incidence\ngeometries admitting trialities but no dualities from the groups PSL(2,q)\n(where $q=p^{3n}$ with $p$ a prime and $n>0$ a positive integer). Unfortunately\nthese geometries are not flag transitive. In this paper, we construct the first\ninfinite family of incidence geometries of rank three that are flag transitive\nand have trialities but no dualities. These geometries are constructed using\nchamber systems of Suzuki groups Sz(q) (where $q=2^{2e+1}$ with $e$ a positive\ninteger and $2e+1$ is divisible by 3) and the trialities come from field\nautomorphisms. We also construct an infinite family of regular hypermaps with\nautomorphism group Sz(q) that admit trialities but no dualities.","publication_date":1700671978,"paper_link":"http://arxiv.org/pdf/2311.13522v1","categories":["Mathematics"],"abstract":"Recently, Leemans and Stokes constructed an infinite family of incidence geometries admitting trialities but no dualities from the groups PSL(2,q) (where __FORMULA__ with __FORMULA__ a prime and __FORMULA__ a positive integer). Unfortunately these geometries are not flag transitive. In this paper, we construct the first infinite family of incidence geometries of rank three that are flag transitive and have trialities but no dualities. These geometries are constructed using chamber systems of Suzuki groups Sz(q) (where __FORMULA__ with __FORMULA__ a positive integer and __FORMULA__ is divisible by 3) and the trialities come from field automorphisms. We also construct an infinite family of regular hypermaps with automorphism group Sz(q) that admit trialities but no dualities."}
{"title":"Varieties of truth definitions","authors":["Piotr Gruza","Mateusz \u0141e\u0142yk"],"raw_abstract":"We study the structure of the partial order induced by the definability\nrelation on definitions of truth for the language of arithmetic. Formally, a\ndefinition of truth is any sentence $\\alpha$ which extends a weak arithmetical\ntheory (which we take to be EA) such that for some formula $\\Theta$ and any\narithmetical sentence $\\varphi$, $\\Theta(\\ulcorner\\varphi\\urcorner)\\equiv\n\\varphi$ is provable in $\\alpha$. We say that a sentence $\\beta$ is definable\nin a sentence $\\alpha$, if there exists an unrelativized translation from the\nlanguage of $\\beta$ to the language of $\\alpha$ which is identity on the\narithmetical symbols and such that the translation of $\\beta$ is provable in\n$\\alpha$. Our main result is that the structure consisting of truth definitions\nwhich are conservative over the basic arithmetical theory forms a countable\nuniversal distributive lattice. Additionally, we generalize the result of\nPakhomov and Visser showing that the set of (G\\\"odel codes of) definitions of\ntruth is not $\\Sigma_2$-definable in the standard model of arithmetic. We\nconclude by remarking that no $\\Sigma_2$-sentence, satisfying certain further\nnatural conditions, can be a definition of truth for the language of\narithmetic.","publication_date":1700671463,"paper_link":"http://arxiv.org/pdf/2311.13519v1","categories":["Mathematics"],"abstract":"We study the structure of the partial order induced by the definability relation on definitions of truth for the language of arithmetic. Formally, a definition of truth is any sentence __FORMULA__ which extends a weak arithmetical theory (which we take to be EA) such that for some formula __FORMULA__ and any arithmetical sentence __FORMULA__, __FORMULA__ is provable in __FORMULA__. We say that a sentence __FORMULA__ is definable in a sentence __FORMULA__, if there exists an unrelativized translation from the language of __FORMULA__ to the language of __FORMULA__ which is identity on the arithmetical symbols and such that the translation of __FORMULA__ is provable in __FORMULA__. Our main result is that the structure consisting of truth definitions which are conservative over the basic arithmetical theory forms a countable universal distributive lattice. Additionally, we generalize the result of Pakhomov and Visser showing that the set of (G\\\"odel codes of) definitions of truth is not __FORMULA__-definable in the standard model of arithmetic. We conclude by remarking that no __FORMULA__-sentence, satisfying certain further natural conditions, can be a definition of truth for the language of arithmetic."}
{"title":"Learning-Based Relaxation of Completeness Requirements for Data Entry Forms","authors":["Hichem Belgacem","Xiaochen Li","Domenico Bianculli","Lionel C. Briand"],"raw_abstract":"Data entry forms use completeness requirements to specify the fields that are\nrequired or optional to fill for collecting necessary information from\ndifferent types of users.\n  However, some required fields may not be applicable for certain types of\nusers anymore. Nevertheless, they may still be incorrectly marked as required\nin the form; we call such fields obsolete required fields.\n  Since obsolete required fields usually have not-null validation checks before\nsubmitting the form, users have to enter meaningless values in such fields in\norder to complete the form submission. These meaningless values threaten the\nquality of the filled data. To avoid users filling meaningless values, existing\ntechniques usually rely on manually written rules to identify the obsolete\nrequired fields and relax their completeness requirements. However, these\ntechniques are ineffective and costly. In this paper, we propose LACQUER, a\nlearning-based automated approach for relaxing the completeness requirements of\ndata entry forms. LACQUER builds Bayesian Network models to automatically learn\nconditions under which users had to fill meaningless values. To improve its\nlearning ability, LACQUER identifies the cases where a required field is only\napplicable for a small group of users, and uses SMOTE, an oversampling\ntechnique, to generate more instances on such fields for effectively mining\ndependencies on them. Our experimental results show that LACQUER can accurately\nrelax the completeness requirements of required fields in data entry forms with\nprecision values ranging between 0.76 and 0.90 on different datasets. LACQUER\ncan prevent users from filling 20% to 64% of meaningless values, with negative\npredictive values between 0.72 and 0.91. Furthermore, LACQUER is efficient; it\ntakes at most 839 ms to predict the completeness requirement of an instance.","publication_date":1700671226,"paper_link":"http://arxiv.org/pdf/2311.13517v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Data entry forms use completeness requirements to specify the fields that are required or optional to fill for collecting necessary information from different types of users.   However, some required fields may not be applicable for certain types of users anymore. Nevertheless, they may still be incorrectly marked as required in the form; we call such fields obsolete required fields.   Since obsolete required fields usually have not-null validation checks before submitting the form, users have to enter meaningless values in such fields in order to complete the form submission. These meaningless values threaten the quality of the filled data. To avoid users filling meaningless values, existing techniques usually rely on manually written rules to identify the obsolete required fields and relax their completeness requirements. However, these techniques are ineffective and costly. In this paper, we propose LACQUER, a learning-based automated approach for relaxing the completeness requirements of data entry forms. LACQUER builds Bayesian Network models to automatically learn conditions under which users had to fill meaningless values. To improve its learning ability, LACQUER identifies the cases where a required field is only applicable for a small group of users, and uses SMOTE, an oversampling technique, to generate more instances on such fields for effectively mining dependencies on them. Our experimental results show that LACQUER can accurately relax the completeness requirements of required fields in data entry forms with precision values ranging between 0.76 and 0.90 on different datasets. LACQUER can prevent users from filling 20% to 64% of meaningless values, with negative predictive values between 0.72 and 0.91. Furthermore, LACQUER is efficient; it takes at most 839 ms to predict the completeness requirement of an instance."}
{"title":"Linearity of compact $R$-analytic groups","authors":["Montserrat Casals-Ruiz","Andoni Zozaya"],"raw_abstract":"We prove that any compact $R$-analytic group is linear when $R$ is a pro-$p$\ndomain of characteristic zero.","publication_date":1700671177,"paper_link":"http://arxiv.org/pdf/2311.13516v1","categories":["Mathematics"],"abstract":"We prove that any compact __FORMULA__-analytic group is linear when __FORMULA__ is a pro-__FORMULA__ domain of characteristic zero."}
{"title":"Ordinary parts and local-global compatibility at $\\ell=p$","authors":["Bence Hevesi"],"raw_abstract":"We prove local-global compatibility results at $\\ell=p$ for the torsion\nautomorphic Galois representations constructed by Scholze, generalising the\nwork of Caraiani--Newton. In particular, we verify, up to a nilpotent ideal,\nthe local-global compatibility conjecture at $\\ell=p$ of Gee--Newton in the\ncase of imaginary CM fields under some technical assumptions. The key new\ningredient is a local-global compatibility result for $Q$-ordinary self-dual\nautomorphic representations for arbitrary parabolic subgroups.","publication_date":1700671157,"paper_link":"http://arxiv.org/pdf/2311.13514v1","categories":["Mathematics"],"abstract":"We prove local-global compatibility results at __FORMULA__ for the torsion automorphic Galois representations constructed by Scholze, generalising the work of Caraiani--Newton. In particular, we verify, up to a nilpotent ideal, the local-global compatibility conjecture at __FORMULA__ of Gee--Newton in the case of imaginary CM fields under some technical assumptions. The key new ingredient is a local-global compatibility result for __FORMULA__-ordinary self-dual automorphic representations for arbitrary parabolic subgroups."}
{"title":"The SOPHIE search for northern extrasolar planets-XIX. A system including a cold sub-Neptune potentially transiting a V = 6.5 star HD88986","authors":["N. Heidari","I. Boisse","N. C. Hara","T. G. Wilson","F. Kiefer","G. H\u00e9brard","F. Philipot","S. Hoyer","K. G. Stassun","G. W. Henry","N. C. Santos","L. Acu\u00f1a","D. Almasian","L. Arnold","N. Astudillo-Defru","O. Attia","X. Bonfils","F. Bouchy","V. Bourrier","B. Collet","P. Cort\u00e9s-Zuleta","A. Carmona","X. Delfosse","S. Dalal","M. Deleuil","O. D. S. Demangeon","R. F. D\u00edaz","X. Dumusque","D. Ehrenreich","T. Forveille","M. J. Hobson","J. S. Jenkins","J. M. Jenkins","A. M. Lagrange","D. W. Latham","P. Larue","J. Liu","C. Moutou","L. Mignon","H. P. Osborn","F. Pepe","D. Rapetti","J. Rodrigues","A. Santerne","D. Segransan","A. Shporer","S. Sulis","G. Torres","S. Udry","F. Vakili","A. Vanderburg","O. Venot","H. G. Vivien","J. I. Vines"],"raw_abstract":"Transiting planets with orbital periods longer than 40 d are extremely rare\namong the 5000+ planets discovered so far. The lack of discoveries of this\npopulation poses a challenge to research into planetary demographics,\nformation, and evolution. Here, we present the detection and characterization\nof HD88986b, a potentially transiting sub-Neptune, possessing the longest\norbital period among known transiting small planets (< 4 R$_{\\oplus}$) with a\nprecise mass measurement ($\\sigma_M/M$ > 25%). Additionally, we identified the\npresence of a massive companion in a wider orbit around HD88986. Our analysis\nreveals that HD88986b, based on two potential single transits on sector 21 and\nsector 48 which are both consistent with the predicted transit time from the RV\nmodel, is potentially transiting. The joint analysis of RV and photometric data\nshow that HD88986b has a radius of 2.49$\\pm$0.18 R$_{\\oplus}$, a mass of\n17.2$^{+4.0}_{-3.8}$ M$_{\\oplus}$, and it orbits every 146.05$^{+0.43}_{-0.40}$\nd around a subgiant HD88986 which is one of the closest and brightest exoplanet\nhost stars (G2V type, R=1.543 $\\pm$0.065 R$_{\\odot}$, V=$6.47\\pm 0.01$ mag,\ndistance=33.37$\\pm$0.04 pc). The nature of the outer, massive companion is\nstill to be confirmed; a joint analysis of RVs, Hipparcos, and Gaia astrometric\ndata shows that with a 3$\\sigma$ confidence interval, its semi-major axis is\nbetween 16.7 and 38.8 au and its mass is between 68 and 284 M$_{Jup}$.\nHD88986b's wide orbit suggests the planet did not undergo significant mass loss\ndue to extreme-ultraviolet radiation from its host star. Therefore, it probably\nmaintained its primordial composition, allowing us to probe its formation\nscenario. Furthermore, the cold nature of HD88986b (460$\\pm$8 K), thanks to its\nlong orbital period, will open up exciting opportunities for future studies of\ncold atmosphere composition characterization.","publication_date":1700671096,"paper_link":"http://arxiv.org/pdf/2311.13513v1","categories":["Physics"],"abstract":"Transiting planets with orbital periods longer than 40 d are extremely rare among the 5000+ planets discovered so far. The lack of discoveries of this population poses a challenge to research into planetary demographics, formation, and evolution. Here, we present the detection and characterization of HD88986b, a potentially transiting sub-Neptune, possessing the longest orbital period among known transiting small planets (< 4 R__FORMULA__) with a precise mass measurement (__FORMULA__ > 25%). Additionally, we identified the presence of a massive companion in a wider orbit around HD88986. Our analysis reveals that HD88986b, based on two potential single transits on sector 21 and sector 48 which are both consistent with the predicted transit time from the RV model, is potentially transiting. The joint analysis of RV and photometric data show that HD88986b has a radius of 2.49__FORMULA__0.18 R__FORMULA__, a mass of 17.2__FORMULA__ M__FORMULA__, and it orbits every 146.05__FORMULA__ d around a subgiant HD88986 which is one of the closest and brightest exoplanet host stars (G2V type, R=1.543 __FORMULA__0.065 R__FORMULA__, V=__FORMULA__ mag, distance=33.37__FORMULA__0.04 pc). The nature of the outer, massive companion is still to be confirmed; a joint analysis of RVs, Hipparcos, and Gaia astrometric data shows that with a 3__FORMULA__ confidence interval, its semi-major axis is between 16.7 and 38.8 au and its mass is between 68 and 284 M__FORMULA__. HD88986b's wide orbit suggests the planet did not undergo significant mass loss due to extreme-ultraviolet radiation from its host star. Therefore, it probably maintained its primordial composition, allowing us to probe its formation scenario. Furthermore, the cold nature of HD88986b (460__FORMULA__8 K), thanks to its long orbital period, will open up exciting opportunities for future studies of cold atmosphere composition characterization."}
{"title":"Jordan correspondence and block distribution of characters","authors":["Radha Kessar","Gunter Malle"],"raw_abstract":"We complete the determination of the $\\ell$-block distribution of characters\nfor quasi-simple exceptional groups of Lie type up to some minor ambiguities\nrelating to non-uniqueness of Jordan decomposition. For this, we first\ndetermine the $\\ell$-block distribution for finite reductive groups whose\nambient algebraic group defined in characteristic different from $\\ell$ has\nconnected centre. As a consequence we derive a compatibility between\n$\\ell$-blocks, $e$-Harish-Chandra series and Jordan decomposition. Further we\napply our results to complete the proof of Robinson's conjecture on defects of\ncharacters.","publication_date":1700670869,"paper_link":"http://arxiv.org/pdf/2311.13510v1","categories":["Mathematics"],"abstract":"We complete the determination of the __FORMULA__-block distribution of characters for quasi-simple exceptional groups of Lie type up to some minor ambiguities relating to non-uniqueness of Jordan decomposition. For this, we first determine the __FORMULA__-block distribution for finite reductive groups whose ambient algebraic group defined in characteristic different from __FORMULA__ has connected centre. As a consequence we derive a compatibility between __FORMULA__-blocks, __FORMULA__-Harish-Chandra series and Jordan decomposition. Further we apply our results to complete the proof of Robinson's conjecture on defects of characters."}
{"title":"Naturalness of Attention: Revisiting Attention in Code Language Models","authors":["Mootez Saad","Tushar Sharma"],"raw_abstract":"Language models for code such as CodeBERT offer the capability to learn\nadvanced source code representation, but their opacity poses barriers to\nunderstanding of captured properties. Recent attention analysis studies provide\ninitial interpretability insights by focusing solely on attention weights\nrather than considering the wider context modeling of Transformers. This study\naims to shed some light on the previously ignored factors of the attention\nmechanism beyond the attention weights. We conduct an initial empirical study\nanalyzing both attention distributions and transformed representations in\nCodeBERT. Across two programming languages, Java and Python, we find that the\nscaled transformation norms of the input better capture syntactic structure\ncompared to attention weights alone. Our analysis reveals characterization of\nhow CodeBERT embeds syntactic code properties. The findings demonstrate the\nimportance of incorporating factors beyond just attention weights for\nrigorously understanding neural code models. This lays the groundwork for\ndeveloping more interpretable models and effective uses of attention mechanisms\nin program analysis.","publication_date":1700670852,"paper_link":"http://arxiv.org/pdf/2311.13508v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Language models for code such as CodeBERT offer the capability to learn advanced source code representation, but their opacity poses barriers to understanding of captured properties. Recent attention analysis studies provide initial interpretability insights by focusing solely on attention weights rather than considering the wider context modeling of Transformers. This study aims to shed some light on the previously ignored factors of the attention mechanism beyond the attention weights. We conduct an initial empirical study analyzing both attention distributions and transformed representations in CodeBERT. Across two programming languages, Java and Python, we find that the scaled transformation norms of the input better capture syntactic structure compared to attention weights alone. Our analysis reveals characterization of how CodeBERT embeds syntactic code properties. The findings demonstrate the importance of incorporating factors beyond just attention weights for rigorously understanding neural code models. This lays the groundwork for developing more interpretable models and effective uses of attention mechanisms in program analysis."}
{"title":"The Bifurcation Coalescence Problem for Feedforward Coalescence Networks","authors":["Manuela Aguiar","Pedro Soares"],"raw_abstract":"Consider two networks and combine them through the coalescence operation to\nget a larger network. Is it possible to study the steady-state bifurcations of\nthe coalescence network by studying the steady-state bifurcations of the\ncomponent networks? We conclude that this is not possible for general\ncoalescence networks. We show, however, that in the case of feedforward\ncoalescence networks this is possible and we cover the simplest cases. In\nparticular, we prove how the growth rate of the bifurcation branches in the\nfeedforward coalescence network depends on the connections from one network to\nthe other.","publication_date":1700670800,"paper_link":"http://arxiv.org/pdf/2311.13506v1","categories":["Mathematics"],"abstract":"Consider two networks and combine them through the coalescence operation to get a larger network. Is it possible to study the steady-state bifurcations of the coalescence network by studying the steady-state bifurcations of the component networks? We conclude that this is not possible for general coalescence networks. We show, however, that in the case of feedforward coalescence networks this is possible and we cover the simplest cases. In particular, we prove how the growth rate of the bifurcation branches in the feedforward coalescence network depends on the connections from one network to the other."}
{"title":"The Potsdam astroComb (POCO) Part I: Mode crossing effect in feedback resonators","authors":["Daniel Bodenm\u00fcller","Kalaga Madhav","Martin Roth"],"raw_abstract":"We investigate theoretically and experimentally the mode interaction in an\nintegrated Silicon Nitride (Si3N4) microring resonator with interferometric\ncoupling realized by a feedback loop as an adjustable optical path path length\nconnecting the ring to the bus waveguide at two coupling sections. From the\ntransmission spectra recorded at different optical path lengths, two\nresonances, 1596.5~nm and 1570.5~nm, were selected for detailed investigation.\nBoth resonances show the possibility of adjusting the resonance width and\ndepth. However, the transmission spectra around the first resonance also show\nthe effect of mode interaction. This is also well captured in the theoretical\nmodel, from which we can derive a coupling rate for the mode interaction of\n$3.4~\\textrm{rad}~\\textrm{ns}^{-1}$.","publication_date":1700670354,"paper_link":"http://arxiv.org/pdf/2311.13505v1","categories":["Physics"],"abstract":"We investigate theoretically and experimentally the mode interaction in an integrated Silicon Nitride (Si3N4) microring resonator with interferometric coupling realized by a feedback loop as an adjustable optical path path length connecting the ring to the bus waveguide at two coupling sections. From the transmission spectra recorded at different optical path lengths, two resonances, 1596.5~nm and 1570.5~nm, were selected for detailed investigation. Both resonances show the possibility of adjusting the resonance width and depth. However, the transmission spectra around the first resonance also show the effect of mode interaction. This is also well captured in the theoretical model, from which we can derive a coupling rate for the mode interaction of __FORMULA__."}
{"title":"Arithmetic varieties of numerical semigroups","authors":["Manuel B. Branco","Ignacio Ojeda","Jos\u00e9 Carlos Rosales"],"raw_abstract":"In this paper we present the notion of arithmetic variety for numerical\nsemigroups. We study various aspects related to these varieties such as the\nsmallest arithmetic that contains a set of numerical semigroups and we exhibit\nthe root three associated with an arithmetic variety. This tree is not locally\nfinite; however, if the Frobenius number is fixed, the tree has finitely many\nnodes and algorithms can be developed. All algorithms provided in this article\ninclude their (non-debugged) implementation in GAP.","publication_date":1700669927,"paper_link":"http://arxiv.org/pdf/2311.13500v1","categories":["Mathematics"],"abstract":"In this paper we present the notion of arithmetic variety for numerical semigroups. We study various aspects related to these varieties such as the smallest arithmetic that contains a set of numerical semigroups and we exhibit the root three associated with an arithmetic variety. This tree is not locally finite; however, if the Frobenius number is fixed, the tree has finitely many nodes and algorithms can be developed. All algorithms provided in this article include their (non-debugged) implementation in GAP."}
{"title":"Unveiling $\u03a9_{\\rm m0}$ independently: a journey and consistency quest with first-order perturbation theory","authors":["Bikash R. Dinda"],"raw_abstract":"This study combines cosmic chronometer (CC) Hubble parameter data with growth\nrate (f) observations to constrain the $\\Omega_{\\rm m0}$ parameter. Utilizing a\nconsistency relation independent of cosmological models, we employ Gaussian\nprocess regression to reconstruct Hubble parameter and growth rate values. The\nresulting $\\Omega_{\\rm m0}h^2$ constraint is $\\Omega_{\\rm\nm0}h^2=0.139\\pm0.017$. Incorporating $H_0$ measurements, we find $\\Omega_{\\rm\nm0}$ values from CC data ($0.308\\pm0.057$), tip of the Red Giant Branch\n($0.285\\pm0.038$), and SHOES measurements ($0.259\\pm0.033$). Interestingly, a\nhigher mean $H_0$ correlates with a lower mean $\\Omega_{\\rm m0}$. In summary,\nour cosmological model-independent approach offers valuable constraints on\n$\\Omega_{\\rm m0}$, affirming the consistency of FLRW and Newtonian perturbation\ntheory.","publication_date":1700669802,"paper_link":"http://arxiv.org/pdf/2311.13498v1","categories":["Physics"],"abstract":"This study combines cosmic chronometer (CC) Hubble parameter data with growth rate (f) observations to constrain the __FORMULA__ parameter. Utilizing a consistency relation independent of cosmological models, we employ Gaussian process regression to reconstruct Hubble parameter and growth rate values. The resulting __FORMULA__ constraint is __FORMULA__. Incorporating __FORMULA__ measurements, we find __FORMULA__ values from CC data (__FORMULA__), tip of the Red Giant Branch (__FORMULA__), and SHOES measurements (__FORMULA__). Interestingly, a higher mean __FORMULA__ correlates with a lower mean __FORMULA__. In summary, our cosmological model-independent approach offers valuable constraints on __FORMULA__, affirming the consistency of FLRW and Newtonian perturbation theory."}
{"title":"Gravitational fields of axially symmetric compact objects in 5D space-time-matter gravity","authors":["J. L. Hern\u00e1ndez-Pastora"],"raw_abstract":"In the standard Einstein's theory the exterior gravitational field of any\nstatic and axially symmetric stellar object can be described by means of a\nsingle function from which we obtain a metric into a four-dimensional\nspace-time. In this work we present a generalization of those so called Weyl\nsolutions to a space-time-matter metric in a five-dimensional manifold within a\nnon-compactified Kaluza-Klein theory of gravity. The arising field equations\nreduce to those of vacuum Einstein's gravity when the metric function\nassociated to the fifth dimension is considered to be constant. The calculation\nof the geodesics allows to identify the existence or not of different\nbehaviours of test particles, in orbits on a constant plane, between the two\nmetrics. In addition, static solutions on the hypersurface orthogonal to the\nadded dimension but with time dependence in the five-dimensional metric are\nalso obtained. The consequences on the variation of the rest mass, if the fifth\ndimension is identified with it, are studied.","publication_date":1700669700,"paper_link":"http://arxiv.org/pdf/2311.13497v1","categories":["Physics"],"abstract":"In the standard Einstein's theory the exterior gravitational field of any static and axially symmetric stellar object can be described by means of a single function from which we obtain a metric into a four-dimensional space-time. In this work we present a generalization of those so called Weyl solutions to a space-time-matter metric in a five-dimensional manifold within a non-compactified Kaluza-Klein theory of gravity. The arising field equations reduce to those of vacuum Einstein's gravity when the metric function associated to the fifth dimension is considered to be constant. The calculation of the geodesics allows to identify the existence or not of different behaviours of test particles, in orbits on a constant plane, between the two metrics. In addition, static solutions on the hypersurface orthogonal to the added dimension but with time dependence in the five-dimensional metric are also obtained. The consequences on the variation of the rest mass, if the fifth dimension is identified with it, are studied."}
{"title":"A Comparative Analysis of Supportive Navigation on Movie Recommenders","authors":["Mohammad Sualeh Ali","Muhammed Maaz Tariq","Alina Ahmed","Abdul Razaque Soomro","Danysh Syed"],"raw_abstract":"This literature review covers the research and thought process that went into\nmaking a solution for the infinite scrolling problem faced in streaming\nservices such as Netflix. Using the data collected, we have come to the\nconclusion that an alternate layout can somewhat alleviate the problems it\ntakes in navigating a list of movies. We also found out by a comparative\nanalysis that some layouts, the circular one in particular, is advantageous in\ncertain settings making it an ideal candidate for a movie recommender system.","publication_date":1700669561,"paper_link":"http://arxiv.org/pdf/2311.13494v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This literature review covers the research and thought process that went into making a solution for the infinite scrolling problem faced in streaming services such as Netflix. Using the data collected, we have come to the conclusion that an alternate layout can somewhat alleviate the problems it takes in navigating a list of movies. We also found out by a comparative analysis that some layouts, the circular one in particular, is advantageous in certain settings making it an ideal candidate for a movie recommender system."}
{"title":"Every atom-atom map can be explained by electron pushing diagrams","authors":["Christoph Flamm","Stefan M\u00fcller","Peter F. Stadler"],"raw_abstract":"Chemical reactions can be understood as transformations of multigraphs\n(molecules) that preserve vertex labels (atoms) and degrees (sums of bonding\nand non-bonding electrons), thereby implying the atom-atom map of a reaction.\nThe corresponding reaction mechanism is often described by an electron pushing\ndiagram that explains the transformation by consecutive local relocations of\ninvidudal edges (electron pairs). Here, we show that every degree-preserving\nmap between multigraphs, and thus every atom-atom map, can be generated by\ncyclic electron pushing. Moreover, it is always possible to decompose such an\nexplanation into electron pushing diagrams involving only four electron pairs.\nThis in turn implies that every reaction can be decomposed into a sequence of\nelementary reactions that involve at most two educt molecules and two product\nmolecules. Hence, the requirement of a mechanistic explantion in terms of\nelectron pushing and small imaginary transition states does not impose a\ncombinatorial constraint on the feasibility of hypothetical chemical reactions.","publication_date":1700669341,"paper_link":"http://arxiv.org/pdf/2311.13492v1","categories":["Mathematics","Quantitative Biology"],"abstract":"Chemical reactions can be understood as transformations of multigraphs (molecules) that preserve vertex labels (atoms) and degrees (sums of bonding and non-bonding electrons), thereby implying the atom-atom map of a reaction. The corresponding reaction mechanism is often described by an electron pushing diagram that explains the transformation by consecutive local relocations of invidudal edges (electron pairs). Here, we show that every degree-preserving map between multigraphs, and thus every atom-atom map, can be generated by cyclic electron pushing. Moreover, it is always possible to decompose such an explanation into electron pushing diagrams involving only four electron pairs. This in turn implies that every reaction can be decomposed into a sequence of elementary reactions that involve at most two educt molecules and two product molecules. Hence, the requirement of a mechanistic explantion in terms of electron pushing and small imaginary transition states does not impose a combinatorial constraint on the feasibility of hypothetical chemical reactions."}
{"title":"Evidence for enhanced star formation rates in z~0.35 cluster galaxies undergoing ram pressure stripping","authors":["Benedetta Vulcani","Alessia Moretti","Bianca M. Poggianti","Mario Radovich","Ariel Werle","Marco Gullieuszik","Jacopo Fritz","Cecilia Bacchini","Johan Richard"],"raw_abstract":"Ram pressure stripping (RPS) is one of the most invoked mechanisms to explain\nthe observed differences between cluster and field galaxies. In the local\nUniverse, its effect on the galaxy star forming properties has been largely\ntackled and the general consensus is that this process first compresses the gas\navailable in the galaxy disks, boosting the star formation for a limited amount\nof time, and then removes the remaining gas leading to quenching. Much less is\nknown on the effect and preponderance of RPS at higher redshift, due to the\nlack of statistical samples. Exploiting VLT/MUSE observations of galaxies at\n0.2<z<0.55 and the catalog of ram pressure stripped galaxies by Moretti et al.,\nwe compare the global star formation rate-mass (SFR-M*) relation of 29 cluster\ngalaxies undergoing RPS to that of 26 field and cluster undisturbed galaxies\nthat constitute our control sample. Stripping galaxies occupy the upper\nenvelope of the control sample SFR-M* relation, showing a systematic\nenhancement of the SFR at any given mass. The boost is >3sigma when considering\nthe SFR occurring in both the tail and disk of galaxies. The enhancement is\nretrieved also on local scales: considering spatially resolved data, ram\npressure stripped galaxies overall have large {\\Sigma}SFR values, especially\nfor Sigma_*>10^7.5M_sun kpc~2. RPS seems to leave the same imprint on the\nSFR-M* and Sigma_SFR-Sigma_* relations both in the Local Universe and at\nz~0.35.","publication_date":1700668945,"paper_link":"http://arxiv.org/pdf/2311.13486v1","categories":["Physics"],"abstract":"Ram pressure stripping (RPS) is one of the most invoked mechanisms to explain the observed differences between cluster and field galaxies. In the local Universe, its effect on the galaxy star forming properties has been largely tackled and the general consensus is that this process first compresses the gas available in the galaxy disks, boosting the star formation for a limited amount of time, and then removes the remaining gas leading to quenching. Much less is known on the effect and preponderance of RPS at higher redshift, due to the lack of statistical samples. Exploiting VLT/MUSE observations of galaxies at 0.2<z<0.55 and the catalog of ram pressure stripped galaxies by Moretti et al., we compare the global star formation rate-mass (SFR-M*) relation of 29 cluster galaxies undergoing RPS to that of 26 field and cluster undisturbed galaxies that constitute our control sample. Stripping galaxies occupy the upper envelope of the control sample SFR-M* relation, showing a systematic enhancement of the SFR at any given mass. The boost is >3sigma when considering the SFR occurring in both the tail and disk of galaxies. The enhancement is retrieved also on local scales: considering spatially resolved data, ram pressure stripped galaxies overall have large {\\Sigma}SFR values, especially for Sigma_*>10^7.5M_sun kpc~2. RPS seems to leave the same imprint on the SFR-M* and Sigma_SFR-Sigma_* relations both in the Local Universe and at z~0.35."}
{"title":"Polarimetry of Didymos-Dimorphos: Unexpected Long-Term Effects of the DART Impact","authors":["Zuri Gray","Stefano Bagnulo","Mikael Granvik","Alberto Cellino","Geraint H. Jones","Ludmilla Kolokolova","Fernando Moreno","Karri Muinonen","Olga Mu\u00f1oz","Cyrielle Opitom","Antti Penttil\u00e4","Colin Snodgrass"],"raw_abstract":"We have monitored the Didymos-Dimorphos binary system in imaging polarimetric\nmode before and after the impact from the Double Asteroid Redirection Test\n(DART) mission. A previous spectropolarimetric study showed that the impact\ncaused a dramatic drop in polarisation. Our longer-term monitoring shows that\nthe polarisation of the post-impact system remains lower than the pre-impact\nsystem even months after the impact, suggesting that some fresh ejecta material\nremains in the system at the time of our observations, either in orbit or\nsettled on the surface. The slope of the post-impact polarimetric curve is\nshallower than that of the pre-impact system, implying an increase in albedo of\nthe system. This suggests that the ejected material is composed of smaller and\npossibly brighter particles than those present on the pre-impact surface of the\nasteroid. Our polarimetric maps show that the dust cloud ejected immediately\nafter the impact polarises light in a spatially uniform manner (and at a lower\nlevel than pre-impact). Later maps exhibit a gradient in polarisation between\nthe photocentre (which probes the asteroid surface) and the surrounding cloud\nand tail. The polarisation occasionally shows some small-scale variations, the\nsource of which is not yet clear. The polarimetric phase curve of\nDidymos-Dimorphos resembles that of the S-type asteroid class.","publication_date":1700668732,"paper_link":"http://arxiv.org/pdf/2311.13483v1","categories":["Physics"],"abstract":"We have monitored the Didymos-Dimorphos binary system in imaging polarimetric mode before and after the impact from the Double Asteroid Redirection Test (DART) mission. A previous spectropolarimetric study showed that the impact caused a dramatic drop in polarisation. Our longer-term monitoring shows that the polarisation of the post-impact system remains lower than the pre-impact system even months after the impact, suggesting that some fresh ejecta material remains in the system at the time of our observations, either in orbit or settled on the surface. The slope of the post-impact polarimetric curve is shallower than that of the pre-impact system, implying an increase in albedo of the system. This suggests that the ejected material is composed of smaller and possibly brighter particles than those present on the pre-impact surface of the asteroid. Our polarimetric maps show that the dust cloud ejected immediately after the impact polarises light in a spatially uniform manner (and at a lower level than pre-impact). Later maps exhibit a gradient in polarisation between the photocentre (which probes the asteroid surface) and the surrounding cloud and tail. The polarisation occasionally shows some small-scale variations, the source of which is not yet clear. The polarimetric phase curve of Didymos-Dimorphos resembles that of the S-type asteroid class."}
{"title":"Synergizing Roughness Penalization and Basis Selection in Bayesian Spline Regression","authors":["Sunwoo Lim","Seonghyun Jeong"],"raw_abstract":"Bayesian P-splines and basis determination through Bayesian model selection\nare both commonly employed strategies for nonparametric regression using spline\nbasis expansions within the Bayesian framework. Although both methods are\nwidely employed, they each have particular limitations that may introduce\npotential estimation bias depending on the nature of the target function. To\novercome the limitations associated with each method while capitalizing on\ntheir respective strengths, we propose a new prior distribution that integrates\nthe essentials of both approaches. The proposed prior distribution assesses the\ncomplexity of the spline model based on a penalty term formed by a convex\ncombination of the penalties from both methods. The proposed method exhibits\nadaptability to the unknown level of smoothness while achieving the\nminimax-optimal posterior contraction rate up to a logarithmic factor. We\nprovide an efficient Markov chain Monte Carlo algorithm for implementing the\nproposed approach. Our extensive simulation study reveals that the proposed\nmethod outperforms other competitors in terms of performance metrics or model\ncomplexity. An application to a real dataset substantiates the validity of our\nproposed approach.","publication_date":1700668161,"paper_link":"http://arxiv.org/pdf/2311.13481v1","categories":["Statistics"],"abstract":"Bayesian P-splines and basis determination through Bayesian model selection are both commonly employed strategies for nonparametric regression using spline basis expansions within the Bayesian framework. Although both methods are widely employed, they each have particular limitations that may introduce potential estimation bias depending on the nature of the target function. To overcome the limitations associated with each method while capitalizing on their respective strengths, we propose a new prior distribution that integrates the essentials of both approaches. The proposed prior distribution assesses the complexity of the spline model based on a penalty term formed by a convex combination of the penalties from both methods. The proposed method exhibits adaptability to the unknown level of smoothness while achieving the minimax-optimal posterior contraction rate up to a logarithmic factor. We provide an efficient Markov chain Monte Carlo algorithm for implementing the proposed approach. Our extensive simulation study reveals that the proposed method outperforms other competitors in terms of performance metrics or model complexity. An application to a real dataset substantiates the validity of our proposed approach."}
{"title":"Interacting urn models with strong reinforcement","authors":["Shuo Qin"],"raw_abstract":"We disprove a conjecture by Launay that almost surely one color monopolizes\nall the urns for the interacting urn model with polynomial reinforcement and\nparameter $p>0$. Moreover, we prove that the critical parameter $p_m$ converges\nto $1/2$ as $m$, the degree of the polynomial, goes to infinity.\n  In the case $p=1$, Launay proved that a.s. one color monopolizes all the urns\nif the reinforcement sequence is non-decreasing and reciprocally summable. We\ngive a short proof of that result and generalize it to a larger class of\nreinforcement sequences.","publication_date":1700668147,"paper_link":"http://arxiv.org/pdf/2311.13480v1","categories":["Mathematics"],"abstract":"We disprove a conjecture by Launay that almost surely one color monopolizes all the urns for the interacting urn model with polynomial reinforcement and parameter __FORMULA__. Moreover, we prove that the critical parameter __FORMULA__ converges to __FORMULA__ as __FORMULA__, the degree of the polynomial, goes to infinity.   In the case __FORMULA__, Launay proved that a.s. one color monopolizes all the urns if the reinforcement sequence is non-decreasing and reciprocally summable. We give a short proof of that result and generalize it to a larger class of reinforcement sequences."}
{"title":"Topology of moduli of parabolic connections with fixed determinant","authors":["Nilkantha Das","Sumit Roy"],"raw_abstract":"Let $X$ be a compact Riemann surface of genus $g \\geq 2$ and $D\\subset X$ be\na fixed finite subset. Let $\\xi$ be a line bundle of degree $d$ over $X$. Let\n$\\mathcal{M}(\\alpha, r, \\xi)$ (respectively,\n$\\mathcal{M}_{\\mathrm{conn}}(\\alpha, r, \\xi)$) denote the moduli space of\nstable parabolic bundles (respectively, parabolic connections) of rank $r$\n$(\\geq 2)$, determinant $\\xi$ and full flag generic rational parabolic weight\ntype $\\alpha$. We show that $\n  \\pi_k(\\mathcal{M}_{\\mathrm{conn}}(\\alpha, r, \\xi)) \\cong\n\\pi_k(\\mathcal{M}(\\alpha, r, \\xi)) $ for $k \\leq2(r-1)(g-1)-1$. As a\nconsequence, we deduce that the moduli space\n$\\mathcal{M}_{\\mathrm{conn}}(\\alpha, r, \\xi)$ is simply connected. We also show\nthat the Hodge structures on the torsion-free parts of both the cohomologies\n$H^k(\\mathcal{M}_{\\mathrm{conn}}(\\alpha, r, \\xi),\\mathbb{Z})$ and\n$H^k(\\mathcal{M}(\\alpha, r, \\xi),\\mathbb{Z})$ are isomorphic for all $k\\leq\n2(r-1)(g-1)+1$.","publication_date":1700668090,"paper_link":"http://arxiv.org/pdf/2311.13477v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be a compact Riemann surface of genus __FORMULA__ and __FORMULA__ be a fixed finite subset. Let __FORMULA__ be a line bundle of degree __FORMULA__ over __FORMULA__. Let __FORMULA__ (respectively, __FORMULA__) denote the moduli space of stable parabolic bundles (respectively, parabolic connections) of rank __FORMULA__ __FORMULA__, determinant __FORMULA__ and full flag generic rational parabolic weight type __FORMULA__. We show that __FORMULA__ for __FORMULA__. As a consequence, we deduce that the moduli space __FORMULA__ is simply connected. We also show that the Hodge structures on the torsion-free parts of both the cohomologies __FORMULA__ and __FORMULA__ are isomorphic for all __FORMULA__."}
{"title":"Analysis of a multi-species Cahn-Hilliard-Keller-Segel tumor growth model with chemotaxis and angiogenesis","authors":["Abramo Agosti","Andrea Signori"],"raw_abstract":"We introduce a multi-species diffuse interface model for tumor growth,\ncharacterized by its incorporation of essential features related to chemotaxis,\nangiogenesis and proliferation mechanisms. We establish the weak well-posedness\nof the system within an appropriate variational framework, accommodating\nvarious choices for the nonlinear potentials. One of the primary novelties of\nthe work lies in the rigorous establishment of the existence of a weak solution\nthrough the introduction of delicate approximation schemes. To our knowledge,\nthis represents a novel advancement for both the intricate\nCahn-Hilliard-Keller-Segel system and the Keller-Segel subsystem with source\nterms. Moreover, when specific conditions are met, such as having more regular\ninitial data, a smallness condition on the chemotactic constant with respect to\nthe magnitude of initial conditions and potentially focusing solely on the\ntwo-dimensional case, we provide regularity results for the weak solutions.\nFinally, we derive a continuous dependence estimate, which, in turn, leads to\nthe uniqueness of the smoothed solution as a natural consequence.","publication_date":1700667319,"paper_link":"http://arxiv.org/pdf/2311.13470v1","categories":["Mathematics"],"abstract":"We introduce a multi-species diffuse interface model for tumor growth, characterized by its incorporation of essential features related to chemotaxis, angiogenesis and proliferation mechanisms. We establish the weak well-posedness of the system within an appropriate variational framework, accommodating various choices for the nonlinear potentials. One of the primary novelties of the work lies in the rigorous establishment of the existence of a weak solution through the introduction of delicate approximation schemes. To our knowledge, this represents a novel advancement for both the intricate Cahn-Hilliard-Keller-Segel system and the Keller-Segel subsystem with source terms. Moreover, when specific conditions are met, such as having more regular initial data, a smallness condition on the chemotactic constant with respect to the magnitude of initial conditions and potentially focusing solely on the two-dimensional case, we provide regularity results for the weak solutions. Finally, we derive a continuous dependence estimate, which, in turn, leads to the uniqueness of the smoothed solution as a natural consequence."}
{"title":"Span-Based Optimal Sample Complexity for Average Reward MDPs","authors":["Matthew Zurek","Yudong Chen"],"raw_abstract":"We study the sample complexity of learning an $\\varepsilon$-optimal policy in\nan average-reward Markov decision process (MDP) under a generative model. We\nestablish the complexity bound $\\widetilde{O}\\left(SA\\frac{H}{\\varepsilon^2}\n\\right)$, where $H$ is the span of the bias function of the optimal policy and\n$SA$ is the cardinality of the state-action space. Our result is the first that\nis minimax optimal (up to log factors) in all parameters $S,A,H$ and\n$\\varepsilon$, improving on existing work that either assumes uniformly bounded\nmixing times for all policies or has suboptimal dependence on the parameters.\n  Our result is based on reducing the average-reward MDP to a discounted MDP.\nTo establish the optimality of this reduction, we develop improved bounds for\n$\\gamma$-discounted MDPs, showing that\n$\\widetilde{O}\\left(SA\\frac{H}{(1-\\gamma)^2\\varepsilon^2} \\right)$ samples\nsuffice to learn a $\\varepsilon$-optimal policy in weakly communicating MDPs\nunder the regime that $\\gamma \\geq 1 - \\frac{1}{H}$, circumventing the\nwell-known lower bound of\n$\\widetilde{\\Omega}\\left(SA\\frac{1}{(1-\\gamma)^3\\varepsilon^2} \\right)$ for\ngeneral $\\gamma$-discounted MDPs. Our analysis develops upper bounds on certain\ninstance-dependent variance parameters in terms of the span parameter. These\nbounds are tighter than those based on the mixing time or diameter of the MDP\nand may be of broader use.","publication_date":1700667284,"paper_link":"http://arxiv.org/pdf/2311.13469v1","categories":["Mathematics","Statistics"],"abstract":"We study the sample complexity of learning an __FORMULA__-optimal policy in an average-reward Markov decision process (MDP) under a generative model. We establish the complexity bound __FORMULA__, where __FORMULA__ is the span of the bias function of the optimal policy and __FORMULA__ is the cardinality of the state-action space. Our result is the first that is minimax optimal (up to log factors) in all parameters __FORMULA__ and __FORMULA__, improving on existing work that either assumes uniformly bounded mixing times for all policies or has suboptimal dependence on the parameters.   Our result is based on reducing the average-reward MDP to a discounted MDP. To establish the optimality of this reduction, we develop improved bounds for __FORMULA__-discounted MDPs, showing that __FORMULA__ samples suffice to learn a __FORMULA__-optimal policy in weakly communicating MDPs under the regime that __FORMULA__, circumventing the well-known lower bound of __FORMULA__ for general __FORMULA__-discounted MDPs. Our analysis develops upper bounds on certain instance-dependent variance parameters in terms of the span parameter. These bounds are tighter than those based on the mixing time or diameter of the MDP and may be of broader use."}
{"title":"Fast-varying time lags in the Quasi-periodic Oscillation in GRS 1915+105","authors":["Tomaso M. Belloni","Mariano Mendez","Federico Garcia","Dipankar Bhattacharya"],"raw_abstract":"The properties of sub-second time variability of the X-ray emission of the\nblack-hole binary GRS 1915+105 are very complex and strictly connected to its\npatterns of variability observed on long time scales. A key aspect for\ndetermining the geometry of the accretion flow is the study of time lags\nbetween emission at different energies, as they are associated to key time\nscales of the system. In particular, it is important to examine the lags\nassociated to the strong low-frequency Quasi-periodic Oscillations (QPOs), as\nthe QPOs provide unambiguous special frequencies to sample the variability. We\nhave analyzed data from an observation with the AstroSat satellite, in which\nthe frequency of the low-frequency QPO varies smoothly between 2.5 and 6.6 Hz\non a time scale of ~10 hours. The derived phase lags show the same properties\nand evolution of those observed on time scales of a few hundred days,\nindicating that changes in the system geometry can take place on times below\none day. We fit selected energy spectra of the source and rms and phase-lag\nspectra of the QPO with a time-variable Comptonization model, as done\npreviously to RossiXTE data of the same source, and find that indeed the\nderived parameters match those obtained for variations on much longer time\nscales.","publication_date":1700667246,"paper_link":"http://arxiv.org/pdf/2311.13467v1","categories":["Physics"],"abstract":"The properties of sub-second time variability of the X-ray emission of the black-hole binary GRS 1915+105 are very complex and strictly connected to its patterns of variability observed on long time scales. A key aspect for determining the geometry of the accretion flow is the study of time lags between emission at different energies, as they are associated to key time scales of the system. In particular, it is important to examine the lags associated to the strong low-frequency Quasi-periodic Oscillations (QPOs), as the QPOs provide unambiguous special frequencies to sample the variability. We have analyzed data from an observation with the AstroSat satellite, in which the frequency of the low-frequency QPO varies smoothly between 2.5 and 6.6 Hz on a time scale of ~10 hours. The derived phase lags show the same properties and evolution of those observed on time scales of a few hundred days, indicating that changes in the system geometry can take place on times below one day. We fit selected energy spectra of the source and rms and phase-lag spectra of the QPO with a time-variable Comptonization model, as done previously to RossiXTE data of the same source, and find that indeed the derived parameters match those obtained for variations on much longer time scales."}
{"title":"Continuous-time vertex-reinforced random walks on complete-like graphs","authors":["Shuo Qin","Pierre Tarres"],"raw_abstract":"We introduce the continuous-time vertex-reinforced random walk (cVRRW) as a\ncontinuous-time version of the vertex-reinforced random walk (VRRW), which\nmight open a new perspective on the study of the VRRW.\n  It has been proved by Limic and Volkov that for the VRRW on a complete-like\ngraph $K_d \\cup \\partial K_d$, the asymptotic frequency of visits is uniform\nover the non-leaf vertices. We give short proofs of those results by\nestablishing a stochastic approximation result for the cVRRW on complete-like\ngraphs. We also prove that almost surely, the number of visits to each leaf up\nto time n divided by $n^{\\frac{1}{d-1}}$ converges to a non-zero limit. We\nsolve a conjecture by Limic and Volkov on the rate of convergence in the case\nof the complete graph.","publication_date":1700667139,"paper_link":"http://arxiv.org/pdf/2311.13465v1","categories":["Mathematics"],"abstract":"We introduce the continuous-time vertex-reinforced random walk (cVRRW) as a continuous-time version of the vertex-reinforced random walk (VRRW), which might open a new perspective on the study of the VRRW.   It has been proved by Limic and Volkov that for the VRRW on a complete-like graph __FORMULA__, the asymptotic frequency of visits is uniform over the non-leaf vertices. We give short proofs of those results by establishing a stochastic approximation result for the cVRRW on complete-like graphs. We also prove that almost surely, the number of visits to each leaf up to time n divided by __FORMULA__ converges to a non-zero limit. We solve a conjecture by Limic and Volkov on the rate of convergence in the case of the complete graph."}
{"title":"Enhancement of stability of metastable states in the presence of L\u00e9vy noise","authors":["A. A. Dubkov","C. Guarcello","B. Spagnolo"],"raw_abstract":"The barrier crossing event for superdiffusion in the form of symmetric\nL\\'{e}vy flights is investigated. We derive from the fractional Fokker-Planck\nequation a general differential equation with the corresponding conditions\nuseful to calculate the mean residence time of a particle in a fixed interval\nfor an arbitrary smooth potential profile, in particular metastable, with a\nsink and a L\\'{e}vy noise with an arbitrary index $\\alpha$. A closed expression\nin quadrature of the nonlinear relaxation time for L\\'{e}vy flights with the\nindex $\\alpha =1$ in cubic metastable potential is obtained. Enhancement of the\nmean residence time in the metastable state, analytically derived, due to\nL\\'{e}vy noise is found.","publication_date":1700667091,"paper_link":"http://arxiv.org/pdf/2311.13464v1","categories":["Physics"],"abstract":"The barrier crossing event for superdiffusion in the form of symmetric L\\'{e}vy flights is investigated. We derive from the fractional Fokker-Planck equation a general differential equation with the corresponding conditions useful to calculate the mean residence time of a particle in a fixed interval for an arbitrary smooth potential profile, in particular metastable, with a sink and a L\\'{e}vy noise with an arbitrary index __FORMULA__. A closed expression in quadrature of the nonlinear relaxation time for L\\'{e}vy flights with the index __FORMULA__ in cubic metastable potential is obtained. Enhancement of the mean residence time in the metastable state, analytically derived, due to L\\'{e}vy noise is found."}
{"title":"Variance of squarefull numbers in short intervals II","authors":["Tsz Ho Chan"],"raw_abstract":"In this paper, we continue the study on variance of the number of squarefull\nnumbers in short intervals $(x, x + 2 \\sqrt{x} H + H^2]$ with $X \\le x \\le 2X$.\nWe obtain the expected asymptotic for this variance over the range $X^\\epsilon\n\\le H \\le X^{0.180688...}$ unconditionally and over the optimal range\n$X^\\epsilon \\le H \\le X^{0.25 - \\epsilon}$ conditionally on the Riemann\nHypothesis or the Lindel\\\"{o}f Hypothesis.","publication_date":1700667026,"paper_link":"http://arxiv.org/pdf/2311.13463v1","categories":["Mathematics"],"abstract":"In this paper, we continue the study on variance of the number of squarefull numbers in short intervals __FORMULA__ with __FORMULA__. We obtain the expected asymptotic for this variance over the range __FORMULA__ unconditionally and over the optimal range __FORMULA__ conditionally on the Riemann Hypothesis or the Lindel\\\"{o}f Hypothesis."}
{"title":"Experimentation in Early-Stage Video Game Startups: Practices and Challenges","authors":["Henry Edison","Jorge Melegati","Elizabeth Bjarnason"],"raw_abstract":"Experimentation has been considered critical for successful software product\nand business development, including in video game startups. Video game startups\nneed \"wow\" qualities that distinguish them from the competition. Thus, they\nneed to continuously experiment to find these qualities before running out of\ntime and resources. In this study, we aimed to explore how these companies\nperform experimentation. We interviewed four co-founders of video game\nstartups. Our findings identify six practices, or scenarios, through which\nvideo game startups conduct experiments and challenges associated with these.\nThe initial results could inform these startups about the possibilities and\nchallenges and guide future research.","publication_date":1700666830,"paper_link":"http://arxiv.org/pdf/2311.13462v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Experimentation has been considered critical for successful software product and business development, including in video game startups. Video game startups need \"wow\" qualities that distinguish them from the competition. Thus, they need to continuously experiment to find these qualities before running out of time and resources. In this study, we aimed to explore how these companies perform experimentation. We interviewed four co-founders of video game startups. Our findings identify six practices, or scenarios, through which video game startups conduct experiments and challenges associated with these. The initial results could inform these startups about the possibilities and challenges and guide future research."}
{"title":"An exact bandit model for the risk-volatility tradeoff","authors":["Max-Olivier Hongler","Renaud Rivier"],"raw_abstract":"We revisit the two-armed bandit (TAB) problem where both arms are driven by\ndiffusive stochastic processes with a common instantaneous reward. We focus on\nsituations where the Radon-Nikodym derivative between the transition\nprobability densities of the first arm with respect to the second is explicitly\nknown. We calculate how the corresponding Gittins' indices behave under such a\nchange of probability measure. This general framework is used to solve the\noptimal allocation of a TAB problem where the first arm is driven by a pure\nBrownian motion and the second is driven by a centered super-diffusive\nnon-Gaussian process with variance quadratically growing in time. The\nprobability spread due to the super-diffusion introduces an extra risk into the\nallocation problem. This drastically affects the optimal decision rule. Our\nmodeling illustrates the interplay between the notions of risk and volatility.","publication_date":1700666764,"paper_link":"http://arxiv.org/pdf/2311.13461v1","categories":["Mathematics"],"abstract":"We revisit the two-armed bandit (TAB) problem where both arms are driven by diffusive stochastic processes with a common instantaneous reward. We focus on situations where the Radon-Nikodym derivative between the transition probability densities of the first arm with respect to the second is explicitly known. We calculate how the corresponding Gittins' indices behave under such a change of probability measure. This general framework is used to solve the optimal allocation of a TAB problem where the first arm is driven by a pure Brownian motion and the second is driven by a centered super-diffusive non-Gaussian process with variance quadratically growing in time. The probability spread due to the super-diffusion introduces an extra risk into the allocation problem. This drastically affects the optimal decision rule. Our modeling illustrates the interplay between the notions of risk and volatility."}
{"title":"Multi-Objective Bayesian Optimization with Active Preference Learning","authors":["Ryota Ozaki","Kazuki Ishikawa","Youhei Kanzaki","Shinya Suzuki","Shion Takeno","Ichiro Takeuchi","Masayuki Karasuyama"],"raw_abstract":"There are a lot of real-world black-box optimization problems that need to\noptimize multiple criteria simultaneously. However, in a multi-objective\noptimization (MOO) problem, identifying the whole Pareto front requires the\nprohibitive search cost, while in many practical scenarios, the decision maker\n(DM) only needs a specific solution among the set of the Pareto optimal\nsolutions. We propose a Bayesian optimization (BO) approach to identifying the\nmost preferred solution in the MOO with expensive objective functions, in which\na Bayesian preference model of the DM is adaptively estimated by an interactive\nmanner based on the two types of supervisions called the pairwise preference\nand improvement request. To explore the most preferred solution, we define an\nacquisition function in which the uncertainty both in the objective functions\nand the DM preference is incorporated. Further, to minimize the interaction\ncost with the DM, we also propose an active learning strategy for the\npreference estimation. We empirically demonstrate the effectiveness of our\nproposed method through the benchmark function optimization and the\nhyper-parameter optimization problems for machine learning models.","publication_date":1700666676,"paper_link":"http://arxiv.org/pdf/2311.13460v1","categories":["Statistics"],"abstract":"There are a lot of real-world black-box optimization problems that need to optimize multiple criteria simultaneously. However, in a multi-objective optimization (MOO) problem, identifying the whole Pareto front requires the prohibitive search cost, while in many practical scenarios, the decision maker (DM) only needs a specific solution among the set of the Pareto optimal solutions. We propose a Bayesian optimization (BO) approach to identifying the most preferred solution in the MOO with expensive objective functions, in which a Bayesian preference model of the DM is adaptively estimated by an interactive manner based on the two types of supervisions called the pairwise preference and improvement request. To explore the most preferred solution, we define an acquisition function in which the uncertainty both in the objective functions and the DM preference is incorporated. Further, to minimize the interaction cost with the DM, we also propose an active learning strategy for the preference estimation. We empirically demonstrate the effectiveness of our proposed method through the benchmark function optimization and the hyper-parameter optimization problems for machine learning models."}
{"title":"The Tempered Hilbert Simplex Distance and Its Application To Non-linear Embeddings of TEMs","authors":["Ehsan Amid","Frank Nielsen","Richard Nock","Manfred K. Warmuth"],"raw_abstract":"Tempered Exponential Measures (TEMs) are a parametric generalization of the\nexponential family of distributions maximizing the tempered entropy function\namong positive measures subject to a probability normalization of their power\ndensities. Calculus on TEMs relies on a deformed algebra of arithmetic\noperators induced by the deformed logarithms used to define the tempered\nentropy. In this work, we introduce three different parameterizations of finite\ndiscrete TEMs via Legendre functions of the negative tempered entropy function.\nIn particular, we establish an isometry between such parameterizations in terms\nof a generalization of the Hilbert log cross-ratio simplex distance to a\ntempered Hilbert co-simplex distance. Similar to the Hilbert geometry, the\ntempered Hilbert distance is characterized as a $t$-symmetrization of the\noriented tempered Funk distance. We motivate our construction by introducing\nthe notion of $t$-lengths of smooth curves in a tautological Finsler manifold.\nWe then demonstrate the properties of our generalized structure in different\nsettings and numerically examine the quality of its differentiable\napproximations for optimization in machine learning settings.","publication_date":1700666669,"paper_link":"http://arxiv.org/pdf/2311.13459v1","categories":["Statistics"],"abstract":"Tempered Exponential Measures (TEMs) are a parametric generalization of the exponential family of distributions maximizing the tempered entropy function among positive measures subject to a probability normalization of their power densities. Calculus on TEMs relies on a deformed algebra of arithmetic operators induced by the deformed logarithms used to define the tempered entropy. In this work, we introduce three different parameterizations of finite discrete TEMs via Legendre functions of the negative tempered entropy function. In particular, we establish an isometry between such parameterizations in terms of a generalization of the Hilbert log cross-ratio simplex distance to a tempered Hilbert co-simplex distance. Similar to the Hilbert geometry, the tempered Hilbert distance is characterized as a __FORMULA__-symmetrization of the oriented tempered Funk distance. We motivate our construction by introducing the notion of __FORMULA__-lengths of smooth curves in a tautological Finsler manifold. We then demonstrate the properties of our generalized structure in different settings and numerically examine the quality of its differentiable approximations for optimization in machine learning settings."}
{"title":"Real-time unobtrusive sleep monitoring of in-patients with affective disorders: a feasibility study","authors":["Samuel Askjer","Kim Mathiasen","Ali Amidi","Christine Parsons","Nicolai Ladegaard"],"raw_abstract":"Sleep and mental health are highly related concepts, and it is an important\nresearch and clinical priority to understand their interactions. In-bed sensors\nusing ballistocardiography provide the possibility of unobtrusive measurements\nof sleep. In this study, we examined the feasibility of ballistocardiography in\nmeasuring key aspects of sleep in psychiatric in-patients. Specifically, we\nexamined a sample of patients diagnosed with depression and bipolar disorder.\nThe subjective experiences of the researchers conducting the study are explored\nand descriptive analyses of patient sleep are subsequently presented. The\npracticalities of using the ballistocardiography device seem to be favourable.\nThere were no apparent issues regarding data quality or data integrity. Of\nclinical interest, we found no link between length of stay and reduced time in\nbed (b = -0.06, SE = 0.03, t = -1.76, p = .08). Using ballistocardiography for\nmeasurements on in-patients with affective disorders seems to be a feasible\napproach.","publication_date":1700666626,"paper_link":"http://arxiv.org/pdf/2311.13457v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Sleep and mental health are highly related concepts, and it is an important research and clinical priority to understand their interactions. In-bed sensors using ballistocardiography provide the possibility of unobtrusive measurements of sleep. In this study, we examined the feasibility of ballistocardiography in measuring key aspects of sleep in psychiatric in-patients. Specifically, we examined a sample of patients diagnosed with depression and bipolar disorder. The subjective experiences of the researchers conducting the study are explored and descriptive analyses of patient sleep are subsequently presented. The practicalities of using the ballistocardiography device seem to be favourable. There were no apparent issues regarding data quality or data integrity. Of clinical interest, we found no link between length of stay and reduced time in bed (b = -0.06, SE = 0.03, t = -1.76, p = .08). Using ballistocardiography for measurements on in-patients with affective disorders seems to be a feasible approach."}
{"title":"Infinite-dimensional flats in the space of positive metrics on an ample line bundle","authors":["Reboulet R\u00e9mi","Witt Nystr\u00f6m David"],"raw_abstract":"We show that any continuous positive metric on an ample line bundle L lies at\nthe apex of many infinite-dimensional Mabuchi-flat cones. More precisely, given\nany bounded graded filtration F of the section ring of L, the set of bounded\ndecreasing convex functions on the support of the Duistermaat--Heckman measure\nof F embeds L^p-isometrically into the space of bounded positive metrics on L\nwith respect to Darvas' d_p distance for all p\\in[1,\\infty), and in particular\nwith respect to the Mabuchi metric (p=2).","publication_date":1700666193,"paper_link":"http://arxiv.org/pdf/2311.13451v1","categories":["Mathematics"],"abstract":"We show that any continuous positive metric on an ample line bundle L lies at the apex of many infinite-dimensional Mabuchi-flat cones. More precisely, given any bounded graded filtration F of the section ring of L, the set of bounded decreasing convex functions on the support of the Duistermaat--Heckman measure of F embeds L^p-isometrically into the space of bounded positive metrics on L with respect to Darvas' d_p distance for all p\\in[1,\\infty), and in particular with respect to the Mabuchi metric (p=2)."}
{"title":"Millimeter Wave Thin-Film Bulk Acoustic Resonator in Sputtered Scandium Aluminum Nitride Using Platinum Electrodes","authors":["Sinwoo Cho","Omar Barrera","Pietro Simeoni","Ellie Y. Wang","Jack Kramer","Vakhtang Chulukhadze","Joshua Campbell","Matteo Rinaldi","Ruochen Lu"],"raw_abstract":"This work describes sputtered scandium aluminum nitride (ScAlN) thin-film\nbulk acoustic resonators (FBAR) at millimeter wave (mmWave) with high quality\nfactor (Q) using platinum (Pt) electrodes. FBARs with combinations of Pt and\naluminum (Al) electrodes, i.e., Al top Al bottom, Pt top Al bottom, Al top Pt\nbottom, and Pt top Pt bottom, are built to study the impact of electrodes on\nmmWave FBARs. The demonstrated FBAR with Pt top and bottom electrodes achieve\nelectromechanical coupling (k2) of 4.0% and Q of 116 for the first-order\nsymmetric (S1) mode at 13.7 GHz, and k2 of 1.8% and Q of 94 for third-order\nsymmetric (S3) mode at 61.6 GHz. Through these results, we confirmed that even\nin the frequency band of approximately 60 GHz, ScAlN FBAR can achieve a Q\nfactor approaching 100 with optimized fabrication and acoustic/EM design.\nFurther development calls for stacks with better quality in piezoelectric and\nmetallic layers.","publication_date":1700665994,"paper_link":"http://arxiv.org/pdf/2311.13448v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This work describes sputtered scandium aluminum nitride (ScAlN) thin-film bulk acoustic resonators (FBAR) at millimeter wave (mmWave) with high quality factor (Q) using platinum (Pt) electrodes. FBARs with combinations of Pt and aluminum (Al) electrodes, i.e., Al top Al bottom, Pt top Al bottom, Al top Pt bottom, and Pt top Pt bottom, are built to study the impact of electrodes on mmWave FBARs. The demonstrated FBAR with Pt top and bottom electrodes achieve electromechanical coupling (k2) of 4.0% and Q of 116 for the first-order symmetric (S1) mode at 13.7 GHz, and k2 of 1.8% and Q of 94 for third-order symmetric (S3) mode at 61.6 GHz. Through these results, we confirmed that even in the frequency band of approximately 60 GHz, ScAlN FBAR can achieve a Q factor approaching 100 with optimized fabrication and acoustic/EM design. Further development calls for stacks with better quality in piezoelectric and metallic layers."}
{"title":"Differentially Private Non-Convex Optimization under the KL Condition with Optimal Rates","authors":["Michael Menart","Enayat Ullah","Raman Arora","Raef Bassily","Crist\u00f3bal Guzm\u00e1n"],"raw_abstract":"We study private empirical risk minimization (ERM) problem for losses\nsatisfying the $(\\gamma,\\kappa)$-Kurdyka-{\\L}ojasiewicz (KL) condition. The\nPolyak-{\\L}ojasiewicz (PL) condition is a special case of this condition when\n$\\kappa=2$. Specifically, we study this problem under the constraint of $\\rho$\nzero-concentrated differential privacy (zCDP). When $\\kappa\\in[1,2]$ and the\nloss function is Lipschitz and smooth over a sufficiently large region, we\nprovide a new algorithm based on variance reduced gradient descent that\nachieves the rate\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$ on the\nexcess empirical risk, where $n$ is the dataset size and $d$ is the dimension.\nWe further show that this rate is nearly optimal. When $\\kappa \\geq 2$ and the\nloss is instead Lipschitz and weakly convex, we show it is possible to achieve\nthe rate $\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^\\kappa\\big)$\nwith a private implementation of the proximal point method. When the KL\nparameters are unknown, we provide a novel modification and analysis of the\nnoisy gradient descent algorithm and show that this algorithm achieves a rate\nof\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^{\\frac{2\\kappa}{4-\\kappa}}\\big)$\nadaptively, which is nearly optimal when $\\kappa = 2$. We further show that,\nwithout assuming the KL condition, the same gradient descent algorithm can\nachieve fast convergence to a stationary point when the gradient stays\nsufficiently large during the run of the algorithm. Specifically, we show that\nthis algorithm can approximate stationary points of Lipschitz, smooth (and\npossibly nonconvex) objectives with rate as fast as\n$\\tilde{O}\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)$ and never worse than\n$\\tilde{O}\\big(\\big(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}\\big)^{1/2}\\big)$. The latter\nrate matches the best known rate for methods that do not rely on variance\nreduction.","publication_date":1700665962,"paper_link":"http://arxiv.org/pdf/2311.13447v1","categories":["Mathematics","Statistics"],"abstract":"We study private empirical risk minimization (ERM) problem for losses satisfying the __FORMULA__-Kurdyka-{\\L}ojasiewicz (KL) condition. The Polyak-{\\L}ojasiewicz (PL) condition is a special case of this condition when __FORMULA__. Specifically, we study this problem under the constraint of __FORMULA__ zero-concentrated differential privacy (zCDP). When __FORMULA__ and the loss function is Lipschitz and smooth over a sufficiently large region, we provide a new algorithm based on variance reduced gradient descent that achieves the rate __FORMULA__ on the excess empirical risk, where __FORMULA__ is the dataset size and __FORMULA__ is the dimension. We further show that this rate is nearly optimal. When __FORMULA__ and the loss is instead Lipschitz and weakly convex, we show it is possible to achieve the rate __FORMULA__ with a private implementation of the proximal point method. When the KL parameters are unknown, we provide a novel modification and analysis of the noisy gradient descent algorithm and show that this algorithm achieves a rate of __FORMULA__ adaptively, which is nearly optimal when __FORMULA__. We further show that, without assuming the KL condition, the same gradient descent algorithm can achieve fast convergence to a stationary point when the gradient stays sufficiently large during the run of the algorithm. Specifically, we show that this algorithm can approximate stationary points of Lipschitz, smooth (and possibly nonconvex) objectives with rate as fast as __FORMULA__ and never worse than __FORMULA__. The latter rate matches the best known rate for methods that do not rely on variance reduction."}
{"title":"Transfer Attacks and Defenses for Large Language Models on Coding Tasks","authors":["Chi Zhang","Zifan Wang","Ravi Mangal","Matt Fredrikson","Limin Jia","Corina Pasareanu"],"raw_abstract":"Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.","publication_date":1700665895,"paper_link":"http://arxiv.org/pdf/2311.13445v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Modern large language models (LLMs), such as ChatGPT, have demonstrated impressive capabilities for coding tasks including writing and reasoning about code. They improve upon previous neural network models of code, such as code2seq or seq2seq, that already demonstrated competitive results when performing tasks such as code summarization and identifying code vulnerabilities. However, these previous code models were shown vulnerable to adversarial examples, i.e. small syntactic perturbations that do not change the program's semantics, such as the inclusion of \"dead code\" through false conditions or the addition of inconsequential print statements, designed to \"fool\" the models. LLMs can also be vulnerable to the same adversarial perturbations but a detailed study on this concern has been lacking so far. In this paper we aim to investigate the effect of adversarial perturbations on coding tasks with LLMs. In particular, we study the transferability of adversarial examples, generated through white-box attacks on smaller code models, to LLMs. Furthermore, to make the LLMs more robust against such adversaries without incurring the cost of retraining, we propose prompt-based defenses that involve modifying the prompt to include additional information such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations. Our experiments show that adversarial examples obtained with a smaller code model are indeed transferable, weakening the LLMs' performance. The proposed defenses show promise in improving the model's resilience, paving the way to more robust defensive solutions for LLMs in code-related applications."}
{"title":"On convergence of points to limiting processes, with an application to zeta zeros","authors":["Juan Arias de Reyna","Brad Rodgers"],"raw_abstract":"This paper considers sequences of points on the real line which have been\nrandomly translated, and provides conditions under which various notions of\nconvergence to a limiting point process are equivalent. In particular we\nconsider convergence in correlation, convergence in distribution, and\nconvergence of spacings between points. We also prove a simple Tauberian\ntheorem regarding rescaled correlations. The results are applied to zeros of\nthe Riemann zeta-function to show that several ways to state the GUE Hypothesis\nare equivalent. The proof relies on a moment bound of A. Fujii.","publication_date":1700665651,"paper_link":"http://arxiv.org/pdf/2311.13441v1","categories":["Mathematics"],"abstract":"This paper considers sequences of points on the real line which have been randomly translated, and provides conditions under which various notions of convergence to a limiting point process are equivalent. In particular we consider convergence in correlation, convergence in distribution, and convergence of spacings between points. We also prove a simple Tauberian theorem regarding rescaled correlations. The results are applied to zeros of the Riemann zeta-function to show that several ways to state the GUE Hypothesis are equivalent. The proof relies on a moment bound of A. Fujii."}
{"title":"Pro-$\\mathcal{C}$ RAAGs","authors":["Montserrat Casals-Ruiz","Matteo Pintonello","Pavel Zalesskii"],"raw_abstract":"Let $\\mathcal{C}$ be a class of finite groups closed under taking subgroups,\nquotients, and extensions with abelian kernel. The right-angled Artin\npro-$\\mathcal{C}$ group $G_\\Gamma$ (pro-$\\mathcal{C}$ RAAG for short) is the\npro-$\\mathcal{C}$ completion of the right-angled Artin group $G(\\Gamma)$\nassociated with the finite simplicial graph $\\Gamma$.\n  In the first part, we describe structural properties of pro-$\\mathcal{C}$\nRAAGs. Among others, we describe the centraliser of an element and show that\npro-$\\mathcal{C}$ RAAGs satisfy the Tits' alternative, that standard subgroups\nare isolated, and that 2-generated pro-$p$ subgroups of pro-$\\mathcal{C}$ RAAGs\nare either free pro-$p$ or free abelian pro-$p$.\n  In the second part, we characterise splittings of pro-$\\mathcal{C}$ RAAGs in\nterms of the defining graph. More precisely, we prove that a pro-$\\mathcal{C}$\nRAAG $G_\\Gamma$ splits as a non-trivial direct product if and only if $\\Gamma$\nis a join and it splits over an abelian pro-$\\mathcal{C}$ group if and only if\na connected component of $\\Gamma$ is a complete graph or it has a complete\ndisconnecting subgraph. We then use this characterisation to describe an\nabelian JSJ decomposition of a pro-$\\mathcal{C}$ RAAG, in the sense of\nGuirardel and Levitt.","publication_date":1700665116,"paper_link":"http://arxiv.org/pdf/2311.13439v1","categories":["Mathematics"],"abstract":"Let __FORMULA__ be a class of finite groups closed under taking subgroups, quotients, and extensions with abelian kernel. The right-angled Artin pro-__FORMULA__ group __FORMULA__ (pro-__FORMULA__ RAAG for short) is the pro-__FORMULA__ completion of the right-angled Artin group __FORMULA__ associated with the finite simplicial graph __FORMULA__.   In the first part, we describe structural properties of pro-__FORMULA__ RAAGs. Among others, we describe the centraliser of an element and show that pro-__FORMULA__ RAAGs satisfy the Tits' alternative, that standard subgroups are isolated, and that 2-generated pro-__FORMULA__ subgroups of pro-__FORMULA__ RAAGs are either free pro-__FORMULA__ or free abelian pro-__FORMULA__.   In the second part, we characterise splittings of pro-__FORMULA__ RAAGs in terms of the defining graph. More precisely, we prove that a pro-__FORMULA__ RAAG __FORMULA__ splits as a non-trivial direct product if and only if __FORMULA__ is a join and it splits over an abelian pro-__FORMULA__ group if and only if a connected component of __FORMULA__ is a complete graph or it has a complete disconnecting subgraph. We then use this characterisation to describe an abelian JSJ decomposition of a pro-__FORMULA__ RAAG, in the sense of Guirardel and Levitt."}
{"title":"Sparsity-Driven EEG Channel Selection for Brain-Assisted Speech Enhancement","authors":["Jie Zhang","Qing-Tian Xu","Zhen-Hua Ling"],"raw_abstract":"Speech enhancement is widely used as a front-end to improve the speech\nquality in many audio systems, while it is still hard to extract the target\nspeech in multi-talker conditions without prior information on the speaker\nidentity. It was shown by auditory attention decoding that the attended speaker\ncan be revealed by the electroencephalogram (EEG) of the listener implicitly.\nIn this work, we therefore propose a novel end-to-end brain-assisted speech\nenhancement network (BASEN), which incorporates the listeners' EEG signals and\nadopts a temporal convolutional network together with a convolutional\nmulti-layer cross attention module to fuse EEG-audio features. Considering that\nan EEG cap with sparse channels exhibits multiple benefits and in practice many\nelectrodes might contribute marginally, we further propose two channel\nselection methods, called residual Gumbel selection and convolutional\nregularization selection. They are dedicated to tackling the issues of training\ninstability and duplicated channel selections, respectively. Experimental\nresults on a public dataset show the superiority of the proposed baseline BASEN\nover existing approaches. The proposed channel selection methods can\nsignificantly reduce the amount of informative EEG channels with a negligible\nimpact on the performance.","publication_date":1700664611,"paper_link":"http://arxiv.org/pdf/2311.13436v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Speech enhancement is widely used as a front-end to improve the speech quality in many audio systems, while it is still hard to extract the target speech in multi-talker conditions without prior information on the speaker identity. It was shown by auditory attention decoding that the attended speaker can be revealed by the electroencephalogram (EEG) of the listener implicitly. In this work, we therefore propose a novel end-to-end brain-assisted speech enhancement network (BASEN), which incorporates the listeners' EEG signals and adopts a temporal convolutional network together with a convolutional multi-layer cross attention module to fuse EEG-audio features. Considering that an EEG cap with sparse channels exhibits multiple benefits and in practice many electrodes might contribute marginally, we further propose two channel selection methods, called residual Gumbel selection and convolutional regularization selection. They are dedicated to tackling the issues of training instability and duplicated channel selections, respectively. Experimental results on a public dataset show the superiority of the proposed baseline BASEN over existing approaches. The proposed channel selection methods can significantly reduce the amount of informative EEG channels with a negligible impact on the performance."}
{"title":"Extracting individual variable information for their decoupling, direct mutual information and multi-feature Granger causality","authors":["Jarek Duda"],"raw_abstract":"Working with multiple variables they usually contain difficult to control\ncomplex dependencies. This article proposes extraction of their individual\ninformation, e.g. $\\overline{X|Y}$ as random variable containing information\nfrom $X$, but with removed information about $Y$, by using $(x,y)\n\\leftrightarrow (\\bar{x}=\\textrm{CDF}_{X|Y=y}(x),y)$ reversible normalization.\nOne application can be decoupling of individual information of variables:\nreversibly transform $(X_1,\\ldots,X_n)\\leftrightarrow(\\tilde{X}_1,\\ldots\n\\tilde{X}_n)$ together containing the same information, but being independent:\n$\\forall_{i\\neq j} \\tilde{X}_i\\perp \\tilde{X}_j, \\tilde{X}_i\\perp X_j$. It\nrequires detailed models of complex conditional probability distributions - it\nis generally a difficult task, but here can be done through multiple dependency\nreducing iterations, using imperfect methods (here HCR: Hierarchical\nCorrelation Reconstruction). It could be also used for direct mutual\ninformation - evaluating direct information transfer: without use of\nintermediate variables. For causality direction there is discussed\nmulti-feature Granger causality, e.g. to trace various types of individual\ninformation transfers between such decoupled variables, including propagation\ntime (delay).","publication_date":1700664330,"paper_link":"http://arxiv.org/pdf/2311.13431v1","categories":["Mathematics","Statistics"],"abstract":"Working with multiple variables they usually contain difficult to control complex dependencies. This article proposes extraction of their individual information, e.g. __FORMULA__ as random variable containing information from __FORMULA__, but with removed information about __FORMULA__, by using __FORMULA__ reversible normalization. One application can be decoupling of individual information of variables: reversibly transform __FORMULA__ together containing the same information, but being independent: __FORMULA__. It requires detailed models of complex conditional probability distributions - it is generally a difficult task, but here can be done through multiple dependency reducing iterations, using imperfect methods (here HCR: Hierarchical Correlation Reconstruction). It could be also used for direct mutual information - evaluating direct information transfer: without use of intermediate variables. For causality direction there is discussed multi-feature Granger causality, e.g. to trace various types of individual information transfers between such decoupled variables, including propagation time (delay)."}
{"title":"Cosmic evolution of FRI and FRII sources out to z=2.5","authors":["J. M. G. H. J. de Jong","H. J. A. R\u00f6ttgering","R. Kondapally","B. Mingo","R. J. van Weeren","P. N. Best","L. K. Morabito","M. Magliocchetti","J. B. R. Oonk","A. Villarrubia-Aguilar","F. F. Vecchi"],"raw_abstract":"Radio-loud active galactic nuclei (RLAGN) play an important role in the\nevolution of galaxies through the effects on their environment. The two major\nmorphological classes are core-bright (FRI) and edge-bright (FRII) sources.\nWith the LOw-Frequency ARray (LOFAR) we compare the FRI and FRII evolution down\nto lower flux densities and with larger samples than before with the aim to\nexamine the cosmic space density evolution for FRIs and FRIIs by analyzing\ntheir space density evolution between L_150~10^24.5 W/Hz and L_150~10^28.5 W/Hz\nand up to z=2.5. We construct radio luminosity functions (RLFs) from FRI and\nFRII catalogues based on recent data from LOFAR at 150MHz to study the space\ndensities as a function of radio luminosity and redshift. To partly correct for\nselection biases and completeness, we simulate how sources appear at a range of\nredshifts. We report a space density enhancement from low to high redshift for\nFRI and FRII sources brighter than L_150~10^27 W/Hz. This is possibly related\nto the higher gas availability in the earlier denser universe. The constant\nFRI/FRII space density ratio evolution as a function of radio luminosity and\nredshift in our results suggests that the jet-disruption of FRIs might be\nprimarily caused by events occurring on scales within the host galaxy, rather\nthan being driven by changes in the overall large-scale environment. Remaining\nselection biases in our results also highlight the need to resolve more sources\nat angular scales below 40 arcsec and therefore strengthens the motivation for\nthe further development and automation of the calibration and imaging pipeline\nof LOFAR data to produce images at sub-arcsecond resolution.","publication_date":1700664082,"paper_link":"http://arxiv.org/pdf/2311.13427v1","categories":["Physics"],"abstract":"Radio-loud active galactic nuclei (RLAGN) play an important role in the evolution of galaxies through the effects on their environment. The two major morphological classes are core-bright (FRI) and edge-bright (FRII) sources. With the LOw-Frequency ARray (LOFAR) we compare the FRI and FRII evolution down to lower flux densities and with larger samples than before with the aim to examine the cosmic space density evolution for FRIs and FRIIs by analyzing their space density evolution between L_150~10^24.5 W/Hz and L_150~10^28.5 W/Hz and up to z=2.5. We construct radio luminosity functions (RLFs) from FRI and FRII catalogues based on recent data from LOFAR at 150MHz to study the space densities as a function of radio luminosity and redshift. To partly correct for selection biases and completeness, we simulate how sources appear at a range of redshifts. We report a space density enhancement from low to high redshift for FRI and FRII sources brighter than L_150~10^27 W/Hz. This is possibly related to the higher gas availability in the earlier denser universe. The constant FRI/FRII space density ratio evolution as a function of radio luminosity and redshift in our results suggests that the jet-disruption of FRIs might be primarily caused by events occurring on scales within the host galaxy, rather than being driven by changes in the overall large-scale environment. Remaining selection biases in our results also highlight the need to resolve more sources at angular scales below 40 arcsec and therefore strengthens the motivation for the further development and automation of the calibration and imaging pipeline of LOFAR data to produce images at sub-arcsecond resolution."}
{"title":"Numerical investigation of effective nonlinear coefficient model for coupled third harmonic generation","authors":["Zihua Zheng","Ziwen Tang","Zhiyi Wei","Jinghua Sun"],"raw_abstract":"In this paper, the optimal solution of effective nonlinear coefficient of\nquasi-phase-matching (QPM) crystals for coupled third harmonic generation\n(CTHG) was numerically investigated. The effective nonlinear coefficient of\nCTHG was converted to an Ising model for optimizing domain length distributions\nof aperiodically poled lithium niobate (APPLN) crystals with lengths as 0.5 mm\nand 1 mm, and fundamental wavelengths ranging from 1000 nm to 6000 nm. A method\nfor reconstructing crystal domain poling weight curve of coupled nonlinear\nprocesses was also proposed, which demonstrated the optimal conversion ratio\nbetween two coupled nonlinear processes at each place along the crystal. In\naddition, by applying the semidefinite programming, the upper bound on the\neffective nonlinear coefficients deff for different fundamental wavelengths\nwere calculated. The research can be extended to any coupled dual \\c{hi}(2)\nprocess and will help us to understand better the dynamics of coupled nonlinear\ninteractions based on QPM crystals.","publication_date":1700664006,"paper_link":"http://arxiv.org/pdf/2311.13426v1","categories":["Physics"],"abstract":"In this paper, the optimal solution of effective nonlinear coefficient of quasi-phase-matching (QPM) crystals for coupled third harmonic generation (CTHG) was numerically investigated. The effective nonlinear coefficient of CTHG was converted to an Ising model for optimizing domain length distributions of aperiodically poled lithium niobate (APPLN) crystals with lengths as 0.5 mm and 1 mm, and fundamental wavelengths ranging from 1000 nm to 6000 nm. A method for reconstructing crystal domain poling weight curve of coupled nonlinear processes was also proposed, which demonstrated the optimal conversion ratio between two coupled nonlinear processes at each place along the crystal. In addition, by applying the semidefinite programming, the upper bound on the effective nonlinear coefficients deff for different fundamental wavelengths were calculated. The research can be extended to any coupled dual hi(2) process and will help us to understand better the dynamics of coupled nonlinear interactions based on QPM crystals."}
{"title":"Towards a general description of the cavitation threshold in acoustic systems","authors":["Gianmaria Viciconte","Paolo Guida","Tadd Truscott","William L. Roberts"],"raw_abstract":"Traditionally, the occurrence of cavitation has been related to the ratio\nbetween flow velocity and pressure gradient in the case of hydrodynamic\ncavitation, or some combination of vapor pressure and surface tension. However,\nboth formulations present a large discrepancy with experimental data for cases\nin which cavitation is induced by acoustic waves. The present study aims to\nidentify a more suitable cavitation threshold for such cases. The methodology\nadopted in this work consists of a combination of visualization with high-speed\ncameras and direct measurements using a hydrophone. The data collected\nconfirmed that the vapor pressure is not a proper indicator of cavitation\noccurrence for an acoustic system characterized by high frequencies. The main\nreason behind the inability of vapor pressure to predict incipient cavitation\nin acoustic systems is that they evolve very quickly toward strong gradients in\npressure, and the quasi-static assumptions used by traditional models are not\nvalid. Instead, the system evolves towards a metastable state [Brennen, 2013],\nwhere the liquid exhibits an elastic behavior and can withstand negative\npressures. A new cavitation number accounting for the tensile strength of the\nliquid was defined. An acoustic analogy is also proposed for the description,\nwith the same framework, of an impulsive cavitation phenomenon.","publication_date":1700663934,"paper_link":"http://arxiv.org/pdf/2311.13425v1","categories":["Physics"],"abstract":"Traditionally, the occurrence of cavitation has been related to the ratio between flow velocity and pressure gradient in the case of hydrodynamic cavitation, or some combination of vapor pressure and surface tension. However, both formulations present a large discrepancy with experimental data for cases in which cavitation is induced by acoustic waves. The present study aims to identify a more suitable cavitation threshold for such cases. The methodology adopted in this work consists of a combination of visualization with high-speed cameras and direct measurements using a hydrophone. The data collected confirmed that the vapor pressure is not a proper indicator of cavitation occurrence for an acoustic system characterized by high frequencies. The main reason behind the inability of vapor pressure to predict incipient cavitation in acoustic systems is that they evolve very quickly toward strong gradients in pressure, and the quasi-static assumptions used by traditional models are not valid. Instead, the system evolves towards a metastable state [Brennen, 2013], where the liquid exhibits an elastic behavior and can withstand negative pressures. A new cavitation number accounting for the tensile strength of the liquid was defined. An acoustic analogy is also proposed for the description, with the same framework, of an impulsive cavitation phenomenon."}
{"title":"Nonlocal Schr\u00f6dinger-Poisson systems in $\\mathbb R^N$: the fractional Sobolev limiting case","authors":["Daniele Cassani","Zhisu Liu","Giulio Romani"],"raw_abstract":"We study the existence of positive solutions for nonlocal systems in gradient\nform and set in the whole $\\mathbb R^N$. A quasilinear fractional Schr\\\"odinger\nequation, where the leading operator is the $\\frac Ns$-fractional Laplacian, is\ncoupled with a higher-order and possibly fractional Poisson equation. For both\noperators the dimension $N\\geq 2$ corresponds to the limiting case of the\nSobolev embedding, hence we consider nonlinearities with exponential growth.\nSince standard variational tools cannot be applied due to the sign changing\nlogarithmic Riesz kernel of the Poisson equation, we employ a variational\napproximating procedure for an auxiliary Choquard equation, where the Riesz\nkernel is uniformly approximated by polynomial kernels. Qualitative properties\nof solutions such as symmetry, regularity and decay are also established. Our\nresults extend and complete the analysis carried out in the planar case in [D.\nCassani, Z. Liu, G. Romani. arxiv:2305.15274].","publication_date":1700663864,"paper_link":"http://arxiv.org/pdf/2311.13424v1","categories":["Mathematics"],"abstract":"We study the existence of positive solutions for nonlocal systems in gradient form and set in the whole __FORMULA__. A quasilinear fractional Schr\\\"odinger equation, where the leading operator is the __FORMULA__-fractional Laplacian, is coupled with a higher-order and possibly fractional Poisson equation. For both operators the dimension __FORMULA__ corresponds to the limiting case of the Sobolev embedding, hence we consider nonlinearities with exponential growth. Since standard variational tools cannot be applied due to the sign changing logarithmic Riesz kernel of the Poisson equation, we employ a variational approximating procedure for an auxiliary Choquard equation, where the Riesz kernel is uniformly approximated by polynomial kernels. Qualitative properties of solutions such as symmetry, regularity and decay are also established. Our results extend and complete the analysis carried out in the planar case in [D. Cassani, Z. Liu, G. Romani. arxiv:2305.15274]."}
{"title":"Fast vanishing cycles on perturbations of (complex) weighted-homogeneous germs","authors":["Dmitry Kerner","Rodrigo Mendes"],"raw_abstract":"Let X_o be a complex weighted-homogeneous complete intersection germ,\n(possibly non-reduced). Let X be a perturbation of X_o by\n``higher-order-terms\". We give sufficient criteria to detect fast cycles on X,\nvia the weights of X_o. This is an easy obstruction to be non-metrically\nconical.\n  A simple application of our results gives, e.g.\n  * Suppose the germs X_o,X,X\\cap V(x_1) are ICIS. If X is IMC then the n\nlowest weights of X_o coincide.\n  * Let the surface germ X=V(f)\\subset (C^3,o) be Newton-non-degenerate and\nIMC. Then for each of the faces of the Newton diagram the two lowest weights\ncoincide.\n  As an auxiliary result we prove (under certain assumptions, for \\k=\\R,\\C):\nthe weighted-homogeneous foliation of the pair X_o \\sset (\\k^N,o) deforms to a\nfoliation of the pair X \\sset (\\k^N,o). In particular, the deformation by\nhigher order terms is ambient-trivializable by a semialgebraic Lipschitz\nhomeomorphism.","publication_date":1700663811,"paper_link":"http://arxiv.org/pdf/2311.13423v1","categories":["Mathematics"],"abstract":"Let X_o be a complex weighted-homogeneous complete intersection germ, (possibly non-reduced). Let X be a perturbation of X_o by ``higher-order-terms\". We give sufficient criteria to detect fast cycles on X, via the weights of X_o. This is an easy obstruction to be non-metrically conical.   A simple application of our results gives, e.g.   * Suppose the germs X_o,X,X\\cap V(x_1) are ICIS. If X is IMC then the n lowest weights of X_o coincide.   * Let the surface germ X=V(f)\\subset (C^3,o) be Newton-non-degenerate and IMC. Then for each of the faces of the Newton diagram the two lowest weights coincide.   As an auxiliary result we prove (under certain assumptions, for \\k=\\R,\\C): the weighted-homogeneous foliation of the pair X_o \\sset (\\k^N,o) deforms to a foliation of the pair X \\sset (\\k^N,o). In particular, the deformation by higher order terms is ambient-trivializable by a semialgebraic Lipschitz homeomorphism."}
{"title":"Moduli of K3 families over $\\mathbb{P}^1$, cycle spaces of IHS period domains, and deformations of complex-hyperk\u00e4hler metrics","authors":["Daniel Greb","Martin Schwald"],"raw_abstract":"In the spirit of the classical theory for K3 surfaces, we establish a moduli\ntheory for families of marked K3 surfaces over smooth rational curves in the K3\nperiod domain. In particular, we study the geometry of the space of such\nrational curves, which assumes the role of a period space for this moduli\nproblem. Over an open subset containing all twistor cycles, we construct a\nfamily of such families that is a universal small deformation for every twistor\nfamily. Finally, small deformations of K3 twistor families are shown to induce\ncomplex-hyperk\\\"ahler metrics on members of the families via the original\ncomplex version of Penrose's Non-linear Graviton construction. This answers a\nquestion posed independently by Fels--Huckleberry--Wolf and Looijenga.\nGeneralizations to higher-dimensional irreducible holomorphic-symplectic\nmanifolds are studied throughout.","publication_date":1700663461,"paper_link":"http://arxiv.org/pdf/2311.13420v1","categories":["Mathematics"],"abstract":"In the spirit of the classical theory for K3 surfaces, we establish a moduli theory for families of marked K3 surfaces over smooth rational curves in the K3 period domain. In particular, we study the geometry of the space of such rational curves, which assumes the role of a period space for this moduli problem. Over an open subset containing all twistor cycles, we construct a family of such families that is a universal small deformation for every twistor family. Finally, small deformations of K3 twistor families are shown to induce complex-hyperk\\\"ahler metrics on members of the families via the original complex version of Penrose's Non-linear Graviton construction. This answers a question posed independently by Fels--Huckleberry--Wolf and Looijenga. Generalizations to higher-dimensional irreducible holomorphic-symplectic manifolds are studied throughout."}
{"title":"Liouville theorem for quasilinear elliptic equations in $\\mathbb R^N$","authors":["Wangzhe Wu","Qiqi Zhang"],"raw_abstract":"We prove Liouville theorem for the equation $\\Delta_m v + v^p + M |\\nabla\nv|^{q}= 0$ in a domain $\\Omega\\subset\\mathbb R^n$, with $M\\in \\mathbb{R}$ in\nthe critical and subcritical case. As a natural extension of our recent work\n\\cite{MWZ}, the proof is based on an integral identity and Young's inequality.","publication_date":1700663356,"paper_link":"http://arxiv.org/pdf/2311.13418v1","categories":["Mathematics"],"abstract":"We prove Liouville theorem for the equation __FORMULA__ in a domain __FORMULA__, with __FORMULA__ in the critical and subcritical case. As a natural extension of our recent work MWZ, the proof is based on an integral identity and Young's inequality."}
{"title":"Revisiting Machine Learning based Test Case Prioritization for Continuous Integration","authors":["Yifan Zhao","Dan Hao","Lu Zhang"],"raw_abstract":"To alleviate the cost of regression testing in continuous integration (CI), a\nlarge number of machine learning-based (ML-based) test case prioritization\ntechniques have been proposed. However, it is yet unknown how they perform\nunder the same experimental setup, because they are evaluated on different\ndatasets with different metrics. To bridge this gap, we conduct the first\ncomprehensive study on these ML-based techniques in this paper. We investigate\nthe performance of 11 representative ML-based prioritization techniques for CI\non 11 open-source subjects and obtain a series of findings. For example, the\nperformance of the techniques changes across CI cycles, mainly resulting from\nthe changing amount of training data, instead of code evolution and test\nremoval/addition. Based on the findings, we give some actionable suggestions on\nenhancing the effectiveness of ML-based techniques, e.g., pretraining a\nprioritization technique with cross-subject data to get it thoroughly trained\nand then finetuning it with within-subject data dramatically improves its\nperformance. In particular, the pretrained MART achieves state-of-the-art\nperformance, producing the optimal sequence on 80% subjects, while the existing\nbest technique, the original MART, only produces the optimal sequence on 50%\nsubjects.","publication_date":1700662764,"paper_link":"http://arxiv.org/pdf/2311.13413v1","categories":["Electrical Engineering and Systems Science"],"abstract":"To alleviate the cost of regression testing in continuous integration (CI), a large number of machine learning-based (ML-based) test case prioritization techniques have been proposed. However, it is yet unknown how they perform under the same experimental setup, because they are evaluated on different datasets with different metrics. To bridge this gap, we conduct the first comprehensive study on these ML-based techniques in this paper. We investigate the performance of 11 representative ML-based prioritization techniques for CI on 11 open-source subjects and obtain a series of findings. For example, the performance of the techniques changes across CI cycles, mainly resulting from the changing amount of training data, instead of code evolution and test removal/addition. Based on the findings, we give some actionable suggestions on enhancing the effectiveness of ML-based techniques, e.g., pretraining a prioritization technique with cross-subject data to get it thoroughly trained and then finetuning it with within-subject data dramatically improves its performance. In particular, the pretrained MART achieves state-of-the-art performance, producing the optimal sequence on 80% subjects, while the existing best technique, the original MART, only produces the optimal sequence on 50% subjects."}
{"title":"Improving tensor regression by optimal model averaging","authors":["Qiushi Bu","Hua Liang","Xinyu Zhang","Jiahui Zou"],"raw_abstract":"Tensors have broad applications in neuroimaging, data mining, digital\nmarketing, etc. CANDECOMP/PARAFAC (CP) tensor decomposition can effectively\nreduce the number of parameters to gain dimensionality-reduction and thus plays\na key role in tensor regression. However, in CP decomposition, there is\nuncertainty which rank to use. In this article, we develop a model averaging\nmethod to handle this uncertainty by weighting the estimators from candidate\ntensor regression models with different ranks. When all candidate models are\nmisspecified, we prove that the model averaging estimator is asymptotically\noptimal. When correct models are included in the candidate models, we prove the\nconsistency of parameters and the convergence of the model averaging weight.\nSimulations and empirical studies illustrate that the proposed method has\nsuperiority over the competition methods and has promising applications.","publication_date":1700662726,"paper_link":"http://arxiv.org/pdf/2311.13412v1","categories":["Mathematics","Statistics"],"abstract":"Tensors have broad applications in neuroimaging, data mining, digital marketing, etc. CANDECOMP/PARAFAC (CP) tensor decomposition can effectively reduce the number of parameters to gain dimensionality-reduction and thus plays a key role in tensor regression. However, in CP decomposition, there is uncertainty which rank to use. In this article, we develop a model averaging method to handle this uncertainty by weighting the estimators from candidate tensor regression models with different ranks. When all candidate models are misspecified, we prove that the model averaging estimator is asymptotically optimal. When correct models are included in the candidate models, we prove the consistency of parameters and the convergence of the model averaging weight. Simulations and empirical studies illustrate that the proposed method has superiority over the competition methods and has promising applications."}
{"title":"Bayesian inference of a new Mallows model for characterising symptom sequences applied in primary progressive aphasia","authors":["Beatrice Taylor","Cameron Shand","Chris J. D. Hardy","Neil Oxtoby"],"raw_abstract":"Machine learning models offer the potential to understand diverse datasets in\na data-driven way, powering insights into individual disease experiences and\nensuring equitable healthcare. In this study, we explore Bayesian inference for\ncharacterising symptom sequences, and the associated modelling challenges. We\nadapted the Mallows model to account for partial rankings and right-censored\ndata, employing custom MCMC fitting. Our evaluation, encompassing synthetic\ndata and a primary progressive aphasia dataset, highlights the model's efficacy\nin revealing mean orderings and estimating ranking variance. This holds the\npotential to enhance clinical comprehension of symptom occurrence. However, our\nwork encounters limitations concerning model scalability and small dataset\nsizes.","publication_date":1700662580,"paper_link":"http://arxiv.org/pdf/2311.13411v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Machine learning models offer the potential to understand diverse datasets in a data-driven way, powering insights into individual disease experiences and ensuring equitable healthcare. In this study, we explore Bayesian inference for characterising symptom sequences, and the associated modelling challenges. We adapted the Mallows model to account for partial rankings and right-censored data, employing custom MCMC fitting. Our evaluation, encompassing synthetic data and a primary progressive aphasia dataset, highlights the model's efficacy in revealing mean orderings and estimating ranking variance. This holds the potential to enhance clinical comprehension of symptom occurrence. However, our work encounters limitations concerning model scalability and small dataset sizes."}
{"title":"Towards Sensitivity Analysis: A Workflow","authors":["Cheng Lin","Sourabh Balgi","Jose Pena","Adel Daoud"],"raw_abstract":"Establishing causal claims is one of the primary endeavors in sociological\nresearch. Statistical causal inference is a promising way to achieve this\nthrough the potential outcome framework or structural causal models, which are\nbased on a set of identification assumptions. However, identification\nassumptions are often not fully discussed in practice, which harms the validity\nof causal claims. In this article, we focus on the unmeasurededness assumption\nthat assumes no unmeasured confounders in models, which is often violated in\npractice. This article reviews a set of papers in two leading sociological\njournals to check the practice of causal inference and relevant identification\nassumptions, indicating the lack of discussion on sensitivity analysis methods\non unconfoundedness in practice. And then, a blueprint of how to conduct\nsensitivity analysis methods on unconfoundedness is built, including six steps\nof proper choices on practices of sensitivity analysis to evaluate the impacts\nof unmeasured confounders.","publication_date":1700662411,"paper_link":"http://arxiv.org/pdf/2311.13410v1","categories":["Statistics"],"abstract":"Establishing causal claims is one of the primary endeavors in sociological research. Statistical causal inference is a promising way to achieve this through the potential outcome framework or structural causal models, which are based on a set of identification assumptions. However, identification assumptions are often not fully discussed in practice, which harms the validity of causal claims. In this article, we focus on the unmeasurededness assumption that assumes no unmeasured confounders in models, which is often violated in practice. This article reviews a set of papers in two leading sociological journals to check the practice of causal inference and relevant identification assumptions, indicating the lack of discussion on sensitivity analysis methods on unconfoundedness in practice. And then, a blueprint of how to conduct sensitivity analysis methods on unconfoundedness is built, including six steps of proper choices on practices of sensitivity analysis to evaluate the impacts of unmeasured confounders."}
{"title":"Fppf-descent for condensed animated rings","authors":["Yutaro Mikami"],"raw_abstract":"In this paper, we define \"animated affinoid algebras\" and prove some basic\nproperties of them. Then we generalize the result of the previous paper of the\nauthor and the result of Lucas Mann (fppf-descent for discrete rings or\naffinoid algebras) to discrete animated rings or animated affinoid algebras.","publication_date":1700662309,"paper_link":"http://arxiv.org/pdf/2311.13408v1","categories":["Mathematics"],"abstract":"In this paper, we define \"animated affinoid algebras\" and prove some basic properties of them. Then we generalize the result of the previous paper of the author and the result of Lucas Mann (fppf-descent for discrete rings or affinoid algebras) to discrete animated rings or animated affinoid algebras."}
{"title":"Lozenge Tilings of a Hexagon and q-Racah Ensembles","authors":["Maurice Duits","Erik Duse","Wenkui Liu"],"raw_abstract":"We study the limiting behavior of random lozenge tilings of the hexagon with\na q-Racah weight as the size of the hexagon grows large. Based on the\nasymptotic behavior of the recurrence coefficients of the q-Racah polynomials,\nwe give a new proof for the fact that that the height function for a random\ntiling concentrates near a deterministic limit shape and that the global\nfluctuations are described by the Gaussian Free Field. These results were\nrecently proved using (dynamic) loop equation techniques. In this paper we\nextend the recurrence coefficient approach that was developed for (dynamic)\northogonal polynomial ensembles to the setting of q-orthogonal polynomials. An\ninteresting feature is that the complex structure is easily found from the\nlimiting behavior of the (explicitly known) recurrence coefficients. A\nparticular motivation for studying this model is that the variational\ncharacterization of the limiting height function has an inhomogeneous term. The\nstudy of the regularity properties of the minimizer for general variation\nproblems with such inhomogeuous terms is a challenging open problem. We show\nthat, in a general setup, the variational problem gives rise to a natural\ncomplex structure that is associated to the same Beltrami equation as in the\nhomogeneous situation. We also derive a relation between the complex structure\nand the complex slope. In case of the q-Racah weighting of lozenge tilings of\nthe hexagon, our representation of the limit shape and their fluctuations in\nterms of the recurrence coefficients allows us to verify this relation\nexplicitly.","publication_date":1700661885,"paper_link":"http://arxiv.org/pdf/2311.13407v1","categories":["Mathematics"],"abstract":"We study the limiting behavior of random lozenge tilings of the hexagon with a q-Racah weight as the size of the hexagon grows large. Based on the asymptotic behavior of the recurrence coefficients of the q-Racah polynomials, we give a new proof for the fact that that the height function for a random tiling concentrates near a deterministic limit shape and that the global fluctuations are described by the Gaussian Free Field. These results were recently proved using (dynamic) loop equation techniques. In this paper we extend the recurrence coefficient approach that was developed for (dynamic) orthogonal polynomial ensembles to the setting of q-orthogonal polynomials. An interesting feature is that the complex structure is easily found from the limiting behavior of the (explicitly known) recurrence coefficients. A particular motivation for studying this model is that the variational characterization of the limiting height function has an inhomogeneous term. The study of the regularity properties of the minimizer for general variation problems with such inhomogeuous terms is a challenging open problem. We show that, in a general setup, the variational problem gives rise to a natural complex structure that is associated to the same Beltrami equation as in the homogeneous situation. We also derive a relation between the complex structure and the complex slope. In case of the q-Racah weighting of lozenge tilings of the hexagon, our representation of the limit shape and their fluctuations in terms of the recurrence coefficients allows us to verify this relation explicitly."}
{"title":"Triple-sinusoid hedgehog lattice in a centrosymmetric Kondo metal","authors":["Soohyeon Shin","Jin-Hong Park","Romain Sibille","Harim Jang","Tae Beom Park","Suyoung Kim","Tian Shang","Marisa Medarde","Eric D. Bauer","Oksana Zaharko","Michel Kenzelmann","Tuson Park"],"raw_abstract":"Superposed symmetry-equivalent magnetic ordering wave vectors can lead to\ntopologically non-trivial spin textures, such as magnetic skyrmions and\nhedgehogs, and give rise to novel quantum phenomena due to fictitious magnetic\nfields associated with a non-zero Berry curvature of these spin textures. To\ndate, all known spin textures are constructed through the superposition of\nmultiple spiral orders, where spins vary in directions with constant amplitude.\nRecent theoretical studies have suggested that multiple sinusoidal orders,\nwhere collinear spins vary in amplitude, can construct distinct topological\nspin textures regarding chirality properties. However, such textures have yet\nto be experimentally realised. In this work, we report the observation of a\nzero-field magnetic hedgehog lattice from a superposition of triple sinusoidal\nwave vectors in the magnetically frustrated Kondo lattice CePtAl4Ge2. Notably,\nwe also observe the emergence of anomalous electrical and thermodynamic\nbehaviours near the field-induced transition from the zero-field topological\nhedgehog lattice to a non-topological sinusoidal state. These observations\nhighlight the role of Kondo coupling in stabilising the zero-field hedgehog\nstate in the Kondo lattice and warrant an expedited search for other\ntopological magnetic structures coupled with Kondo coupling.","publication_date":1700661689,"paper_link":"http://arxiv.org/pdf/2311.13405v1","categories":["Physics"],"abstract":"Superposed symmetry-equivalent magnetic ordering wave vectors can lead to topologically non-trivial spin textures, such as magnetic skyrmions and hedgehogs, and give rise to novel quantum phenomena due to fictitious magnetic fields associated with a non-zero Berry curvature of these spin textures. To date, all known spin textures are constructed through the superposition of multiple spiral orders, where spins vary in directions with constant amplitude. Recent theoretical studies have suggested that multiple sinusoidal orders, where collinear spins vary in amplitude, can construct distinct topological spin textures regarding chirality properties. However, such textures have yet to be experimentally realised. In this work, we report the observation of a zero-field magnetic hedgehog lattice from a superposition of triple sinusoidal wave vectors in the magnetically frustrated Kondo lattice CePtAl4Ge2. Notably, we also observe the emergence of anomalous electrical and thermodynamic behaviours near the field-induced transition from the zero-field topological hedgehog lattice to a non-topological sinusoidal state. These observations highlight the role of Kondo coupling in stabilising the zero-field hedgehog state in the Kondo lattice and warrant an expedited search for other topological magnetic structures coupled with Kondo coupling."}
{"title":"Explicit height estimates for CM curves of genus 2","authors":["Linda Frey","Samuel Le Fourn","Elisa Lorenzo Garc\u00eda"],"raw_abstract":"In this paper we make explicit the constants of Habegger and Pazuki's work\nfrom 2017 on bounding the discriminant of cyclic Galois CM fields corresponding\nto genus 2 curves with CM by them and potentially good reduction outside a\npredefined set of primes. We also simplify some of the arguments.","publication_date":1700661402,"paper_link":"http://arxiv.org/pdf/2311.13403v1","categories":["Mathematics"],"abstract":"In this paper we make explicit the constants of Habegger and Pazuki's work from 2017 on bounding the discriminant of cyclic Galois CM fields corresponding to genus 2 curves with CM by them and potentially good reduction outside a predefined set of primes. We also simplify some of the arguments."}
{"title":"Volatility and irregularity Capturing in stock price indices using time series Generative adversarial networks (TimeGAN)","authors":["Leonard Mushunje","David Allen","Shelton Peiris"],"raw_abstract":"This paper captures irregularities in financial time series data,\nparticularly stock prices, in the presence of COVID-19 shock. We conjectured\nthat jumps and irregularities are embedded in stock data due to the pandemic\nshock, which brings forth irregular trends in the time series data. We put\nforward that efficient and robust forecasting methods are needed to predict\nstock closing prices in the presence of the pandemic shock. This piece of\ninformation is helpful to investors as far as confidence risk and return boost\nare concerned. Generative adversarial networks of a time series nature are used\nto provide new ways of modeling and learning the proper and suitable\ndistribution for the financial time series data under complex setups. Ideally,\nthese traditional models are liable to producing high forecasting errors, and\nthey need to be more robust to capture dependency structures and other stylized\nfacts like volatility in stock markets. The TimeGAN model is used, effectively\ndealing with this risk of poor forecasts. Using the DAX stock index from\nJanuary 2010 to November 2022, we trained the LSTM, GRU, WGAN, and TimeGAN\nmodels as benchmarks and forecasting errors were noted, and our TimeGAN\noutperformed them all as indicated by a small forecasting error.","publication_date":1700599568,"paper_link":"http://arxiv.org/pdf/2311.12987v1","categories":["Quantitative Finance"],"abstract":"This paper captures irregularities in financial time series data, particularly stock prices, in the presence of COVID-19 shock. We conjectured that jumps and irregularities are embedded in stock data due to the pandemic shock, which brings forth irregular trends in the time series data. We put forward that efficient and robust forecasting methods are needed to predict stock closing prices in the presence of the pandemic shock. This piece of information is helpful to investors as far as confidence risk and return boost are concerned. Generative adversarial networks of a time series nature are used to provide new ways of modeling and learning the proper and suitable distribution for the financial time series data under complex setups. Ideally, these traditional models are liable to producing high forecasting errors, and they need to be more robust to capture dependency structures and other stylized facts like volatility in stock markets. The TimeGAN model is used, effectively dealing with this risk of poor forecasts. Using the DAX stock index from January 2010 to November 2022, we trained the LSTM, GRU, WGAN, and TimeGAN models as benchmarks and forecasting errors were noted, and our TimeGAN outperformed them all as indicated by a small forecasting error."}
{"title":"Neural-Integrated Meshfree (NIM) Method: A differentiable programming-based hybrid solver for computational mechanics","authors":["Honghui Du","QiZhi He"],"raw_abstract":"We present the neural-integrated meshfree (NIM) method, a differentiable\nprogramming-based hybrid meshfree approach within the field of computational\nmechanics. NIM seamlessly integrates traditional physics-based meshfree\ndiscretization techniques with deep learning architectures. It employs a hybrid\napproximation scheme, NeuroPU, to effectively represent the solution by\ncombining continuous DNN representations with partition of unity (PU) basis\nfunctions associated with the underlying spatial discretization. This\nneural-numerical hybridization not only enhances the solution representation\nthrough functional space decomposition but also reduces both the size of DNN\nmodel and the need for spatial gradient computations based on automatic\ndifferentiation, leading to a significant improvement in training efficiency.\nUnder the NIM framework, we propose two truly meshfree solvers: the strong\nform-based NIM (S-NIM) and the local variational form-based NIM (V-NIM). In the\nS-NIM solver, the strong-form governing equation is directly considered in the\nloss function, while the V-NIM solver employs a local Petrov-Galerkin approach\nthat allows the construction of variational residuals based on arbitrary\noverlapping subdomains. This ensures both the satisfaction of underlying\nphysics and the preservation of meshfree property. We perform extensive\nnumerical experiments on both stationary and transient benchmark problems to\nassess the effectiveness of the proposed NIM methods in terms of accuracy,\nscalability, generalizability, and convergence properties. Moreover,\ncomparative analysis with other physics-informed machine learning methods\ndemonstrates that NIM, especially V-NIM, significantly enhances both accuracy\nand efficiency in end-to-end predictive capabilities.","publication_date":1700589432,"paper_link":"http://arxiv.org/pdf/2311.12915v1","categories":["Quantitative Finance"],"abstract":"We present the neural-integrated meshfree (NIM) method, a differentiable programming-based hybrid meshfree approach within the field of computational mechanics. NIM seamlessly integrates traditional physics-based meshfree discretization techniques with deep learning architectures. It employs a hybrid approximation scheme, NeuroPU, to effectively represent the solution by combining continuous DNN representations with partition of unity (PU) basis functions associated with the underlying spatial discretization. This neural-numerical hybridization not only enhances the solution representation through functional space decomposition but also reduces both the size of DNN model and the need for spatial gradient computations based on automatic differentiation, leading to a significant improvement in training efficiency. Under the NIM framework, we propose two truly meshfree solvers: the strong form-based NIM (S-NIM) and the local variational form-based NIM (V-NIM). In the S-NIM solver, the strong-form governing equation is directly considered in the loss function, while the V-NIM solver employs a local Petrov-Galerkin approach that allows the construction of variational residuals based on arbitrary overlapping subdomains. This ensures both the satisfaction of underlying physics and the preservation of meshfree property. We perform extensive numerical experiments on both stationary and transient benchmark problems to assess the effectiveness of the proposed NIM methods in terms of accuracy, scalability, generalizability, and convergence properties. Moreover, comparative analysis with other physics-informed machine learning methods demonstrates that NIM, especially V-NIM, significantly enhances both accuracy and efficiency in end-to-end predictive capabilities."}
{"title":"From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design","authors":["Cyril Picard","Kristen M. Edwards","Anna C. Doris","Brandon Man","Giorgio Giannone","Md Ferdous Alam","Faez Ahmed"],"raw_abstract":"Engineering Design is undergoing a transformative shift with the advent of\nAI, marking a new era in how we approach product, system, and service planning.\nLarge language models have demonstrated impressive capabilities in enabling\nthis shift. Yet, with text as their only input modality, they cannot leverage\nthe large body of visual artifacts that engineers have used for centuries and\nare accustomed to. This gap is addressed with the release of multimodal vision\nlanguage models, such as GPT-4V, enabling AI to impact many more types of\ntasks. In light of these advancements, this paper presents a comprehensive\nevaluation of GPT-4V, a vision language model, across a wide spectrum of\nengineering design tasks, categorized into four main areas: Conceptual Design,\nSystem-Level and Detailed Design, Manufacturing and Inspection, and Engineering\nEducation Tasks. Our study assesses GPT-4V's capabilities in design tasks such\nas sketch similarity analysis, concept selection using Pugh Charts, material\nselection, engineering drawing analysis, CAD generation, topology optimization,\ndesign for additive and subtractive manufacturing, spatial reasoning\nchallenges, and textbook problems. Through this structured evaluation, we not\nonly explore GPT-4V's proficiency in handling complex design and manufacturing\nchallenges but also identify its limitations in complex engineering design\napplications. Our research establishes a foundation for future assessments of\nvision language models, emphasizing their immense potential for innovating and\nenhancing the engineering design and manufacturing landscape. It also\ncontributes a set of benchmark testing datasets, with more than 1000 queries,\nfor ongoing advancements and applications in this field.","publication_date":1700580048,"paper_link":"http://arxiv.org/pdf/2311.12668v1","categories":["Quantitative Finance"],"abstract":"Engineering Design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision language models, such as GPT-4V, enabling AI to impact many more types of tasks. In light of these advancements, this paper presents a comprehensive evaluation of GPT-4V, a vision language model, across a wide spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Our study assesses GPT-4V's capabilities in design tasks such as sketch similarity analysis, concept selection using Pugh Charts, material selection, engineering drawing analysis, CAD generation, topology optimization, design for additive and subtractive manufacturing, spatial reasoning challenges, and textbook problems. Through this structured evaluation, we not only explore GPT-4V's proficiency in handling complex design and manufacturing challenges but also identify its limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models, emphasizing their immense potential for innovating and enhancing the engineering design and manufacturing landscape. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field."}
{"title":"A new paradigm for the efficient inclusion of stochasticity in engineering simulations","authors":["Hendrik Geisler","Cem Erdogan","Jan Nagel","Philipp Junker"],"raw_abstract":"As a physical fact, randomness is an inherent and ineliminable aspect in all\nphysical measurements and engineering production. As a consequence, material\nparameters, serving as input data, are only known in a stochastic sense and\nthus, also output parameters, e.g., stresses, fluctuate. For the estimation of\nthose fluctuations it is imperative to incoporate randomness into engineering\nsimulations. Unfortunately, incorporating uncertain parameters into the\nmodeling and simulation of inelastic materials is often computationally\nexpensive, as many individual simulations may have to be performed. The promise\nof the proposed method is simple: using extended material models to include\nstochasticity reduces the number of needed simulations to one. This single\ncomputation is cheap, i.e., it has a comparable numerical effort as a single\nstandard simulation. The extended material models are easily derived from\nstandard deterministic material models and account for the effect of\nuncertainty by an extended set of deterministic material parameters. The\ntime-dependent and stochastic material behavior are separated, such that only\nthe deterministic time-dependent behavior of the extended material model needs\nto be simulated. The effect of stochasticity is then included during\npost-processing. The feasibility of this approach is demonstrated for three\ndifferent and highly non-linear material models: viscous damage, viscous phase\ntransformations and elasto-viscoplasticity. A comparison to the Monte Carlo\nmethod showcases that the method is indeed able to provide reliable estimates\nof the expectation and variance of internal variables and stress at a minimal\nfraction of the computation cost.","publication_date":1700576964,"paper_link":"http://arxiv.org/pdf/2311.12636v1","categories":["Quantitative Finance"],"abstract":"As a physical fact, randomness is an inherent and ineliminable aspect in all physical measurements and engineering production. As a consequence, material parameters, serving as input data, are only known in a stochastic sense and thus, also output parameters, e.g., stresses, fluctuate. For the estimation of those fluctuations it is imperative to incoporate randomness into engineering simulations. Unfortunately, incorporating uncertain parameters into the modeling and simulation of inelastic materials is often computationally expensive, as many individual simulations may have to be performed. The promise of the proposed method is simple: using extended material models to include stochasticity reduces the number of needed simulations to one. This single computation is cheap, i.e., it has a comparable numerical effort as a single standard simulation. The extended material models are easily derived from standard deterministic material models and account for the effect of uncertainty by an extended set of deterministic material parameters. The time-dependent and stochastic material behavior are separated, such that only the deterministic time-dependent behavior of the extended material model needs to be simulated. The effect of stochasticity is then included during post-processing. The feasibility of this approach is demonstrated for three different and highly non-linear material models: viscous damage, viscous phase transformations and elasto-viscoplasticity. A comparison to the Monte Carlo method showcases that the method is indeed able to provide reliable estimates of the expectation and variance of internal variables and stress at a minimal fraction of the computation cost."}
{"title":"Fast calculation of Counterparty Credit exposures and associated sensitivities using fourier series expansion","authors":["Gijs Mast","Xiaoyu Shen","Fang Fang"],"raw_abstract":"This paper introduces a novel approach for computing netting--set level and\ncounterparty level exposures, such as Potential Future Exposure (PFE) and\nExpected Exposure (EE), along with associated sensitivities. The method is\nessentially an extension of the Fourier-cosine series expansion (COS) method,\noriginally proposed for option pricing. This method can accommodate a broad\nrange of models where the joint distribution of involved risk factors is\nanalytically or semi-analytically tractable. This inclusivity encompasses\nnearly all CCR models commonly employed in practice. A notable advantage of the\nCOS method is its sustained efficiency, particularly when handling large\nportfolios. A theoretical error analysis is also provided to justify the\nmethod's theoretical stability and accuracy. Various numerical tests are\nconducted using real-sized portfolios, and the results underscore its potential\nas a significantly more efficient alternative to the Monte Carlo method for\npractical usage, particularly applicable to portfolios involving a relatively\nmodest number of risk factors. Furthermore, the observed error convergence\nrates align closely with the theoretical error analysis.","publication_date":1700570454,"paper_link":"http://arxiv.org/pdf/2311.12575v1","categories":["Quantitative Finance"],"abstract":"This paper introduces a novel approach for computing netting--set level and counterparty level exposures, such as Potential Future Exposure (PFE) and Expected Exposure (EE), along with associated sensitivities. The method is essentially an extension of the Fourier-cosine series expansion (COS) method, originally proposed for option pricing. This method can accommodate a broad range of models where the joint distribution of involved risk factors is analytically or semi-analytically tractable. This inclusivity encompasses nearly all CCR models commonly employed in practice. A notable advantage of the COS method is its sustained efficiency, particularly when handling large portfolios. A theoretical error analysis is also provided to justify the method's theoretical stability and accuracy. Various numerical tests are conducted using real-sized portfolios, and the results underscore its potential as a significantly more efficient alternative to the Monte Carlo method for practical usage, particularly applicable to portfolios involving a relatively modest number of risk factors. Furthermore, the observed error convergence rates align closely with the theoretical error analysis."}
{"title":"Heuristics for Detecting CoinJoin Transactions on the Bitcoin Blockchain","authors":["Hugo Schnoering","Michalis Vazirgiannis"],"raw_abstract":"This research delves into the intricacies of Bitcoin, a decentralized\npeer-to-peer network, and its associated blockchain, which records all\ntransactions since its inception. While this ensures integrity and\ntransparency, the transparent nature of Bitcoin potentially compromises users'\nprivacy rights. To address this concern, users have adopted CoinJoin, a method\nthat amalgamates multiple transaction intents into a single, larger transaction\nto bolster transactional privacy. This process complicates individual\ntransaction tracing and disrupts many established blockchain analysis\nheuristics. Despite its significance, limited research has been conducted on\nidentifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin\nimplementations such as JoinMarket, Wasabi, and Whirlpool, each presenting\ndistinct challenges due to their unique transaction structures. This study\ndelves deeply into the open-source implementations of these protocols, aiming\nto develop refined heuristics for identifying their transactions on the\nblockchain. Our exhaustive analysis covers transactions up to block 760,000,\noffering a comprehensive insight into CoinJoin transactions and their\nimplications for Bitcoin blockchain analysis.","publication_date":1700561132,"paper_link":"http://arxiv.org/pdf/2311.12491v1","categories":["Quantitative Finance"],"abstract":"This research delves into the intricacies of Bitcoin, a decentralized peer-to-peer network, and its associated blockchain, which records all transactions since its inception. While this ensures integrity and transparency, the transparent nature of Bitcoin potentially compromises users' privacy rights. To address this concern, users have adopted CoinJoin, a method that amalgamates multiple transaction intents into a single, larger transaction to bolster transactional privacy. This process complicates individual transaction tracing and disrupts many established blockchain analysis heuristics. Despite its significance, limited research has been conducted on identifying CoinJoin transactions. Particularly noteworthy are varied CoinJoin implementations such as JoinMarket, Wasabi, and Whirlpool, each presenting distinct challenges due to their unique transaction structures. This study delves deeply into the open-source implementations of these protocols, aiming to develop refined heuristics for identifying their transactions on the blockchain. Our exhaustive analysis covers transactions up to block 760,000, offering a comprehensive insight into CoinJoin transactions and their implications for Bitcoin blockchain analysis."}
{"title":"A General Framework for Importance Sampling with Latent Markov Processes","authors":["Cheng-Der Fuh","Yanwei Jia","Steven Kou"],"raw_abstract":"Although stochastic models driven by latent Markov processes are widely used,\nthe classical importance sampling method based on the exponential tilting\nmethod for these models suffers from the difficulty of computing the eigenvalue\nand associated eigenfunction and the plausibility of the indirect asymptotic\nlarge deviation regime for the variance of the estimator. We propose a general\nimportance sampling framework that twists the observable and latent processes\nseparately based on a link function that directly minimizes the estimator's\nvariance. An optimal choice of the link function is chosen within the locally\nasymptotically normal family. We show the logarithmic efficiency of the\nproposed estimator under the asymptotic normal regime. As applications, we\nestimate an overflow probability under a pandemic model and the CoVaR, a\nmeasurement of the co-dependent financial systemic risk. Both applications are\nbeyond the scope of traditional importance sampling methods due to their\nnonlinear structures.","publication_date":1700538358,"paper_link":"http://arxiv.org/pdf/2311.12330v1","categories":["Quantitative Finance","Statistics"],"abstract":"Although stochastic models driven by latent Markov processes are widely used, the classical importance sampling method based on the exponential tilting method for these models suffers from the difficulty of computing the eigenvalue and associated eigenfunction and the plausibility of the indirect asymptotic large deviation regime for the variance of the estimator. We propose a general importance sampling framework that twists the observable and latent processes separately based on a link function that directly minimizes the estimator's variance. An optimal choice of the link function is chosen within the locally asymptotically normal family. We show the logarithmic efficiency of the proposed estimator under the asymptotic normal regime. As applications, we estimate an overflow probability under a pandemic model and the CoVaR, a measurement of the co-dependent financial systemic risk. Both applications are beyond the scope of traditional importance sampling methods due to their nonlinear structures."}
{"title":"Quantum-inspired nonlinear Galerkin ansatz for high-dimensional HJB equations","authors":["Chuhao Sun","Asaf Cohen","James Stokes","Shravan Veerapaneni"],"raw_abstract":"Neural networks are increasingly recognized as a powerful numerical solution\ntechnique for partial differential equations (PDEs) arising in diverse\nscientific computing domains, including quantum many-body physics. In the\ncontext of time-dependent PDEs, the dominant paradigm involves casting the\napproximate solution in terms of stochastic minimization of an objective\nfunction given by the norm of the PDE residual, viewed as a function of the\nneural network parameters. Recently, advancements have been made in the\ndirection of an alternative approach which shares aspects of nonlinearly\nparametrized Galerkin methods and variational quantum Monte Carlo, especially\nfor high-dimensional, time-dependent PDEs that extend beyond the usual scope of\nquantum physics. This paper is inspired by the potential of solving\nHamilton-Jacobi-Bellman (HJB) PDEs using Neural Galerkin methods and commences\nthe exploration of nonlinearly parametrized trial functions for which the\nevolution equations are analytically tractable. As a precursor to the Neural\nGalerkin scheme, we present trial functions with evolution equations that admit\nclosed-form solutions, focusing on time-dependent HJB equations relevant to\nfinance.","publication_date":1700523101,"paper_link":"http://arxiv.org/pdf/2311.12239v1","categories":["Mathematics","Quantitative Finance"],"abstract":"Neural networks are increasingly recognized as a powerful numerical solution technique for partial differential equations (PDEs) arising in diverse scientific computing domains, including quantum many-body physics. In the context of time-dependent PDEs, the dominant paradigm involves casting the approximate solution in terms of stochastic minimization of an objective function given by the norm of the PDE residual, viewed as a function of the neural network parameters. Recently, advancements have been made in the direction of an alternative approach which shares aspects of nonlinearly parametrized Galerkin methods and variational quantum Monte Carlo, especially for high-dimensional, time-dependent PDEs that extend beyond the usual scope of quantum physics. This paper is inspired by the potential of solving Hamilton-Jacobi-Bellman (HJB) PDEs using Neural Galerkin methods and commences the exploration of nonlinearly parametrized trial functions for which the evolution equations are analytically tractable. As a precursor to the Neural Galerkin scheme, we present trial functions with evolution equations that admit closed-form solutions, focusing on time-dependent HJB equations relevant to finance."}
{"title":"High-performance Effective Scientific Error-bounded Lossy Compression with Auto-tuned Multi-component Interpolation","authors":["Jinyang Liu","Sheng Di","Kai Zhao","Xin Liang","Sian Jin","Zizhe Jian","Jiajun Huang","Shixun Wu","Zizhong Chen","Franck Cappello"],"raw_abstract":"Error-bounded lossy compression has been identified as a promising solution\nfor significantly reducing scientific data volumes upon users' requirements on\ndata distortion. For the existing scientific error-bounded lossy compressors,\nsome of them (such as SPERR and FAZ) can reach fairly high compression ratios\nand some others (such as SZx, SZ, and ZFP) feature high compression speeds, but\nthey rarely exhibit both high ratio and high speed meanwhile. In this paper, we\npropose HPEZ with newly-designed interpolations and quality-metric-driven\nauto-tuning, which features significantly improved compression quality upon the\nexisting high-performance compressors, meanwhile being exceedingly faster than\nhigh-ratio compressors. The key contributions lie in the following points: (1)\nWe develop a series of advanced techniques such as interpolation re-ordering,\nmulti-dimensional interpolation, and natural cubic splines to significantly\nimprove compression qualities with interpolation-based data prediction. (2) The\nauto-tuning module in HPEZ has been carefully designed with novel strategies,\nincluding but not limited to block-wise interpolation tuning, dynamic dimension\nfreezing, and Lorenzo tuning. (3) We thoroughly evaluate HPEZ compared with\nmany other compressors on six real-world scientific datasets. Experiments show\nthat HPEZ outperforms other high-performance error-bounded lossy compressors in\ncompression ratio by up to 140% under the same error bound, and by up to 360%\nunder the same PSNR. In parallel data transfer experiments on the distributed\ndatabase, HPEZ achieves a significant performance gain with up to 40% time cost\nreduction over the second-best compressor.","publication_date":1700508771,"paper_link":"http://arxiv.org/pdf/2311.12133v1","categories":["Quantitative Finance"],"abstract":"Error-bounded lossy compression has been identified as a promising solution for significantly reducing scientific data volumes upon users' requirements on data distortion. For the existing scientific error-bounded lossy compressors, some of them (such as SPERR and FAZ) can reach fairly high compression ratios and some others (such as SZx, SZ, and ZFP) feature high compression speeds, but they rarely exhibit both high ratio and high speed meanwhile. In this paper, we propose HPEZ with newly-designed interpolations and quality-metric-driven auto-tuning, which features significantly improved compression quality upon the existing high-performance compressors, meanwhile being exceedingly faster than high-ratio compressors. The key contributions lie in the following points: (1) We develop a series of advanced techniques such as interpolation re-ordering, multi-dimensional interpolation, and natural cubic splines to significantly improve compression qualities with interpolation-based data prediction. (2) The auto-tuning module in HPEZ has been carefully designed with novel strategies, including but not limited to block-wise interpolation tuning, dynamic dimension freezing, and Lorenzo tuning. (3) We thoroughly evaluate HPEZ compared with many other compressors on six real-world scientific datasets. Experiments show that HPEZ outperforms other high-performance error-bounded lossy compressors in compression ratio by up to 140% under the same error bound, and by up to 360% under the same PSNR. In parallel data transfer experiments on the distributed database, HPEZ achieves a significant performance gain with up to 40% time cost reduction over the second-best compressor."}
{"title":"Rate-Independent Gradient Crystal Plasticity Theory -- Robust Algorithmic Formulations based on Incremental Energy Minimization","authors":["Volker Fohrmeister","J\u00f6rn Mosler"],"raw_abstract":"Numerically robust algorithmic formulations suitable for rate-independent\ncrystal plasticity are presented. They cover classic local models as well as\ngradient-enhanced theories in which the gradients of the plastic slips are\nincorporated by means of the micromorphic approach. The elaborated algorithmic\nformulations rely on the underlying variational structure of (associative)\ncrystal plasticity. To be more precise and in line with so-called variational\nconstitutive updates or incremental energy minimization principles, an\nincrementally defined energy derived from the underlying time-continuous\nconstitutive model represents the starting point of the novel numerically\nrobust algorithmic formulations. This incrementally defined potential allows to\ncompute all variables jointly as minimizers of this energy. While such discrete\nvariational constitutive updates are not new in general, they are considered\nhere in order to employ powerful techniques from non-linear constrained\noptimization theory in order to compute robustly the aforementioned minimizers.\nThe analyzed prototype models are based on (1) nonlinear complementarity\nproblem (NCP) functions as well as on (2) the augmented Lagrangian formulation.\nNumerical experiments show the numerical robustness of the resulting\nalgorithmic formulations. Furthermore, it is shown that the novel algorithmic\nideas can also be integrated into classic, non-variational, return-mapping\nschemes.","publication_date":1700506689,"paper_link":"http://arxiv.org/pdf/2311.12026v1","categories":["Quantitative Finance"],"abstract":"Numerically robust algorithmic formulations suitable for rate-independent crystal plasticity are presented. They cover classic local models as well as gradient-enhanced theories in which the gradients of the plastic slips are incorporated by means of the micromorphic approach. The elaborated algorithmic formulations rely on the underlying variational structure of (associative) crystal plasticity. To be more precise and in line with so-called variational constitutive updates or incremental energy minimization principles, an incrementally defined energy derived from the underlying time-continuous constitutive model represents the starting point of the novel numerically robust algorithmic formulations. This incrementally defined potential allows to compute all variables jointly as minimizers of this energy. While such discrete variational constitutive updates are not new in general, they are considered here in order to employ powerful techniques from non-linear constrained optimization theory in order to compute robustly the aforementioned minimizers. The analyzed prototype models are based on (1) nonlinear complementarity problem (NCP) functions as well as on (2) the augmented Lagrangian formulation. Numerical experiments show the numerical robustness of the resulting algorithmic formulations. Furthermore, it is shown that the novel algorithmic ideas can also be integrated into classic, non-variational, return-mapping schemes."}
{"title":"Weak-Form Latent Space Dynamics Identification","authors":["April Tran","Xiaolong He","Daniel A. Messenger","Youngsoo Choi","David M. Bortz"],"raw_abstract":"Recent work in data-driven modeling has demonstrated that a weak formulation\nof model equations enhances the noise robustness of a wide range of\ncomputational methods. In this paper, we demonstrate the power of the weak form\nto enhance the LaSDI (Latent Space Dynamics Identification) algorithm, a\nrecently developed data-driven reduced order modeling technique.\n  We introduce a weak form-based version WLaSDI (Weak-form Latent Space\nDynamics Identification). WLaSDI first compresses data, then projects onto the\ntest functions and learns the local latent space models. Notably, WLaSDI\ndemonstrates significantly enhanced robustness to noise. With WLaSDI, the local\nlatent space is obtained using weak-form equation learning techniques. Compared\nto the standard sparse identification of nonlinear dynamics (SINDy) used in\nLaSDI, the variance reduction of the weak form guarantees a robust and precise\nlatent space recovery, hence allowing for a fast, robust, and accurate\nsimulation. We demonstrate the efficacy of WLaSDI vs. LaSDI on several common\nbenchmark examples including viscid and inviscid Burgers', radial advection,\nand heat conduction. For instance, in the case of 1D inviscid Burgers'\nsimulations with the addition of up to 100% Gaussian white noise, the relative\nerror remains consistently below 6% for WLaSDI, while it can exceed 10,000% for\nLaSDI. Similarly, for radial advection simulations, the relative errors stay\nbelow 15% for WLaSDI, in stark contrast to the potential errors of up to\n10,000% with LaSDI. Moreover, speedups of several orders of magnitude can be\nobtained with WLaSDI. For example applying WLaSDI to 1D Burgers' yields a 140X\nspeedup compared to the corresponding full order model.\n  Python code to reproduce the results in this work is available at\n(https://github.com/MathBioCU/PyWSINDy_ODE) and\n(https://github.com/MathBioCU/PyWLaSDI).","publication_date":1700505734,"paper_link":"http://arxiv.org/pdf/2311.12880v1","categories":["Mathematics","Electrical Engineering and Systems Science"],"abstract":"Recent work in data-driven modeling has demonstrated that a weak formulation of model equations enhances the noise robustness of a wide range of computational methods. In this paper, we demonstrate the power of the weak form to enhance the LaSDI (Latent Space Dynamics Identification) algorithm, a recently developed data-driven reduced order modeling technique.   We introduce a weak form-based version WLaSDI (Weak-form Latent Space Dynamics Identification). WLaSDI first compresses data, then projects onto the test functions and learns the local latent space models. Notably, WLaSDI demonstrates significantly enhanced robustness to noise. With WLaSDI, the local latent space is obtained using weak-form equation learning techniques. Compared to the standard sparse identification of nonlinear dynamics (SINDy) used in LaSDI, the variance reduction of the weak form guarantees a robust and precise latent space recovery, hence allowing for a fast, robust, and accurate simulation. We demonstrate the efficacy of WLaSDI vs. LaSDI on several common benchmark examples including viscid and inviscid Burgers', radial advection, and heat conduction. For instance, in the case of 1D inviscid Burgers' simulations with the addition of up to 100% Gaussian white noise, the relative error remains consistently below 6% for WLaSDI, while it can exceed 10,000% for LaSDI. Similarly, for radial advection simulations, the relative errors stay below 15% for WLaSDI, in stark contrast to the potential errors of up to 10,000% with LaSDI. Moreover, speedups of several orders of magnitude can be obtained with WLaSDI. For example applying WLaSDI to 1D Burgers' yields a 140X speedup compared to the corresponding full order model.   Python code to reproduce the results in this work is available at (https://github.com/MathBioCU/PyWSINDy_ODE) and (https://github.com/MathBioCU/PyWLaSDI)."}
{"title":"Correlated Attention in Transformers for Multivariate Time Series","authors":["Quang Minh Nguyen","Lam M. Nguyen","Subhro Das"],"raw_abstract":"Multivariate time series (MTS) analysis prevails in real-world applications\nsuch as finance, climate science and healthcare. The various self-attention\nmechanisms, the backbone of the state-of-the-art Transformer-based models,\nefficiently discover the temporal dependencies, yet cannot well capture the\nintricate cross-correlation between different features of MTS data, which\ninherently stems from complex dynamical systems in practice. To this end, we\npropose a novel correlated attention mechanism, which not only efficiently\ncaptures feature-wise dependencies, but can also be seamlessly integrated\nwithin the encoder blocks of existing well-known Transformers to gain\nefficiency improvement. In particular, correlated attention operates across\nfeature channels to compute cross-covariance matrices between queries and keys\nwith different lag values, and selectively aggregate representations at the\nsub-series level. This architecture facilitates automated discovery and\nrepresentation learning of not only instantaneous but also lagged\ncross-correlations, while inherently capturing time series auto-correlation.\nWhen combined with prevalent Transformer baselines, correlated attention\nmechanism constitutes a better alternative for encoder-only architectures,\nwhich are suitable for a wide range of tasks including imputation, anomaly\ndetection and classification. Extensive experiments on the aforementioned tasks\nconsistently underscore the advantages of correlated attention mechanism in\nenhancing base Transformer models, and demonstrate our state-of-the-art results\nin imputation, anomaly detection and classification.","publication_date":1700501744,"paper_link":"http://arxiv.org/pdf/2311.11959v1","categories":["Quantitative Finance"],"abstract":"Multivariate time series (MTS) analysis prevails in real-world applications such as finance, climate science and healthcare. The various self-attention mechanisms, the backbone of the state-of-the-art Transformer-based models, efficiently discover the temporal dependencies, yet cannot well capture the intricate cross-correlation between different features of MTS data, which inherently stems from complex dynamical systems in practice. To this end, we propose a novel correlated attention mechanism, which not only efficiently captures feature-wise dependencies, but can also be seamlessly integrated within the encoder blocks of existing well-known Transformers to gain efficiency improvement. In particular, correlated attention operates across feature channels to compute cross-covariance matrices between queries and keys with different lag values, and selectively aggregate representations at the sub-series level. This architecture facilitates automated discovery and representation learning of not only instantaneous but also lagged cross-correlations, while inherently capturing time series auto-correlation. When combined with prevalent Transformer baselines, correlated attention mechanism constitutes a better alternative for encoder-only architectures, which are suitable for a wide range of tasks including imputation, anomaly detection and classification. Extensive experiments on the aforementioned tasks consistently underscore the advantages of correlated attention mechanism in enhancing base Transformer models, and demonstrate our state-of-the-art results in imputation, anomaly detection and classification."}
{"title":"FinanceBench: A New Benchmark for Financial Question Answering","authors":["Pranab Islam","Anand Kannappan","Douwe Kiela","Rebecca Qian","Nino Scherrer","Bertie Vidgen"],"raw_abstract":"FinanceBench is a first-of-its-kind test suite for evaluating the performance\nof LLMs on open book financial question answering (QA). It comprises 10,231\nquestions about publicly traded companies, with corresponding answers and\nevidence strings. The questions in FinanceBench are ecologically valid and\ncover a diverse set of scenarios. They are intended to be clear-cut and\nstraightforward to answer to serve as a minimum performance standard. We test\n16 state of the art model configurations (including GPT-4-Turbo, Llama2 and\nClaude2, with vector stores and long context prompts) on a sample of 150 cases\nfrom FinanceBench, and manually review their answers (n=2,400). The cases are\navailable open-source. We show that existing LLMs have clear limitations for\nfinancial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly\nanswered or refused to answer 81% of questions. While augmentation techniques\nsuch as using longer context window to feed in relevant evidence improve\nperformance, they are unrealistic for enterprise settings due to increased\nlatency and cannot support larger financial documents. We find that all models\nexamined exhibit weaknesses, such as hallucinations, that limit their\nsuitability for use by enterprises.","publication_date":1700501282,"paper_link":"http://arxiv.org/pdf/2311.11944v1","categories":["Statistics"],"abstract":"FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises."}
{"title":"Deep Calibration of Market Simulations using Neural Density Estimators and Embedding Networks","authors":["Namid R. Stillman","Rory Baggott","Justin Lyon","Jianfei Zhang","Dingqiu Zhu","Tao Chen","Perukrishnen Vytelingum"],"raw_abstract":"The ability to construct a realistic simulator of financial exchanges,\nincluding reproducing the dynamics of the limit order book, can give insight\ninto many counterfactual scenarios, such as a flash crash, a margin call, or\nchanges in macroeconomic outlook. In recent years, agent-based models have been\ndeveloped that reproduce many features of an exchange, as summarised by a set\nof stylised facts and statistics. However, the ability to calibrate simulators\nto a specific period of trading remains an open challenge. In this work, we\ndevelop a novel approach to the calibration of market simulators by leveraging\nrecent advances in deep learning, specifically using neural density estimators\nand embedding networks. We demonstrate that our approach is able to correctly\nidentify high probability parameter sets, both when applied to synthetic and\nhistorical data, and without reliance on manually selected or weighted\nensembles of stylised facts.","publication_date":1700498658,"paper_link":"http://arxiv.org/pdf/2311.11913v1","categories":["Quantitative Finance","Statistics"],"abstract":"The ability to construct a realistic simulator of financial exchanges, including reproducing the dynamics of the limit order book, can give insight into many counterfactual scenarios, such as a flash crash, a margin call, or changes in macroeconomic outlook. In recent years, agent-based models have been developed that reproduce many features of an exchange, as summarised by a set of stylised facts and statistics. However, the ability to calibrate simulators to a specific period of trading remains an open challenge. In this work, we develop a novel approach to the calibration of market simulators by leveraging recent advances in deep learning, specifically using neural density estimators and embedding networks. We demonstrate that our approach is able to correctly identify high probability parameter sets, both when applied to synthetic and historical data, and without reliance on manually selected or weighted ensembles of stylised facts."}
{"title":"Towards Robust Text Retrieval with Progressive Learning","authors":["Tong Wu","Yulei Qin","Enwei Zhang","Zihan Xu","Yuting Gao","Ke Li","Xing Sun"],"raw_abstract":"Retrieval augmentation has become an effective solution to empower large\nlanguage models (LLMs) with external and verified knowledge sources from the\ndatabase, which overcomes the limitations and hallucinations of LLMs in\nhandling up-to-date and domain-specific information. However, existing\nembedding models for text retrieval usually have three non-negligible\nlimitations. First, the number and diversity of samples in a batch are too\nrestricted to supervise the modeling of textual nuances at scale. Second, the\nhigh proportional noise are detrimental to the semantic correctness and\nconsistency of embeddings. Third, the equal treatment to easy and difficult\nsamples would cause sub-optimum convergence of embeddings with poorer\ngeneralization. In this paper, we propose the PEG, a progressively learned\nembeddings for robust text retrieval. Specifically, we increase the training\nin-batch negative samples to 80,000, and for each query, we extracted five hard\nnegatives. Concurrently, we incorporated a progressive learning mechanism,\nenabling the model to dynamically modulate its attention to the samples\nthroughout the entire training process. Additionally, PEG is trained on more\nthan 100 million data, encompassing a wide range of domains (e.g., finance,\nmedicine, and tourism) and covering various tasks (e.g., question-answering,\nmachine reading comprehension, and similarity matching). Extensive experiments\nconducted on C-MTEB and DuReader demonstrate that PEG surpasses\nstate-of-the-art embeddings in retrieving true positives, highlighting its\nsignificant potential for applications in LLMs. Our model is publicly available\nat https://huggingface.co/TownsWu/PEG.","publication_date":1700480641,"paper_link":"http://arxiv.org/pdf/2311.11691v1","categories":["Quantitative Finance"],"abstract":"Retrieval augmentation has become an effective solution to empower large language models (LLMs) with external and verified knowledge sources from the database, which overcomes the limitations and hallucinations of LLMs in handling up-to-date and domain-specific information. However, existing embedding models for text retrieval usually have three non-negligible limitations. First, the number and diversity of samples in a batch are too restricted to supervise the modeling of textual nuances at scale. Second, the high proportional noise are detrimental to the semantic correctness and consistency of embeddings. Third, the equal treatment to easy and difficult samples would cause sub-optimum convergence of embeddings with poorer generalization. In this paper, we propose the PEG, a progressively learned embeddings for robust text retrieval. Specifically, we increase the training in-batch negative samples to 80,000, and for each query, we extracted five hard negatives. Concurrently, we incorporated a progressive learning mechanism, enabling the model to dynamically modulate its attention to the samples throughout the entire training process. Additionally, PEG is trained on more than 100 million data, encompassing a wide range of domains (e.g., finance, medicine, and tourism) and covering various tasks (e.g., question-answering, machine reading comprehension, and similarity matching). Extensive experiments conducted on C-MTEB and DuReader demonstrate that PEG surpasses state-of-the-art embeddings in retrieving true positives, highlighting its significant potential for applications in LLMs. Our model is publicly available at https://huggingface.co/TownsWu/PEG."}
{"title":"Web News Timeline Generation with Extended Task Prompting","authors":["Sha Wang","Yuchen Li","Hanhua Xiao","Lambert Deng","Yanfei Dong"],"raw_abstract":"The creation of news timeline is essential for a comprehensive and contextual\nunderstanding of events as they unfold over time. This approach aids in\ndiscerning patterns and trends that might be obscured when news is viewed in\nisolation. By organizing news in a chronological sequence, it becomes easier to\ntrack the development of stories, understand the interrelation of events, and\ngrasp the broader implications of news items. This is particularly helpful in\nsectors like finance and insurance, where timely understanding of the event\ndevelopment-ranging from extreme weather to political upheavals and health\ncrises-is indispensable for effective risk management. While traditional\nnatural language processing (NLP) techniques have had some success, they often\nfail to capture the news with nuanced relevance that are readily apparent to\ndomain experts, hindering broader industry integration. The advance of Large\nLanguage Models (LLMs) offers a renewed opportunity to tackle this challenge.\nHowever, direct prompting LLMs for this task is often ineffective. Our study\ninvestigates the application of an extended task prompting technique to assess\npast news relevance. We demonstrate that enhancing conventional prompts with\nadditional tasks boosts their effectiveness on various news dataset, rendering\nnews timeline generation practical for professional use. This work has been\ndeployed as a publicly accessible browser extension which is adopted within our\nnetwork.","publication_date":1700476702,"paper_link":"http://arxiv.org/pdf/2311.11652v1","categories":["Quantitative Finance"],"abstract":"The creation of news timeline is essential for a comprehensive and contextual understanding of events as they unfold over time. This approach aids in discerning patterns and trends that might be obscured when news is viewed in isolation. By organizing news in a chronological sequence, it becomes easier to track the development of stories, understand the interrelation of events, and grasp the broader implications of news items. This is particularly helpful in sectors like finance and insurance, where timely understanding of the event development-ranging from extreme weather to political upheavals and health crises-is indispensable for effective risk management. While traditional natural language processing (NLP) techniques have had some success, they often fail to capture the news with nuanced relevance that are readily apparent to domain experts, hindering broader industry integration. The advance of Large Language Models (LLMs) offers a renewed opportunity to tackle this challenge. However, direct prompting LLMs for this task is often ineffective. Our study investigates the application of an extended task prompting technique to assess past news relevance. We demonstrate that enhancing conventional prompts with additional tasks boosts their effectiveness on various news dataset, rendering news timeline generation practical for professional use. This work has been deployed as a publicly accessible browser extension which is adopted within our network."}
{"title":"A novel transformer-based approach for soil temperature prediction","authors":["Muhammet Mucahit Enes Yurtsever","Ayhan Kucukmanisa","Zeynep Hilal Kilimci"],"raw_abstract":"Soil temperature is one of the most significant parameters that plays a\ncrucial role in glacier energy, dynamics of mass balance, processes of surface\nhydrological, coaction of glacier-atmosphere, nutrient cycling, ecological\nstability, the management of soil, water, and field crop. In this work, we\nintroduce a novel approach using transformer models for the purpose of\nforecasting soil temperature prediction. To the best of our knowledge, the\nusage of transformer models in this work is the very first attempt to predict\nsoil temperature. Experiments are carried out using six different FLUXNET\nstations by modeling them with five different transformer models, namely,\nVanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To\ndemonstrate the effectiveness of the proposed model, experiment results are\ncompared with both deep learning approaches and literature studies. Experiment\nresults show that the utilization of transformer models ensures a significant\ncontribution to the literature, thence determining the new state-of-the-art.","publication_date":1700472026,"paper_link":"http://arxiv.org/pdf/2311.11626v1","categories":["Physics"],"abstract":"Soil temperature is one of the most significant parameters that plays a crucial role in glacier energy, dynamics of mass balance, processes of surface hydrological, coaction of glacier-atmosphere, nutrient cycling, ecological stability, the management of soil, water, and field crop. In this work, we introduce a novel approach using transformer models for the purpose of forecasting soil temperature prediction. To the best of our knowledge, the usage of transformer models in this work is the very first attempt to predict soil temperature. Experiments are carried out using six different FLUXNET stations by modeling them with five different transformer models, namely, Vanilla Transformer, Informer, Autoformer, Reformer, and ETSformer. To demonstrate the effectiveness of the proposed model, experiment results are compared with both deep learning approaches and literature studies. Experiment results show that the utilization of transformer models ensures a significant contribution to the literature, thence determining the new state-of-the-art."}
{"title":"Multi-component Predictions of Transient Solution Fields with Sequential Deep Operator Network","authors":["Junyan He","Shashank Kushwaha","Jaewan Park","Seid Koric","Diab Abueidda","Iwona Jasiuk"],"raw_abstract":"The Deep Operator Network (DeepONet) structure has shown great potential in\napproximating complex solution operators with low generalization errors.\nRecently, a sequential DeepONet (S-DeepONet) was proposed to use sequential\nlearning models in the branch of DeepONet to predict final solutions given\ntime-dependent inputs. In this novel work, the S-DeepONet architecture is\nfurther extended by modifying the information combination mechanism between the\nbranch and trunk networks to simultaneously predict vector solutions with\nmultiple components at multiple time steps of the evolution history. Two\nexample problems, one on transient fluid flow and the other on path-dependent\nplastic loading were shown to demonstrate the capabilities of the model to\nhandle different physics problems. The use of a trained S-DeepONet model in\ninverse parameter identification via the genetic algorithm is shown to\ndemonstrate the application of the model. In almost all cases, the trained\nmodel achieved an $R^2$ value of above 0.99 and relative $L_2$ error of less\nthan 10\\% with only 3200 training data points, indicating superior accuracy.\nThe vector S-DeepONet model, having only 0.4\\% more parameters than a scalar\nmodel, can predict two output components simultaneously at an accuracy similar\nto the two independently trained scalar models with a 20.8\\% faster training\ntime. The S-DeepONet inference is at least three orders of magnitude faster\nthan direct numerical simulations, and inverse parameter identifications using\nthe trained model is highly efficient and accurate.","publication_date":1700449150,"paper_link":"http://arxiv.org/pdf/2311.11500v1","categories":["Quantitative Finance"],"abstract":"The Deep Operator Network (DeepONet) structure has shown great potential in approximating complex solution operators with low generalization errors. Recently, a sequential DeepONet (S-DeepONet) was proposed to use sequential learning models in the branch of DeepONet to predict final solutions given time-dependent inputs. In this novel work, the S-DeepONet architecture is further extended by modifying the information combination mechanism between the branch and trunk networks to simultaneously predict vector solutions with multiple components at multiple time steps of the evolution history. Two example problems, one on transient fluid flow and the other on path-dependent plastic loading were shown to demonstrate the capabilities of the model to handle different physics problems. The use of a trained S-DeepONet model in inverse parameter identification via the genetic algorithm is shown to demonstrate the application of the model. In almost all cases, the trained model achieved an __FORMULA__ value of above 0.99 and relative __FORMULA__ error of less than 10\\% with only 3200 training data points, indicating superior accuracy. The vector S-DeepONet model, having only 0.4\\% more parameters than a scalar model, can predict two output components simultaneously at an accuracy similar to the two independently trained scalar models with a 20.8\\% faster training time. The S-DeepONet inference is at least three orders of magnitude faster than direct numerical simulations, and inverse parameter identifications using the trained model is highly efficient and accurate."}
{"title":"Attention-based Multi-fidelity Machine Learning Model for Computational Fractional Flow Reserve Assessment","authors":["Haizhou Yang","C. Alberto Figueroa","Krishna Garikipati"],"raw_abstract":"Coronary Artery Disease (CAD) is one of the most common forms of heart\ndisease, which is caused by a buildup of atherosclerotic plaque (known as\nstenosis) in the coronary arteries, leading to insufficient supplement of\nblood, oxygen, and nutrients to the heart. Fractional Flow Reserve (FFR),\nmeasuring the pressure ratio between the aorta and distal coronary artery, is\nan invasive physiologic gold standard for assessing the severity of coronary\nartery stenosis. Despite its benefits, invasive FFR assessment is still\nunderutilized due to its high cost, time-consuming, experimental variability,\nand increased risk to patients. In this study, an attention-based\nmulti-fidelity machine learning model (AttMulFid) is proposed for\ncomputationally efficient and accurate FFR assessment with uncertainty\nmeasurement. Within AttMulFid, an autoencoder is utilized to intelligently\nselect geometric features from coronary arteries, with additional attention on\nthe key area. Results show that the geometric features are able to represent\nthe entirety of the geometric information and intelligently allocate attention\nbased on crucial properties of geometry. Furthermore, the AttMulFid is a\nfeasible approach for non-invasive, rapid, and accurate FFR assessment (with\n0.002s/simulation).","publication_date":1700419354,"paper_link":"http://arxiv.org/pdf/2311.11397v1","categories":["Quantitative Finance"],"abstract":"Coronary Artery Disease (CAD) is one of the most common forms of heart disease, which is caused by a buildup of atherosclerotic plaque (known as stenosis) in the coronary arteries, leading to insufficient supplement of blood, oxygen, and nutrients to the heart. Fractional Flow Reserve (FFR), measuring the pressure ratio between the aorta and distal coronary artery, is an invasive physiologic gold standard for assessing the severity of coronary artery stenosis. Despite its benefits, invasive FFR assessment is still underutilized due to its high cost, time-consuming, experimental variability, and increased risk to patients. In this study, an attention-based multi-fidelity machine learning model (AttMulFid) is proposed for computationally efficient and accurate FFR assessment with uncertainty measurement. Within AttMulFid, an autoencoder is utilized to intelligently select geometric features from coronary arteries, with additional attention on the key area. Results show that the geometric features are able to represent the entirety of the geometric information and intelligently allocate attention based on crucial properties of geometry. Furthermore, the AttMulFid is a feasible approach for non-invasive, rapid, and accurate FFR assessment (with 0.002s/simulation)."}
{"title":"On the Noise Scheduling for Generating Plausible Designs with Diffusion Models","authors":["Jiajie Fan","Laure Vuaille","Thomas B\u00e4ck","Hao Wang"],"raw_abstract":"Deep Generative Models (DGMs) are widely used to create innovative designs\nacross multiple industries, ranging from fashion to the automotive sector. In\naddition to generating images of high visual quality, the task of structural\ndesign generation imposes more stringent constrains on the semantic expression,\ne.g., no floating material or missing part, which we refer to as plausibility\nin this work. We delve into the impact of noise schedules of diffusion models\non the plausibility of the outcome: there exists a range of noise levels at\nwhich the model's performance decides the result plausibility. Also, we propose\ntwo techniques to determine such a range for a given image set and devise a\nnovel parametric noise schedule for better plausibility. We apply this noise\nschedule to the training and sampling of the well-known diffusion model EDM and\ncompare it to its default noise schedule. Compared to EDM, our schedule\nsignificantly improves the rate of plausible designs from 83.4% to 93.5% and\nFr\\'echet Inception Distance (FID) from 7.84 to 4.87. Further applications of\nadvanced image editing tools demonstrate the model's solid understanding of\nstructure.","publication_date":1700363874,"paper_link":"http://arxiv.org/pdf/2311.11207v1","categories":["Quantitative Finance"],"abstract":"Deep Generative Models (DGMs) are widely used to create innovative designs across multiple industries, ranging from fashion to the automotive sector. In addition to generating images of high visual quality, the task of structural design generation imposes more stringent constrains on the semantic expression, e.g., no floating material or missing part, which we refer to as plausibility in this work. We delve into the impact of noise schedules of diffusion models on the plausibility of the outcome: there exists a range of noise levels at which the model's performance decides the result plausibility. Also, we propose two techniques to determine such a range for a given image set and devise a novel parametric noise schedule for better plausibility. We apply this noise schedule to the training and sampling of the well-known diffusion model EDM and compare it to its default noise schedule. Compared to EDM, our schedule significantly improves the rate of plausible designs from 83.4% to 93.5% and Fr\\'echet Inception Distance (FID) from 7.84 to 4.87. Further applications of advanced image editing tools demonstrate the model's solid understanding of structure."}
{"title":"A Novel Perspective Process Simulation Framework Based on Automatic Differentiation","authors":["Shaoyi Yang"],"raw_abstract":"Thermodynamic and flash equilibrium calculations are the cornerstones of\nsimulation process calculations. The iterative approach, a widely used\nnonlinear problem-solving technique, relies on derivative calculations\nthroughout the procedure that directly affect the stability and effectiveness\nof the solution. In this study, we use state-of-the-art automatic\ndifferentiation frameworks for thermodynamic calculations to obtain precise\nderivatives without altering the logic of the algorithm. This contrasts with\ntraditional numerical differentiation algorithms and significantly improves the\nconvergence and computational efficiency of process simulations in contrast to\nnumerical differentiation algorithms. Standard chemical phase equilibrium\ncalculations such as PT, PV, and PH flash are used to evaluate an automated\ndifferentiation approach with respect to numerical stability and iteration\ncounts. It is used to evaluate the iteration count. The results of the\nexperiment showed that the automatic differentiation method has a more uniform\ngradient distribution and requires fewer convergence iterations. The\nexperimental results show that the system shows that the process is more\nuniform. The gradient distribution and computational convergence curves help to\nhighlight the improvements provided by automatic differentiation. In addition,\nthis method shows greater generalizability and can be used more easily in the\ncalculation of various other chemical simulation modules.","publication_date":1700329053,"paper_link":"http://arxiv.org/pdf/2311.11129v1","categories":["Mathematics"],"abstract":"Thermodynamic and flash equilibrium calculations are the cornerstones of simulation process calculations. The iterative approach, a widely used nonlinear problem-solving technique, relies on derivative calculations throughout the procedure that directly affect the stability and effectiveness of the solution. In this study, we use state-of-the-art automatic differentiation frameworks for thermodynamic calculations to obtain precise derivatives without altering the logic of the algorithm. This contrasts with traditional numerical differentiation algorithms and significantly improves the convergence and computational efficiency of process simulations in contrast to numerical differentiation algorithms. Standard chemical phase equilibrium calculations such as PT, PV, and PH flash are used to evaluate an automated differentiation approach with respect to numerical stability and iteration counts. It is used to evaluate the iteration count. The results of the experiment showed that the automatic differentiation method has a more uniform gradient distribution and requires fewer convergence iterations. The experimental results show that the system shows that the process is more uniform. The gradient distribution and computational convergence curves help to highlight the improvements provided by automatic differentiation. In addition, this method shows greater generalizability and can be used more easily in the calculation of various other chemical simulation modules."}
{"title":"High-Throughput Asset Pricing","authors":["Andrew Y. Chen","Chukwuma Dim"],"raw_abstract":"We use empirical Bayes (EB) to mine for out-of-sample returns among 73,108\nlong-short strategies constructed from accounting ratios, past returns, and\nticker symbols. EB predicts returns are concentrated in accounting and past\nreturn strategies, small stocks, and pre-2004 samples. The cross-section of\nout-of-sample return lines up closely with EB predictions. Data-mined\nportfolios have mean returns comparable with published portfolios, but the\ndata-mined returns are arguably free of data mining bias. In contrast,\ncontrolling for multiple testing following Harvey, Liu, and Zhu (2016) misses\nthe vast majority of returns. This \"high-throughput asset pricing\" provides an\nevidence-based solution for data mining bias.","publication_date":1700244825,"paper_link":"http://arxiv.org/pdf/2311.10685v1","categories":["Quantitative Finance","Economics","Statistics"],"abstract":"We use empirical Bayes (EB) to mine for out-of-sample returns among 73,108 long-short strategies constructed from accounting ratios, past returns, and ticker symbols. EB predicts returns are concentrated in accounting and past return strategies, small stocks, and pre-2004 samples. The cross-section of out-of-sample return lines up closely with EB predictions. Data-mined portfolios have mean returns comparable with published portfolios, but the data-mined returns are arguably free of data mining bias. In contrast, controlling for multiple testing following Harvey, Liu, and Zhu (2016) misses the vast majority of returns. This \"high-throughput asset pricing\" provides an evidence-based solution for data mining bias."}
{"title":"Using Cooperative Game Theory to Prune Neural Networks","authors":["Mauricio Diaz-Ortiz Jr","Benjamin Kempinski","Daphne Cornelisse","Yoram Bachrach","Tal Kachman"],"raw_abstract":"We show how solution concepts from cooperative game theory can be used to\ntackle the problem of pruning neural networks.\n  The ever-growing size of deep neural networks (DNNs) increases their\nperformance, but also their computational requirements. We introduce a method\ncalled Game Theory Assisted Pruning (GTAP), which reduces the neural network's\nsize while preserving its predictive accuracy. GTAP is based on eliminating\nneurons in the network based on an estimation of their joint impact on the\nprediction quality through game theoretic solutions. Specifically, we use a\npower index akin to the Shapley value or Banzhaf index, tailored using a\nprocedure similar to Dropout (commonly used to tackle overfitting problems in\nmachine learning).\n  Empirical evaluation of both feedforward networks and convolutional neural\nnetworks shows that this method outperforms existing approaches in the achieved\ntradeoff between the number of parameters and model accuracy.","publication_date":1700221690,"paper_link":"http://arxiv.org/pdf/2311.10468v1","categories":["Quantitative Finance"],"abstract":"We show how solution concepts from cooperative game theory can be used to tackle the problem of pruning neural networks.   The ever-growing size of deep neural networks (DNNs) increases their performance, but also their computational requirements. We introduce a method called Game Theory Assisted Pruning (GTAP), which reduces the neural network's size while preserving its predictive accuracy. GTAP is based on eliminating neurons in the network based on an estimation of their joint impact on the prediction quality through game theoretic solutions. Specifically, we use a power index akin to the Shapley value or Banzhaf index, tailored using a procedure similar to Dropout (commonly used to tackle overfitting problems in machine learning).   Empirical evaluation of both feedforward networks and convolutional neural networks shows that this method outperforms existing approaches in the achieved tradeoff between the number of parameters and model accuracy."}
{"title":"Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools","authors":["Wentao Zhang","Yilei Zhao","Shuo Sun","Jie Ying","Yonggang Xie","Zitao Song","Xinrun Wang","Bo An"],"raw_abstract":"Portfolio management (PM) is a fundamental financial trading task, which\nexplores the optimal periodical reallocation of capitals into different stocks\nto pursue long-term profits. Reinforcement learning (RL) has recently shown its\npotential to train profitable agents for PM through interacting with financial\nmarkets. However, existing work mostly focuses on fixed stock pools, which is\ninconsistent with investors' practical demand. Specifically, the target stock\npool of different investors varies dramatically due to their discrepancy on\nmarket states and individual investors may temporally adjust stocks they desire\nto trade (e.g., adding one popular stocks), which lead to customizable stock\npools (CSPs). Existing RL methods require to retrain RL agents even with a tiny\nchange of the stock pool, which leads to high computational cost and unstable\nperformance. To tackle this challenge, we propose EarnMore, a rEinforcement\nleARNing framework with Maskable stOck REpresentation to handle PM with CSPs\nthrough one-shot training in a global stock pool (GSP). Specifically, we first\nintroduce a mechanism to mask out the representation of the stocks outside the\ntarget pool. Second, we learn meaningful stock representations through a\nself-supervised masking and reconstruction process. Third, a re-weighting\nmechanism is designed to make the portfolio concentrate on favorable stocks and\nneglect the stocks outside the target pool. Through extensive experiments on 8\nsubset stock pools of the US stock market, we demonstrate that EarnMore\nsignificantly outperforms 14 state-of-the-art baselines in terms of 6 popular\nfinancial metrics with over 40% improvement on profit.","publication_date":1700212619,"paper_link":"http://arxiv.org/pdf/2311.10801v2","categories":["Quantitative Finance"],"abstract":"Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNing framework with Maskable stOck REpresentation to handle PM with CSPs through one-shot training in a global stock pool (GSP). Specifically, we first introduce a mechanism to mask out the representation of the stocks outside the target pool. Second, we learn meaningful stock representations through a self-supervised masking and reconstruction process. Third, a re-weighting mechanism is designed to make the portfolio concentrate on favorable stocks and neglect the stocks outside the target pool. Through extensive experiments on 8 subset stock pools of the US stock market, we demonstrate that EarnMore significantly outperforms 14 state-of-the-art baselines in terms of 6 popular financial metrics with over 40% improvement on profit."}
{"title":"Bi-Level Optimization to Enhance Intensity Modulated Radiation Therapy Planning","authors":["Juan Jos\u00e9 Moreno","Sav\u00edns Puertas-Mart\u00edn","Juana L. Redondo","Pilar M. Ortigosa","Anna Zawadzka","Pawel Kukolowicz","Robert Szmur\u0142o","Ignacy Kaliszewski","Janusz Miroforidis","Ester M. Garz\u00f3n"],"raw_abstract":"Intensity Modulated Radiation Therapy is an effective cancer treatment.\nModels based on the Generalized Equivalent Uniform Dose (gEUD) provide\nradiation plans with excellent planning target volume coverage and low\nradiation for organs at risk. However, manual adjustment of the parameters\ninvolved in gEUD is required to ensure that the plans meet patient-specific\nphysical restrictions. This paper proposes a radiotherapy planning methodology\nbased on bi-level optimization. We evaluated the proposed scheme in a real\npatient and compared the resulting irradiation plans with those prepared by\nclinical planners in hospital devices. The results in terms of efficiency and\neffectiveness are promising.","publication_date":1700184866,"paper_link":"http://arxiv.org/pdf/2311.10272v1","categories":["Mathematics"],"abstract":"Intensity Modulated Radiation Therapy is an effective cancer treatment. Models based on the Generalized Equivalent Uniform Dose (gEUD) provide radiation plans with excellent planning target volume coverage and low radiation for organs at risk. However, manual adjustment of the parameters involved in gEUD is required to ensure that the plans meet patient-specific physical restrictions. This paper proposes a radiotherapy planning methodology based on bi-level optimization. We evaluated the proposed scheme in a real patient and compared the resulting irradiation plans with those prepared by clinical planners in hospital devices. The results in terms of efficiency and effectiveness are promising."}
{"title":"Software Dependability Measurement at the Age Of 36","authors":["Robert V. Binder"],"raw_abstract":"Thirty-six years after the first edition of IEEE standard 982.1, Measures of\nthe Software Aspects of Dependability, the third edition focuses on the\nmeasurement of in-service software dependability. This article explains how\nthis new point of view evolved and shaped the third edition's guidance for\nsoftware dependability measurement.","publication_date":1700156188,"paper_link":"http://arxiv.org/pdf/2311.10039v1","categories":["Quantitative Finance"],"abstract":"Thirty-six years after the first edition of IEEE standard 982.1, Measures of the Software Aspects of Dependability, the third edition focuses on the measurement of in-service software dependability. This article explains how this new point of view evolved and shaped the third edition's guidance for software dependability measurement."}
{"title":"DocMath-Eval: Evaluating Numerical Reasoning Capabilities of LLMs in Understanding Long Documents with Tabular Data","authors":["Yilun Zhao","Yitao Long","Hongjun Liu","Linyong Nan","Lyuhao Chen","Ryo Kamoi","Yixin Liu","Xiangru Tang","Rui Zhang","Arman Cohan"],"raw_abstract":"Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning and problem-solving capabilities of LLMs in the context of\nunderstanding and analyzing financial documents containing both text and\ntables. We evaluate a wide spectrum of 19 LLMs, including those specialized in\ncoding and finance. We also incorporate different prompting strategies (i.e.,\nChain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the\ncapabilities and limitations of existing LLMs in DocMath-Eval. We found that,\nalthough the current best-performing system (i.e., GPT-4), can perform well on\nsimple problems such as calculating the rate of increase in a financial metric\nwithin a short document context, it significantly lags behind human experts in\nmore complex problems grounded in longer contexts. We believe DocMath-Eval can\nbe used as a valuable benchmark to evaluate LLMs' capabilities to solve\nchallenging numerical reasoning problems in expert domains. We will release the\nbenchmark and code at https://github.com/yale-nlp/DocMath-Eval.","publication_date":1700134253,"paper_link":"http://arxiv.org/pdf/2311.09805v1","categories":["Quantitative Finance"],"abstract":"Recent LLMs have demonstrated remarkable performance in solving exam-like math word problems. However, the degree to which these numerical reasoning skills are effective in real-world scenarios, particularly in expert domains, is still largely unexplored. This paper introduces DocMath-Eval, a comprehensive benchmark specifically designed to evaluate the numerical reasoning and problem-solving capabilities of LLMs in the context of understanding and analyzing financial documents containing both text and tables. We evaluate a wide spectrum of 19 LLMs, including those specialized in coding and finance. We also incorporate different prompting strategies (i.e., Chain-of-Thoughts and Program-of-Thoughts) to comprehensively assess the capabilities and limitations of existing LLMs in DocMath-Eval. We found that, although the current best-performing system (i.e., GPT-4), can perform well on simple problems such as calculating the rate of increase in a financial metric within a short document context, it significantly lags behind human experts in more complex problems grounded in longer contexts. We believe DocMath-Eval can be used as a valuable benchmark to evaluate LLMs' capabilities to solve challenging numerical reasoning problems in expert domains. We will release the benchmark and code at https://github.com/yale-nlp/DocMath-Eval."}
{"title":"KnowledgeMath: Knowledge-Intensive Math Word Problem Solving in Finance Domains","authors":["Yilun Zhao","Hongjun Liu","Yitao Long","Rui Zhang","Chen Zhao","Arman Cohan"],"raw_abstract":"We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs'\ncapabilities in applying financial knowledge to solve complex math word\nproblems. Compared to prior works, this study features three core advancements.\nFirst, KnowledgeMath includes 1,259 problems with a hybrid of textual and\ntabular content and require college-level knowledge in the finance domain for\neffective resolution. Second, we provide expert-annotated, detailed solution\nreferences in Python program format, ensuring a high-quality benchmark for LLM\nassessment. Finally, we evaluate a wide spectrum of 14 LLMs with different\nprompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The\ncurrent best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves\nonly 45.4% accuracy, leaving substantial room for improvement. While\nknowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0%\nfor GPT-3.5), it is still significantly lower the estimated human expert\nperformance of 94%. We believe that KnowledgeMath can facilitate future\nresearch on domain-specific knowledge retrieval and augmentation into the math\nword problem-solving process. We will release the benchmark and code at\nhttps://github.com/yale-nlp/KnowledgeMath.","publication_date":1700133728,"paper_link":"http://arxiv.org/pdf/2311.09797v1","categories":["Quantitative Finance"],"abstract":"We introduce KnowledgeMath, a novel benchmark designed to evaluate LLMs' capabilities in applying financial knowledge to solve complex math word problems. Compared to prior works, this study features three core advancements. First, KnowledgeMath includes 1,259 problems with a hybrid of textual and tabular content and require college-level knowledge in the finance domain for effective resolution. Second, we provide expert-annotated, detailed solution references in Python program format, ensuring a high-quality benchmark for LLM assessment. Finally, we evaluate a wide spectrum of 14 LLMs with different prompting strategies like Chain-of-Thoughts and Program-of-Thoughts. The current best-performing system (i.e., GPT-4 with Program-of-Thoughts) achieves only 45.4% accuracy, leaving substantial room for improvement. While knowledge-augmented LLMs can improve the performance (e.g., from 23.9% to 32.0% for GPT-3.5), it is still significantly lower the estimated human expert performance of 94%. We believe that KnowledgeMath can facilitate future research on domain-specific knowledge retrieval and augmentation into the math word problem-solving process. We will release the benchmark and code at https://github.com/yale-nlp/KnowledgeMath."}
{"title":"Low-cost singular value decomposition with optimal sensor placement","authors":["Ashton Hetherington","Soledad Le Clainche"],"raw_abstract":"This paper presents a new method capable of reconstructing datasets with\ngreat precision and very low computational cost using a novel variant of the\nsingular value decomposition (SVD) algorithm that has been named low-cost SVD\n(lcSVD). This algorithm allows to reconstruct a dataset from a minimum amount\nof points, that can be selected randomly, equidistantly or can be calculated\nusing the optimal sensor placement functionality that is also presented in this\npaper, which finds minimizing the reconstruction error to validate the\ncalculated sensor positions. This method also allows to find the optimal number\nof sensors, aiding users in optimizing experimental data recollection. The\nmethod is tested in a series of datasets, which vary between experimental and\nnumerical simulations, two- and three-dimensional data and laminar and\nturbulent flow, which have been used to demonstrate the capacity of this method\nbased on its high reconstruction accuracy, robustness, and computational\nresource optimization. Maximum speed-up factors of 630 and memory reduction of\n37% are found when compared to the application of standard SVD to the dataset.\nThis method will be incorporated into ModelFLOWs-app's next version release.","publication_date":1700133294,"paper_link":"http://arxiv.org/pdf/2311.09791v2","categories":["Quantitative Finance"],"abstract":"This paper presents a new method capable of reconstructing datasets with great precision and very low computational cost using a novel variant of the singular value decomposition (SVD) algorithm that has been named low-cost SVD (lcSVD). This algorithm allows to reconstruct a dataset from a minimum amount of points, that can be selected randomly, equidistantly or can be calculated using the optimal sensor placement functionality that is also presented in this paper, which finds minimizing the reconstruction error to validate the calculated sensor positions. This method also allows to find the optimal number of sensors, aiding users in optimizing experimental data recollection. The method is tested in a series of datasets, which vary between experimental and numerical simulations, two- and three-dimensional data and laminar and turbulent flow, which have been used to demonstrate the capacity of this method based on its high reconstruction accuracy, robustness, and computational resource optimization. Maximum speed-up factors of 630 and memory reduction of 37% are found when compared to the application of standard SVD to the dataset. This method will be incorporated into ModelFLOWs-app's next version release."}
{"title":"Correlation networks: Interdisciplinary approaches beyond thresholding","authors":["Naoki Masuda","Zachary M. Boyd","Diego Garlaschelli","Peter J. Mucha"],"raw_abstract":"Many empirical networks originate from correlational data, arising in domains\nas diverse as psychology, neuroscience, genomics, microbiology, finance, and\nclimate science. Specialized algorithms and theory have been developed in\ndifferent application domains for working with such networks, as well as in\nstatistics, network science, and computer science, often with limited\ncommunication between practitioners in different fields. This leaves\nsignificant room for cross-pollination across disciplines. A central challenge\nis that it is not always clear how to best transform correlation matrix data\ninto networks for the application at hand, and probably the most widespread\nmethod, i.e., thresholding on the correlation value to create either unweighted\nor weighted networks, suffers from multiple problems. In this article, we\nreview various methods of constructing and analyzing correlation networks,\nranging from thresholding and its improvements to weighted networks,\nregularization, dynamic correlation networks, threshold-free approaches, and\nmore. Finally, we propose and discuss a variety of key open questions currently\nconfronting this field.","publication_date":1700105170,"paper_link":"http://arxiv.org/pdf/2311.09536v1","categories":["Physics"],"abstract":"Many empirical networks originate from correlational data, arising in domains as diverse as psychology, neuroscience, genomics, microbiology, finance, and climate science. Specialized algorithms and theory have been developed in different application domains for working with such networks, as well as in statistics, network science, and computer science, often with limited communication between practitioners in different fields. This leaves significant room for cross-pollination across disciplines. A central challenge is that it is not always clear how to best transform correlation matrix data into networks for the application at hand, and probably the most widespread method, i.e., thresholding on the correlation value to create either unweighted or weighted networks, suffers from multiple problems. In this article, we review various methods of constructing and analyzing correlation networks, ranging from thresholding and its improvements to weighted networks, regularization, dynamic correlation networks, threshold-free approaches, and more. Finally, we propose and discuss a variety of key open questions currently confronting this field."}
{"title":"Measure of Dependence for Financial Time-Series","authors":["Martin Winist\u00f6rfer","Ivan Zhdankin"],"raw_abstract":"Assessing the predictive power of both data and models holds paramount\nsignificance in time-series machine learning applications. Yet, preparing time\nseries data accurately and employing an appropriate measure for predictive\npower seems to be a non-trivial task. This work involves reviewing and\nestablishing the groundwork for a comprehensive analysis of shaping time-series\ndata and evaluating various measures of dependence. Lastly, we present a\nmethod, framework, and a concrete example for selecting and evaluating a\nsuitable measure of dependence.","publication_date":1700078910,"paper_link":"http://arxiv.org/pdf/2311.12129v1","categories":["Quantitative Finance"],"abstract":"Assessing the predictive power of both data and models holds paramount significance in time-series machine learning applications. Yet, preparing time series data accurately and employing an appropriate measure for predictive power seems to be a non-trivial task. This work involves reviewing and establishing the groundwork for a comprehensive analysis of shaping time-series data and evaluating various measures of dependence. Lastly, we present a method, framework, and a concrete example for selecting and evaluating a suitable measure of dependence."}
{"title":"Predicting risk/reward ratio in financial markets for asset management using machine learning","authors":["Reza Yarbakhsh","Mahdieh Soleymani Baghshah","Hamidreza Karimaghaie"],"raw_abstract":"Financial market forecasting remains a formidable challenge despite the surge\nin computational capabilities and machine learning advancements. While numerous\nstudies have underscored the precision of computer-generated market\npredictions, many of these forecasts fail to yield profitable trading outcomes.\nThis discrepancy often arises from the unpredictable nature of profit and loss\nratios in the event of successful and unsuccessful predictions. In this study,\nwe introduce a novel algorithm specifically designed for forecasting the profit\nand loss outcomes of trading activities. This is further augmented by an\ninnovative approach for integrating these forecasts with previous predictions\nof market trends. This approach is designed for algorithmic trading, enabling\ntraders to assess the profitability of each trade and calibrate the optimal\ntrade size. Our findings indicate that this method significantly improves the\nperformance of traditional trading strategies as well as algorithmic trading\nsystems, offering a promising avenue for enhancing trading decisions.","publication_date":1700070302,"paper_link":"http://arxiv.org/pdf/2311.09148v1","categories":["Quantitative Finance"],"abstract":"Financial market forecasting remains a formidable challenge despite the surge in computational capabilities and machine learning advancements. While numerous studies have underscored the precision of computer-generated market predictions, many of these forecasts fail to yield profitable trading outcomes. This discrepancy often arises from the unpredictable nature of profit and loss ratios in the event of successful and unsuccessful predictions. In this study, we introduce a novel algorithm specifically designed for forecasting the profit and loss outcomes of trading activities. This is further augmented by an innovative approach for integrating these forecasts with previous predictions of market trends. This approach is designed for algorithmic trading, enabling traders to assess the profitability of each trade and calibrate the optimal trade size. Our findings indicate that this method significantly improves the performance of traditional trading strategies as well as algorithmic trading systems, offering a promising avenue for enhancing trading decisions."}
{"title":"Automatic cable harness layout routing in a customizable 3D environment","authors":["T. Karlsson","E. \u00c5blad","T. Hermansson","J. S. Carlson","G. Tenf\u00e4lt"],"raw_abstract":"Designing cable harnesses can be time-consuming and complex due to many\ndesign and manufacturing aspects and rules. Automating the design process can\nhelp to fulfil these rules, speed up the process, and optimize the design. To\naccommodate this, we formulate a harness routing optimization problem to\nminimize cable lengths, maximize bundling by rewarding shared paths, and\noptimize the cables' spatial location with respect to case-specific information\nof the routing environment, e.g., zones to avoid. A deterministic and\ncomputationally effective cable harness routing algorithm has been developed to\nsolve the routing problem and is used to generate a set of cable harness\ntopology candidates and approximate the Pareto front. Our approach was tested\nagainst a stochastic and an exact solver and our routing algorithm generated\nobjective function values better than the stochastic approach and close to the\nexact solver. Our algorithm was able to find solutions, some of them being\nproven to be near-optimal, for three industrial-sized 3D cases within\nreasonable time (in magnitude of seconds to minutes) and the computation times\nwere comparable to those of the stochastic approach.","publication_date":1700063605,"paper_link":"http://arxiv.org/pdf/2311.09061v1","categories":["Quantitative Finance"],"abstract":"Designing cable harnesses can be time-consuming and complex due to many design and manufacturing aspects and rules. Automating the design process can help to fulfil these rules, speed up the process, and optimize the design. To accommodate this, we formulate a harness routing optimization problem to minimize cable lengths, maximize bundling by rewarding shared paths, and optimize the cables' spatial location with respect to case-specific information of the routing environment, e.g., zones to avoid. A deterministic and computationally effective cable harness routing algorithm has been developed to solve the routing problem and is used to generate a set of cable harness topology candidates and approximate the Pareto front. Our approach was tested against a stochastic and an exact solver and our routing algorithm generated objective function values better than the stochastic approach and close to the exact solver. Our algorithm was able to find solutions, some of them being proven to be near-optimal, for three industrial-sized 3D cases within reasonable time (in magnitude of seconds to minutes) and the computation times were comparable to those of the stochastic approach."}
{"title":"Introducing CHAD -- An ADM1 Solver for Direct Linking to Lagrangian CFD Software","authors":["Prashant Kumar","Zhenghao Yan","Soroush Dabiri","Nikolaus Rauch","Wolfgang Rauch"],"raw_abstract":"Standard methods for modeling anaerobic digestion processes assume\nhomogeneous conditions inside the tank and thus suffer from the negligence of\nhydrodynamics. In this work, we present the software toolbox Coupled\nHydrodynamics and Anaerobic Digestion (CHAD), a novel parallelized solver that\nis capable of utilizing CFD results as the basis for Anaerobic digestion model\nNo.1 (ADMno1) simulations. CHAD uses a particle-based Lagrangian CFD solver\ni.e., DualSPHysics (DSPH) as input and provides for a parallelized, C++ code\nimplementation of the standard ADMno1. This paper demonstrates a conceptual and\nnumerical verification of the toolbox and outlines the future pathway to\nenhance the approach.","publication_date":1700053043,"paper_link":"http://arxiv.org/pdf/2311.08927v1","categories":["Physics"],"abstract":"Standard methods for modeling anaerobic digestion processes assume homogeneous conditions inside the tank and thus suffer from the negligence of hydrodynamics. In this work, we present the software toolbox Coupled Hydrodynamics and Anaerobic Digestion (CHAD), a novel parallelized solver that is capable of utilizing CFD results as the basis for Anaerobic digestion model No.1 (ADMno1) simulations. CHAD uses a particle-based Lagrangian CFD solver i.e., DualSPHysics (DSPH) as input and provides for a parallelized, C++ code implementation of the standard ADMno1. This paper demonstrates a conceptual and numerical verification of the toolbox and outlines the future pathway to enhance the approach."}
{"title":"Reducing 2-QuBit Gate Count for ZX-Calculus based Quantum Circuit Optimization","authors":["Korbinian Staudacher","Tobias Guggemos","Sophia Grundner-Culemann","Wolfgang Gehrke"],"raw_abstract":"In the near term, programming quantum computers will remain severely limited\nby low quantum volumes. Therefore, it is desirable to implement quantum\ncircuits with the fewest resources possible. For the common Clifford+T\ncircuits, most research is focused on reducing the number of T gates, since\nthey are an order of magnitude more expensive than Clifford gates in quantum\nerror corrected encoding schemes. However, this optimization sometimes leads to\nmore 2-qubit gates, which, even though they are less expensive in terms of\nfault-tolerance, contribute significantly to the overall circuit cost.\nApproaches based on the ZX-calculus have recently gained some popularity in the\nfield, but reduction of 2-qubit gates is not their focus. In this work, we\npresent an alternative for improving 2-qubit gate count of a quantum circuit\nwith the ZX-calculus by using heuristics in ZX-diagram simplification. Our\napproach maintains the good reduction of the T gate count provided by other\nstrategies based on ZX-calculus, thus serving as an extension for other\noptimization algorithms. Our results show that combining the available\nZX-calculus-based optimizations with our algorithms can reduce the number of\n2-qubit gates by as much as 40% compared to current approaches using\nZX-calculus. Additionally, we improve the results of the best currently\navailable optimization technique of Nam et. al for some circuits by up to 15%.","publication_date":1700048403,"paper_link":"http://arxiv.org/pdf/2311.08881v1","categories":["Quantitative Finance"],"abstract":"In the near term, programming quantum computers will remain severely limited by low quantum volumes. Therefore, it is desirable to implement quantum circuits with the fewest resources possible. For the common Clifford+T circuits, most research is focused on reducing the number of T gates, since they are an order of magnitude more expensive than Clifford gates in quantum error corrected encoding schemes. However, this optimization sometimes leads to more 2-qubit gates, which, even though they are less expensive in terms of fault-tolerance, contribute significantly to the overall circuit cost. Approaches based on the ZX-calculus have recently gained some popularity in the field, but reduction of 2-qubit gates is not their focus. In this work, we present an alternative for improving 2-qubit gate count of a quantum circuit with the ZX-calculus by using heuristics in ZX-diagram simplification. Our approach maintains the good reduction of the T gate count provided by other strategies based on ZX-calculus, thus serving as an extension for other optimization algorithms. Our results show that combining the available ZX-calculus-based optimizations with our algorithms can reduce the number of 2-qubit gates by as much as 40% compared to current approaches using ZX-calculus. Additionally, we improve the results of the best currently available optimization technique of Nam et. al for some circuits by up to 15%."}
{"title":"A short note on super-hedging an arbitrary number of European options with integer-valued strategies","authors":["Dorsaf Cherif","Meriam El Mansour","Emmanuel Lepinette"],"raw_abstract":"The usual theory of asset pricing in finance assumes that the financial\nstrategies, i.e. the quantity of risky assets to invest, are real-valued so\nthat they are not integer-valued in general, see the Black and Scholes model\nfor instance. This is clearly contrary to what it is possible to do in the real\nworld. Surprisingly, it seems that there is no many contributions in that\ndirection in the literature, except for a finite number of states. In this\npaper, for arbitrary {\\Omega}, we show that, in discrete-time, it is possible\nto evaluate the minimal super-hedging price when we restrict ourselves to\ninteger-valued strategies. To do so, we only consider terminal claims that are\ncontinuous piecewise affine functions of the underlying asset. We formulate a\ndynamic programming principle that can be directly implemented on an historical\ndata and which also provides the optimal integer-valued strategy. The problem\nwith general payoffs remains open but should be solved with the same approach.","publication_date":1700046764,"paper_link":"http://arxiv.org/pdf/2311.08871v1","categories":["Mathematics","Quantitative Finance"],"abstract":"The usual theory of asset pricing in finance assumes that the financial strategies, i.e. the quantity of risky assets to invest, are real-valued so that they are not integer-valued in general, see the Black and Scholes model for instance. This is clearly contrary to what it is possible to do in the real world. Surprisingly, it seems that there is no many contributions in that direction in the literature, except for a finite number of states. In this paper, for arbitrary {\\Omega}, we show that, in discrete-time, it is possible to evaluate the minimal super-hedging price when we restrict ourselves to integer-valued strategies. To do so, we only consider terminal claims that are continuous piecewise affine functions of the underlying asset. We formulate a dynamic programming principle that can be directly implemented on an historical data and which also provides the optimal integer-valued strategy. The problem with general payoffs remains open but should be solved with the same approach."}
{"title":"Multi-stage Euler-Maruyama methods for backward stochastic differential equations driven by continuous-time Markov chains","authors":["Akihiro Kaneko"],"raw_abstract":"Numerical methods for computing the solutions of Markov backward stochastic\ndifferential equations (BSDEs) driven by continuous-time Markov chains (CTMCs)\nare explored. The main contributions of this paper are as follows: (1) we\nobserve that Euler-Maruyama temporal discretization methods for solving Markov\nBSDEs driven by CTMCs are equivalent to exponential integrators for solving the\nassociated systems of ordinary differential equations (ODEs); (2) we introduce\nmulti-stage Euler-Maruyama methods for effectively solving \"stiff\" Markov BSDEs\ndriven by CTMCs; these BSDEs typically arise from the spatial discretization of\nMarkov BSDEs driven by Brownian motion; (3) we propose a multilevel spatial\ndiscretization method on sparse grids that efficiently approximates\nhigh-dimensional Markov BSDEs driven by Brownian motion with a combination of\nmultiple Markov BSDEs driven by CTMCs on grids with different resolutions. We\nalso illustrate the effectiveness of the presented methods with a number of\nnumerical experiments in which we treat nonlinear BSDEs arising from option\npricing problems in finance.","publication_date":1700042400,"paper_link":"http://arxiv.org/pdf/2311.08826v1","categories":["Mathematics","Quantitative Finance"],"abstract":"Numerical methods for computing the solutions of Markov backward stochastic differential equations (BSDEs) driven by continuous-time Markov chains (CTMCs) are explored. The main contributions of this paper are as follows: (1) we observe that Euler-Maruyama temporal discretization methods for solving Markov BSDEs driven by CTMCs are equivalent to exponential integrators for solving the associated systems of ordinary differential equations (ODEs); (2) we introduce multi-stage Euler-Maruyama methods for effectively solving \"stiff\" Markov BSDEs driven by CTMCs; these BSDEs typically arise from the spatial discretization of Markov BSDEs driven by Brownian motion; (3) we propose a multilevel spatial discretization method on sparse grids that efficiently approximates high-dimensional Markov BSDEs driven by Brownian motion with a combination of multiple Markov BSDEs driven by CTMCs on grids with different resolutions. We also illustrate the effectiveness of the presented methods with a number of numerical experiments in which we treat nonlinear BSDEs arising from option pricing problems in finance."}
{"title":"Thermal Finite Element Modeling and Simulation of a Squirrel-Cage Induction Machine","authors":["Christian Bergfried","Yvonne Sp\u00e4ck-Leigsnering","Roland Seebacher","Heinrich Eickhoff","Annette Muetze"],"raw_abstract":"Finite element models of electrical machines allow insights in electrothermal\nstresses which endanger the insulation system of the machine. This paper\npresents a thermal finite element model of a 3.7 kW squirrel-cage induction\nmachine. The model resolves the conductors and the surrounding insulation\nmaterials in the stator slots. A set of transient thermal scenarios is defined\nand measured in the machine laboratory. These data are used to assess the\nfinite element model.","publication_date":1700041880,"paper_link":"http://arxiv.org/pdf/2311.08821v1","categories":["Quantitative Finance"],"abstract":"Finite element models of electrical machines allow insights in electrothermal stresses which endanger the insulation system of the machine. This paper presents a thermal finite element model of a 3.7 kW squirrel-cage induction machine. The model resolves the conductors and the surrounding insulation materials in the stator slots. A set of transient thermal scenarios is defined and measured in the machine laboratory. These data are used to assess the finite element model."}
{"title":"German FinBERT: A German Pre-trained Language Model","authors":["Moritz Scherrmann"],"raw_abstract":"This study presents German FinBERT, a novel pre-trained German language model\ntailored for financial textual data. The model is trained through a\ncomprehensive pre-training process, leveraging a substantial corpus comprising\nfinancial reports, ad-hoc announcements and news related to German companies.\nThe corpus size is comparable to the data sets commonly used for training\nstandard BERT models. I evaluate the performance of German FinBERT on\ndownstream tasks, specifically sentiment prediction, topic recognition and\nquestion answering against generic German language models. My results\ndemonstrate improved performance on finance-specific data, indicating the\nefficacy of German FinBERT in capturing domain-specific nuances. The presented\nfindings suggest that German FinBERT holds promise as a valuable tool for\nfinancial text analysis, potentially benefiting various applications in the\nfinancial domain.","publication_date":1700039249,"paper_link":"http://arxiv.org/pdf/2311.08793v1","categories":["Statistics"],"abstract":"This study presents German FinBERT, a novel pre-trained German language model tailored for financial textual data. The model is trained through a comprehensive pre-training process, leveraging a substantial corpus comprising financial reports, ad-hoc announcements and news related to German companies. The corpus size is comparable to the data sets commonly used for training standard BERT models. I evaluate the performance of German FinBERT on downstream tasks, specifically sentiment prediction, topic recognition and question answering against generic German language models. My results demonstrate improved performance on finance-specific data, indicating the efficacy of German FinBERT in capturing domain-specific nuances. The presented findings suggest that German FinBERT holds promise as a valuable tool for financial text analysis, potentially benefiting various applications in the financial domain."}
{"title":"A Scalable Two-Level Domain Decomposition Eigensolver for Periodic Schr\u00f6dinger Eigenstates in Anisotropically Expanding Domains","authors":["Lambert Theisen","Benjamin Stamm"],"raw_abstract":"Accelerating iterative eigenvalue algorithms is often achieved by employing a\nspectral shifting strategy. Unfortunately, improved shifting typically leads to\na smaller eigenvalue for the resulting shifted operator, which in turn results\nin a high condition number of the underlying solution matrix, posing a major\nchallenge for iterative linear solvers. This paper introduces a two-level\ndomain decomposition preconditioner that addresses this issue for the linear\nSchr\\\"odinger eigenvalue problem, even in the presence of a vanishing\neigenvalue gap in non-uniform, expanding domains. Since the quasi-optimal\nshift, which is already available as the solution to a spectral cell problem,\nis required for the eigenvalue solver, it is logical to also use its associated\neigenfunction as a generator to construct a coarse space. We analyze the\nresulting two-level additive Schwarz preconditioner and obtain a condition\nnumber bound that is independent of the domain's anisotropy, despite the need\nfor only one basis function per subdomain for the coarse solver. Several\nnumerical examples are presented to illustrate its flexibility and efficiency.","publication_date":1700035048,"paper_link":"http://arxiv.org/pdf/2311.08757v1","categories":["Mathematics"],"abstract":"Accelerating iterative eigenvalue algorithms is often achieved by employing a spectral shifting strategy. Unfortunately, improved shifting typically leads to a smaller eigenvalue for the resulting shifted operator, which in turn results in a high condition number of the underlying solution matrix, posing a major challenge for iterative linear solvers. This paper introduces a two-level domain decomposition preconditioner that addresses this issue for the linear Schr\\\"odinger eigenvalue problem, even in the presence of a vanishing eigenvalue gap in non-uniform, expanding domains. Since the quasi-optimal shift, which is already available as the solution to a spectral cell problem, is required for the eigenvalue solver, it is logical to also use its associated eigenfunction as a generator to construct a coarse space. We analyze the resulting two-level additive Schwarz preconditioner and obtain a condition number bound that is independent of the domain's anisotropy, despite the need for only one basis function per subdomain for the coarse solver. Several numerical examples are presented to illustrate its flexibility and efficiency."}
{"title":"A General Theory of Liquidity Provisioning for Automated Market Makers","authors":["Adithya Bhaskara","Rafael Frongillo","Maneesha Papireddygari"],"raw_abstract":"In decentralized finance, it is common for automated market makers to\nprovision liquidity from external parties. The market maker rewards these\nliquidity providers with a cut of the trading fees, in exchange for the risk\nthey take on. A handful of protocols for liquidity provisioning have been\nproposed, such as Uniswap V2 and V3, with specific and sometimes complex rules\nfor collecting liquidity deposits, executing trades, and dividing up fees.\nBeyond these examples, and a broader understanding of liquidity provisioning,\nand particularly the design space from which one could choose a different\nprotocols, has been out of reach. In this work, we show that one can view\nliquidity provisioning very broadly as the practice of running several market\nmakers \"in parallel\": each market maker provides its own liquidity, yet the\ncombined group can operate as a single coherent market. We prove that this\ngeneral protocol, when restricted to specific forms of the constituent market\nmakers, recovers Uniswap V2 and V3 as special cases. We then go on to propose a\nnew restriction which may have advantages over Uniswap V3. In the context of\nprediction markets, where computation costs are less constrained, our general\nprotocol gives a maximally flexible way to provision liquidity. We conclude\nwith remarks about the nature of liquidity and fees in markets with more than 2\nassets, and several open questions.","publication_date":1700030150,"paper_link":"http://arxiv.org/pdf/2311.08725v1","categories":["Quantitative Finance"],"abstract":"In decentralized finance, it is common for automated market makers to provision liquidity from external parties. The market maker rewards these liquidity providers with a cut of the trading fees, in exchange for the risk they take on. A handful of protocols for liquidity provisioning have been proposed, such as Uniswap V2 and V3, with specific and sometimes complex rules for collecting liquidity deposits, executing trades, and dividing up fees. Beyond these examples, and a broader understanding of liquidity provisioning, and particularly the design space from which one could choose a different protocols, has been out of reach. In this work, we show that one can view liquidity provisioning very broadly as the practice of running several market makers \"in parallel\": each market maker provides its own liquidity, yet the combined group can operate as a single coherent market. We prove that this general protocol, when restricted to specific forms of the constituent market makers, recovers Uniswap V2 and V3 as special cases. We then go on to propose a new restriction which may have advantages over Uniswap V3. In the context of prediction markets, where computation costs are less constrained, our general protocol gives a maximally flexible way to provision liquidity. We conclude with remarks about the nature of liquidity and fees in markets with more than 2 assets, and several open questions."}
{"title":"Bank Performance Determinants: State of the Art and Future Research Avenues","authors":["Anas Azzabi","Younes Lahrichi"],"raw_abstract":"Banks' performance is an important topic for both professionals and\nresearchers. Given the important literature on this subject, this paper aims to\nbring an up-to-date and organized review of literature on the determinants of\nbanks performance. This paper discusses the main approaches that molded the\ndebate on banks performance and their main determinants. An in-depth\nunderstanding of these latter may allow on the one hand, bank managers and\nregulators to improve the sector efficiency and to deal with the new trends\nshaping the future of their industry and on the other hand, academicians to\nenrich research and knowledge on this field. Through the analysis of 54 studies\npublished in 42 peer-reviewed journals, we show that despite the importance of\nthe existent literature, the subject of bank performance factors did not reveal\nall its secrets and still constitute a fertile field for critical debates,\nespecially since the COVID-19 and the increasingly pressing rise in power of\ndigital transformation and artificial intelligence in general and FinTechs in\nparticular. The study concludes by suggesting new promising research avenues.","publication_date":1700009239,"paper_link":"http://arxiv.org/pdf/2311.08617v1","categories":["Quantitative Finance"],"abstract":"Banks' performance is an important topic for both professionals and researchers. Given the important literature on this subject, this paper aims to bring an up-to-date and organized review of literature on the determinants of banks performance. This paper discusses the main approaches that molded the debate on banks performance and their main determinants. An in-depth understanding of these latter may allow on the one hand, bank managers and regulators to improve the sector efficiency and to deal with the new trends shaping the future of their industry and on the other hand, academicians to enrich research and knowledge on this field. Through the analysis of 54 studies published in 42 peer-reviewed journals, we show that despite the importance of the existent literature, the subject of bank performance factors did not reveal all its secrets and still constitute a fertile field for critical debates, especially since the COVID-19 and the increasingly pressing rise in power of digital transformation and artificial intelligence in general and FinTechs in particular. The study concludes by suggesting new promising research avenues."}
{"title":"Natural Language Processing for Financial Regulation","authors":["Ixandra Achitouv","Dragos Gorduza","Antoine Jacquier"],"raw_abstract":"This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.","publication_date":1699995501,"paper_link":"http://arxiv.org/pdf/2311.08533v1","categories":["Quantitative Finance"],"abstract":"This article provides an understanding of Natural Language Processing techniques in the framework of financial regulation, more specifically in order to perform semantic matching search between rules and policy when no dataset is available for supervised learning. We outline how to outperform simple pre-trained sentences-transformer models using freely available resources and explain the mathematical concepts behind the key building blocks of Natural Language Processing."}
{"title":"A New Paradigm in Blockchain-based Financial Aid Distribution","authors":["Md. Raisul Hasan Shahrukh","Md. Tabassinur Rahman","Nafees Mansoor"],"raw_abstract":"Blockchain technology has emerged as a game-changer in a variety of\nindustries, providing robust solutions that can supplant conventional\nprocedures. The unique potential of this technology originates from its\ndecentralized ledger systems, which enable enhanced security, transparency, and\nthe validation of transactions without the need for intermediaries. Notably,\nthe financial sector is making substantial progress toward implementing\nblockchain solutions for a variety of operations, including remittances,\nlending, and investments. The healthcare industry is simultaneously\nincorporating this technology into systems for managing medical records,\ntracing supply chains, and data management. Similarly, the capacity of\nblockchain to enhance transparency, traceability, and accountability is widely\nacknowledged in supply chain management, from the procurement of basic\nmaterials to the delivery of finished goods. Diverse industries, including real\nestate, energy, and government, are actively investigating the potential of\nblockchain to improve efficiency, security, and transparency. Notably,\nHyperledger Besu, an open-source blockchain platform, is used to implement\nsmart contracts that automate processes and reduce manual intervention along\ndistribution pathways. This exhaustive review examines the transformative\npotential of blockchain technology across a variety of industries, discussing\nthe obstacles encountered and providing key insights into future research and\ndevelopment directions. This paper seeks to serve as a pivotal resource for\nacademics, industry stakeholders, and policymakers by synthesizing existing\nscholarly literature and shedding light on significant findings.","publication_date":1699990884,"paper_link":"http://arxiv.org/pdf/2311.08494v1","categories":["Quantitative Finance"],"abstract":"Blockchain technology has emerged as a game-changer in a variety of industries, providing robust solutions that can supplant conventional procedures. The unique potential of this technology originates from its decentralized ledger systems, which enable enhanced security, transparency, and the validation of transactions without the need for intermediaries. Notably, the financial sector is making substantial progress toward implementing blockchain solutions for a variety of operations, including remittances, lending, and investments. The healthcare industry is simultaneously incorporating this technology into systems for managing medical records, tracing supply chains, and data management. Similarly, the capacity of blockchain to enhance transparency, traceability, and accountability is widely acknowledged in supply chain management, from the procurement of basic materials to the delivery of finished goods. Diverse industries, including real estate, energy, and government, are actively investigating the potential of blockchain to improve efficiency, security, and transparency. Notably, Hyperledger Besu, an open-source blockchain platform, is used to implement smart contracts that automate processes and reduce manual intervention along distribution pathways. This exhaustive review examines the transformative potential of blockchain technology across a variety of industries, discussing the obstacles encountered and providing key insights into future research and development directions. This paper seeks to serve as a pivotal resource for academics, industry stakeholders, and policymakers by synthesizing existing scholarly literature and shedding light on significant findings."}
{"title":"Exploration of Hyperledger Besu in Designing Private Blockchain-based Financial Distribution Systems","authors":["Md. Raisul Hasan Shahrukh","Md. Tabassinur Rahman","Nafees Mansoor"],"raw_abstract":"Blockchain, a decentralized technology that provides unrivaled security,\ntransparency, and process validation, is redefining the operational landscape\nacross numerous industries. This article focuses on the development of an\ninnovative consortium blockchain based financial distribution application. This\npaper illuminates the transformative role of blockchain technology in a variety\nof sectors by drawing on a plethora of academic literature and current industry\npractices. It demonstrates the diverse applications of blockchain, ranging from\nremittances to lending and investments in finance to data administration in\nhealthcare and supply chain tracking. The paper reveals the design and\npotential of a consortium blockchain based application for financial\ndistribution. Utilizing the capabilities of Hyperledger Besu, the application\nis tailored to improve security, scalability, and interoperability, thereby\ncontributing to a more integrated financial ecosystem. The investigation sheds\nlight on the combination of consortium blockchain controlled access and\nHyprledger Besu comprehensive functionality, proposing a secure, transparent,\nand efficient financial transaction environment. The investigation serves as a\nresource for academics, industry professionals, and policymakers alike,\nhighlighting the vast potential of blockchain technology, enabled by platforms\nsuch as Hyperledger Besu, in accelerating the evolution of traditional systems\ntoward a more decentralized, secure, and efficient future.","publication_date":1699989496,"paper_link":"http://arxiv.org/pdf/2311.08483v1","categories":["Quantitative Finance"],"abstract":"Blockchain, a decentralized technology that provides unrivaled security, transparency, and process validation, is redefining the operational landscape across numerous industries. This article focuses on the development of an innovative consortium blockchain based financial distribution application. This paper illuminates the transformative role of blockchain technology in a variety of sectors by drawing on a plethora of academic literature and current industry practices. It demonstrates the diverse applications of blockchain, ranging from remittances to lending and investments in finance to data administration in healthcare and supply chain tracking. The paper reveals the design and potential of a consortium blockchain based application for financial distribution. Utilizing the capabilities of Hyperledger Besu, the application is tailored to improve security, scalability, and interoperability, thereby contributing to a more integrated financial ecosystem. The investigation sheds light on the combination of consortium blockchain controlled access and Hyprledger Besu comprehensive functionality, proposing a secure, transparent, and efficient financial transaction environment. The investigation serves as a resource for academics, industry professionals, and policymakers alike, highlighting the vast potential of blockchain technology, enabled by platforms such as Hyperledger Besu, in accelerating the evolution of traditional systems toward a more decentralized, secure, and efficient future."}
{"title":"Reduction of large-scale RLCk models via low-rank balanced truncation","authors":["Christos Giamouzis","Dimitrios Garyfallou","Anastasis Vagenas","Nestor Evmorfopoulos"],"raw_abstract":"Model order reduction (MOR) is an important step in the design process of\nintegrated circuits. Specifically, the electromagnetic models extracted from\nmodern complex designs result in a large number of passive elements that\nintroduce limitations in the simulation process. MOR techniques based on\nbalanced truncation (BT) can overcome these limitations by producing compact\nreduced-order models (ROMs) that approximate the behavior of the original\nmodels at the input/output ports. In this paper, we present a low-rank BT\nmethod that exploits the extended Krylov subspace and efficient implementation\ntechniques for the reduction of large-scale models. Experimental evaluation on\na diverse set of analog and mixed-signal circuits with millions of elements\nindicates that up to x5.5 smaller ROMs can be produced with similar accuracy to\nANSYS RaptorX ROMs.","publication_date":1699989051,"paper_link":"http://arxiv.org/pdf/2311.08478v1","categories":["Mathematics"],"abstract":"Model order reduction (MOR) is an important step in the design process of integrated circuits. Specifically, the electromagnetic models extracted from modern complex designs result in a large number of passive elements that introduce limitations in the simulation process. MOR techniques based on balanced truncation (BT) can overcome these limitations by producing compact reduced-order models (ROMs) that approximate the behavior of the original models at the input/output ports. In this paper, we present a low-rank BT method that exploits the extended Krylov subspace and efficient implementation techniques for the reduction of large-scale models. Experimental evaluation on a diverse set of analog and mixed-signal circuits with millions of elements indicates that up to x5.5 smaller ROMs can be produced with similar accuracy to ANSYS RaptorX ROMs."}
{"title":"Real-time topology optimization via learnable mappings","authors":["Gabriel Garayalde","Matteo Torzoni","Matteo Bruggi","Alberto Corigliano"],"raw_abstract":"In traditional topology optimization, the computing time required to\niteratively update the material distribution within a design domain strongly\ndepends on the complexity or size of the problem, limiting its application in\nreal engineering contexts. This work proposes a multi-stage machine learning\nstrategy that aims to predict an optimal topology and the related stress fields\nof interest, either in 2D or 3D, without resorting to any iterative analysis\nand design process. The overall topology optimization is treated as regression\ntask in a low-dimensional latent space, that encodes the variability of the\ntarget designs. First, a fully-connected model is employed to surrogate the\nfunctional link between the parametric input space characterizing the design\nproblem and the latent space representation of the corresponding optimal\ntopology. The decoder branch of an autoencoder is then exploited to reconstruct\nthe desired optimal topology from its latent representation. The deep learning\nmodels are trained on a dataset generated through a standard method of topology\noptimization implementing the solid isotropic material with penalization, for\nvarying boundary and loading conditions. The underlying hypothesis behind the\nproposed strategy is that optimal topologies share enough common patterns to be\ncompressed into small latent space representations without significant\ninformation loss. Results relevant to a 2D Messerschmitt-B\\\"olkow-Blohm beam\nand a 3D bridge case demonstrate the capabilities of the proposed framework to\nprovide accurate optimal topology predictions in a fraction of a second.","publication_date":1699988656,"paper_link":"http://arxiv.org/pdf/2311.08473v1","categories":["Quantitative Finance"],"abstract":"In traditional topology optimization, the computing time required to iteratively update the material distribution within a design domain strongly depends on the complexity or size of the problem, limiting its application in real engineering contexts. This work proposes a multi-stage machine learning strategy that aims to predict an optimal topology and the related stress fields of interest, either in 2D or 3D, without resorting to any iterative analysis and design process. The overall topology optimization is treated as regression task in a low-dimensional latent space, that encodes the variability of the target designs. First, a fully-connected model is employed to surrogate the functional link between the parametric input space characterizing the design problem and the latent space representation of the corresponding optimal topology. The decoder branch of an autoencoder is then exploited to reconstruct the desired optimal topology from its latent representation. The deep learning models are trained on a dataset generated through a standard method of topology optimization implementing the solid isotropic material with penalization, for varying boundary and loading conditions. The underlying hypothesis behind the proposed strategy is that optimal topologies share enough common patterns to be compressed into small latent space representations without significant information loss. Results relevant to a 2D Messerschmitt-B\\\"olkow-Blohm beam and a 3D bridge case demonstrate the capabilities of the proposed framework to provide accurate optimal topology predictions in a fraction of a second."}
{"title":"Aid Nexus : A Blockchain Based Financial Distribution System","authors":["Md. Raisul Hasan Shahrukh","Md. Tabassinur Rahman","Nafees Mansoor"],"raw_abstract":"Blockchain technology has emerged as a disruptive force with transformative\npotential across numerous industries, promising efficient and automated\nsolutions that can revolutionize traditional systems. By leveraging\ndecentralized ledger systems, blockchain offers enhanced security,\ntransparency, and transaction verification without the need for intermediaries.\nThe finance sector is exploring blockchain-based solutions for payments,\nremittances, lending, and investments, while healthcare adopts the technology\nfor medical record keeping, supply chain tracking, and data management.\nSimilarly, supply chain management benefits from blockchain's ability to\nenhance transparency, traceability, and accountability from raw materials to\nfinished products. Other sectors, including real estate, energy, and\ngovernment, are also investigating blockchain-based solutions to improve\nefficiency, security, and transparency. Furthermore, smart contracts within the\nblockchain enable process automation, reducing manual intervention in\ndistribution workflows. AidNeux, a consortium-based blockchain DApp, reimagines\nthe distribution of financial assistance by addressing inefficiencies and\nopaqueness. Using smart contracts ensures the security and directness of money\ntransfers. Its robust digital identity verification and real-time auditability\nreduce fraud risks and strengthen accountability, thereby presenting a\nscalable, transparent solution to problems inherent to conventional financial\naid systems.","publication_date":1699986902,"paper_link":"http://arxiv.org/pdf/2311.08372v1","categories":["Quantitative Finance"],"abstract":"Blockchain technology has emerged as a disruptive force with transformative potential across numerous industries, promising efficient and automated solutions that can revolutionize traditional systems. By leveraging decentralized ledger systems, blockchain offers enhanced security, transparency, and transaction verification without the need for intermediaries. The finance sector is exploring blockchain-based solutions for payments, remittances, lending, and investments, while healthcare adopts the technology for medical record keeping, supply chain tracking, and data management. Similarly, supply chain management benefits from blockchain's ability to enhance transparency, traceability, and accountability from raw materials to finished products. Other sectors, including real estate, energy, and government, are also investigating blockchain-based solutions to improve efficiency, security, and transparency. Furthermore, smart contracts within the blockchain enable process automation, reducing manual intervention in distribution workflows. AidNeux, a consortium-based blockchain DApp, reimagines the distribution of financial assistance by addressing inefficiencies and opaqueness. Using smart contracts ensures the security and directness of money transfers. Its robust digital identity verification and real-time auditability reduce fraud risks and strengthen accountability, thereby presenting a scalable, transparent solution to problems inherent to conventional financial aid systems."}
{"title":"In the Red(dit): Social Media and Stock Prices","authors":["James Baker"],"raw_abstract":"Spearheaded by retail traders on the website reddit, the GameStop short\nsqueeze of early 2021 shows that social media embeds information that\ncorrelates with market movements. This paper seeks to examine this relationship\nby using daily frequencies of classified comments and buzzwords as additional\nfactors in a Fama-French three factor model. Comments are classified using an\nunsupervised clustering method, while past studies have used pretrained models\nthat are not specific to the domains being studied.","publication_date":1699981121,"paper_link":"http://arxiv.org/pdf/2311.09252v1","categories":["Quantitative Finance"],"abstract":"Spearheaded by retail traders on the website reddit, the GameStop short squeeze of early 2021 shows that social media embeds information that correlates with market movements. This paper seeks to examine this relationship by using daily frequencies of classified comments and buzzwords as additional factors in a Fama-French three factor model. Comments are classified using an unsupervised clustering method, while past studies have used pretrained models that are not specific to the domains being studied."}
{"title":"Thermal Fluctuations For a Three-Beads Swimmer","authors":["R. Ferretta","R. Di Leonardo","A. Puglisi"],"raw_abstract":"We discuss a micro-swimmer model made of three spheres actuated by an\ninternal active time-periodic force, tied by an elastic potential and submitted\nto hydrodynamic interactions with thermal noise. The dynamical approach we use,\nreplacing the more common kinetic one, unveils the instability of the original\nmodel and the need of a confining potential to prevent the evaporation of the\nswimmer. We investigate the effect of the main parameters of the model, such as\nthe frequency and phase difference of the periodic active force, the stiffness\nof the confining potential, the length of the swimmer and the temperature and\nviscosity of the fluid. Our observables of interest are the averages of the\nswim velocity, of the energy consumption rate, the diffusion coefficient and\nthe swimming precision, which is limited by the energy consumption through the\ncelebrated Thermodynamic Uncertainty Relations. An optimum for velocity and\nprecision is found for an intermediate frequency. Reducing the potential\nstiffness, the viscosity or the length, is also beneficial for the swimming\nperformance, but these parameters are limited by the consistency of the model.\nAnalytical approximation for many of the interesting observables is obtained\nfor small deformations of the swimmer. We also discuss the efficiency of the\nswimmer in terms of its maximum precision and of the hydrodynamic, or\nLighthill, criterion, and how they are connected.","publication_date":1700649912,"paper_link":"http://arxiv.org/pdf/2311.13302v1","categories":["Physics"],"abstract":"We discuss a micro-swimmer model made of three spheres actuated by an internal active time-periodic force, tied by an elastic potential and submitted to hydrodynamic interactions with thermal noise. The dynamical approach we use, replacing the more common kinetic one, unveils the instability of the original model and the need of a confining potential to prevent the evaporation of the swimmer. We investigate the effect of the main parameters of the model, such as the frequency and phase difference of the periodic active force, the stiffness of the confining potential, the length of the swimmer and the temperature and viscosity of the fluid. Our observables of interest are the averages of the swim velocity, of the energy consumption rate, the diffusion coefficient and the swimming precision, which is limited by the energy consumption through the celebrated Thermodynamic Uncertainty Relations. An optimum for velocity and precision is found for an intermediate frequency. Reducing the potential stiffness, the viscosity or the length, is also beneficial for the swimming performance, but these parameters are limited by the consistency of the model. Analytical approximation for many of the interesting observables is obtained for small deformations of the swimmer. We also discuss the efficiency of the swimmer in terms of its maximum precision and of the hydrodynamic, or Lighthill, criterion, and how they are connected."}
{"title":"Bistability dynamics in the dissipative Dicke-Bose-Hubbard system","authors":["Tianyi Wu","Sayak Ray","Johann Kroha"],"raw_abstract":"We consider a driven-dissipative system consisting of an atomic Bose-Einstein\ncondensates loaded into a two-dimensional Hubbard lattice and coupled to a\nsingle mode of an optical cavity. Due to the interplay between strong,\nrepulsive atomic interaction and the atom-cavity coupling, the system exhibits\nseveral phases of atoms and photons including the atomic superfluid (SF) and\nsupersolid (SS). We investigate the dynamical behaviour of the system, where we\ninclude dissipation by means of Lindblad master equation formalism. Due to the\ndiscontinuous nature of the Dicke transition for strong atomic repulsion, we\nfind extended co-existence region of different phases. We investigate the\nresulting switching dynamics, particularly between the coexisting SF and SS\nphases, which eventually becomes damped by the dissipation.","publication_date":1700649894,"paper_link":"http://arxiv.org/pdf/2311.13301v1","categories":["Physics"],"abstract":"We consider a driven-dissipative system consisting of an atomic Bose-Einstein condensates loaded into a two-dimensional Hubbard lattice and coupled to a single mode of an optical cavity. Due to the interplay between strong, repulsive atomic interaction and the atom-cavity coupling, the system exhibits several phases of atoms and photons including the atomic superfluid (SF) and supersolid (SS). We investigate the dynamical behaviour of the system, where we include dissipation by means of Lindblad master equation formalism. Due to the discontinuous nature of the Dicke transition for strong atomic repulsion, we find extended co-existence region of different phases. We investigate the resulting switching dynamics, particularly between the coexisting SF and SS phases, which eventually becomes damped by the dissipation."}
{"title":"Robust Functional Regression with Discretely Sampled Predictors","authors":["Ioannis Kalogridis","Stanislav Nagy"],"raw_abstract":"The functional linear model is an important extension of the classical\nregression model allowing for scalar responses to be modeled as functions of\nstochastic processes. Yet, despite the usefulness and popularity of the\nfunctional linear model in recent years, most treatments, theoretical and\npractical alike, suffer either from (i) lack of resistance towards the many\ntypes of anomalies one may encounter with functional data or (ii) biases\nresulting from the use of discretely sampled functional data instead of\ncompletely observed data. To address these deficiencies, this paper introduces\nand studies the first class of robust functional regression estimators for\npartially observed functional data. The proposed broad class of estimators is\nbased on thin-plate splines with a novel computationally efficient quadratic\npenalty, is easily implementable and enjoys good theoretical properties under\nweak assumptions. We show that, in the incomplete data setting, both the sample\nsize and discretization error of the processes determine the asymptotic rate of\nconvergence of functional regression estimators and the latter cannot be\nignored. These theoretical properties remain valid even with multi-dimensional\nrandom fields acting as predictors and random smoothing parameters. The\neffectiveness of the proposed class of estimators in practice is demonstrated\nby means of a simulation study and a real-data example.","publication_date":1700648382,"paper_link":"http://arxiv.org/pdf/2311.13291v1","categories":["Mathematics","Statistics"],"abstract":"The functional linear model is an important extension of the classical regression model allowing for scalar responses to be modeled as functions of stochastic processes. Yet, despite the usefulness and popularity of the functional linear model in recent years, most treatments, theoretical and practical alike, suffer either from (i) lack of resistance towards the many types of anomalies one may encounter with functional data or (ii) biases resulting from the use of discretely sampled functional data instead of completely observed data. To address these deficiencies, this paper introduces and studies the first class of robust functional regression estimators for partially observed functional data. The proposed broad class of estimators is based on thin-plate splines with a novel computationally efficient quadratic penalty, is easily implementable and enjoys good theoretical properties under weak assumptions. We show that, in the incomplete data setting, both the sample size and discretization error of the processes determine the asymptotic rate of convergence of functional regression estimators and the latter cannot be ignored. These theoretical properties remain valid even with multi-dimensional random fields acting as predictors and random smoothing parameters. The effectiveness of the proposed class of estimators in practice is demonstrated by means of a simulation study and a real-data example."}
{"title":"Kardar-Parisi-Zhang fluctuations in the synchronization dynamics of limit-cycle oscillators","authors":["Ricardo Guti\u00e9rrez","Rodolfo Cuerno"],"raw_abstract":"The space-time process whereby one-dimensional systems of self-sustained\noscillators synchronize is shown to display generic scale invariance, with\nscaling properties characteristic of the Kardar-Parisi-Zhang equation with\ncolumnar noise, and phase fluctuations that follow a Tracy-Widom probability\ndistribution. This is revealed by a numerical exploration of rings of\nStuart-Landau oscillators (the universal representation of an oscillating\nsystem close to a Hopf bifurcation) and rings of van der Pol oscillators, both\nof which are paradigms of self-sustained oscillators. The critical behavior is\nvery well-defined for limit-cycle oscillations near the bifurcation point, and\nstill dominates the behavior comparatively far from the bifurcation. In\nparticular, the Tracy-Widom fluctuation distribution seems to be an extremely\nrobust feature of the synchronization process. The nonequilibrium criticality\nhere described appears to transcend the details of the coupled dynamical\nsystems that synchronize, making plausible its experimental observation.","publication_date":1700644580,"paper_link":"http://arxiv.org/pdf/2311.13253v1","categories":["Physics"],"abstract":"The space-time process whereby one-dimensional systems of self-sustained oscillators synchronize is shown to display generic scale invariance, with scaling properties characteristic of the Kardar-Parisi-Zhang equation with columnar noise, and phase fluctuations that follow a Tracy-Widom probability distribution. This is revealed by a numerical exploration of rings of Stuart-Landau oscillators (the universal representation of an oscillating system close to a Hopf bifurcation) and rings of van der Pol oscillators, both of which are paradigms of self-sustained oscillators. The critical behavior is very well-defined for limit-cycle oscillations near the bifurcation point, and still dominates the behavior comparatively far from the bifurcation. In particular, the Tracy-Widom fluctuation distribution seems to be an extremely robust feature of the synchronization process. The nonequilibrium criticality here described appears to transcend the details of the coupled dynamical systems that synchronize, making plausible its experimental observation."}
{"title":"Critical edge statistics for deformed GinUEs","authors":["Dang-Zheng Liu","Lu Zhang"],"raw_abstract":"For an additive perturbation of the complex Ginibre ensemble under a\ndeterministic matrix $X_0$, under certain assumption on $X_0$ we prove that\nthere are only two types of local statistical patterns at the spectral\nedge--GinUE statistics and critical statistics, which corresponds to regular or\nquadratic vanishing spectral points. The latter, as a non-Hermitian analogue of\nPearcey statistics in random matrix theory, characterizes a new point process\non the complex plane and will be our major goal in this paper.","publication_date":1700641927,"paper_link":"http://arxiv.org/pdf/2311.13227v1","categories":["Mathematics"],"abstract":"For an additive perturbation of the complex Ginibre ensemble under a deterministic matrix __FORMULA__, under certain assumption on __FORMULA__ we prove that there are only two types of local statistical patterns at the spectral edge--GinUE statistics and critical statistics, which corresponds to regular or quadratic vanishing spectral points. The latter, as a non-Hermitian analogue of Pearcey statistics in random matrix theory, characterizes a new point process on the complex plane and will be our major goal in this paper."}
{"title":"Testing Closeness of Multivariate Distributions via Ramsey Theory","authors":["Ilias Diakonikolas","Daniel M. Kane","Sihan Liu"],"raw_abstract":"We investigate the statistical task of closeness (or equivalence) testing for\nmultidimensional distributions. Specifically, given sample access to two\nunknown distributions $\\mathbf p, \\mathbf q$ on $\\mathbb R^d$, we want to\ndistinguish between the case that $\\mathbf p=\\mathbf q$ versus $\\|\\mathbf\np-\\mathbf q\\|_{A_k} > \\epsilon$, where $\\|\\mathbf p-\\mathbf q\\|_{A_k}$ denotes\nthe generalized ${A}_k$ distance between $\\mathbf p$ and $\\mathbf q$ --\nmeasuring the maximum discrepancy between the distributions over any collection\nof $k$ disjoint, axis-aligned rectangles. Our main result is the first\ncloseness tester for this problem with {\\em sub-learning} sample complexity in\nany fixed dimension and a nearly-matching sample complexity lower bound.\n  In more detail, we provide a computationally efficient closeness tester with\nsample complexity $O\\left((k^{6/7}/ \\mathrm{poly}_d(\\epsilon))\n\\log^d(k)\\right)$. On the lower bound side, we establish a qualitatively\nmatching sample complexity lower bound of\n$\\Omega(k^{6/7}/\\mathrm{poly}(\\epsilon))$, even for $d=2$. These sample\ncomplexity bounds are surprising because the sample complexity of the problem\nin the univariate setting is $\\Theta(k^{4/5}/\\mathrm{poly}(\\epsilon))$. This\nhas the interesting consequence that the jump from one to two dimensions leads\nto a substantial increase in sample complexity, while increases beyond that do\nnot.\n  As a corollary of our general $A_k$ tester, we obtain $d_{\\mathrm\nTV}$-closeness testers for pairs of $k$-histograms on $\\mathbb R^d$ over a\ncommon unknown partition, and pairs of uniform distributions supported on the\nunion of $k$ unknown disjoint axis-aligned rectangles.\n  Both our algorithm and our lower bound make essential use of tools from\nRamsey theory.","publication_date":1700627649,"paper_link":"http://arxiv.org/pdf/2311.13154v1","categories":["Mathematics","Statistics"],"abstract":"We investigate the statistical task of closeness (or equivalence) testing for multidimensional distributions. Specifically, given sample access to two unknown distributions __FORMULA__ on __FORMULA__, we want to distinguish between the case that __FORMULA__ versus __FORMULA__, where __FORMULA__ denotes the generalized __FORMULA__ distance between __FORMULA__ and __FORMULA__ -- measuring the maximum discrepancy between the distributions over any collection of __FORMULA__ disjoint, axis-aligned rectangles. Our main result is the first closeness tester for this problem with {\\em sub-learning} sample complexity in any fixed dimension and a nearly-matching sample complexity lower bound.   In more detail, we provide a computationally efficient closeness tester with sample complexity __FORMULA__. On the lower bound side, we establish a qualitatively matching sample complexity lower bound of __FORMULA__, even for __FORMULA__. These sample complexity bounds are surprising because the sample complexity of the problem in the univariate setting is __FORMULA__. This has the interesting consequence that the jump from one to two dimensions leads to a substantial increase in sample complexity, while increases beyond that do not.   As a corollary of our general __FORMULA__ tester, we obtain __FORMULA__-closeness testers for pairs of __FORMULA__-histograms on __FORMULA__ over a common unknown partition, and pairs of uniform distributions supported on the union of __FORMULA__ unknown disjoint axis-aligned rectangles.   Both our algorithm and our lower bound make essential use of tools from Ramsey theory."}
{"title":"A Note on Improved Multivariate Normal Mean Estimation With Unknown Covariance When p Is Greater Than n","authors":["Arash A. Foroushani","Severien Nkurunziza"],"raw_abstract":"In this paper, we highlight a major error in the proofs of the important\nresults of [D.Ch\\'etelat and M. T. Wells(2012). Improved Multivariate Normal\nMean Estimation with Unknown Covariance when p is Greater than n. The Annals of\nStatistics, Vol. 40, No.6, 3137--3160]. In particular, the proofs of some of\ntheir main results are based on Theorem 2 whose proof needs to be revisited.\nMore precisely, there are some major mistakes in the derivation of this\nimportant result. Further, under a very realistic assumption about the rank of\nthe estimator of the variance-covariance matrix, we correct the proof of the\nquoted result.","publication_date":1700625420,"paper_link":"http://arxiv.org/pdf/2311.13140v1","categories":["Mathematics","Statistics"],"abstract":"In this paper, we highlight a major error in the proofs of the important results of [D.Ch\\'etelat and M. T. Wells(2012). Improved Multivariate Normal Mean Estimation with Unknown Covariance when p is Greater than n. The Annals of Statistics, Vol. 40, No.6, 3137--3160]. In particular, the proofs of some of their main results are based on Theorem 2 whose proof needs to be revisited. More precisely, there are some major mistakes in the derivation of this important result. Further, under a very realistic assumption about the rank of the estimator of the variance-covariance matrix, we correct the proof of the quoted result."}
{"title":"Double shrinkage priors for a normal mean matrix","authors":["Takeru Matsuda","Fumiyasu Komaki","William E. Strawderman"],"raw_abstract":"We consider estimation of a normal mean matrix under the Frobenius loss.\nMotivated by the Efron--Morris estimator, a generalization of Stein's prior has\nbeen recently developed, which is superharmonic and shrinks the singular values\ntowards zero. The generalized Bayes estimator with respect to this prior is\nminimax and dominates the maximum likelihood estimator. However, here we show\nthat it is inadmissible by using Brown's condition. Then, we develop two types\nof priors that provide improved generalized Bayes estimators and examine their\nperformance numerically. The proposed priors attain risk reduction by adding\nscalar shrinkage or column-wise shrinkage to singular value shrinkage. Parallel\nresults for Bayesian predictive densities are also given.","publication_date":1700625275,"paper_link":"http://arxiv.org/pdf/2311.13137v1","categories":["Mathematics","Statistics"],"abstract":"We consider estimation of a normal mean matrix under the Frobenius loss. Motivated by the Efron--Morris estimator, a generalization of Stein's prior has been recently developed, which is superharmonic and shrinks the singular values towards zero. The generalized Bayes estimator with respect to this prior is minimax and dominates the maximum likelihood estimator. However, here we show that it is inadmissible by using Brown's condition. Then, we develop two types of priors that provide improved generalized Bayes estimators and examine their performance numerically. The proposed priors attain risk reduction by adding scalar shrinkage or column-wise shrinkage to singular value shrinkage. Parallel results for Bayesian predictive densities are also given."}
{"title":"KMT-2023-BLG-1431Lb: A New $q < 10^{-4}$ Microlensing Planet from a Subtle Signature","authors":["Aislyn Bell","Jiyuan Zhang","Youn Kil Jung","Jennifer C. Yee","Hongjing Yang","Takahiro Sumi","Andrzej Udalski","Michael D. Albrow","Sun-Ju Chung","Andrew Gould","Cheongho Han","Kyu-Ha Hwang","Yoon-Hyun Ryu","In-Gu Shin","Yossi Shvartzvald","Weicheng Zang","Sang-Mok Cha","Dong-Jin Kim","Seung-Lee Kim","Chung-Uk Lee","Dong-Joo Lee","Yongseok Lee","Byeong-Gon Park","Richard W. Pogge","Yunyi Tang","Jennie McCormick","Subo Dong","Zhuokai Liu","Shude Mao","Dan Maoz","Wei Zhu","Fumio Abe","Richard Barry","David P. Bennett","Aparna Bhattacharya","Ian A. Bond","Hirosane Fujii","Akihiko Fukui","Ryusei Hamada","Yuki Hirao","Stela Ishitani Silva","Yoshitaka Itow","Rintaro Kirikawa","Iona Kondo","Naoki Koshimoto","Yutaka Matsubara","Sho Matsumoto","Shota Miyazaki","Yasushi Muraki","Arisa Okamura","Greg Olmschenk","Cl\u00e9ment Ranc","Nicholas J. Rattenbury","Yuki Satoh","Daisuke Suzuki","Taiga Toda","Mio Tomoyoshi","Paul J. Tristram","Aikaterini Vandorou","Hibiki Yama","Kansuke Yamashita","Przemek Mr\u00f3z","Jan Skowron","Radoslaw Poleski","Micha\u0142 K. Szyma\u0144ski","Igor Soszy\u0144ski","Pawe\u0142 Pietrukowicz","Szymon Koz\u0142owski","Krzysztof Ulaczyk","Krzysztof A. Rybicki","Patryk Iwanek","Marcin Wrona","Mariusz Gromadzki"],"raw_abstract":"The current studies of microlensing planets are limited by small number\nstatistics. Follow-up observations of high-magnification microlensing events\ncan efficiently form a statistical planetary sample. Since 2020, the Korea\nMicrolensing Telescope Network (KMTNet) and the Las Cumbres Observatory (LCO)\nglobal network have been conducting a follow-up program for high-magnification\nKMTNet events. Here, we report the detection and analysis of a microlensing\nplanetary event, KMT-2023-BLG-1431, for which the subtle (0.05 magnitude) and\nshort-lived (5 hours) planetary signature was characterized by the follow-up\nfrom KMTNet and LCO. A binary-lens single-source (2L1S) analysis reveals a\nplanet/host mass ratio of $q = (0.72 \\pm 0.07) \\times 10^{-4}$, and the\nsingle-lens binary-source (1L2S) model is excluded by $\\Delta\\chi^2 = 80$. A\nBayesian analysis using a Galactic model yields estimates of the host star mass\nof $M_{\\rm host} = 0.57^{+0.33}_{-0.29}~M_\\odot$, the planetary mass of $M_{\\rm\nplanet} = 13.5_{-6.8}^{+8.1}~M_{\\oplus}$, and the lens distance of $D_{\\rm L} =\n6.9_{-1.7}^{+0.8}$ kpc. The projected planet-host separation of $a_\\perp =\n2.3_{-0.5}^{+0.5}$ au or $a_\\perp = 3.2_{-0.8}^{+0.7}$, subject to the\nclose/wide degeneracy. We also find that without the follow-up data, the\nsurvey-only data cannot break the degeneracy of central/resonant caustics and\nthe degeneracy of 2L1S/1L2S models, showing the importance of follow-up\nobservations for current microlensing surveys.","publication_date":1700618198,"paper_link":"http://arxiv.org/pdf/2311.13097v1","categories":["Physics"],"abstract":"The current studies of microlensing planets are limited by small number statistics. Follow-up observations of high-magnification microlensing events can efficiently form a statistical planetary sample. Since 2020, the Korea Microlensing Telescope Network (KMTNet) and the Las Cumbres Observatory (LCO) global network have been conducting a follow-up program for high-magnification KMTNet events. Here, we report the detection and analysis of a microlensing planetary event, KMT-2023-BLG-1431, for which the subtle (0.05 magnitude) and short-lived (5 hours) planetary signature was characterized by the follow-up from KMTNet and LCO. A binary-lens single-source (2L1S) analysis reveals a planet/host mass ratio of __FORMULA__, and the single-lens binary-source (1L2S) model is excluded by __FORMULA__. A Bayesian analysis using a Galactic model yields estimates of the host star mass of __FORMULA__, the planetary mass of __FORMULA__, and the lens distance of __FORMULA__ kpc. The projected planet-host separation of __FORMULA__ au or __FORMULA__, subject to the close/wide degeneracy. We also find that without the follow-up data, the survey-only data cannot break the degeneracy of central/resonant caustics and the degeneracy of 2L1S/1L2S models, showing the importance of follow-up observations for current microlensing surveys."}
{"title":"Measuring $f_{\\mathrm{NL}}$ with the SPHEREx Multi-tracer Redshift Space Bispectrum","authors":["Chen Heinrich","Olivier Dore","Elisabeth Krause"],"raw_abstract":"The bispectrum is an important statistics helpful for measuring the\nprimordial non-Gaussianity parameter $f_{\\mathrm{NL}}$ to less than order unity\nin error, which would allow us to distinguish between single and multi-field\ninflation models. The Spectro-Photometer for the History of the Universe, Epoch\nof Reionization and Ices Explorer (SPHEREx) mission is particularly well-suited\nfor making this measurement with its $\\sim$100-band all-sky observations in the\nnear-infrared. Consequently, the SPHEREx data will contain galaxies with\nspectroscopic-like redshift measurements as well as those with much larger\nerrors. In this paper, we evaluate the impact of photometric redshift errors on\n$f_{\\mathrm{NL}}$ constraints in the context of an updated multi-tracer\nforecast for SPHEREx, finding that the azimuthal averages of the first three\neven bispectrum multipoles are no longer sufficient for capturing most of the\ninformation (as opposed to the case of spectroscopic surveys shown in the\nliterature). The final SPHEREx result with all five galaxy samples and six\nredshift bins is however not severely impacted because the total result is\ndominated by the samples with the best redshift errors, while the worse samples\nserve to reduce cosmic variance. Our fiducial result of\n$\\sigma_{f_{\\mathrm{NL}}} = 0.7$ from bispectrum alone is increased by $18\\%$\nand $3\\%$ when using $l_{\\mathrm{max}}=0$ and 2 respectively. We also explore\nthe impact on parameter constraints when varying the fiducial redshift errors,\nas well as using subsets of multi-tracer combinations or triangles with\ndifferent squeezing factors. Note that the fiducial result here is not the\nfinal SPHEREx capability, which is still on target for being\n$\\sigma_{f_{\\mathrm{NL}}} = 0.5$ once the power spectrum will be included.","publication_date":1700615539,"paper_link":"http://arxiv.org/pdf/2311.13082v1","categories":["Physics"],"abstract":"The bispectrum is an important statistics helpful for measuring the primordial non-Gaussianity parameter __FORMULA__ to less than order unity in error, which would allow us to distinguish between single and multi-field inflation models. The Spectro-Photometer for the History of the Universe, Epoch of Reionization and Ices Explorer (SPHEREx) mission is particularly well-suited for making this measurement with its __FORMULA__100-band all-sky observations in the near-infrared. Consequently, the SPHEREx data will contain galaxies with spectroscopic-like redshift measurements as well as those with much larger errors. In this paper, we evaluate the impact of photometric redshift errors on __FORMULA__ constraints in the context of an updated multi-tracer forecast for SPHEREx, finding that the azimuthal averages of the first three even bispectrum multipoles are no longer sufficient for capturing most of the information (as opposed to the case of spectroscopic surveys shown in the literature). The final SPHEREx result with all five galaxy samples and six redshift bins is however not severely impacted because the total result is dominated by the samples with the best redshift errors, while the worse samples serve to reduce cosmic variance. Our fiducial result of __FORMULA__ from bispectrum alone is increased by __FORMULA__ and __FORMULA__ when using __FORMULA__ and 2 respectively. We also explore the impact on parameter constraints when varying the fiducial redshift errors, as well as using subsets of multi-tracer combinations or triangles with different squeezing factors. Note that the fiducial result here is not the final SPHEREx capability, which is still on target for being __FORMULA__ once the power spectrum will be included."}
{"title":"A dark siren measurement of the Hubble constant with the LIGO/Virgo gravitational wave event GW190412 and DESI galaxies","authors":["W. Ballard","A. Palmese","I. Maga\u00f1a Hernandez","S. BenZvi","J. Moon","A. J. Ross","G. Rossi","J. Aguilar","S. Ahlen","R. Blum","D. Brooks","T. Claybaugh","A. de la Macorra","A. Dey","P. Doel","J. E. Forero-Romero","S. Gontcho A Gontcho","K. Honscheid","A. Kremin","M. Manera","A. Meisner","R. Miquel","J. Moustakas","F. Prada","E. Sanchez","G. Tarl\u00e9","Z. Zhou"],"raw_abstract":"We present a measurement of the Hubble Constant $H_0$ using the gravitational\nwave event GW190412, an asymmetric binary black hole merger detected by\nLIGO/Virgo, as a dark standard siren. This event does not have an\nelectromagnetic counterpart, so we use the statistical standard siren method\nand marginalize over potential host galaxies from the Dark Energy Spectroscopic\nInstrument (DESI) survey. GW190412 is well-localized to 12 deg$^2$ (90%\ncredible interval), so it is promising for a dark siren analysis. The dark\nsiren value for $H_0=85.4_{-33.9}^{+29.1}$ km/s/Mpc, with a posterior shape\nthat is consistent with redshift overdensities. When combined with the bright\nstandard siren measurement from GW170817 we recover $H_0=77.96_{-5.03}^{+23.0}$\nkm/s/Mpc, consistent with both early and late-time Universe measurements of\n$H_0$. This work represents the first standard siren analysis performed with\nDESI data, and includes the most complete spectroscopic sample used in a dark\nsiren analysis to date.","publication_date":1700610799,"paper_link":"http://arxiv.org/pdf/2311.13062v1","categories":["Physics"],"abstract":"We present a measurement of the Hubble Constant __FORMULA__ using the gravitational wave event GW190412, an asymmetric binary black hole merger detected by LIGO/Virgo, as a dark standard siren. This event does not have an electromagnetic counterpart, so we use the statistical standard siren method and marginalize over potential host galaxies from the Dark Energy Spectroscopic Instrument (DESI) survey. GW190412 is well-localized to 12 deg__FORMULA__ (90% credible interval), so it is promising for a dark siren analysis. The dark siren value for __FORMULA__ km/s/Mpc, with a posterior shape that is consistent with redshift overdensities. When combined with the bright standard siren measurement from GW170817 we recover __FORMULA__ km/s/Mpc, consistent with both early and late-time Universe measurements of __FORMULA__. This work represents the first standard siren analysis performed with DESI data, and includes the most complete spectroscopic sample used in a dark siren analysis to date."}
{"title":"A note on estimating the dimension from a random geometric graph","authors":["Caelan Atamanchuk","Luc Devroye","Gabor Lugosi"],"raw_abstract":"Let $G_n$ be a random geometric graph with vertex set $[n]$ based on $n$\ni.i.d.\\ random vectors $X_1,\\ldots,X_n$ drawn from an unknown density $f$ on\n$\\R^d$. An edge $(i,j)$ is present when $\\|X_i -X_j\\| \\le r_n$, for a given\nthreshold $r_n$ possibly depending upon $n$, where $\\| \\cdot \\|$ denotes\nEuclidean distance. We study the problem of estimating the dimension $d$ of the\nunderlying space when we have access to the adjacency matrix of the graph but\ndo not know $r_n$ or the vectors $X_i$. The main result of the paper is that\nthere exists an estimator of $d$ that converges to $d$ in probability as $n \\to\n\\infty$ for all densities with $\\int f^5 < \\infty$ whenever $n^{3/2} r_n^d \\to\n\\infty$ and $r_n = o(1)$. The conditions allow very sparse graphs since when\n$n^{3/2} r_n^d \\to 0$, the graph contains isolated edges only, with high\nprobability. We also show that, without any condition on the density, a\nconsistent estimator of $d$ exists when $n r_n^d \\to \\infty$ and $r_n = o(1)$.","publication_date":1700610404,"paper_link":"http://arxiv.org/pdf/2311.13059v1","categories":["Mathematics","Statistics"],"abstract":"Let __FORMULA__ be a random geometric graph with vertex set __FORMULA__ based on __FORMULA__ i.i.d.\\ random vectors __FORMULA__ drawn from an unknown density __FORMULA__ on __FORMULA__. An edge __FORMULA__ is present when __FORMULA__, for a given threshold __FORMULA__ possibly depending upon __FORMULA__, where __FORMULA__ denotes Euclidean distance. We study the problem of estimating the dimension __FORMULA__ of the underlying space when we have access to the adjacency matrix of the graph but do not know __FORMULA__ or the vectors __FORMULA__. The main result of the paper is that there exists an estimator of __FORMULA__ that converges to __FORMULA__ in probability as __FORMULA__ for all densities with __FORMULA__ whenever __FORMULA__ and __FORMULA__. The conditions allow very sparse graphs since when __FORMULA__, the graph contains isolated edges only, with high probability. We also show that, without any condition on the density, a consistent estimator of __FORMULA__ exists when __FORMULA__ and __FORMULA__."}
{"title":"W-kernel and essential subspace for frequencist's evaluation of Bayesian estimators","authors":["Yukito Iba"],"raw_abstract":"The posterior covariance matrix W defined by the log-likelihood of each\nobservation plays important roles both in the sensitivity analysis and\nfrequencist's evaluation of the Bayesian estimators. This study focused on the\nmatrix W and its principal space; we term the latter as an essential subspace.\nFirst, it is shown that they appear in various statistical settings, such as\nthe evaluation of the posterior sensitivity, assessment of the frequencist's\nuncertainty from posterior samples, and stochastic expansion of the loss; a key\ntool to treat frequencist's properties is the recently proposed Bayesian\ninfinitesimal jackknife approximation (Giordano and Broderick (2023)). In the\nfollowing part, we show that the matrix W can be interpreted as a reproducing\nkernel; it is named as W-kernel. Using the W-kernel, the essential subspace is\nexpressed as a principal space given by the kernel PCA. A relation to the\nFisher kernel and neural tangent kernel is established, which elucidates the\nconnection to the classical asymptotic theory; it also leads to a sort of\nBayesian-frequencist's duality. Finally, two applications, selection of a\nrepresentative set of observations and dimensional reduction in the approximate\nbootstrap, are discussed. In the former, incomplete Cholesky decomposition is\nintroduced as an efficient method to compute the essential subspace. In the\nlatter, different implementations of the approximate bootstrap for posterior\nmeans are compared.","publication_date":1700603164,"paper_link":"http://arxiv.org/pdf/2311.13017v1","categories":["Physics","Statistics"],"abstract":"The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequencist's evaluation of the Bayesian estimators. This study focused on the matrix W and its principal space; we term the latter as an essential subspace. First, it is shown that they appear in various statistical settings, such as the evaluation of the posterior sensitivity, assessment of the frequencist's uncertainty from posterior samples, and stochastic expansion of the loss; a key tool to treat frequencist's properties is the recently proposed Bayesian infinitesimal jackknife approximation (Giordano and Broderick (2023)). In the following part, we show that the matrix W can be interpreted as a reproducing kernel; it is named as W-kernel. Using the W-kernel, the essential subspace is expressed as a principal space given by the kernel PCA. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory; it also leads to a sort of Bayesian-frequencist's duality. Finally, two applications, selection of a representative set of observations and dimensional reduction in the approximate bootstrap, are discussed. In the former, incomplete Cholesky decomposition is introduced as an efficient method to compute the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared."}
{"title":"Beyond Catoni: Sharper Rates for Heavy-Tailed and Robust Mean Estimation","authors":["Shivam Gupta","Samuel B. Hopkins","Eric Price"],"raw_abstract":"We study the fundamental problem of estimating the mean of a $d$-dimensional\ndistribution with covariance $\\Sigma \\preccurlyeq \\sigma^2 I_d$ given $n$\nsamples. When $d = 1$, Catoni \\cite{catoni} showed an estimator with error\n$(1+o(1)) \\cdot \\sigma \\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}$, with\nprobability $1 - \\delta$, matching the Gaussian error rate. For $d>1$, a\nnatural estimator outputs the center of the minimum enclosing ball of\none-dimensional confidence intervals to achieve a $1-\\delta$ confidence radius\nof $\\sqrt{\\frac{2 d}{d+1}} \\cdot \\sigma \\left(\\sqrt{\\frac{d}{n}} +\n\\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}\\right)$, incurring a\n$\\sqrt{\\frac{2d}{d+1}}$-factor loss over the Gaussian rate. When the\n$\\sqrt{\\frac{d}{n}}$ term dominates by a $\\sqrt{\\log \\frac{1}{\\delta}}$ factor,\n\\cite{lee2022optimal-highdim} showed an improved estimator matching the\nGaussian rate. This raises a natural question: is the Gaussian rate achievable\nin general? Or is the $\\sqrt{\\frac{2 d}{d+1}}$ loss \\emph{necessary} when the\n$\\sqrt{\\frac{2 \\log \\frac{1}{\\delta}}{n}}$ term dominates?\n  We show that the answer to both these questions is \\emph{no} -- we show that\n\\emph{some} constant-factor loss over the Gaussian rate is necessary, but\nconstruct an estimator that improves over the above naive estimator by a\nconstant factor. We also consider robust estimation, where an adversary is\nallowed to corrupt an $\\epsilon$-fraction of samples arbitrarily: in this case,\nwe show that the above strategy of combining one-dimensional estimates and\nincurring the $\\sqrt{\\frac{2d}{d+1}}$-factor \\emph{is} optimal in the\ninfinite-sample limit.","publication_date":1700602646,"paper_link":"http://arxiv.org/pdf/2311.13010v1","categories":["Mathematics","Statistics"],"abstract":"We study the fundamental problem of estimating the mean of a __FORMULA__-dimensional distribution with covariance __FORMULA__ given __FORMULA__ samples. When __FORMULA__, Catoni catoni showed an estimator with error __FORMULA__, with probability __FORMULA__, matching the Gaussian error rate. For __FORMULA__, a natural estimator outputs the center of the minimum enclosing ball of one-dimensional confidence intervals to achieve a __FORMULA__ confidence radius of __FORMULA__, incurring a __FORMULA__-factor loss over the Gaussian rate. When the __FORMULA__ term dominates by a __FORMULA__ factor, lee2022optimal-highdim showed an improved estimator matching the Gaussian rate. This raises a natural question: is the Gaussian rate achievable in general? Or is the __FORMULA__ loss necessary when the __FORMULA__ term dominates?   We show that the answer to both these questions is no -- we show that some constant-factor loss over the Gaussian rate is necessary, but construct an estimator that improves over the above naive estimator by a constant factor. We also consider robust estimation, where an adversary is allowed to corrupt an __FORMULA__-fraction of samples arbitrarily: in this case, we show that the above strategy of combining one-dimensional estimates and incurring the __FORMULA__-factor is optimal in the infinite-sample limit."}
{"title":"Secondary halo bias through cosmic time I: Scaling relations and the connection with the cosmic web","authors":["Andr\u00e9s Balaguera-Antol\u00ednez","Antonio D. Montero-Dorta","Ginevra Favole"],"raw_abstract":"We measure the signal of secondary halo bias as a function of a variety of\nintrinsic and environmental halo properties, and characterize its statistical\nsignificance as a function of cosmological redshift. Using fixed and paired\n$N$-body simulations of dark-matter halos -- the \\texttt{UNIT} simulation --\nwith masses above $10^{11}M_{\\odot}h^{-1}$ identified over a wide range of\ncosmological redshifts ($0<z<5$), we explore the behavior of the scaling\nrelations among different halo properties. We include novel environmental\nproperties based on the halo distribution as well as the underlying dark-matter\nfield. We implement an object-by-object estimator of large-scale effective bias\nand test its validity against standard approaches. With a bias assigned to each\ntracer, we perform a statistical analysis aiming at characterizing the\ndistribution of bias and the signal of secondary halo bias. We show how the\nhalo scaling relations linking direct probes of the halo potential well do not\ndepend on the environment. On the contrary, links between halo mass and the so\ncalled set of secondary halo properties are sensitive to the cosmological\nenvironment. We show that the signal of secondary bias derives statistically\nfrom secondary correlations beyond the standard link to halo mass. We show that\nthe secondary bias arise through non-local and/or environmental properties\nrelated either to the halo distribution or to the properties of the underlying\ndark-matter field. Properties such as the tidal field and the local Mach number\ngenerates the signals of secondary bias with the highest significance. We\npropose applications of the assignment of individual bias for the generation of\nmock catalogs containing the signal of secondary bias, as well as a series of\ncosmological analyses aiming at mining large galaxy data sets.","publication_date":1700599940,"paper_link":"http://arxiv.org/pdf/2311.12991v1","categories":["Physics"],"abstract":"We measure the signal of secondary halo bias as a function of a variety of intrinsic and environmental halo properties, and characterize its statistical significance as a function of cosmological redshift. Using fixed and paired __FORMULA__-body simulations of dark-matter halos -- the UNIT simulation -- with masses above __FORMULA__ identified over a wide range of cosmological redshifts (__FORMULA__), we explore the behavior of the scaling relations among different halo properties. We include novel environmental properties based on the halo distribution as well as the underlying dark-matter field. We implement an object-by-object estimator of large-scale effective bias and test its validity against standard approaches. With a bias assigned to each tracer, we perform a statistical analysis aiming at characterizing the distribution of bias and the signal of secondary halo bias. We show how the halo scaling relations linking direct probes of the halo potential well do not depend on the environment. On the contrary, links between halo mass and the so called set of secondary halo properties are sensitive to the cosmological environment. We show that the signal of secondary bias derives statistically from secondary correlations beyond the standard link to halo mass. We show that the secondary bias arise through non-local and/or environmental properties related either to the halo distribution or to the properties of the underlying dark-matter field. Properties such as the tidal field and the local Mach number generates the signals of secondary bias with the highest significance. We propose applications of the assignment of individual bias for the generation of mock catalogs containing the signal of secondary bias, as well as a series of cosmological analyses aiming at mining large galaxy data sets."}
{"title":"A Controlled Study on Evaluation of Thermal Stimulation Influence on Affective Measures of Uninformed Individuals","authors":["Mehdi Hojatmadani","Samantha Shepard","Kristen Salomon","Kyle Reed"],"raw_abstract":"Although the relationship between temperature and emotional states has been\ninvestigated in the field of haptics, it remains unknown if, or in what\ndirection, temperature affects emotional states. We approach this question at\nthe intersection of haptics and psychology using a custom-built thermal device\nand emotional responses based on photos from the International Affective\nPicture System (IAPS) library. Unlike past research, this study incorporates\ndeception and a control (i.e., neutral temperature) condition. One hundred and\ntwenty naive subjects reported their emotional responses to fifty-six images\nvarying on normative arousal and valence ratings while being exposed to a\ncool~(30{\\deg}C), neutral (33{\\deg}C), or warm (36{\\deg}C) temperature applied\nto the upper back. Participants exposed to warm temperatures reported higher\narousal ratings in some image categories than participants exposed to neutral\nor cool temperatures. Valence ratings were decreased in warm conditions\ncompared to neutral conditions. The emotion wheel was used as a complementary\nmethod of affective response measurement, and exploratory analysis methods were\nimplemented. Although the valence and arousal showed statistical significance,\nthe emotion wheel results did not demonstrate any significant differences\nbetween the temperature conditions.","publication_date":1700599800,"paper_link":"http://arxiv.org/pdf/2311.12989v1","categories":["Statistics"],"abstract":"Although the relationship between temperature and emotional states has been investigated in the field of haptics, it remains unknown if, or in what direction, temperature affects emotional states. We approach this question at the intersection of haptics and psychology using a custom-built thermal device and emotional responses based on photos from the International Affective Picture System (IAPS) library. Unlike past research, this study incorporates deception and a control (i.e., neutral temperature) condition. One hundred and twenty naive subjects reported their emotional responses to fifty-six images varying on normative arousal and valence ratings while being exposed to a cool~(30{\\deg}C), neutral (33{\\deg}C), or warm (36{\\deg}C) temperature applied to the upper back. Participants exposed to warm temperatures reported higher arousal ratings in some image categories than participants exposed to neutral or cool temperatures. Valence ratings were decreased in warm conditions compared to neutral conditions. The emotion wheel was used as a complementary method of affective response measurement, and exploratory analysis methods were implemented. Although the valence and arousal showed statistical significance, the emotion wheel results did not demonstrate any significant differences between the temperature conditions."}
{"title":"An $\\mathcal{O}(\\log_2N)$ SMC$^2$ Algorithm on Distributed Memory with an Approx. Optimal L-Kernel","authors":["Conor Rosato","Alessandro Varsi","Joshua Murphy","Simon Maskell"],"raw_abstract":"Calibrating statistical models using Bayesian inference often requires both\naccurate and timely estimates of parameters of interest. Particle Markov Chain\nMonte Carlo (p-MCMC) and Sequential Monte Carlo Squared (SMC$^2$) are two\nmethods that use an unbiased estimate of the log-likelihood obtained from a\nparticle filter (PF) to evaluate the target distribution. P-MCMC constructs a\nsingle Markov chain which is sequential by nature so cannot be readily\nparallelized using Distributed Memory (DM) architectures. This is in contrast\nto SMC$^2$ which includes processes, such as importance sampling, that are\ndescribed as \\textit{embarrassingly parallel}. However, difficulties arise when\nattempting to parallelize resampling. None-the-less, the choice of backward\nkernel, recycling scheme and compatibility with DM architectures makes SMC$^2$\nan attractive option when compared with p-MCMC. In this paper, we present an\nSMC$^2$ framework that includes the following features: an optimal (in terms of\ntime complexity) $\\mathcal{O}(\\log_2N)$ parallelization for DM architectures,\nan approximately optimal (in terms of accuracy) backward kernel, and an\nefficient recycling scheme. On a cluster of $128$ DM processors, the results on\na biomedical application show that SMC$^2$ achieves up to a $70\\times$ speed-up\nvs its sequential implementation. It is also more accurate and roughly\n$54\\times$ faster than p-MCMC. A GitHub link is given which provides access to\nthe code.","publication_date":1700598230,"paper_link":"http://arxiv.org/pdf/2311.12973v1","categories":["Statistics"],"abstract":"Calibrating statistical models using Bayesian inference often requires both accurate and timely estimates of parameters of interest. Particle Markov Chain Monte Carlo (p-MCMC) and Sequential Monte Carlo Squared (SMC__FORMULA__) are two methods that use an unbiased estimate of the log-likelihood obtained from a particle filter (PF) to evaluate the target distribution. P-MCMC constructs a single Markov chain which is sequential by nature so cannot be readily parallelized using Distributed Memory (DM) architectures. This is in contrast to SMC__FORMULA__ which includes processes, such as importance sampling, that are described as embarrassingly parallel. However, difficulties arise when attempting to parallelize resampling. None-the-less, the choice of backward kernel, recycling scheme and compatibility with DM architectures makes SMC__FORMULA__ an attractive option when compared with p-MCMC. In this paper, we present an SMC__FORMULA__ framework that includes the following features: an optimal (in terms of time complexity) __FORMULA__ parallelization for DM architectures, an approximately optimal (in terms of accuracy) backward kernel, and an efficient recycling scheme. On a cluster of __FORMULA__ DM processors, the results on a biomedical application show that SMC__FORMULA__ achieves up to a __FORMULA__ speed-up vs its sequential implementation. It is also more accurate and roughly __FORMULA__ faster than p-MCMC. A GitHub link is given which provides access to the code."}
{"title":"Clustered Policy Decision Ranking","authors":["Mark Levin","Hana Chockler"],"raw_abstract":"Policies trained via reinforcement learning (RL) are often very complex even\nfor simple tasks. In an episode with n time steps, a policy will make n\ndecisions on actions to take, many of which may appear non-intuitive to the\nobserver. Moreover, it is not clear which of these decisions directly\ncontribute towards achieving the reward and how significant their contribution\nis. Given a trained policy, we propose a black-box method based on statistical\ncovariance estimation that clusters the states of the environment and ranks\neach cluster according to the importance of decisions made in its states. We\ncompare our measure against a previous statistical fault localization based\nranking procedure.","publication_date":1700597762,"paper_link":"http://arxiv.org/pdf/2311.12970v1","categories":["Statistics"],"abstract":"Policies trained via reinforcement learning (RL) are often very complex even for simple tasks. In an episode with n time steps, a policy will make n decisions on actions to take, many of which may appear non-intuitive to the observer. Moreover, it is not clear which of these decisions directly contribute towards achieving the reward and how significant their contribution is. Given a trained policy, we propose a black-box method based on statistical covariance estimation that clusters the states of the environment and ranks each cluster according to the importance of decisions made in its states. We compare our measure against a previous statistical fault localization based ranking procedure."}
{"title":"Bit Error Rate Performance and Diversity Analysis for Mediumband Wireless Communication","authors":["Dushyantha A Basnayaka","Jiabin Jia"],"raw_abstract":"Mediumband wireless communication refers to wireless communication through a\nclass of channels known as mediumband that exists on the TmTs-plane. This\npaper, through statistical analysis and computer simulations, studies the\nperformance limits of this class of channels in terms of uncoded bit error rate\n(BER) and diversity order. We show that, owing mainly to the effect of the deep\nfading avoidance, which is unique to the channels in the mediumband region,\nmediumband wireless systems, if designed judiciously, have the potential to\nachieve significantly superior error rate and higher order diversity even in\nnon-line-of-sight (NLoS) propagation environments where the achievable\ndiversity order is otherwise low.","publication_date":1700597654,"paper_link":"http://arxiv.org/pdf/2311.12968v1","categories":["Mathematics","Electrical Engineering and Systems Science"],"abstract":"Mediumband wireless communication refers to wireless communication through a class of channels known as mediumband that exists on the TmTs-plane. This paper, through statistical analysis and computer simulations, studies the performance limits of this class of channels in terms of uncoded bit error rate (BER) and diversity order. We show that, owing mainly to the effect of the deep fading avoidance, which is unique to the channels in the mediumband region, mediumband wireless systems, if designed judiciously, have the potential to achieve significantly superior error rate and higher order diversity even in non-line-of-sight (NLoS) propagation environments where the achievable diversity order is otherwise low."}
{"title":"Similarity Means: A Study on Stability and Symmetry","authors":["Gonzalo Travieso","Luciando da F. Costa"],"raw_abstract":"The arithmetic mean plays a central role in science and technology, being\ndirectly related to the concepts of statistical expectance and centrality. Yet,\nit is highly susceptible to the presence of ouliers or biased interference in\nthe original dataset to which it is applied. Described recently, the concept of\nsimilarity means has been preliminary found to have marked robustness to those\nsame effects, especially when adopting the Jaccard similarity index. The\npresent work is aimed at investigating further the properties of similarity\nmeans, especially regarding their range, translating and scaling properties,\nsensitivity and robustness to outliers. Several interesting contributions are\nreported, including an effective algorithm for obtaining the similarity mean,\nthe analytic and experimental identification of a number of properties, as well\nas the confirmation of the potential stability of the similarity mean to the\npresence of outliers. The present work also describes an application\ncase-example in which the Jaccard similarity is succesfully employed to study\ncycles of sunspots, with interesting results.","publication_date":1700596484,"paper_link":"http://arxiv.org/pdf/2311.12959v1","categories":["Physics"],"abstract":"The arithmetic mean plays a central role in science and technology, being directly related to the concepts of statistical expectance and centrality. Yet, it is highly susceptible to the presence of ouliers or biased interference in the original dataset to which it is applied. Described recently, the concept of similarity means has been preliminary found to have marked robustness to those same effects, especially when adopting the Jaccard similarity index. The present work is aimed at investigating further the properties of similarity means, especially regarding their range, translating and scaling properties, sensitivity and robustness to outliers. Several interesting contributions are reported, including an effective algorithm for obtaining the similarity mean, the analytic and experimental identification of a number of properties, as well as the confirmation of the potential stability of the similarity mean to the presence of outliers. The present work also describes an application case-example in which the Jaccard similarity is succesfully employed to study cycles of sunspots, with interesting results."}
{"title":"Effective (moderate) random RPF theorems and applications to limit theorems for non-uniformly expanding RDS","authors":["Yeor Hafouta"],"raw_abstract":"We prove central limit theorems and related results for non-uniformly\nexpanding random dynamical systems and general random Gibbs measures under some\nintegrability conditions and some (relatively weak) mixing related assumptions\non the base map. Some of our conditions also apply to maps which only expand on\naverage, in an appropriate sense. This is the first set of conditions that\nallows to treat general non-uniformly expanding cases. A crucial step in our\nproofs can be viewed as effective moderate versions of the random\nRuelle-Perron-Frobenius (RPF) theorem for the random transfer operator cocycle\ncorresponding to the random Gibbs measures (namely, effective moderate\nestimates on the rate of \"convergence\" to the projection on the top Lyapunov\nspace in the corresponding multiplicative ergodic theorem). As a byproduct of\nour methods we are also able to prove statistical properties of the\ncorresponding skew products and some estimates on the tails of random mixing\ntimes. Compared with recent progress [38] on the subject, we are able to treat\nrandom Gibbs measures corresponding to potentials which do not necessarily have\nsmall local oscillation along inverse branches. Moreover, we are able to\nconsider more general classes of random expanding maps with unbounded\nmixing/covering times and (only) local pairing property. Thus, our results\napply to a wide family of C^2-random expanding maps on compact C^2 manifolds,\ntogether with the random geometric potential which corresponds to the unique\nequivariant family of measures which are absolutely continuous with respect to\nthe volume measure. Other applications include high dimensional piecewise\nexpanding maps and random subshifts of finite type, which have applications to\nrandom Markov interval maps and finite state Markov chains in random dynamical\nenvironment.","publication_date":1700595018,"paper_link":"http://arxiv.org/pdf/2311.12950v1","categories":["Mathematics"],"abstract":"We prove central limit theorems and related results for non-uniformly expanding random dynamical systems and general random Gibbs measures under some integrability conditions and some (relatively weak) mixing related assumptions on the base map. Some of our conditions also apply to maps which only expand on average, in an appropriate sense. This is the first set of conditions that allows to treat general non-uniformly expanding cases. A crucial step in our proofs can be viewed as effective moderate versions of the random Ruelle-Perron-Frobenius (RPF) theorem for the random transfer operator cocycle corresponding to the random Gibbs measures (namely, effective moderate estimates on the rate of \"convergence\" to the projection on the top Lyapunov space in the corresponding multiplicative ergodic theorem). As a byproduct of our methods we are also able to prove statistical properties of the corresponding skew products and some estimates on the tails of random mixing times. Compared with recent progress [38] on the subject, we are able to treat random Gibbs measures corresponding to potentials which do not necessarily have small local oscillation along inverse branches. Moreover, we are able to consider more general classes of random expanding maps with unbounded mixing/covering times and (only) local pairing property. Thus, our results apply to a wide family of C^2-random expanding maps on compact C^2 manifolds, together with the random geometric potential which corresponds to the unique equivariant family of measures which are absolutely continuous with respect to the volume measure. Other applications include high dimensional piecewise expanding maps and random subshifts of finite type, which have applications to random Markov interval maps and finite state Markov chains in random dynamical environment."}
{"title":"Optimality in Mean Estimation: Beyond Worst-Case, Beyond Sub-Gaussian, and Beyond $1+\u03b1$ Moments","authors":["Trung Dang","Jasper C. H. Lee","Maoyuan Song","Paul Valiant"],"raw_abstract":"There is growing interest in improving our algorithmic understanding of\nfundamental statistical problems such as mean estimation, driven by the goal of\nunderstanding the limits of what we can extract from valuable data. The state\nof the art results for mean estimation in $\\mathbb{R}$ are 1) the optimal\nsub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant for\nall distributions with finite but unknown variance, and 2) the analysis of the\nmedian-of-means algorithm by [BCL13] and a lower bound by [DLLO16],\ncharacterizing the big-O optimal errors for distributions for which only a\n$1+\\alpha$ moment exists for $\\alpha \\in (0,1)$. Both results, however, are\noptimal only in the worst case. We initiate the fine-grained study of the mean\nestimation problem: Can algorithms leverage useful features of the input\ndistribution to beat the sub-Gaussian rate, without explicit knowledge of such\nfeatures?\n  We resolve this question with an unexpectedly nuanced answer: \"Yes in limited\nregimes, but in general no\". For any distribution $p$ with a finite mean, we\nconstruct a distribution $q$ whose mean is well-separated from $p$'s, yet $p$\nand $q$ are not distinguishable with high probability, and $q$ further\npreserves $p$'s moments up to constants. The main consequence is that no\nreasonable estimator can asymptotically achieve better than the sub-Gaussian\nerror rate for any distribution, matching the worst-case result of [LV22]. More\ngenerally, we introduce a new definitional framework to analyze the\nfine-grained optimality of algorithms, which we call \"neighborhood optimality\",\ninterpolating between the unattainably strong \"instance optimality\" and the\ntrivially weak \"admissibility\" definitions. Applying the new framework, we show\nthat median-of-means is neighborhood optimal, up to constant factors. It is\nopen to find a neighborhood-optimal estimator without constant factor\nslackness.","publication_date":1700592638,"paper_link":"http://arxiv.org/pdf/2311.12784v1","categories":["Mathematics","Statistics"],"abstract":"There is growing interest in improving our algorithmic understanding of fundamental statistical problems such as mean estimation, driven by the goal of understanding the limits of what we can extract from valuable data. The state of the art results for mean estimation in __FORMULA__ are 1) the optimal sub-Gaussian mean estimator by [LV22], with the tight sub-Gaussian constant for all distributions with finite but unknown variance, and 2) the analysis of the median-of-means algorithm by [BCL13] and a lower bound by [DLLO16], characterizing the big-O optimal errors for distributions for which only a __FORMULA__ moment exists for __FORMULA__. Both results, however, are optimal only in the worst case. We initiate the fine-grained study of the mean estimation problem: Can algorithms leverage useful features of the input distribution to beat the sub-Gaussian rate, without explicit knowledge of such features?   We resolve this question with an unexpectedly nuanced answer: \"Yes in limited regimes, but in general no\". For any distribution __FORMULA__ with a finite mean, we construct a distribution __FORMULA__ whose mean is well-separated from __FORMULA__'s, yet __FORMULA__ and __FORMULA__ are not distinguishable with high probability, and __FORMULA__ further preserves __FORMULA__'s moments up to constants. The main consequence is that no reasonable estimator can asymptotically achieve better than the sub-Gaussian error rate for any distribution, matching the worst-case result of [LV22]. More generally, we introduce a new definitional framework to analyze the fine-grained optimality of algorithms, which we call \"neighborhood optimality\", interpolating between the unattainably strong \"instance optimality\" and the trivially weak \"admissibility\" definitions. Applying the new framework, we show that median-of-means is neighborhood optimal, up to constant factors. It is open to find a neighborhood-optimal estimator without constant factor slackness."}
{"title":"Estimating time of arrival of vehicle fleets with GCN based traffic prediction","authors":["Shivika Sharma","Nandini Mawane","Dhruthick Gowda M","Mayur Taware","Chetan Kumar","Yash Chandrashekhar Dixit","Rakshit Ramesh"],"raw_abstract":"This paper presents an effective framework for estimating time of arrival of\nvehicles (buses) in an Intelligent Transit Management System (ITMS) having\nsparse position updates. Our contributions towards this is firstly in\nimplementing a constrained optimization based road linestring segmenting\nframework ensuring ideal segment lengths and segments with sufficient density\nof vehicle position measurements which will result in valid statistics for\nscenarios involving sparse position measurements. Over this we propose a\ncomprehensive approach for predicting traffic delays and estimated time of\nvehicle arrival addressing both the spatial and temporal dependencies of\ntraffic. The traffic delay model is built on top of the T-GCN architecture on\nwhich we optimally augment an adjacency matrix which models a complexly\nconnected road network considering the degree of influence between road\nsegments, enabling the traffic delay model to look beyond physical road\nconnectivity in predicting traffic delays and therefore producing better\nestimates of arrival times to points along the designated route of the\nvehicles.","publication_date":1700590202,"paper_link":"http://arxiv.org/pdf/2311.12758v1","categories":["Electrical Engineering and Systems Science"],"abstract":"This paper presents an effective framework for estimating time of arrival of vehicles (buses) in an Intelligent Transit Management System (ITMS) having sparse position updates. Our contributions towards this is firstly in implementing a constrained optimization based road linestring segmenting framework ensuring ideal segment lengths and segments with sufficient density of vehicle position measurements which will result in valid statistics for scenarios involving sparse position measurements. Over this we propose a comprehensive approach for predicting traffic delays and estimated time of vehicle arrival addressing both the spatial and temporal dependencies of traffic. The traffic delay model is built on top of the T-GCN architecture on which we optimally augment an adjacency matrix which models a complexly connected road network considering the degree of influence between road segments, enabling the traffic delay model to look beyond physical road connectivity in predicting traffic delays and therefore producing better estimates of arrival times to points along the designated route of the vehicles."}
{"title":"Phylogenetic least squares estimation without genetic distances","authors":["Peter B. Chi","Volodymyr M. Minin"],"raw_abstract":"Least squares estimation of phylogenies is an established family of methods\nwith good statistical properties. State-of-the-art least squares phylogenetic\nestimation proceeds by first estimating a distance matrix, which is then used\nto determine the phylogeny by minimizing a squared-error loss function. Here,\nwe develop a method for least squares phylogenetic inference that does not rely\non a pre-estimated distance matrix. Our approach allows us to circumvent the\ntypical need to first estimate a distance matrix by forming a new loss function\ninspired by the phylogenetic likelihood score function; in this manner,\ninference is not based on a summary statistic of the sequence data, but\ndirectly on the sequence data itself. We use a Jukes-Cantor substitution model\nto show that our method leads to improvements over ordinary least squares\nphylogenetic inference, and is even observed to rival maximum likelihood\nestimation in terms of topology estimation efficiency. Using a Kimura\n2-parameter model, we show that our method also allows for estimation of the\nglobal transition/transversion ratio simultaneously with the phylogeny and its\nbranch lengths. This is impossible to accomplish with any other distance-based\nmethod as far as we know. Our developments pave the way for more optimal\nphylogenetic inference under the least squares framework, particularly in\nsettings under which likelihood-based inference is infeasible, including when\none desires to build a phylogeny based on information provided by only a subset\nof all possible nucleotide substitutions such as synonymous or non-synonymous\nsubstitutions.","publication_date":1700585059,"paper_link":"http://arxiv.org/pdf/2311.12717v1","categories":["Quantitative Biology","Statistics"],"abstract":"Least squares estimation of phylogenies is an established family of methods with good statistical properties. State-of-the-art least squares phylogenetic estimation proceeds by first estimating a distance matrix, which is then used to determine the phylogeny by minimizing a squared-error loss function. Here, we develop a method for least squares phylogenetic inference that does not rely on a pre-estimated distance matrix. Our approach allows us to circumvent the typical need to first estimate a distance matrix by forming a new loss function inspired by the phylogenetic likelihood score function; in this manner, inference is not based on a summary statistic of the sequence data, but directly on the sequence data itself. We use a Jukes-Cantor substitution model to show that our method leads to improvements over ordinary least squares phylogenetic inference, and is even observed to rival maximum likelihood estimation in terms of topology estimation efficiency. Using a Kimura 2-parameter model, we show that our method also allows for estimation of the global transition/transversion ratio simultaneously with the phylogeny and its branch lengths. This is impossible to accomplish with any other distance-based method as far as we know. Our developments pave the way for more optimal phylogenetic inference under the least squares framework, particularly in settings under which likelihood-based inference is infeasible, including when one desires to build a phylogeny based on information provided by only a subset of all possible nucleotide substitutions such as synonymous or non-synonymous substitutions."}
{"title":"Human perceptual decision making of nonequilibrium fluctuations","authors":["Ayb\u00fcke Durmaz","Yonathan Sarmiento","Gianfranco Fortunato","Debraj Das","Mathew Ernst Diamond","Domenica Bueti","\u00c9dgar Rold\u00e1n"],"raw_abstract":"Perceptual decision-making frequently requires making rapid, reliable choices\nupon encountering noisy sensory inputs. To better define the statistical\nprocesses underlying perceptual decision-making, here we characterize the\nchoices of human participants visualizing a system of nonequilibrium stationary\nphysical dynamics and compare such choices to the performance of an optimal\nagent computing Wald's sequential probability ratio test (SPRT). Participants\nviewed movies of a particle endowed with drifted Brownian dynamics and had to\njudge the motion as leftward or rightward. Overall, the results uncovered\nfundamental performance limits, consistent with recently established\nthermodynamic trade-offs involving speed, accuracy, and dissipation.\nSpecifically, decision times are sensitive to entropy production rates.\nMoreover, to achieve a given level of observed accuracy, participants require\nmore time than predicted by SPRT, indicating suboptimal integration of\navailable information. In view of such suboptimality, we develop an alternative\naccount based on evidence integration with a memory time constant. Setting the\ntime constant proportionately to the deviation from equilibrium in the stimuli\nsignificantly improved trial-by-trial predictions of decision metrics with\nrespect to SPRT. This study shows that perceptual psychophysics using stimuli\nrooted in nonequilibrium physical processes provides a robust platform for\nunderstanding how the brain takes decisions on stochastic information inputs.","publication_date":1700582113,"paper_link":"http://arxiv.org/pdf/2311.12692v2","categories":["Physics"],"abstract":"Perceptual decision-making frequently requires making rapid, reliable choices upon encountering noisy sensory inputs. To better define the statistical processes underlying perceptual decision-making, here we characterize the choices of human participants visualizing a system of nonequilibrium stationary physical dynamics and compare such choices to the performance of an optimal agent computing Wald's sequential probability ratio test (SPRT). Participants viewed movies of a particle endowed with drifted Brownian dynamics and had to judge the motion as leftward or rightward. Overall, the results uncovered fundamental performance limits, consistent with recently established thermodynamic trade-offs involving speed, accuracy, and dissipation. Specifically, decision times are sensitive to entropy production rates. Moreover, to achieve a given level of observed accuracy, participants require more time than predicted by SPRT, indicating suboptimal integration of available information. In view of such suboptimality, we develop an alternative account based on evidence integration with a memory time constant. Setting the time constant proportionately to the deviation from equilibrium in the stimuli significantly improved trial-by-trial predictions of decision metrics with respect to SPRT. This study shows that perceptual psychophysics using stimuli rooted in nonequilibrium physical processes provides a robust platform for understanding how the brain takes decisions on stochastic information inputs."}
{"title":"Virtual potential created by a feedback loop: taming the feedback demon to explore stochastic thermodynamics of underdamped systems","authors":["Salamb\u00f4 Dago","Nicolas Barros","Jorge Pereda","Sergio Ciliberto","Ludovic Bellon"],"raw_abstract":"Virtual potentials are an elegant, precise and flexible tool to manipulate\nsmall systems and explore fundamental questions in stochastic thermodynamics.\nIn particular double-well potentials have applications in information\nprocessing, such as the demonstration of Landauer's principle. In this chapter,\nwe detail the implementation of a feedback loop for an underdamped system, in\norder to build a tunable virtual double-well potential. This feedback behaves\nas a demon acting on the system depending on the outcome of a continuously\nrunning measurement. It can thus modify the energy exchanges with the\nthermostat and create an out-of-equilibrium state. To create a bi-stable\npotential, the feedback consists only in switching an external force between\ntwo steady values when the measured position crosses a threshold. We show that\na small delay of the feedback loop in the switches between the two wells\nresults in a modified velocity distribution. The latter can be interpreted as a\ncooling of the kinetic temperature of the system. Using a fast digital\nfeedback, we successfully address all experimental issues to create a virtual\npotential that is statistically indistinguishable from a physical one, with a\ntunable barrier height and energy step between the two wells.","publication_date":1700581782,"paper_link":"http://arxiv.org/pdf/2311.12687v1","categories":["Physics"],"abstract":"Virtual potentials are an elegant, precise and flexible tool to manipulate small systems and explore fundamental questions in stochastic thermodynamics. In particular double-well potentials have applications in information processing, such as the demonstration of Landauer's principle. In this chapter, we detail the implementation of a feedback loop for an underdamped system, in order to build a tunable virtual double-well potential. This feedback behaves as a demon acting on the system depending on the outcome of a continuously running measurement. It can thus modify the energy exchanges with the thermostat and create an out-of-equilibrium state. To create a bi-stable potential, the feedback consists only in switching an external force between two steady values when the measured position crosses a threshold. We show that a small delay of the feedback loop in the switches between the two wells results in a modified velocity distribution. The latter can be interpreted as a cooling of the kinetic temperature of the system. Using a fast digital feedback, we successfully address all experimental issues to create a virtual potential that is statistically indistinguishable from a physical one, with a tunable barrier height and energy step between the two wells."}
{"title":"An entanglement asymmetry study of black hole radiation","authors":["Filiberto Ares","Sara Murciano","Lorenzo Piroli","Pasquale Calabrese"],"raw_abstract":"Hawking discovery that black holes can evaporate through radiation emission\nhas posed a number of questions that with time became fundamental hallmarks for\na quantum theory of gravity. The most famous one is likely the information\nparadox, which finds an elegant explanation in the Page argument suggesting\nthat a black hole and its radiation can be effectively represented by a random\nstate of qubits. Leveraging the same assumption, we ponder the extent to which\na black hole may display emergent symmetries, employing the entanglement\nasymmetry as a modern, information-based indicator of symmetry breaking. We\nfind that for a random state devoid of any symmetry, a $U(1)$ symmetry emerges\nand it is exact in the thermodynamic limit before the Page time. At the Page\ntime, the entanglement asymmetry shows a finite jump to a large value. Our\nfindings imply that the emitted radiation is symmetric up to the Page time and\nthen undergoes a sharp transition. Conversely the black hole is symmetric only\nafter the Page time.","publication_date":1700581220,"paper_link":"http://arxiv.org/pdf/2311.12683v1","categories":["Physics"],"abstract":"Hawking discovery that black holes can evaporate through radiation emission has posed a number of questions that with time became fundamental hallmarks for a quantum theory of gravity. The most famous one is likely the information paradox, which finds an elegant explanation in the Page argument suggesting that a black hole and its radiation can be effectively represented by a random state of qubits. Leveraging the same assumption, we ponder the extent to which a black hole may display emergent symmetries, employing the entanglement asymmetry as a modern, information-based indicator of symmetry breaking. We find that for a random state devoid of any symmetry, a __FORMULA__ symmetry emerges and it is exact in the thermodynamic limit before the Page time. At the Page time, the entanglement asymmetry shows a finite jump to a large value. Our findings imply that the emitted radiation is symmetric up to the Page time and then undergoes a sharp transition. Conversely the black hole is symmetric only after the Page time."}
{"title":"The DURel Annotation Tool: Human and Computational Measurement of Semantic Proximity, Sense Clusters and Semantic Change","authors":["Dominik Schlechtweg","Shafqat Mumtaz Virk","Pauline Sander","Emma Sk\u00f6ldberg","Lukas Theuer Linke","Tuo Zhang","Nina Tahmasebi","Jonas Kuhn","Sabine Schulte im Walde"],"raw_abstract":"We present the DURel tool that implements the annotation of semantic\nproximity between uses of words into an online, open source interface. The tool\nsupports standardized human annotation as well as computational annotation,\nbuilding on recent advances with Word-in-Context models. Annotator judgments\nare clustered with automatic graph clustering techniques and visualized for\nanalysis. This allows to measure word senses with simple and intuitive\nmicro-task judgments between use pairs, requiring minimal preparation efforts.\nThe tool offers additional functionalities to compare the agreement between\nannotators to guarantee the inter-subjectivity of the obtained judgments and to\ncalculate summary statistics giving insights into sense frequency\ndistributions, semantic variation or changes of senses over time.","publication_date":1700579694,"paper_link":"http://arxiv.org/pdf/2311.12664v1","categories":["Statistics"],"abstract":"We present the DURel tool that implements the annotation of semantic proximity between uses of words into an online, open source interface. The tool supports standardized human annotation as well as computational annotation, building on recent advances with Word-in-Context models. Annotator judgments are clustered with automatic graph clustering techniques and visualized for analysis. This allows to measure word senses with simple and intuitive micro-task judgments between use pairs, requiring minimal preparation efforts. The tool offers additional functionalities to compare the agreement between annotators to guarantee the inter-subjectivity of the obtained judgments and to calculate summary statistics giving insights into sense frequency distributions, semantic variation or changes of senses over time."}
{"title":"On $q$-Order Statistics","authors":["Malvina Vamvakari"],"raw_abstract":"Building on the notion of $q$-integral introduced by Thomae in 1869, we\nintroduce $q$-order statistics (that, is $q$-analogues of the classical order\nstatistics, for $0<q<1$) which arise from dependent and not identically\ndistributed $q$-continuous random variables and study their distributional\nproperties. We study the $q$-distribution functions and the $q$-density\nfunctions of the relative $q$-ordered random variables. We focus on $q$-ordered\nvariables arising from dependent and not identically $q$-uniformly distributed\nrandom variables and we derive their $q$-distributions, including $q$-power\nlaw, $q$-beta and $q$-Dirichlet distributions.","publication_date":1700576871,"paper_link":"http://arxiv.org/pdf/2311.12634v1","categories":["Mathematics","Statistics"],"abstract":"Building on the notion of __FORMULA__-integral introduced by Thomae in 1869, we introduce __FORMULA__-order statistics (that, is __FORMULA__-analogues of the classical order statistics, for __FORMULA__) which arise from dependent and not identically distributed __FORMULA__-continuous random variables and study their distributional properties. We study the __FORMULA__-distribution functions and the __FORMULA__-density functions of the relative __FORMULA__-ordered random variables. We focus on __FORMULA__-ordered variables arising from dependent and not identically __FORMULA__-uniformly distributed random variables and we derive their __FORMULA__-distributions, including __FORMULA__-power law, __FORMULA__-beta and __FORMULA__-Dirichlet distributions."}
{"title":"Exact solution to quantum dynamical activity","authors":["Tomohiro Nishiyama","Yoshihiko Hasegawa"],"raw_abstract":"The quantum dynamical activity constitutes a thermodynamic cost in trade-off\nrelations such as the quantum speed limit and the quantum thermodynamic\nuncertainty relation. However, calculating the quantum dynamical activity has\nbeen a challenge and its exact solution has not been hitherto derived. In this\nLetter, we present the exact solution to the quantum dynamical activity,\ndeploying the continuous matrix product state method. Moreover, using the\nderived exact solution, we find the upper bound to the dynamical activity,\nwhich comprises the standard deviation of the system Hamiltonian and the jump\noperators. We confirm the exact solution and the upper bound by performing\nnumerical simulations.","publication_date":1700576555,"paper_link":"http://arxiv.org/pdf/2311.12627v1","categories":["Physics"],"abstract":"The quantum dynamical activity constitutes a thermodynamic cost in trade-off relations such as the quantum speed limit and the quantum thermodynamic uncertainty relation. However, calculating the quantum dynamical activity has been a challenge and its exact solution has not been hitherto derived. In this Letter, we present the exact solution to the quantum dynamical activity, deploying the continuous matrix product state method. Moreover, using the derived exact solution, we find the upper bound to the dynamical activity, which comprises the standard deviation of the system Hamiltonian and the jump operators. We confirm the exact solution and the upper bound by performing numerical simulations."}
{"title":"Bridging Algorithmic Information Theory and Machine Learning: A New Approach to Kernel Learning","authors":["Boumediene Hamzi","Marcus Hutter","Houman Owhadi"],"raw_abstract":"Machine Learning (ML) and Algorithmic Information Theory (AIT) look at\nComplexity from different points of view. We explore the interface between AIT\nand Kernel Methods (that are prevalent in ML) by adopting an AIT perspective on\nthe problem of learning kernels from data, in kernel ridge regression, through\nthe method of Sparse Kernel Flows. In particular, by looking at the differences\nand commonalities between Minimal Description Length (MDL) and Regularization\nin Machine Learning (RML), we prove that the method of Sparse Kernel Flows is\nthe natural approach to adopt to learn kernels from data. This paper shows that\nit is not necessary to use the statistical route to derive Sparse Kernel Flows\nand that one can directly work with code-lengths and complexities that are\nconcepts that show up in AIT.","publication_date":1700576308,"paper_link":"http://arxiv.org/pdf/2311.12624v1","categories":["Mathematics","Statistics"],"abstract":"Machine Learning (ML) and Algorithmic Information Theory (AIT) look at Complexity from different points of view. We explore the interface between AIT and Kernel Methods (that are prevalent in ML) by adopting an AIT perspective on the problem of learning kernels from data, in kernel ridge regression, through the method of Sparse Kernel Flows. In particular, by looking at the differences and commonalities between Minimal Description Length (MDL) and Regularization in Machine Learning (RML), we prove that the method of Sparse Kernel Flows is the natural approach to adopt to learn kernels from data. This paper shows that it is not necessary to use the statistical route to derive Sparse Kernel Flows and that one can directly work with code-lengths and complexities that are concepts that show up in AIT."}
{"title":"A New Type Of Upper And Lower Bounds On Right-Tail Probabilities Of Continuous Random Variables","authors":["Nikola Zlatanov"],"raw_abstract":"In this paper, I present a completely new type of upper and lower bounds on\nthe right-tail probabilities of continuous random variables with unbounded\nsupport and with semi-bounded support from the left. The presented upper and\nlower right-tail bounds depend only on the probability density function (PDF),\nits first derivative, and two parameters that are used for tightening the\nbounds. These tail bounds hold under certain conditions that depend on the PDF,\nits first and second derivatives, and the two parameters. The new tail bounds\nare shown to be tight for a wide range of continuous random variables via\nnumerical examples.","publication_date":1700574848,"paper_link":"http://arxiv.org/pdf/2311.12612v1","categories":["Mathematics","Statistics"],"abstract":"In this paper, I present a completely new type of upper and lower bounds on the right-tail probabilities of continuous random variables with unbounded support and with semi-bounded support from the left. The presented upper and lower right-tail bounds depend only on the probability density function (PDF), its first derivative, and two parameters that are used for tightening the bounds. These tail bounds hold under certain conditions that depend on the PDF, its first and second derivatives, and the two parameters. The new tail bounds are shown to be tight for a wide range of continuous random variables via numerical examples."}
{"title":"Optimal Functional Bilinear Regression with Two-way Functional Covariates via Reproducing Kernel Hilbert Space","authors":["Dan Yang","Jianlong Shao","Haipeng Shen","Dong Wang","Hongtu Zhu"],"raw_abstract":"Traditional functional linear regression usually takes a one-dimensional\nfunctional predictor as input and estimates the continuous coefficient\nfunction. Modern applications often generate two-dimensional covariates, which\nbecome matrices when observed at grid points. To avoid the inefficiency of the\nclassical method involving estimation of a two-dimensional coefficient\nfunction, we propose a functional bilinear regression model, and introduce an\ninnovative three-term penalty to impose roughness penalty in the estimation.\nThe proposed estimator exhibits minimax optimal property for prediction under\nthe framework of reproducing kernel Hilbert space. An iterative generalized\ncross-validation approach is developed to choose tuning parameters, which\nsignificantly improves the computational efficiency over the traditional\ncross-validation approach. The statistical and computational advantages of the\nproposed method over existing methods are further demonstrated via simulated\nexperiments, the Canadian weather data, and a biochemical long-range infrared\nlight detection and ranging data.","publication_date":1700573692,"paper_link":"http://arxiv.org/pdf/2311.12597v1","categories":["Statistics"],"abstract":"Traditional functional linear regression usually takes a one-dimensional functional predictor as input and estimates the continuous coefficient function. Modern applications often generate two-dimensional covariates, which become matrices when observed at grid points. To avoid the inefficiency of the classical method involving estimation of a two-dimensional coefficient function, we propose a functional bilinear regression model, and introduce an innovative three-term penalty to impose roughness penalty in the estimation. The proposed estimator exhibits minimax optimal property for prediction under the framework of reproducing kernel Hilbert space. An iterative generalized cross-validation approach is developed to choose tuning parameters, which significantly improves the computational efficiency over the traditional cross-validation approach. The statistical and computational advantages of the proposed method over existing methods are further demonstrated via simulated experiments, the Canadian weather data, and a biochemical long-range infrared light detection and ranging data."}
{"title":"ChronoPscychosis: Temporal Segmentation and Its Impact on Schizophrenia Classification Using Motor Activity Data","authors":["Pradnya Rajendra Jadhav","Raviprasad Aduri"],"raw_abstract":"Schizophrenia is a complicated mental illness characterized by a broad\nspectrum of symptoms affecting cognition, behavior, and emotion. The task of\nidentifying reliable biomarkers to classify Schizophrenia accurately continues\nto be a challenge in the field of psychiatry. We investigate the temporal\npatterns within the motor activity data as a potential key to enhancing the\ncategorization of individuals with Schizophrenia, using the dataset having\nmotor activity recordings of 22 Schizophrenia patients and 32 control subjects.\nThe dataset contains per-minute motor activity measurements collected for an\naverage of 12.7 days in a row for each participant. We dissect each day into\nsegments (Twelve, Eight, six, four, three, and two parts) and evaluate their\nimpact on classification. We employ sixteen statistical features within these\ntemporal segments and train them on Seven machine learning models to get deeper\ninsights. LightGBM model outperforms the other six models. Our results indicate\nthat the temporal segmentation significantly improves the classification, with\nAUC-ROC = 0.93, F1 score = 0.84( LightGBM- without any segmentation) and\nAUC-ROC = 0.98, F1 score = 0.93( LightGBM- with segmentation). Distinguishing\nbetween diurnal and nocturnal segments amplifies the differences between\nSchizophrenia patients and controls. However, further subdivisions into smaller\ntime segments do not affect the AUC- ROC significantly. Morning, afternoon,\nevening, and night partitioning gives similar classification performance to\nday-night partitioning. These findings are valuable as they indicate that\nextensive temporal classification beyond distinguishing between day and night\ndoes not yield substantial results, offering an efficient approach for further\nclassification, early diagnosis, and monitoring of Schizophrenia.","publication_date":1700573193,"paper_link":"http://arxiv.org/pdf/2311.12590v1","categories":["Statistics"],"abstract":"Schizophrenia is a complicated mental illness characterized by a broad spectrum of symptoms affecting cognition, behavior, and emotion. The task of identifying reliable biomarkers to classify Schizophrenia accurately continues to be a challenge in the field of psychiatry. We investigate the temporal patterns within the motor activity data as a potential key to enhancing the categorization of individuals with Schizophrenia, using the dataset having motor activity recordings of 22 Schizophrenia patients and 32 control subjects. The dataset contains per-minute motor activity measurements collected for an average of 12.7 days in a row for each participant. We dissect each day into segments (Twelve, Eight, six, four, three, and two parts) and evaluate their impact on classification. We employ sixteen statistical features within these temporal segments and train them on Seven machine learning models to get deeper insights. LightGBM model outperforms the other six models. Our results indicate that the temporal segmentation significantly improves the classification, with AUC-ROC = 0.93, F1 score = 0.84( LightGBM- without any segmentation) and AUC-ROC = 0.98, F1 score = 0.93( LightGBM- with segmentation). Distinguishing between diurnal and nocturnal segments amplifies the differences between Schizophrenia patients and controls. However, further subdivisions into smaller time segments do not affect the AUC- ROC significantly. Morning, afternoon, evening, and night partitioning gives similar classification performance to day-night partitioning. These findings are valuable as they indicate that extensive temporal classification beyond distinguishing between day and night does not yield substantial results, offering an efficient approach for further classification, early diagnosis, and monitoring of Schizophrenia."}
{"title":"Phi4tools: Compilation of Feynman diagrams for Landau-Ginzburg-Wilson theories","authors":["Giacomo Sberveglieri","Gabriele Spada"],"raw_abstract":"Scalar field theories with quartic interactions are of central interest in\nthe study of second-order phase transitions. For three-dimensional theories,\nnumerous studies make use of the fixed-dimensional perturbative computation of\n[B. Nickel, D. Meiron, and G. Baker Jr, Compilation of 2-pt and 4-pt graphs for\ncontinuous spin model, University of Guelph report (1977)], unfortunately left\nunpublished. We independently verify the results of Nickel et al., and we\nextend the computation to the eighth order in the coupling constant. The\nresults of our calculations, together with the tools developed, are made\navailable in Phi4tools, a user-friendly package that allows displaying the\ninformation about the individual Feynman diagrams, including the numerical\nvalues for the diagrams for zero, two, and four-point functions. We also\nprovide the perturbative series up to order eight for the renormalization-group\nfunctions for the $O(N)$ and cubic anisotropic models.","publication_date":1700570496,"paper_link":"http://arxiv.org/pdf/2311.12576v1","categories":["Physics"],"abstract":"Scalar field theories with quartic interactions are of central interest in the study of second-order phase transitions. For three-dimensional theories, numerous studies make use of the fixed-dimensional perturbative computation of [B. Nickel, D. Meiron, and G. Baker Jr, Compilation of 2-pt and 4-pt graphs for continuous spin model, University of Guelph report (1977)], unfortunately left unpublished. We independently verify the results of Nickel et al., and we extend the computation to the eighth order in the coupling constant. The results of our calculations, together with the tools developed, are made available in Phi4tools, a user-friendly package that allows displaying the information about the individual Feynman diagrams, including the numerical values for the diagrams for zero, two, and four-point functions. We also provide the perturbative series up to order eight for the renormalization-group functions for the __FORMULA__ and cubic anisotropic models."}
{"title":"Minkowski Functionals for composite smooth random fields","authors":["Pravabati Chingangbam","Fazlu Rahman"],"raw_abstract":"Minkowski functionals quantify the morphology of smooth random fields. They\nare widely used to probe statistical properties of cosmological fields.\nAnalytic formulae for ensemble expectations of Minkowski functionals are well\nknown for Gaussian and mildly non-Gaussian fields. In this paper we extend the\nformulae to composite fields which are sums of two fields and explicitly derive\nthe expressions for the sum of uncorrelated mildly non-Gaussian and Gaussian\nfields. These formulae are applicable to observed data which is usually a sum\nof the true signal and one or more secondary fields that can be either noise,\nor some residual contaminating signal. Our formulae provide explicit\nquantification of the effect of the secondary field on the morphology and\nstatistical nature of the true signal. As examples, we apply the formulae to\ndetermine how the presence of Gaussian noise can bias the morphological\nproperties and statistical nature of Gaussian and non-Gaussian CMB temperature\nmaps.","publication_date":1700570048,"paper_link":"http://arxiv.org/pdf/2311.12571v1","categories":["Physics"],"abstract":"Minkowski functionals quantify the morphology of smooth random fields. They are widely used to probe statistical properties of cosmological fields. Analytic formulae for ensemble expectations of Minkowski functionals are well known for Gaussian and mildly non-Gaussian fields. In this paper we extend the formulae to composite fields which are sums of two fields and explicitly derive the expressions for the sum of uncorrelated mildly non-Gaussian and Gaussian fields. These formulae are applicable to observed data which is usually a sum of the true signal and one or more secondary fields that can be either noise, or some residual contaminating signal. Our formulae provide explicit quantification of the effect of the secondary field on the morphology and statistical nature of the true signal. As examples, we apply the formulae to determine how the presence of Gaussian noise can bias the morphological properties and statistical nature of Gaussian and non-Gaussian CMB temperature maps."}
{"title":"Cooperative RIS and STAR-RIS assisted mMIMO Communication: Analysis and Optimization","authors":["Anastasios Papazafeiropoulos","Ahmet M. Elbir","Pandelis Kourtessis","Ioannis Krikidis","Symeon Chatzinotas"],"raw_abstract":"Reconfigurable intelligent surface (RIS) has emerged as a cost-effective and\npromising solution to extend the wireless signal coverage and improve the\nperformance via passive signal reflection. Different from existing works which\ndo not account for the cooperation between RISs or do not provide full space\ncoverage, we propose the marriage of cooperative double-RIS with simultaneously\ntransmitting and reflecting RIS (STAR-RIS) technologies denoted as RIS/STAR-RIS\nunder correlated Rayleigh fading conditions to assist the communication in a\nmassive multiple-input multiple-output (mMIMO) setup. The proposed architecture\nis superior since it enjoys the benefits of the individual designs. We\nintroduce a channel estimation approach of the cascaded channels with reduced\noverhead. Also, we obtain the deterministic equivalent (DE) of the downlink\nachievable sum spectral efficiency (SE) in closed form based on large-scale\nstatistics. Notably, relied on statistical channel state information (CSI), we\noptimise both surfaces by means of the projected gradient ascent method (PGAM),\nand obtain the gradients in closed form. The proposed optimization achieves to\nmaximise the sum SE of such a complex system, and has low complexity and low\noverhead since it can be performed at every several coherence intervals.\nNumerical results show the benefit of the proposed architecture and verify the\nanalytical framework. In particular, we show that the RIS/STAR-RIS architecture\noutperforms the conventional double-RIS or its single-RIS counterparts.","publication_date":1700559246,"paper_link":"http://arxiv.org/pdf/2311.12473v1","categories":["Mathematics"],"abstract":"Reconfigurable intelligent surface (RIS) has emerged as a cost-effective and promising solution to extend the wireless signal coverage and improve the performance via passive signal reflection. Different from existing works which do not account for the cooperation between RISs or do not provide full space coverage, we propose the marriage of cooperative double-RIS with simultaneously transmitting and reflecting RIS (STAR-RIS) technologies denoted as RIS/STAR-RIS under correlated Rayleigh fading conditions to assist the communication in a massive multiple-input multiple-output (mMIMO) setup. The proposed architecture is superior since it enjoys the benefits of the individual designs. We introduce a channel estimation approach of the cascaded channels with reduced overhead. Also, we obtain the deterministic equivalent (DE) of the downlink achievable sum spectral efficiency (SE) in closed form based on large-scale statistics. Notably, relied on statistical channel state information (CSI), we optimise both surfaces by means of the projected gradient ascent method (PGAM), and obtain the gradients in closed form. The proposed optimization achieves to maximise the sum SE of such a complex system, and has low complexity and low overhead since it can be performed at every several coherence intervals. Numerical results show the benefit of the proposed architecture and verify the analytical framework. In particular, we show that the RIS/STAR-RIS architecture outperforms the conventional double-RIS or its single-RIS counterparts."}
{"title":"Towards a Gateway for Knowledge Graph Schemas Collection, Analysis, and Embedding","authors":["Mattia Fumagalli","Marco Boffo","Daqian Shi","Mayukh Bagchi","Fausto Giunchiglia"],"raw_abstract":"One of the significant barriers to the training of statistical models on\nknowledge graphs is the difficulty that scientists have in finding the best\ninput data to address their prediction goal. In addition to this, a key\nchallenge is to determine how to manipulate these relational data, which are\noften in the form of particular triples (i.e., subject, predicate, object), to\nenable the learning process. Currently, many high-quality catalogs of knowledge\ngraphs, are available. However, their primary goal is the re-usability of these\nresources, and their interconnection, in the context of the Semantic Web. This\npaper describes the LiveSchema initiative, namely, a first version of a gateway\nthat has the main scope of leveraging the gold mine of data collected by many\nexisting catalogs collecting relational data like ontologies and knowledge\ngraphs. At the current state, LiveSchema contains - 1000 datasets from 4 main\nsources and offers some key facilities, which allow to: i) evolving LiveSchema,\nby aggregating other source catalogs and repositories as input sources; ii)\nquerying all the collected resources; iii) transforming each given dataset into\nformal concept analysis matrices that enable analysis and visualization\nservices; iv) generating models and tensors from each given dataset.","publication_date":1700558522,"paper_link":"http://arxiv.org/pdf/2311.12465v1","categories":["Statistics"],"abstract":"One of the significant barriers to the training of statistical models on knowledge graphs is the difficulty that scientists have in finding the best input data to address their prediction goal. In addition to this, a key challenge is to determine how to manipulate these relational data, which are often in the form of particular triples (i.e., subject, predicate, object), to enable the learning process. Currently, many high-quality catalogs of knowledge graphs, are available. However, their primary goal is the re-usability of these resources, and their interconnection, in the context of the Semantic Web. This paper describes the LiveSchema initiative, namely, a first version of a gateway that has the main scope of leveraging the gold mine of data collected by many existing catalogs collecting relational data like ontologies and knowledge graphs. At the current state, LiveSchema contains - 1000 datasets from 4 main sources and offers some key facilities, which allow to: i) evolving LiveSchema, by aggregating other source catalogs and repositories as input sources; ii) querying all the collected resources; iii) transforming each given dataset into formal concept analysis matrices that enable analysis and visualization services; iv) generating models and tensors from each given dataset."}
{"title":"Autoencoder-assisted study of directed percolation with spatial long-range interactions","authors":["Yanyang Wang","Yuxiang Yang","Wei Li"],"raw_abstract":"Spatial L{\\'{e}}vy-like flights are introduced as a way to absorbing phase\ntransitions to produce non-local interactions. We utilize the autoencoder, an\nunsupervised learning method, to predict the critical points for $(1+1)$-d\ndirected percolation with such spatial long-range interactions. After making a\nglobal coverage of the reaction-diffusion distance and taking a series of\ndifferent values for the parameter \\;${\\beta}$\\; in the distribution\n\\;$P(r){\\sim}1/r^{\\beta}$\\;, the critical points $P_c$ that can be continuously\nvaried are obtained. And the dynamic decay of the particle density under the\ncritical points was counted as a way to determine the critical exponent\n\\;${\\delta}$\\; of the survival rate. We also investigate the active behavior of\nthe system's particles under the critical point with increasing time steps,\nwhich allows us to determine the characteristic time $t_f$ of the finite-scale\nsystems. And the dynamic exponents \\;$z$\\; are obtained using the scaling\nrelation \\;$t_f{\\sim}L^{z}$\\;. We find that the autoencoder can well identify\nthis characteristic evolutionary behavior of particles. Finally, we discuss the\ncompliance of the scaling form \\;$1/{\\delta}-({\\beta}-2)/{\\delta}z=2$\\; in\ndifferent \\;${\\beta}$\\; intervals as well as a method to introduce a global\nscaling mechanism by generating a random walking step using the L{\\'{e}}vy\ndistribution.","publication_date":1700555880,"paper_link":"http://arxiv.org/pdf/2311.12426v1","categories":["Physics"],"abstract":"Spatial L{\\'{e}}vy-like flights are introduced as a way to absorbing phase transitions to produce non-local interactions. We utilize the autoencoder, an unsupervised learning method, to predict the critical points for __FORMULA__-d directed percolation with such spatial long-range interactions. After making a global coverage of the reaction-diffusion distance and taking a series of different values for the parameter \\;__FORMULA__\\; in the distribution \\;__FORMULA__\\;, the critical points __FORMULA__ that can be continuously varied are obtained. And the dynamic decay of the particle density under the critical points was counted as a way to determine the critical exponent \\;__FORMULA__\\; of the survival rate. We also investigate the active behavior of the system's particles under the critical point with increasing time steps, which allows us to determine the characteristic time __FORMULA__ of the finite-scale systems. And the dynamic exponents \\;__FORMULA__\\; are obtained using the scaling relation \\;__FORMULA__\\;. We find that the autoencoder can well identify this characteristic evolutionary behavior of particles. Finally, we discuss the compliance of the scaling form \\;__FORMULA__\\; in different \\;__FORMULA__\\; intervals as well as a method to introduce a global scaling mechanism by generating a random walking step using the L{\\'{e}}vy distribution."}
{"title":"Data Dialogue with ChatGPT: Using Code Interpreter to Simulate and Analyse Experimental Data","authors":["Andrew Low","Z. Yasemin Kalender"],"raw_abstract":"Artificial intelligence (AI) has the potential to revolutionise many aspects\nof physics education, including introductory laboratory courses. In this study,\nwe explored the capability of OpenAI's ChatGPT-4 to interpret and complete an\nintroductory mechanics laboratory activity using Code Interpreter, a recent\nplug-in that allows users to upload, create, manipulate, and analyse data. By\nuploading an introductory lab activity document via Code Interpreter, we\ninvestigated how effectively ChatGPT could summarize the activity, generate\nmodel data, fit a line to the data, and calculate the reduced chi-square\nstatistic. By analysing the interactions with ChatGPT, as well as the Python\ncode generated by Code Interpreter, we assessed how ChatGPT responded to\ndifferent levels of prompt detail, as well as the accuracy of the data\nanalysis. We found that while Code Interpreter was capable of generating and\nanalysing data that led to plausible-looking answers, the quality and accuracy\nof ChatGPT's outputs were largely dependent on the specificity and detail of\nthe prompts provided. This work offers new insights into the capabilities of\nCode Interpreter within a laboratory setting and highlights a variety of\ntext-prompt strategies for the effective use of Code Interpreter in a lab\ncontext.","publication_date":1700554137,"paper_link":"http://arxiv.org/pdf/2311.12415v1","categories":["Physics"],"abstract":"Artificial intelligence (AI) has the potential to revolutionise many aspects of physics education, including introductory laboratory courses. In this study, we explored the capability of OpenAI's ChatGPT-4 to interpret and complete an introductory mechanics laboratory activity using Code Interpreter, a recent plug-in that allows users to upload, create, manipulate, and analyse data. By uploading an introductory lab activity document via Code Interpreter, we investigated how effectively ChatGPT could summarize the activity, generate model data, fit a line to the data, and calculate the reduced chi-square statistic. By analysing the interactions with ChatGPT, as well as the Python code generated by Code Interpreter, we assessed how ChatGPT responded to different levels of prompt detail, as well as the accuracy of the data analysis. We found that while Code Interpreter was capable of generating and analysing data that led to plausible-looking answers, the quality and accuracy of ChatGPT's outputs were largely dependent on the specificity and detail of the prompts provided. This work offers new insights into the capabilities of Code Interpreter within a laboratory setting and highlights a variety of text-prompt strategies for the effective use of Code Interpreter in a lab context."}
{"title":"Local resetting in non-conserving zero-range processes with extensive rates","authors":["Pascal Grange"],"raw_abstract":"We subject a non-conserving zero-range process with extensive creation,\nannihilation and hopping rates to local resetting. The model is formulated on a\nlarge, fully-connected network of states. The states are equipped with a\nfitness level: particles are added to each state at a rate proportional to the\nfitness level of the state. The fitness is bounded. Moreover, particles are\nannihilated at a constant rate, and hop at a fixed rate to a uniformly-drawn\nstate in the network. This model has been interpreted in terms of population\ndynamics: the fitness is the reproductive fitness in a haploid population, and\nthe hopping process models mutation. It has also been interpreted as a model of\nnetwork growth with a fixed set of nodes. If particles occupying a state are\ninterpreted as links pointing to this state, the fitness is the rate of\nacquisition of links by states, and the hopping rate is the rewiring rate of\nthe network. In the absence of resetting, the model reaches a steady state,\nwhich in a certain limit may exhibit a condensate at maximum fitness: this\nbehavior is a feature of the Kingman model in population dynamics and of the\nBianconi--Barab\\'asi model of growing networks. The occupation numbers of each\nstate are reset to zero at independent random times. These times are\ndistributed according to a Poisson process whose rate (the resetting rate)\ndepends on the fitness. We derive the evolution equation satisfied by the\nprobability law of the occupation numbers, and calculate their average value in\nthe steady state. The existence of a condensate is found to depend on the local\nbehavior of the resetting rate at maximum fitness: if the resetting rate\nvanishes at least linearly at high fitness, a condensate appears at maximum\nfitness in the limit where the sum of the annihilation and hopping rates is\nequal to the maximum fitness.","publication_date":1700554047,"paper_link":"http://arxiv.org/pdf/2311.12414v1","categories":["Physics"],"abstract":"We subject a non-conserving zero-range process with extensive creation, annihilation and hopping rates to local resetting. The model is formulated on a large, fully-connected network of states. The states are equipped with a fitness level: particles are added to each state at a rate proportional to the fitness level of the state. The fitness is bounded. Moreover, particles are annihilated at a constant rate, and hop at a fixed rate to a uniformly-drawn state in the network. This model has been interpreted in terms of population dynamics: the fitness is the reproductive fitness in a haploid population, and the hopping process models mutation. It has also been interpreted as a model of network growth with a fixed set of nodes. If particles occupying a state are interpreted as links pointing to this state, the fitness is the rate of acquisition of links by states, and the hopping rate is the rewiring rate of the network. In the absence of resetting, the model reaches a steady state, which in a certain limit may exhibit a condensate at maximum fitness: this behavior is a feature of the Kingman model in population dynamics and of the Bianconi--Barab\\'asi model of growing networks. The occupation numbers of each state are reset to zero at independent random times. These times are distributed according to a Poisson process whose rate (the resetting rate) depends on the fitness. We derive the evolution equation satisfied by the probability law of the occupation numbers, and calculate their average value in the steady state. The existence of a condensate is found to depend on the local behavior of the resetting rate at maximum fitness: if the resetting rate vanishes at least linearly at high fitness, a condensate appears at maximum fitness in the limit where the sum of the annihilation and hopping rates is equal to the maximum fitness."}
{"title":"Data reduction strategy in the PandaX-4T experiment","authors":["Yubo Zhou","Xun Chen"],"raw_abstract":"The PandaX-4T experiment is designed for multiple purposes, including\nsearches for solar neutrinos, weakly interacting massive particles, and rare\ndouble beta decays of xenon isotopes. The experiment produces a huge amount of\nraw data that needs to be stored for related physical analyses in a wide energy\nrange. With the upgrading of the PandaX-4T experiment, the doubled sampling\nrate resulted in a larger data size, which challenges both the cost and the\ndata processing speed. To address this issue, we propose a data reduction\nstrategy by removing the noise tail of large signals and downsampling the\nremaining parts of them. This strategy reduces the requirement for storage by\n65% while increasing data processing speed. The influences on physical analyses\non different topics at different energy regions are negligible.","publication_date":1700553588,"paper_link":"http://arxiv.org/pdf/2311.12412v1","categories":["Physics"],"abstract":"The PandaX-4T experiment is designed for multiple purposes, including searches for solar neutrinos, weakly interacting massive particles, and rare double beta decays of xenon isotopes. The experiment produces a huge amount of raw data that needs to be stored for related physical analyses in a wide energy range. With the upgrading of the PandaX-4T experiment, the doubled sampling rate resulted in a larger data size, which challenges both the cost and the data processing speed. To address this issue, we propose a data reduction strategy by removing the noise tail of large signals and downsampling the remaining parts of them. This strategy reduces the requirement for storage by 65% while increasing data processing speed. The influences on physical analyses on different topics at different energy regions are negligible."}
{"title":"Note on Granda-Oliveros Holographic Dark Energy","authors":["Manosh T. Manoharan"],"raw_abstract":"This article revisits Granda-Oliveros holographic dark energy (GOHDE) model,\nexplores the features of the parameter space and shows that its ability to\nexplain the late-time acceleration is only due to the integration constant that\nappears by construction. We show that the GOHDE behaves like the dominant\nenergy component and naturally behaves like dark energy in the late phase. In\nthe matter-dominated period, it acts like pressureless matter and exhibits\nradiation-like behaviour in the very early stages. Adjusting the free\nparameters could subdue or enhance these characteristics. Depending on the\nparameters, GOHDE can act similarly to concordance, phantom or\nquintessence-like dark energies with singular equation of states. From the\npoint of local observations, we show that the GOHDE model is observationally\nindistinguishable from $w$CDM and encompasses $\\Lambda$CDM as a specific case.\nOur analysis reveals that while a departure from $\\Lambda$CDM could account for\nlate-time acceleration, it falls short of a consistent description of the\nentire cosmic history. Furthermore, we utilize various datasets, including OHD,\nPantheon, CMB Shift parameter, BAO, and QSO, to constrain the free parameters\nof the GOHDE model. Our analysis indicates that the best fit, assuming the GO\ncut-off, aligns with the $\\Lambda$CDM model. Additionally, we use statistical\nquantifiers such as AIC, BIC, and $\\chi^2$ to test the model's goodness with\nvarious data combinations and estimate the Bayes factor to contrast the models.\nOur study suggests that the standard GOHDE model is equally likely as the\n$\\Lambda$CDM model, ultimately favouring the latter, with\n$\\beta=0.686^{+0.048}_{-0.044}$ and $w{z_0}=-0.988^{+0.042}_{-0.044}$. Given\nthe current observations, we conclude that, for a flat universe, the\n$\\Lambda$CDM case is statistically the best and probably the only consistent\nsolution from the GOHDE construction.","publication_date":1700553351,"paper_link":"http://arxiv.org/pdf/2311.12409v1","categories":["Physics"],"abstract":"This article revisits Granda-Oliveros holographic dark energy (GOHDE) model, explores the features of the parameter space and shows that its ability to explain the late-time acceleration is only due to the integration constant that appears by construction. We show that the GOHDE behaves like the dominant energy component and naturally behaves like dark energy in the late phase. In the matter-dominated period, it acts like pressureless matter and exhibits radiation-like behaviour in the very early stages. Adjusting the free parameters could subdue or enhance these characteristics. Depending on the parameters, GOHDE can act similarly to concordance, phantom or quintessence-like dark energies with singular equation of states. From the point of local observations, we show that the GOHDE model is observationally indistinguishable from __FORMULA__CDM and encompasses __FORMULA__CDM as a specific case. Our analysis reveals that while a departure from __FORMULA__CDM could account for late-time acceleration, it falls short of a consistent description of the entire cosmic history. Furthermore, we utilize various datasets, including OHD, Pantheon, CMB Shift parameter, BAO, and QSO, to constrain the free parameters of the GOHDE model. Our analysis indicates that the best fit, assuming the GO cut-off, aligns with the __FORMULA__CDM model. Additionally, we use statistical quantifiers such as AIC, BIC, and __FORMULA__ to test the model's goodness with various data combinations and estimate the Bayes factor to contrast the models. Our study suggests that the standard GOHDE model is equally likely as the __FORMULA__CDM model, ultimately favouring the latter, with __FORMULA__ and __FORMULA__. Given the current observations, we conclude that, for a flat universe, the __FORMULA__CDM case is statistically the best and probably the only consistent solution from the GOHDE construction."}
{"title":"Problems of Non-equivalent Words in Technical Translation","authors":["Mohammad Ibrahim Qani"],"raw_abstract":"Translating words which do not have equivalent in target language is not easy\nand finding proper equivalent of those words are very important to render\ncorrectly and understandably, the article defines some thoughts and ideas of\nscientists on the common problems of non-equivalent words from English to\nRussian language and includes English and Russian examples and ideas of certain\nscientist. The English language is worldwide spoken and there are 1.35 billion\nEnglish speakers and over 258 million Russian speakers according to the 2021s\nstatistics. Inevitably, these billions of speakers around the world have\nconnection and they may have deal in different criteria. In order to understand\none another they need to have a pure and fully-understood language. These pure\nlanguages understanding directly relates to translation knowledge where\nlinguists and translators need to work and research to eradicate\nmisunderstanding. Misunderstandings mostly appear in non-equivalent words\nbecause there are different local and internal words like food, garment,\ncultural and traditional words and others in every notion. Truly, most of these\nwords do not have equivalent in the target language and these words need to be\nworked and find their equivalent in the target language to fully understand the\nboth languages. However, some of these non-equivalent words are already\nprofessionally rendered to the target language but still there many other words\nto be rendered. Hence, this research paper includes different ways and rules of\nrendering non-equivalent words from source language to the target language.","publication_date":1700550699,"paper_link":"http://arxiv.org/pdf/2311.12395v1","categories":["Statistics"],"abstract":"Translating words which do not have equivalent in target language is not easy and finding proper equivalent of those words are very important to render correctly and understandably, the article defines some thoughts and ideas of scientists on the common problems of non-equivalent words from English to Russian language and includes English and Russian examples and ideas of certain scientist. The English language is worldwide spoken and there are 1.35 billion English speakers and over 258 million Russian speakers according to the 2021s statistics. Inevitably, these billions of speakers around the world have connection and they may have deal in different criteria. In order to understand one another they need to have a pure and fully-understood language. These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. Hence, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language."}
{"title":"Individualized Dynamic Model for Multi-resolutional Data","authors":["Jiuchen Zhang","Fei Xue","Qi Xu","Jung-Ah Lee","Annie Qu"],"raw_abstract":"Mobile health has emerged as a major success in tracking individual health\nstatus, due to the popularity and power of smartphones and wearable devices.\nThis has also brought great challenges in handling heterogeneous,\nmulti-resolution data which arise ubiquitously in mobile health due to\nirregular multivariate measurements collected from individuals. In this paper,\nwe propose an individualized dynamic latent factor model for irregular\nmulti-resolution time series data to interpolate unsampled measurements of time\nseries with low resolution. One major advantage of the proposed method is the\ncapability to integrate multiple irregular time series and multiple subjects by\nmapping the multi-resolution data to the latent space. In addition, the\nproposed individualized dynamic latent factor model is applicable to capturing\nheterogeneous longitudinal information through individualized dynamic latent\nfactors. In theory, we provide the integrated interpolation error bound of the\nproposed estimator and derive the convergence rate with B-spline approximation\nmethods. Both the simulation studies and the application to smartwatch data\ndemonstrate the superior performance of the proposed method compared to\nexisting methods.","publication_date":1700550358,"paper_link":"http://arxiv.org/pdf/2311.12392v2","categories":["Mathematics","Statistics"],"abstract":"Mobile health has emerged as a major success in tracking individual health status, due to the popularity and power of smartphones and wearable devices. This has also brought great challenges in handling heterogeneous, multi-resolution data which arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. In this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. One major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. In addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. In theory, we provide the integrated interpolation error bound of the proposed estimator and derive the convergence rate with B-spline approximation methods. Both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods."}
{"title":"Hierarchical Framework for Predicting Entropies in Bottom-Up Coarse-Grained Models","authors":["Jaehyeok Jin","David R. Reichman"],"raw_abstract":"The thermodynamic entropy of coarse-grained (CG) models stands as one of the\nmost important properties for quantifying the missing information during the CG\nprocess and for establishing transferable (or extendible) CG interactions.\nHowever, performing additional CG simulations on top of model construction\noften leads to significant additional computational overhead. In this work, we\npropose a simple hierarchical framework for predicting the thermodynamic\nentropies of various molecular CG systems. Our approach employs a decomposition\nof the CG interactions, enabling the estimation of the CG partition function\nand thermodynamic properties a priori. Starting from the ideal gas description,\nwe leverage classical perturbation theory to systematically incorporate simple\nyet essential interactions, ranging from the hard sphere model to the\ngeneralized van der Waals model. Additionally, we propose an alternative\napproach based on multiparticle correlation functions, allowing for systematic\nimprovements through higher-order correlations. Numerical applications to\nmolecular liquids validate the high fidelity of our approach, and our\ncomputational protocols demonstrate that a reduced model with simple energetics\ncan reasonably estimate the thermodynamic entropy of CG models without\nperforming any CG simulations. Overall, our findings present a systematic\nframework for estimating not only the entropy but also other thermodynamic\nproperties of CG models, relying solely on information from the reference\nsystem.","publication_date":1700543407,"paper_link":"http://arxiv.org/pdf/2311.12353v1","categories":["Physics"],"abstract":"The thermodynamic entropy of coarse-grained (CG) models stands as one of the most important properties for quantifying the missing information during the CG process and for establishing transferable (or extendible) CG interactions. However, performing additional CG simulations on top of model construction often leads to significant additional computational overhead. In this work, we propose a simple hierarchical framework for predicting the thermodynamic entropies of various molecular CG systems. Our approach employs a decomposition of the CG interactions, enabling the estimation of the CG partition function and thermodynamic properties a priori. Starting from the ideal gas description, we leverage classical perturbation theory to systematically incorporate simple yet essential interactions, ranging from the hard sphere model to the generalized van der Waals model. Additionally, we propose an alternative approach based on multiparticle correlation functions, allowing for systematic improvements through higher-order correlations. Numerical applications to molecular liquids validate the high fidelity of our approach, and our computational protocols demonstrate that a reduced model with simple energetics can reasonably estimate the thermodynamic entropy of CG models without performing any CG simulations. Overall, our findings present a systematic framework for estimating not only the entropy but also other thermodynamic properties of CG models, relying solely on information from the reference system."}
{"title":"Bayesian Cluster Geographically Weighted Regression for Spatial Heterogeneous Data","authors":["Wala Draidi Areed","Aiden Price","Helen Thompson","Conor Hassan","Reid Malseed","Kerrie Mengersen"],"raw_abstract":"Spatial statistical models are commonly used in geographical scenarios to\nensure spatial variation is captured effectively. However, spatial models and\ncluster algorithms can be complicated and expensive. This paper pursues three\nmain objectives. First, it introduces covariate effect clustering by\nintegrating a Bayesian Geographically Weighted Regression (BGWR) with a\nGaussian mixture model and the Dirichlet process mixture model. Second, this\npaper examines situations in which a particular covariate holds significant\nimportance in one region but not in another in the Bayesian framework. Lastly,\nit addresses computational challenges present in existing BGWR, leading to\nnotable enhancements in Markov chain Monte Carlo estimation suitable for large\nspatial datasets. The efficacy of the proposed method is demonstrated using\nsimulated data and is further validated in a case study examining children's\ndevelopment domains in Queensland, Australia, using data provided by Children's\nHealth Queensland and Australia's Early Development Census.","publication_date":1700541752,"paper_link":"http://arxiv.org/pdf/2311.12347v1","categories":["Statistics"],"abstract":"Spatial statistical models are commonly used in geographical scenarios to ensure spatial variation is captured effectively. However, spatial models and cluster algorithms can be complicated and expensive. This paper pursues three main objectives. First, it introduces covariate effect clustering by integrating a Bayesian Geographically Weighted Regression (BGWR) with a Gaussian mixture model and the Dirichlet process mixture model. Second, this paper examines situations in which a particular covariate holds significant importance in one region but not in another in the Bayesian framework. Lastly, it addresses computational challenges present in existing BGWR, leading to notable enhancements in Markov chain Monte Carlo estimation suitable for large spatial datasets. The efficacy of the proposed method is demonstrated using simulated data and is further validated in a case study examining children's development domains in Queensland, Australia, using data provided by Children's Health Queensland and Australia's Early Development Census."}
{"title":"Acceleration and Implicit Regularization in Gaussian Phase Retrieval","authors":["Tyler Maunu","Martin Molina-Fructuoso"],"raw_abstract":"We study accelerated optimization methods in the Gaussian phase retrieval\nproblem. In this setting, we prove that gradient methods with Polyak or\nNesterov momentum have similar implicit regularization to gradient descent.\nThis implicit regularization ensures that the algorithms remain in a nice\nregion, where the cost function is strongly convex and smooth despite being\nnonconvex in general. This ensures that these accelerated methods achieve\nfaster rates of convergence than gradient descent. Experimental evidence\ndemonstrates that the accelerated methods converge faster than gradient descent\nin practice.","publication_date":1700539803,"paper_link":"http://arxiv.org/pdf/2311.12888v1","categories":["Mathematics","Statistics"],"abstract":"We study accelerated optimization methods in the Gaussian phase retrieval problem. In this setting, we prove that gradient methods with Polyak or Nesterov momentum have similar implicit regularization to gradient descent. This implicit regularization ensures that the algorithms remain in a nice region, where the cost function is strongly convex and smooth despite being nonconvex in general. This ensures that these accelerated methods achieve faster rates of convergence than gradient descent. Experimental evidence demonstrates that the accelerated methods converge faster than gradient descent in practice."}
{"title":"A unified consensus-based parallel ADMM algorithm for high-dimensional regression with combined regularizations","authors":["Xiaofei Wu","Zhimin Zhang","Zhenyu Cui"],"raw_abstract":"The parallel alternating direction method of multipliers (ADMM) algorithm is\nwidely recognized for its effectiveness in handling large-scale datasets stored\nin a distributed manner, making it a popular choice for solving statistical\nlearning models. However, there is currently limited research on parallel\nalgorithms specifically designed for high-dimensional regression with combined\n(composite) regularization terms. These terms, such as elastic-net, sparse\ngroup lasso, sparse fused lasso, and their nonconvex variants, have gained\nsignificant attention in various fields due to their ability to incorporate\nprior information and promote sparsity within specific groups or fused\nvariables. The scarcity of parallel algorithms for combined regularizations can\nbe attributed to the inherent nonsmoothness and complexity of these terms, as\nwell as the absence of closed-form solutions for certain proximal operators\nassociated with them. In this paper, we propose a unified constrained\noptimization formulation based on the consensus problem for these types of\nconvex and nonconvex regression problems and derive the corresponding parallel\nADMM algorithms. Furthermore, we prove that the proposed algorithm not only has\nglobal convergence but also exhibits linear convergence rate. Extensive\nsimulation experiments, along with a financial example, serve to demonstrate\nthe reliability, stability, and scalability of our algorithm. The R package for\nimplementing the proposed algorithms can be obtained at\nhttps://github.com/xfwu1016/CPADMM.","publication_date":1700537438,"paper_link":"http://arxiv.org/pdf/2311.12319v1","categories":["Mathematics","Statistics"],"abstract":"The parallel alternating direction method of multipliers (ADMM) algorithm is widely recognized for its effectiveness in handling large-scale datasets stored in a distributed manner, making it a popular choice for solving statistical learning models. However, there is currently limited research on parallel algorithms specifically designed for high-dimensional regression with combined (composite) regularization terms. These terms, such as elastic-net, sparse group lasso, sparse fused lasso, and their nonconvex variants, have gained significant attention in various fields due to their ability to incorporate prior information and promote sparsity within specific groups or fused variables. The scarcity of parallel algorithms for combined regularizations can be attributed to the inherent nonsmoothness and complexity of these terms, as well as the absence of closed-form solutions for certain proximal operators associated with them. In this paper, we propose a unified constrained optimization formulation based on the consensus problem for these types of convex and nonconvex regression problems and derive the corresponding parallel ADMM algorithms. Furthermore, we prove that the proposed algorithm not only has global convergence but also exhibits linear convergence rate. Extensive simulation experiments, along with a financial example, serve to demonstrate the reliability, stability, and scalability of our algorithm. The R package for implementing the proposed algorithms can be obtained at https://github.com/xfwu1016/CPADMM."}
{"title":"Deciphering Non-Gaussianity of Diffusion Based on the Evolution of Diffusivity","authors":["Haolan Xu","Xu Zheng","Xinghua Shi"],"raw_abstract":"Non-Gaussianity indicates complex dynamics related to extreme events or\nsignificant outliers. However, the correlation between non-Gaussianity and the\ndynamics of heterogeneous environments in anomalous diffusion remains\nuncertain. Inspired by a recent study by Alexandre et al. [Phys. Rev. Lett.\n130, 077101 (2023)], we demonstrate that non-Gaussianity can be deciphered\nthrough the spatiotemporal evolution of heterogeneity-dependent diffusivity.\nUsing diffusion experiments in a linear temperature field and Brownian dynamics\nsimulations, we found that short- and long-time non-Gaussianity can be\npredicted based on diffusivity distribution. Non-Gaussianity variation is\ndetermined by an effective P\\'eclet number (a ratio of the varying rate of\ndiffusivity to the diffusivity of diffusivity), which clarifies whether the\ntail distribution expands or contracts. The tail is more Gaussian than\nexponential over long times, with exceptions significantly dependent on the\ndiffusivity distribution. Our findings shed light on heterogeneity mapping in\ncomplex environments using non-Gaussian statistics.","publication_date":1700537151,"paper_link":"http://arxiv.org/pdf/2311.12317v1","categories":["Physics"],"abstract":"Non-Gaussianity indicates complex dynamics related to extreme events or significant outliers. However, the correlation between non-Gaussianity and the dynamics of heterogeneous environments in anomalous diffusion remains uncertain. Inspired by a recent study by Alexandre et al. [Phys. Rev. Lett. 130, 077101 (2023)], we demonstrate that non-Gaussianity can be deciphered through the spatiotemporal evolution of heterogeneity-dependent diffusivity. Using diffusion experiments in a linear temperature field and Brownian dynamics simulations, we found that short- and long-time non-Gaussianity can be predicted based on diffusivity distribution. Non-Gaussianity variation is determined by an effective P\\'eclet number (a ratio of the varying rate of diffusivity to the diffusivity of diffusivity), which clarifies whether the tail distribution expands or contracts. The tail is more Gaussian than exponential over long times, with exceptions significantly dependent on the diffusivity distribution. Our findings shed light on heterogeneity mapping in complex environments using non-Gaussian statistics."}
{"title":"Causality is all you need","authors":["Ning Xu","Yifei Gao","Hongshuo Tian","Yongdong Zhang","An-An Liu"],"raw_abstract":"In the fundamental statistics course, students are taught to remember the\nwell-known saying: \"Correlation is not Causation\". Till now, statistics (i.e.,\ncorrelation) have developed various successful frameworks, such as Transformer\nand Pre-training large-scale models, which have stacked multiple parallel\nself-attention blocks to imitate a wide range of tasks. However, in the\ncausation community, how to build an integrated causal framework still remains\nan untouched domain despite its excellent intervention capabilities. In this\npaper, we propose the Causal Graph Routing (CGR) framework, an integrated\ncausal scheme relying entirely on the intervention mechanisms to reveal the\ncause-effect forces hidden in data. Specifically, CGR is composed of a stack of\ncausal layers. Each layer includes a set of parallel deconfounding blocks from\ndifferent causal graphs. We combine these blocks via the concept of the\nproposed sufficient cause, which allows the model to dynamically select the\nsuitable deconfounding methods in each layer. CGR is implemented as the stacked\nnetworks, integrating no confounder, back-door adjustment, front-door\nadjustment, and probability of sufficient cause. We evaluate this framework on\ntwo classical tasks of CV and NLP. Experiments show CGR can surpass the current\nstate-of-the-art methods on both Visual Question Answer and Long Document\nClassification tasks. In particular, CGR has great potential in building the\n\"causal\" pre-training large-scale model that effectively generalizes to diverse\ntasks. It will improve the machines' comprehension of causal relationships\nwithin a broader semantic space.","publication_date":1700535220,"paper_link":"http://arxiv.org/pdf/2311.12307v1","categories":["Statistics"],"abstract":"In the fundamental statistics course, students are taught to remember the well-known saying: \"Correlation is not Causation\". Till now, statistics (i.e., correlation) have developed various successful frameworks, such as Transformer and Pre-training large-scale models, which have stacked multiple parallel self-attention blocks to imitate a wide range of tasks. However, in the causation community, how to build an integrated causal framework still remains an untouched domain despite its excellent intervention capabilities. In this paper, we propose the Causal Graph Routing (CGR) framework, an integrated causal scheme relying entirely on the intervention mechanisms to reveal the cause-effect forces hidden in data. Specifically, CGR is composed of a stack of causal layers. Each layer includes a set of parallel deconfounding blocks from different causal graphs. We combine these blocks via the concept of the proposed sufficient cause, which allows the model to dynamically select the suitable deconfounding methods in each layer. CGR is implemented as the stacked networks, integrating no confounder, back-door adjustment, front-door adjustment, and probability of sufficient cause. We evaluate this framework on two classical tasks of CV and NLP. Experiments show CGR can surpass the current state-of-the-art methods on both Visual Question Answer and Long Document Classification tasks. In particular, CGR has great potential in building the \"causal\" pre-training large-scale model that effectively generalizes to diverse tasks. It will improve the machines' comprehension of causal relationships within a broader semantic space."}
{"title":"Detecting subtle macroscopic changes in a finite temperature classical scalar field with machine learning","authors":["Jiming Yang","Yutong Zheng","Jiahong Zhou","Huiyu Li","Jun Yin"],"raw_abstract":"The ability to detect macroscopic changes is important for probing the\nbehaviors of experimental many-body systems from the classical to the quantum\nrealm. Although abrupt changes near phase boundaries can easily be detected,\nsubtle macroscopic changes are much more difficult to detect as the changes can\nbe obscured by noise. In this study, as a toy model for detecting subtle\nmacroscopic changes in many-body systems, we try to differentiate scalar field\nsamples at varying temperatures. We compare different methods for making such\ndifferentiations, from physics method, statistics method, to AI method. Our\nfinding suggests that the AI method outperforms both the statistical method and\nthe physics method in its sensitivity. Our result provides a proof-of-concept\nthat AI can potentially detect macroscopic changes in many-body systems that\nelude physical measures.","publication_date":1700534753,"paper_link":"http://arxiv.org/pdf/2311.12303v1","categories":["Physics"],"abstract":"The ability to detect macroscopic changes is important for probing the behaviors of experimental many-body systems from the classical to the quantum realm. Although abrupt changes near phase boundaries can easily be detected, subtle macroscopic changes are much more difficult to detect as the changes can be obscured by noise. In this study, as a toy model for detecting subtle macroscopic changes in many-body systems, we try to differentiate scalar field samples at varying temperatures. We compare different methods for making such differentiations, from physics method, statistics method, to AI method. Our finding suggests that the AI method outperforms both the statistical method and the physics method in its sensitivity. Our result provides a proof-of-concept that AI can potentially detect macroscopic changes in many-body systems that elude physical measures."}
{"title":"Sample size calculation based on the difference in restricted mean time lost for clinical trials with competing risks","authors":["Xiang Geng","Zhaojin Li","Chengfeng Zhang","Yanjie Wang","Haoning Shen","Zhiheng Huang","Yawen Hou","Zheng Chen"],"raw_abstract":"Computation of sample size is important when designing clinical trials. The\npresence of competing risks makes the design of clinical trials with\ntime-to-event endpoints cumbersome. A model based on the subdistribution hazard\nratio (SHR) is commonly used for trials under competing risks. However, this\napproach has some limitations related to model assumptions and clinical\ninterpretation. Considering such limitations, the difference in restricted mean\ntime lost (RMTLd) is recommended as an alternative indicator. In this paper, we\npropose a sample size calculation method based on the RMTLd for the Weibull\ndistribution (RMTLdWeibull) for clinical trials, which considers experimental\nconditions such as equal allocation, uniform accrual, uniform loss to\nfollow-up, and administrative censoring. Simulation results show that sample\nsize calculation based on the RMTLdWeibull can generally achieve a predefined\npower level and maintain relative robustness. Moreover, the performance of the\nsample size calculation based on the RMTLdWeibull is similar or superior to\nthat based on the SHR. Even if the event time does not follow the Weibull\ndistribution, the sample size calculation based on the RMTLdWeibull still\nperforms well. The results also verify the performance of the sample size\ncalculation method based on the RMTLdWeibull. From the perspective of the\nresults of this study, clinical interpretation, application conditions and\nstatistical performance, we recommend that when designing clinical trials in\nthe presence of competing risks, the RMTLd indicator be applied for sample size\ncalculation and subsequent effect size measurement.","publication_date":1700533887,"paper_link":"http://arxiv.org/pdf/2311.12293v1","categories":["Statistics"],"abstract":"Computation of sample size is important when designing clinical trials. The presence of competing risks makes the design of clinical trials with time-to-event endpoints cumbersome. A model based on the subdistribution hazard ratio (SHR) is commonly used for trials under competing risks. However, this approach has some limitations related to model assumptions and clinical interpretation. Considering such limitations, the difference in restricted mean time lost (RMTLd) is recommended as an alternative indicator. In this paper, we propose a sample size calculation method based on the RMTLd for the Weibull distribution (RMTLdWeibull) for clinical trials, which considers experimental conditions such as equal allocation, uniform accrual, uniform loss to follow-up, and administrative censoring. Simulation results show that sample size calculation based on the RMTLdWeibull can generally achieve a predefined power level and maintain relative robustness. Moreover, the performance of the sample size calculation based on the RMTLdWeibull is similar or superior to that based on the SHR. Even if the event time does not follow the Weibull distribution, the sample size calculation based on the RMTLdWeibull still performs well. The results also verify the performance of the sample size calculation method based on the RMTLdWeibull. From the perspective of the results of this study, clinical interpretation, application conditions and statistical performance, we recommend that when designing clinical trials in the presence of competing risks, the RMTLd indicator be applied for sample size calculation and subsequent effect size measurement."}
{"title":"Context-Dependent Memory in Situated Visualization","authors":["Kadek Ananta Satriadi","Benjamin Tag","Tim Dwyer"],"raw_abstract":"Situated visualization presents data alongside their source context (physical\nreferent). While environmental factors influence memory recall (known as\nContext-Dependent Memory or CDM), how physical context affects cognition in\nreal-world tasks such as working with visualizations in situated contexts is\nunclear. This study explores the design space of information memorability in\nsituated visualization through the lens of CDM. We investigate the presence of\nphysical referents for creating contextual cues in desktop and Virtual Reality\n(VR) environments. Across three studies (n=144), we observe a trend suggesting\na CDM effect due to contextual referent is more apparent in VR. Overall, we did\nnot find statistically significant evidence of a CDM effect due to the presence\nof a referent. However, we did find a significant CDM effect for lighting\nconditions. This suggests that representing the entire environment, rather than\nthe physical objects alone, may be necessary to provide sufficiently strong\ncontextual memory cues.","publication_date":1700532072,"paper_link":"http://arxiv.org/pdf/2311.12288v1","categories":["Statistics"],"abstract":"Situated visualization presents data alongside their source context (physical referent). While environmental factors influence memory recall (known as Context-Dependent Memory or CDM), how physical context affects cognition in real-world tasks such as working with visualizations in situated contexts is unclear. This study explores the design space of information memorability in situated visualization through the lens of CDM. We investigate the presence of physical referents for creating contextual cues in desktop and Virtual Reality (VR) environments. Across three studies (n=144), we observe a trend suggesting a CDM effect due to contextual referent is more apparent in VR. Overall, we did not find statistically significant evidence of a CDM effect due to the presence of a referent. However, we did find a significant CDM effect for lighting conditions. This suggests that representing the entire environment, rather than the physical objects alone, may be necessary to provide sufficiently strong contextual memory cues."}
{"title":"Geometric Characterization of Many Body Localization","authors":["W. N. Faugno","Tomoki Ozawa"],"raw_abstract":"Many body localization (MBL) represents a unique physical phenomenon,\nproviding a testing ground for exploring thermalization, or more precisely its\nfailure. Here we characterize the MBL phase geometrically by the many-body\nquantum metric (MBQM), defined in the parameter space of twist boundary. We\nfind that MBQM scales linearly as a function of the inverse system length in\nthe MBL phase, and grows faster in the ergodic phase. We validate our theory\nusing the disordered hardcore Bose-Hubbard model, and characterize the ergodic\nto MBL phase transition via the localization length scale defined from the\nMBQM. MBQM provides an intuitive and experimentally accessible method to\ncharacterize MBL phases.","publication_date":1700531442,"paper_link":"http://arxiv.org/pdf/2311.12280v1","categories":["Physics"],"abstract":"Many body localization (MBL) represents a unique physical phenomenon, providing a testing ground for exploring thermalization, or more precisely its failure. Here we characterize the MBL phase geometrically by the many-body quantum metric (MBQM), defined in the parameter space of twist boundary. We find that MBQM scales linearly as a function of the inverse system length in the MBL phase, and grows faster in the ergodic phase. We validate our theory using the disordered hardcore Bose-Hubbard model, and characterize the ergodic to MBL phase transition via the localization length scale defined from the MBQM. MBQM provides an intuitive and experimentally accessible method to characterize MBL phases."}
{"title":"Procedural Generation of Grain Orientations using the Wave Function Collapse Algorithm","authors":["G. Magny-Fokam","D. Madisetti","J. El-Awady"],"raw_abstract":"Statistics of grain sizes and orientations in metals correlate to the\nmaterial's mechanical properties. Reproducing representative volume elements\nfor further analysis of deformation and failure in metals, like 316L stainless\nsteel, is particularly important due to their wide use in manufacturing goods\ntoday. Two approaches, initially created for video games, were considered for\nthe procedural generation of representative grain microstructures. The first is\nthe Wave Function Collapse (WFC) algorithm, and the second is constraint\npropagation and probabilistic inference through Markov Junior, a free and\nopen-source software. This study aimed to investigate these two algorithms'\neffectiveness in using reference electron backscatter diffraction (EBSD) maps\nand recreating a statistically similar one that could be used in further\nresearch. It utilized two stainless steel EBSD maps as references to test both\nalgorithms. First, the WFC algorithm was too constricting and, thus, incapable\nof producing images that resembled EBSDs. The second, MarkovJunior, was much\nmore effective in creating a Voronoi tessellation that could be used to create\nan EBSD map in Python. When comparing the results between the reference and the\ngenerated EBSD, we discovered that the orientation and volume fractions were\nextremely similar. With the study, it was concluded that MarkovJunior is an\neffective machine learning tool that can reproduce representative grain\nmicrostructures.","publication_date":1700529924,"paper_link":"http://arxiv.org/pdf/2311.12272v1","categories":["Statistics"],"abstract":"Statistics of grain sizes and orientations in metals correlate to the material's mechanical properties. Reproducing representative volume elements for further analysis of deformation and failure in metals, like 316L stainless steel, is particularly important due to their wide use in manufacturing goods today. Two approaches, initially created for video games, were considered for the procedural generation of representative grain microstructures. The first is the Wave Function Collapse (WFC) algorithm, and the second is constraint propagation and probabilistic inference through Markov Junior, a free and open-source software. This study aimed to investigate these two algorithms' effectiveness in using reference electron backscatter diffraction (EBSD) maps and recreating a statistically similar one that could be used in further research. It utilized two stainless steel EBSD maps as references to test both algorithms. First, the WFC algorithm was too constricting and, thus, incapable of producing images that resembled EBSDs. The second, MarkovJunior, was much more effective in creating a Voronoi tessellation that could be used to create an EBSD map in Python. When comparing the results between the reference and the generated EBSD, we discovered that the orientation and volume fractions were extremely similar. With the study, it was concluded that MarkovJunior is an effective machine learning tool that can reproduce representative grain microstructures."}
{"title":"Provable Representation with Efficient Planning for Partially Observable Reinforcement Learning","authors":["Hongming Zhang","Tongzheng Ren","Chenjun Xiao","Dale Schuurmans","Bo Dai"],"raw_abstract":"In real-world reinforcement learning problems, the state information is often\nonly partially observable, which breaks the basic assumption in Markov decision\nprocesses, and thus, leads to inferior performances. Partially Observable\nMarkov Decision Processes have been introduced to explicitly take the issue\ninto account for learning, exploration, and planning, but presenting\nsignificant computational and statistical challenges. To address these\ndifficulties, we exploit the representation view, which leads to a coherent\ndesign framework for a practically tractable reinforcement learning algorithm\nupon partial observations. We provide a theoretical analysis for justifying the\nstatistical efficiency of the proposed algorithm. We also empirically\ndemonstrate the proposed algorithm can surpass state-of-the-art performance\nwith partial observations across various benchmarks, therefore, pushing\nreliable reinforcement learning towards more practical applications.","publication_date":1700524618,"paper_link":"http://arxiv.org/pdf/2311.12244v1","categories":["Statistics"],"abstract":"In real-world reinforcement learning problems, the state information is often only partially observable, which breaks the basic assumption in Markov decision processes, and thus, leads to inferior performances. Partially Observable Markov Decision Processes have been introduced to explicitly take the issue into account for learning, exploration, and planning, but presenting significant computational and statistical challenges. To address these difficulties, we exploit the representation view, which leads to a coherent design framework for a practically tractable reinforcement learning algorithm upon partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm. We also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, therefore, pushing reliable reinforcement learning towards more practical applications."}
{"title":"Properties of Intense Electromagnetic Ion Cyclotron Waves: Implications for Quasi-linear, Nonlinear, and Nonresonant Wave-Particle Interactions","authors":["Xiaofei Shi","Anton Artemyev","Xiao-Jia Zhang","Didier Mourenas","Xin An","Vassilis Angelopoulos"],"raw_abstract":"Resonant interactions between relativistic electrons and electromagnetic ion\ncyclotron (EMIC) waves provide an effective loss mechanism for this important\nelectron population in the outer radiation belt. The diffusive regime of\nelectron scattering and loss has been well incorporated into radiation belt\nmodels within the framework of the quasi-linear diffusion theory, whereas the\nnonlinear regime has been mostly studied with test particle simulations. There\nis also a less investigated, nonresonant regime of electron scattering by EMIC\nwaves. All three regimes should be present, depending on the EMIC waves and\nambient plasma properties, but the occurrence rates of these regimes have not\nbeen previously quantified. This study provides a statistical investigation of\nthe most important EMIC wave-packet characteristics for the diffusive,\nnonlinear, and nonresonant regimes of electron scattering. We utilize 3 years\nof Van Allen Probe observations to derive distributions of wave amplitudes,\nwave-packet sizes, and rates of frequency variations within individual\nwave-packets. We demonstrate that EMIC waves typically propagate as\nwave-packets with $\\sim 10$ wave periods each, and that $\\sim 3-10$\\% of such\nwave-packets can reach the regime of nonlinear resonant interaction with 2 to 6\nMeV electrons. We show that EMIC frequency variations within wave-packets reach\n$50-100$\\% of the center frequency, corresponding to a significant\nhigh-frequency tail in their wave power spectrum. We explore the consequences\nof these wave-packet characteristics for high and low energy electron\nprecipitation by H-band EMIC waves and for the relative importance of\nquasi-linear and nonlinear regimes of wave-particle interactions.","publication_date":1700524443,"paper_link":"http://arxiv.org/pdf/2311.12243v1","categories":["Physics"],"abstract":"Resonant interactions between relativistic electrons and electromagnetic ion cyclotron (EMIC) waves provide an effective loss mechanism for this important electron population in the outer radiation belt. The diffusive regime of electron scattering and loss has been well incorporated into radiation belt models within the framework of the quasi-linear diffusion theory, whereas the nonlinear regime has been mostly studied with test particle simulations. There is also a less investigated, nonresonant regime of electron scattering by EMIC waves. All three regimes should be present, depending on the EMIC waves and ambient plasma properties, but the occurrence rates of these regimes have not been previously quantified. This study provides a statistical investigation of the most important EMIC wave-packet characteristics for the diffusive, nonlinear, and nonresonant regimes of electron scattering. We utilize 3 years of Van Allen Probe observations to derive distributions of wave amplitudes, wave-packet sizes, and rates of frequency variations within individual wave-packets. We demonstrate that EMIC waves typically propagate as wave-packets with __FORMULA__ wave periods each, and that __FORMULA__\\% of such wave-packets can reach the regime of nonlinear resonant interaction with 2 to 6 MeV electrons. We show that EMIC frequency variations within wave-packets reach __FORMULA__\\% of the center frequency, corresponding to a significant high-frequency tail in their wave power spectrum. We explore the consequences of these wave-packet characteristics for high and low energy electron precipitation by H-band EMIC waves and for the relative importance of quasi-linear and nonlinear regimes of wave-particle interactions."}
{"title":"Type A Partially-Symmetric Macdonald Polynomials","authors":["Ben Goodberry"],"raw_abstract":"We construct type A partially-symmetric Macdonald polynomials $P_{(\\lambda\n\\mid \\gamma)}$, where $\\lambda \\in \\mathbb{Z}_{\\geq 0}^{n-k}$ is a partition\nand $\\gamma \\in \\mathbb{Z}_{\\geq 0}^k$ is a composition. These are polynomials\nwhich are symmetric in the first $n-k$ variables, but not necessarily in the\nfinal $k$ variables. We establish their stability and an integral form defined\nusing Young diagram statistics. Finally, we build Pieri-type rules for degree 1\nproducts $x_j P_{(\\lambda \\mid \\gamma)}$ for $j > n-k$ and $e_1[x_1, \\dotsc,\nx_{n-k}] P_{(\\lambda \\mid \\gamma)}$, along with substantial combinatorial\nsimplification of the $e_1$ multiplication. The $P_{(\\lambda \\mid \\gamma)}$ are\nthe same as the $m$-symmetric Macdonald polynomials defined by Lapointe up to a\nchange of variables.","publication_date":1700518511,"paper_link":"http://arxiv.org/pdf/2311.12216v1","categories":["Mathematics"],"abstract":"We construct type A partially-symmetric Macdonald polynomials __FORMULA__, where __FORMULA__ is a partition and __FORMULA__ is a composition. These are polynomials which are symmetric in the first __FORMULA__ variables, but not necessarily in the final __FORMULA__ variables. We establish their stability and an integral form defined using Young diagram statistics. Finally, we build Pieri-type rules for degree 1 products __FORMULA__ for __FORMULA__ and __FORMULA__, along with substantial combinatorial simplification of the __FORMULA__ multiplication. The __FORMULA__ are the same as the __FORMULA__-symmetric Macdonald polynomials defined by Lapointe up to a change of variables."}
{"title":"A bump statistic on permutations resulting from the Robinson-Schensted correspondence","authors":["Mark Dukes","Andrew Mullins"],"raw_abstract":"In this paper we introduce a new permutation statistic. This statistic counts\nthe number of bumps that occur during the execution of the Robinson-Schensted\nprocedure when applied to a given permutation. We provide several\ninterpretations of this bump statistic that include the tableaux shape and also\nas an extremal problem concerning permutations and increasing subsequences.\nSeveral aspects of this bump statistic are investigated from both structural\nand enumerative viewpoints.","publication_date":1700518472,"paper_link":"http://arxiv.org/pdf/2311.12215v1","categories":["Mathematics"],"abstract":"In this paper we introduce a new permutation statistic. This statistic counts the number of bumps that occur during the execution of the Robinson-Schensted procedure when applied to a given permutation. We provide several interpretations of this bump statistic that include the tableaux shape and also as an extremal problem concerning permutations and increasing subsequences. Several aspects of this bump statistic are investigated from both structural and enumerative viewpoints."}
{"title":"Sensitivity of polarizations and spin correlations of Z boson to anomalous neutral triple gauge couplings at lepton collider with polarized beams","authors":["Amir Subba","Ritesh K. Singh"],"raw_abstract":"We investigate the effects of anomalous neutral triple gauge couplings in\n$ZZ$ and $Z\\gamma$ production processes, followed by the leptonic decay of the\n$Z$ boson, at a lepton collider with center-of-mass energy $\\sqrt{s}=250$~GeV\nand polarized beams. We use an effective Lagrangian formalism to parameterize\nthe anomalous couplings in terms of dimension-8 operators $c_{\\widetilde{B}W}$,\n$c_{BW}$, $c_{WW}$, and $c_{BB}$, and study the sensitivity of observables such\nas cross~section, polarization, and spin correlation as functions of these\ncouplings. We perform a Bayesian statistical analysis using Markov Chain Monte\nCarlo methods to determine simultaneous limits on the anomalous couplings,\ntaking into account various luminosities $\\mathcal{L} \\in \\{0.1~\\text{ab}^{-1},\n0.3~\\text{ab}^{-1}, 1~\\text{ab}^{-1}, 3~\\text{ab}^{-1}, 10~\\text{ab}^{-1}\\}$\nand systematic uncertainties. We find that polarization and spin correlation\nobservables significantly enhance the sensitivity to anomalous couplings,\nproviding stringent constraints on these couplings.","publication_date":1700512634,"paper_link":"http://arxiv.org/pdf/2311.12170v1","categories":["Physics"],"abstract":"We investigate the effects of anomalous neutral triple gauge couplings in __FORMULA__ and __FORMULA__ production processes, followed by the leptonic decay of the __FORMULA__ boson, at a lepton collider with center-of-mass energy __FORMULA__~GeV and polarized beams. We use an effective Lagrangian formalism to parameterize the anomalous couplings in terms of dimension-8 operators __FORMULA__, __FORMULA__, __FORMULA__, and __FORMULA__, and study the sensitivity of observables such as cross~section, polarization, and spin correlation as functions of these couplings. We perform a Bayesian statistical analysis using Markov Chain Monte Carlo methods to determine simultaneous limits on the anomalous couplings, taking into account various luminosities __FORMULA__ and systematic uncertainties. We find that polarization and spin correlation observables significantly enhance the sensitivity to anomalous couplings, providing stringent constraints on these couplings."}
{"title":"Quantum Inception Score","authors":["Akira Sone","Naoki Yamamoto"],"raw_abstract":"Motivated by the great success of classical generative models in machine\nlearning, enthusiastic exploration of their quantum version has recently\nstarted. To depart on this journey, it is important to develop a relevant\nmetric to evaluate the quality of quantum generative models; in the classical\ncase, one such examples is the inception score. In this paper, we propose the\nquantum inception score, which relates the quality to the classical capacity of\nthe quantum channel that classifies a given dataset. We prove that, under this\nproposed measure, the quantum generative models provide better quality than\ntheir classical counterparts because of the presence of quantum coherence and\nentanglement. Finally, we harness the quantum fluctuation theorem to\ncharacterize the physical limitation of the quality of quantum generative\nmodels.","publication_date":1700512110,"paper_link":"http://arxiv.org/pdf/2311.12163v1","categories":["Statistics","Physics"],"abstract":"Motivated by the great success of classical generative models in machine learning, enthusiastic exploration of their quantum version has recently started. To depart on this journey, it is important to develop a relevant metric to evaluate the quality of quantum generative models; in the classical case, one such examples is the inception score. In this paper, we propose the quantum inception score, which relates the quality to the classical capacity of the quantum channel that classifies a given dataset. We prove that, under this proposed measure, the quantum generative models provide better quality than their classical counterparts because of the presence of quantum coherence and entanglement. Finally, we harness the quantum fluctuation theorem to characterize the physical limitation of the quality of quantum generative models."}
{"title":"Gene expression in growing cells: A biophysical primer","authors":["Ido Golding","Ariel Amir"],"raw_abstract":"Cell growth and gene expression, essential elements of all living systems,\nhave long been the focus of biophysical interrogation. Advances in single-cell\nmethods have invigorated theoretical studies into these processes. However,\nuntil recently, there was little dialog between the two areas of study. Most\ntheoretical models for gene regulation assumed gene activity to be oblivious to\nthe progression of the cell cycle between birth and division. But there are\nnumerous ways in which the periodic character of all cellular observables can\nmodulate gene expression. The molecular factors required for transcription and\ntranslation increase in number during the cell cycle, but are also diluted due\nto the continuous increase in cell volume. The replication of the genome\nchanges the dosage of those same cellular players but also provides competing\ntargets for regulatory binding. Finally, cell division reduces their number\nagain, and so forth. Stochasticity is inherent to all these biological\nprocesses, manifested in fluctuations in the synthesis and degradation of new\ncellular components as well as the random partitioning of molecules at each\ncell division. The notion of gene expression as stationary is thus hard to\njustify. In this review, we survey the emerging paradigm of cell-cycle\nregulated gene expression, with an emphasis on the global expression patterns\nrather than gene-specific regulation. We discuss recent experimental reports\nwhere cell growth and gene expression were simultaneously measured in\nindividual cells, providing first glimpses into the coupling between the two.\nWhile the experimental findings, not surprisingly, differ among genes and\norganisms, several theoretical models have emerged that attempt to reconcile\nthese differences and form a unifying framework for understanding gene\nexpression in growing cells.","publication_date":1700509522,"paper_link":"http://arxiv.org/pdf/2311.12143v1","categories":["Quantitative Biology","Physics"],"abstract":"Cell growth and gene expression, essential elements of all living systems, have long been the focus of biophysical interrogation. Advances in single-cell methods have invigorated theoretical studies into these processes. However, until recently, there was little dialog between the two areas of study. Most theoretical models for gene regulation assumed gene activity to be oblivious to the progression of the cell cycle between birth and division. But there are numerous ways in which the periodic character of all cellular observables can modulate gene expression. The molecular factors required for transcription and translation increase in number during the cell cycle, but are also diluted due to the continuous increase in cell volume. The replication of the genome changes the dosage of those same cellular players but also provides competing targets for regulatory binding. Finally, cell division reduces their number again, and so forth. Stochasticity is inherent to all these biological processes, manifested in fluctuations in the synthesis and degradation of new cellular components as well as the random partitioning of molecules at each cell division. The notion of gene expression as stationary is thus hard to justify. In this review, we survey the emerging paradigm of cell-cycle regulated gene expression, with an emphasis on the global expression patterns rather than gene-specific regulation. We discuss recent experimental reports where cell growth and gene expression were simultaneously measured in individual cells, providing first glimpses into the coupling between the two. While the experimental findings, not surprisingly, differ among genes and organisms, several theoretical models have emerged that attempt to reconcile these differences and form a unifying framework for understanding gene expression in growing cells."}
{"title":"Optimal photometry of point sources: Joint source flux and background determination on array detectors -- from theory to practical implementation","authors":["Mario L. Vicu\u00f1a","Jorge F. Silva","Rene A. Mendez","Marcos E. Orchard","Sebastian Espinosa","Jeremy Tregloan-Reed"],"raw_abstract":"In this paper we study the joint determination of source and background flux\nfor point sources as observed by digital array detectors. We explicitly compute\nthe two-dimensional Cram\\'er-Rao absolute lower bound (CRLB) as well as the\nperformance bounds for high-dimensional implicit estimators from a generalized\nTaylor expansion. This later approach allows us to obtain computable\nprescriptions for the bias and variance of the joint estimators. We compare\nthese prescriptions with empirical results from numerical simulations in the\ncase of the weighted least squares estimator (introducing an improved version,\ndenoted stochastic weighted least-squares) as well as with the maximum\nlikelihood estimator, finding excellent agreement. We demonstrate that these\nestimators provide quasi-unbiased joint estimations of the flux and background,\nwith a variance that approaches the CRLB very tightly and are, hence, optimal,\nunlike the case of sequential estimation used commonly in astronomical\nphotometry which is sub-optimal. We compare our predictions with numerical\nsimulations of realistic observations, as well as with observations of a\nbona-fide non-variable stellar source observed with TESS, and compare it to the\nresults from the sequential estimation of background and flux, confirming our\ntheoretical expectations. Our practical estimators can be used as benchmarks\nfor general photometric pipelines, or for applications that require maximum\nprecision and accuracy in absolute photometry.","publication_date":1700509435,"paper_link":"http://arxiv.org/pdf/2311.12142v1","categories":["Physics"],"abstract":"In this paper we study the joint determination of source and background flux for point sources as observed by digital array detectors. We explicitly compute the two-dimensional Cram\\'er-Rao absolute lower bound (CRLB) as well as the performance bounds for high-dimensional implicit estimators from a generalized Taylor expansion. This later approach allows us to obtain computable prescriptions for the bias and variance of the joint estimators. We compare these prescriptions with empirical results from numerical simulations in the case of the weighted least squares estimator (introducing an improved version, denoted stochastic weighted least-squares) as well as with the maximum likelihood estimator, finding excellent agreement. We demonstrate that these estimators provide quasi-unbiased joint estimations of the flux and background, with a variance that approaches the CRLB very tightly and are, hence, optimal, unlike the case of sequential estimation used commonly in astronomical photometry which is sub-optimal. We compare our predictions with numerical simulations of realistic observations, as well as with observations of a bona-fide non-variable stellar source observed with TESS, and compare it to the results from the sequential estimation of background and flux, confirming our theoretical expectations. Our practical estimators can be used as benchmarks for general photometric pipelines, or for applications that require maximum precision and accuracy in absolute photometry."}
{"title":"Scramblon loops","authors":["Douglas Stanford","Shreya Vardhan","Shunyu Yao"],"raw_abstract":"In large $N$ chaotic quantum systems, the butterfly effect is mediated by a\ncollective field mode known as the ``scramblon.'' We study self-interactions of\nthe scramblon in variants of the Sachdev-Ye-Kitaev model. In spatially extended\nversions of the model and for large spatial separation, fluctuations described\nby loop diagrams can invalidate the single-scramblon approximation well before\nits contribution to out-of-time-order correlators becomes of order one. We find\na qualitative difference between an incoherent regime at high temperaure (or in\na Brownian version of the model) and a coherent regime at low temperature.","publication_date":1700506992,"paper_link":"http://arxiv.org/pdf/2311.12121v1","categories":["Physics"],"abstract":"In large __FORMULA__ chaotic quantum systems, the butterfly effect is mediated by a collective field mode known as the ``scramblon.'' We study self-interactions of the scramblon in variants of the Sachdev-Ye-Kitaev model. In spatially extended versions of the model and for large spatial separation, fluctuations described by loop diagrams can invalidate the single-scramblon approximation well before its contribution to out-of-time-order correlators becomes of order one. We find a qualitative difference between an incoherent regime at high temperaure (or in a Brownian version of the model) and a coherent regime at low temperature."}
{"title":"Calibrating signal-to-noise ratio detection thresholds using gravitational-wave catalogs","authors":["Matthew Mould","Christopher J. Moore","Davide Gerosa"],"raw_abstract":"Searching for gravitational-wave signals is a challenging and computationally\nintensive endeavor undertaken by multiple independent analysis pipelines. While\ndetection should depend only on the observed noisy data, there are situations\nwhere it is instead unphysically defined in terms also of the true source\nparameters through the optimal signal-to-noise ratio (SNR). For example, the\ncomputational cost of applying real searches is often too prohibitive for large\nsimulated astrophysical populations to create predictive catalogs. By\nsimultaneously modeling selection effects and the intrinsic SNR distribution,\nwe infer the corresponding detection threshold from the real catalog in a\nBayesian setting, thereby calibrating the unphysical model to the physical one.\nFrom the latest open data we infer detection thresholds in the network optimal\nSNR of $10.5_{-2.4}^{+2.1}$, $11.2_{-1.4}^{+1.2}$, and $9.1_{-0.5}^{+0.5}$\n(medians and symmetric 90% credible intervals) for the first, second, and third\nobserving runs, respectively, and that the distribution is consistent with a\nfourth-order power law. We additionally relax the assumption of a step-function\nthreshold and show how the assumed distribution can be validated with more\nflexible models such as normalizing flows, which is useful for ranking\nstatistics with no obvious prior. Employing the same detection criteria that we\ninfer will allow modelers to apply approximate but efficient selection effects\ncalibrated to full searches on real signals.","publication_date":1700506838,"paper_link":"http://arxiv.org/pdf/2311.12117v1","categories":["Physics"],"abstract":"Searching for gravitational-wave signals is a challenging and computationally intensive endeavor undertaken by multiple independent analysis pipelines. While detection should depend only on the observed noisy data, there are situations where it is instead unphysically defined in terms also of the true source parameters through the optimal signal-to-noise ratio (SNR). For example, the computational cost of applying real searches is often too prohibitive for large simulated astrophysical populations to create predictive catalogs. By simultaneously modeling selection effects and the intrinsic SNR distribution, we infer the corresponding detection threshold from the real catalog in a Bayesian setting, thereby calibrating the unphysical model to the physical one. From the latest open data we infer detection thresholds in the network optimal SNR of __FORMULA__, __FORMULA__, and __FORMULA__ (medians and symmetric 90% credible intervals) for the first, second, and third observing runs, respectively, and that the distribution is consistent with a fourth-order power law. We additionally relax the assumption of a step-function threshold and show how the assumed distribution can be validated with more flexible models such as normalizing flows, which is useful for ranking statistics with no obvious prior. Employing the same detection criteria that we infer will allow modelers to apply approximate but efficient selection effects calibrated to full searches on real signals."}
{"title":"Nuclear regions as seen with LOFAR international baselines: A high-resolution study of the recurrent activity","authors":["N. Jurlin","R. Morganti","F. Sweijen","L. K. Morabito","M. Brienza","P. Barthel","G. K. Miley"],"raw_abstract":"Radio galaxies dominate the radio sky and are essential to the galaxy\nevolution puzzle. High-resolution studies of statistical samples of radio\ngalaxies are expected to shed light on the triggering mechanisms of the AGN,\nalternating between the phases of activity and quiescence. In this work, we\nfocus on the sub-arcsec radio structures in the central regions of the 35 radio\ngalaxies over 6.6 $deg^2$ of the Lockman Hole region. These sources were\npreviously classified as active, remnant, and candidate restarted radio\ngalaxies using 150 MHz LOFAR observations. We examine the morphologies and\nstudy the spectral properties of their central regions to explore their\nevolutionary stages and revise the criteria used to select the initial sample.\nWe use the newly available LOFAR 150 MHz image obtained using international\nbaselines, achieving 0.38'' x 0.30'' resolution, making this the first\nsystematic study of the nuclear regions at high resolution and low frequency.\nWe use publicly available images from the FIRST survey at 1.4 GHz and the Karl\nG. Jansky VLA Sky Survey at 3 GHz to achieve our goals. In addition, for one\nrestarted candidate we present new dedicated observations with the VLA at 3\nGHz. We have found various morphologies of the central regions of the radio\ngalaxies in our sample, some resembling miniature double-double radio galaxies.\nWe also see the beginnings of active jets or distinct detections unrelated to\nthe large-scale structure. Furthermore, we have found diverse radio spectra in\nour sample - flat, steep, or peaked between 150 MHz and 3 GHz, indicative of\nthe different life-cycle phases. Based on these analyses, we confirm five of\nsix previously considered restarted candidates and identify three more from the\nactive sample, supporting previous results suggesting that the restarted phase\ncan occur after a relatively short remnant phase (i.e. a few tens of millions\nof years).","publication_date":1700506805,"paper_link":"http://arxiv.org/pdf/2311.12114v1","categories":["Physics"],"abstract":"Radio galaxies dominate the radio sky and are essential to the galaxy evolution puzzle. High-resolution studies of statistical samples of radio galaxies are expected to shed light on the triggering mechanisms of the AGN, alternating between the phases of activity and quiescence. In this work, we focus on the sub-arcsec radio structures in the central regions of the 35 radio galaxies over 6.6 __FORMULA__ of the Lockman Hole region. These sources were previously classified as active, remnant, and candidate restarted radio galaxies using 150 MHz LOFAR observations. We examine the morphologies and study the spectral properties of their central regions to explore their evolutionary stages and revise the criteria used to select the initial sample. We use the newly available LOFAR 150 MHz image obtained using international baselines, achieving 0.38'' x 0.30'' resolution, making this the first systematic study of the nuclear regions at high resolution and low frequency. We use publicly available images from the FIRST survey at 1.4 GHz and the Karl G. Jansky VLA Sky Survey at 3 GHz to achieve our goals. In addition, for one restarted candidate we present new dedicated observations with the VLA at 3 GHz. We have found various morphologies of the central regions of the radio galaxies in our sample, some resembling miniature double-double radio galaxies. We also see the beginnings of active jets or distinct detections unrelated to the large-scale structure. Furthermore, we have found diverse radio spectra in our sample - flat, steep, or peaked between 150 MHz and 3 GHz, indicative of the different life-cycle phases. Based on these analyses, we confirm five of six previously considered restarted candidates and identify three more from the active sample, supporting previous results suggesting that the restarted phase can occur after a relatively short remnant phase (i.e. a few tens of millions of years)."}
{"title":"Characterizing Structure Formation through Instance Segmentation","authors":["Daniel L\u00f3pez-Cano","Jens St\u00fccker","Marcos Pellejero Iba\u00f1ez Ra\u00fal E. Angulo","Daniel Franco-Barranco"],"raw_abstract":"Dark matter haloes form from small perturbations to the almost homogeneous\ndensity field of the early universe. Although it is known how large these\ninitial perturbations must be to form haloes, it is rather poorly understood\nhow to predict which particles will end up belonging to which halo. However, it\nis this process that determines the Lagrangian shape of protohaloes and is\ntherefore essential to understand their mass, spin and formation history. Here,\nwe present a machine-learning framework to learn how the protohalo regions of\ndifferent haloes emerge from the initial density field. This involves one\nneural network to distinguish semantically which particles become part of any\nhalo and a second neural network that groups these particles by halo membership\ninto different instances. This instance segmentation is done through the\nWeinberger method, in which the network maps particles into a pseudo-space\nrepresentation where different instances can be distinguished easily through a\nsimple clustering algorithm. Our model reliably predicts the masses and\nLagrangian shapes of haloes object-by-object, as well as summary statistics\nlike the halo-mass function. We find that our model extracts information close\nto optimal by comparing it to the degree of agreement between two N-body\nsimulations with slight differences in their initial conditions. We publish our\nmodel open-source and suggest that it can be used to inform analytical methods\nof structure formation by studying the effect of systematic manipulations of\nthe initial conditions.","publication_date":1700506804,"paper_link":"http://arxiv.org/pdf/2311.12110v1","categories":["Physics"],"abstract":"Dark matter haloes form from small perturbations to the almost homogeneous density field of the early universe. Although it is known how large these initial perturbations must be to form haloes, it is rather poorly understood how to predict which particles will end up belonging to which halo. However, it is this process that determines the Lagrangian shape of protohaloes and is therefore essential to understand their mass, spin and formation history. Here, we present a machine-learning framework to learn how the protohalo regions of different haloes emerge from the initial density field. This involves one neural network to distinguish semantically which particles become part of any halo and a second neural network that groups these particles by halo membership into different instances. This instance segmentation is done through the Weinberger method, in which the network maps particles into a pseudo-space representation where different instances can be distinguished easily through a simple clustering algorithm. Our model reliably predicts the masses and Lagrangian shapes of haloes object-by-object, as well as summary statistics like the halo-mass function. We find that our model extracts information close to optimal by comparing it to the degree of agreement between two N-body simulations with slight differences in their initial conditions. We publish our model open-source and suggest that it can be used to inform analytical methods of structure formation by studying the effect of systematic manipulations of the initial conditions."}
{"title":"Astronomy & Astrophysics in ICAD History","authors":["Rub\u00e9n Garc\u00eda-Benito"],"raw_abstract":"The International Conference on Auditory Display (ICAD) is a significant\nevent for researchers and practitioners interested in exploring the use of\nsound in conveying information and data. Since its inception in 1994, the\nconference has served as a vital forum for exchanging ideas and presenting\nresearch findings in the field of auditory display. While the conference\nprimarily focuses on auditory display and sound design, astronomy has made its\npresence felt in the proceedings of the conference over the years. However, its\nnot until the current ICAD conference where astronomy features a dedicated\nsession. This paper aims to provide an statistical overview of the presence of\nastronomy in the ICAD conference's history from 1994 to 2022, highlighting some\nof the contributions made by researchers in this area, as well as the topics of\ninterest that have captured the attention of sound artists.","publication_date":1700506802,"paper_link":"http://arxiv.org/pdf/2311.12101v1","categories":["Physics"],"abstract":"The International Conference on Auditory Display (ICAD) is a significant event for researchers and practitioners interested in exploring the use of sound in conveying information and data. Since its inception in 1994, the conference has served as a vital forum for exchanging ideas and presenting research findings in the field of auditory display. While the conference primarily focuses on auditory display and sound design, astronomy has made its presence felt in the proceedings of the conference over the years. However, its not until the current ICAD conference where astronomy features a dedicated session. This paper aims to provide an statistical overview of the presence of astronomy in the ICAD conference's history from 1994 to 2022, highlighting some of the contributions made by researchers in this area, as well as the topics of interest that have captured the attention of sound artists."}
{"title":"Euclid preparation TBD. Spectroscopy of active galactic nuclei with NISP","authors":["Euclid Collaboration","E. Lusso","S. Fotopoulou","M. Selwood","V. Allevato","G. Calderone","C. Mancini","M. Mignoli","M. Scodeggio","L. Bisigello","A. Feltre","F. Ricci","F. La Franca","D. Vergani","L. Gabarra","V. Le Brun","E. Maiorano","E. Palazzi","M. Moresco","G. Zamorani","G. Cresci","K. Jahnke","A. Humphrey","H. Landt","F. Mannucci","A. Marconi","L. Pozzetti","P. Salucci","M. Salvato","F. Shankar","L. Spinoglio","D. Stern","S. Serjeant","N. Aghanim","B. Altieri","A. Amara","S. Andreon","T. Auphan","N. Auricchio","M. Baldi","S. Bardelli","R. Bender","D. Bonino","E. Branchini","M. Brescia","J. Brinchmann","S. Camera","V. Capobianco","C. Carbone","J. Carretero","S. Casas","M. Castellano","S. Cavuoti","A. Cimatti","G. Congedo","C. J. Conselice","L. Conversi","Y. Copin","L. Corcione","F. Courbin","H. M. Courtois","J. Dinis","F. Dubath","C. A. J. Duncan","X. Dupac","S. Dusini","M. Farina","S. Farrens","S. Ferriol","N. Fourmanoit","M. Frailis","E. Franceschi","P. Franzetti","M. Fumana","S. Galeotta","B. Garilli","W. Gillard","B. Gillis","C. Giocoli","A. Grazian","F. Grupp","S. V. H. Haugan","W. Holmes","I. Hook","F. Hormuth","A. Hornstrup","M. K\u00fcmmel","E. Keih\u00e4nen","S. Kermiche","B. Kubik","M. Kunz","H. Kurki-Suonio","S. Ligori","P. B. Lilje","V. Lindholm","I. Lloro","O. Mansutti","O. Marggraf","K. Markovic","N. Martinet","F. Marulli","R. Massey","E. Medinaceli","S. Mei","Y. Mellier","E. Merlin","G. Meylan","L. Moscardini","E. Munari","S. -M. Niemi","C. Padilla","S. Paltani","F. Pasian","K. Pedersen","W. J. Percival","V. Pettorino","G. Polenta","M. Poncet","L. A. Popa","F. Raison","R. Rebolo","A. Renzi","J. Rhodes","G. Riccio","E. Romelli","M. Roncarelli","E. Rossetti","R. Saglia","D. Sapone","B. Sartoris","P. Schneider","A. Secroun","G. Seidel","S. Serrano","C. Sirignano","G. Sirri","L. Stanco","C. Surace","P. Tallada-Cresp\u00ed","A. N. Taylor","H. I. Teplitz","I. Tereno","R. Toledo-Moreo","F. Torradeflot","I. Tutusaus","E. A. Valentijn","L. Valenziano","T. Vassallo","A. Veropalumbo","D. Vibert","Y. Wang","J. Weller","J. Zoubian","E. Zucca","A. Biviano","M. Bolzonella","E. Bozzo","C. Burigana","C. Colodro-Conde","D. Di Ferdinando","J. Graci\u00e1-Carpio","G. Mainetti","N. Mauri","C. Neissner","Z. Sakr","V. Scottez","M. Tenti","M. Viel","M. Wiesmann","Y. Akrami","S. Anselmi","C. Baccigalupi","M. Ballardini","M. Bethermin","S. Borgani","A. S. Borlaff","S. Bruton","R. Cabanac","A. Calabro","A. Cappi","C. S. Carvalho","G. Castignani","T. Castro","G. Ca\u00f1as-Herrera","K. C. Chambers","A. R. Cooray","J. Coupon","O. Cucciati","S. Davini","G. De Lucia","G. Desprez","S. Di Domizio","H. Dole","A. D\u00edaz-S\u00e1nchez","J. A. Escartin Vigo","S. Escoffier","I. Ferrero","K. Ganga","J. Garc\u00eda-Bellido","F. Giacomini","G. Gozaliasl","D. Guinet","A. Hall","H. Hildebrandt","A. Jiminez Mu\u00f1oz","J. J. E. Kajava","V. Kansal","C. C. Kirkpatrick","L. Legrand","A. Loureiro","J. Macias-Perez","M. Magliocchetti","R. Maoli","M. Martinelli","C. J. A. P. Martins","S. Matthew","M. Maturi","L. Maurin","R. B. Metcalf","M. Migliaccio","P. Monaco","G. Morgante","S. Nadathur","L. Patrizii","A. Pezzotta","V. Popa","C. Porciani","D. Potter","M. P\u00f6ntinen","P. -F. Rocci","A. G. S\u00e1nchez","A. Schneider","E. Sefusatti","M. Sereno","A. Shulevski","P. Simon","A. Spurio Mancini","J. Stadel","S. A. Stanford","J. Steinwagner","G. Testera","R. Teyssier","S. Toft","S. Tosi","A. Troja","M. Tucci","C. Valieri","J. Valiviita","I. A. Zinchenko"],"raw_abstract":"The statistical distribution and evolution of key properties (e.g. accretion\nrate, mass, or spin) of active galactic nuclei (AGN), remain an open debate in\nastrophysics. The ESA Euclid space mission, launched on July 1st 2023, promises\na breakthrough in this field. We create detailed mock catalogues of AGN\nspectra, from the rest-frame near-infrared down to the ultraviolet, including\nemission lines, to simulate what Euclid will observe for both obscured (type 2)\nand unobscured (type 1) AGN. We concentrate on the red grisms of the NISP\ninstrument, which will be used for the wide-field survey, opening a new window\nfor spectroscopic AGN studies in the near-infrared. We quantify the efficiency\nin the redshift determination as well as in retrieving the emission line flux\nof the H$\\alpha$+[NII] complex as Euclid is mainly focused on this emission\nline as it is expected to be the brightest one in the probed redshift range.\nSpectroscopic redshifts are measured for 83% of the simulated AGN in the\ninterval where the H$\\alpha$+[NII] is visible (0.89<z<1.83 at a line flux\n$>2x10^{-16}$ erg s$^{-1}$ cm$^{-2}$, encompassing the peak of AGN activity at\n$z\\simeq 1-1.5$) within the spectral coverage of the red grism. Outside this\nredshift range, the measurement efficiency decreases significantly. Overall, a\nspectroscopic redshift is correctly determined for ~90% of type 2 AGN down to\nan emission line flux of $3x10^{-16}$ erg s$^{-1}$ cm$^{-2}$, and for type 1\nAGN down to $8.5x10^{-16}$ erg s$^{-1}$ cm$^{-2}$. Recovered black hole mass\nvalues show a small offset with respect to the input values ~10%, but the\nagreement is good overall. With such a high spectroscopic coverage at z<2, we\nwill be able to measure AGN demography, scaling relations, and clustering from\nthe epoch of the peak of AGN activity down to the present-day Universe for\nhundreds of thousand AGN with homogeneous spectroscopic information.","publication_date":1700506801,"paper_link":"http://arxiv.org/pdf/2311.12096v1","categories":["Physics"],"abstract":"The statistical distribution and evolution of key properties (e.g. accretion rate, mass, or spin) of active galactic nuclei (AGN), remain an open debate in astrophysics. The ESA Euclid space mission, launched on July 1st 2023, promises a breakthrough in this field. We create detailed mock catalogues of AGN spectra, from the rest-frame near-infrared down to the ultraviolet, including emission lines, to simulate what Euclid will observe for both obscured (type 2) and unobscured (type 1) AGN. We concentrate on the red grisms of the NISP instrument, which will be used for the wide-field survey, opening a new window for spectroscopic AGN studies in the near-infrared. We quantify the efficiency in the redshift determination as well as in retrieving the emission line flux of the H__FORMULA__+[NII] complex as Euclid is mainly focused on this emission line as it is expected to be the brightest one in the probed redshift range. Spectroscopic redshifts are measured for 83% of the simulated AGN in the interval where the H__FORMULA__+[NII] is visible (0.89<z<1.83 at a line flux __FORMULA__ erg s__FORMULA__ cm__FORMULA__, encompassing the peak of AGN activity at __FORMULA__) within the spectral coverage of the red grism. Outside this redshift range, the measurement efficiency decreases significantly. Overall, a spectroscopic redshift is correctly determined for ~90% of type 2 AGN down to an emission line flux of __FORMULA__ erg s__FORMULA__ cm__FORMULA__, and for type 1 AGN down to __FORMULA__ erg s__FORMULA__ cm__FORMULA__. Recovered black hole mass values show a small offset with respect to the input values ~10%, but the agreement is good overall. With such a high spectroscopic coverage at z<2, we will be able to measure AGN demography, scaling relations, and clustering from the epoch of the peak of AGN activity down to the present-day Universe for hundreds of thousand AGN with homogeneous spectroscopic information."}
{"title":"Monte Carlo solver and renormalization of Migdal-Eliashberg spin chain","authors":["Yang-Zhi Chou","Zhentao Wang","Sankar Das Sarma"],"raw_abstract":"Motivated by the recently developed classical spin model for\nMigdal-Eliashberg theory, we develop new numerical and analytical methods based\non this spin-chain representation and apply these methods to the\nBogoliuov-Tomachov-Morel-Anderson pairing potential, which incorporates the\nphonon-mediated attraction and Coulomb repulsion. We show that the Monte Carlo\nmethod with heat bath updates can efficiently obtain the gap functions even for\nthe situations challenging for the iterative solvers, suggesting an\nunprecedented robust approach for solving the full nonlinear Migdal-Eliashberg\ntheory. Moreover, we derive the renormalization of all the couplings by tracing\nout the high-frequency spins in the partition function. The derived analytical\nrenormalization equations produce the well-known $\\mu^*$ effect for the\nBogoliuov-Tomachov-Morel-Anderson pairing potential and can be generalized to\nother superconductivity problems. We further point out that several interesting\nfeatures (e.g., sign changing in the frequency-dependent gap function) can be\nintuitively understood using the classical spin-chain representation for\nMigdal-Eliasherg theory. Our results show the advantage of using the spin-chain\nrepresentation for solving Migdal-Eliashberg theory and provide new ways for\ntackling general superconductivity problems.","publication_date":1700506800,"paper_link":"http://arxiv.org/pdf/2311.12094v1","categories":["Physics"],"abstract":"Motivated by the recently developed classical spin model for Migdal-Eliashberg theory, we develop new numerical and analytical methods based on this spin-chain representation and apply these methods to the Bogoliuov-Tomachov-Morel-Anderson pairing potential, which incorporates the phonon-mediated attraction and Coulomb repulsion. We show that the Monte Carlo method with heat bath updates can efficiently obtain the gap functions even for the situations challenging for the iterative solvers, suggesting an unprecedented robust approach for solving the full nonlinear Migdal-Eliashberg theory. Moreover, we derive the renormalization of all the couplings by tracing out the high-frequency spins in the partition function. The derived analytical renormalization equations produce the well-known __FORMULA__ effect for the Bogoliuov-Tomachov-Morel-Anderson pairing potential and can be generalized to other superconductivity problems. We further point out that several interesting features (e.g., sign changing in the frequency-dependent gap function) can be intuitively understood using the classical spin-chain representation for Migdal-Eliasherg theory. Our results show the advantage of using the spin-chain representation for solving Migdal-Eliashberg theory and provide new ways for tackling general superconductivity problems."}
{"title":"Macroscopic description of a heavy particle immersed within a flow of light particles","authors":["Radek Erban","Robert A. Van Gorder"],"raw_abstract":"A micro-hydrodynamics model based on elastic collisions of light point\nsolvent particles with a heavy solute particle is investigated in the setting\nwhere the light particles have velocity distribution corresponding to a\nbackground flow. Considering a range of stationary background flows and\ndistributions for the solvent particle velocities, the macroscopic\nLangevin-type description of the behaviour of the heavy particle is derived in\nthe form of a generalized Ornstein-Uhlenbeck process. At leading order, the\ndrift term in this process depends upon both the geometric structure of the\nbackground flow and the size of the heavy particle, while both drift and\ndiffusion terms scale with moments of the light particle velocity distribution.\nComputational methods for simulating the micro-hydrodynamics model are then\ndesigned to confirm the theoretical results. To enable long-time stochastic\nsimulations, simulations are performed in a frame co-moving with the heavy\nparticle. Efficient methods for sampling the position and velocity\ndistributions of incoming solvent particles at the boundaries of the co-moving\nframe are derived for a range of distributions of solvent particles. The\nsimulations show good agreement with the theoretical results.","publication_date":1700506646,"paper_link":"http://arxiv.org/pdf/2311.12021v1","categories":["Physics"],"abstract":"A micro-hydrodynamics model based on elastic collisions of light point solvent particles with a heavy solute particle is investigated in the setting where the light particles have velocity distribution corresponding to a background flow. Considering a range of stationary background flows and distributions for the solvent particle velocities, the macroscopic Langevin-type description of the behaviour of the heavy particle is derived in the form of a generalized Ornstein-Uhlenbeck process. At leading order, the drift term in this process depends upon both the geometric structure of the background flow and the size of the heavy particle, while both drift and diffusion terms scale with moments of the light particle velocity distribution. Computational methods for simulating the micro-hydrodynamics model are then designed to confirm the theoretical results. To enable long-time stochastic simulations, simulations are performed in a frame co-moving with the heavy particle. Efficient methods for sampling the position and velocity distributions of incoming solvent particles at the boundaries of the co-moving frame are derived for a range of distributions of solvent particles. The simulations show good agreement with the theoretical results."}
{"title":"A Heterogeneous Spatial Model for Soil Carbon Mapping of the Contiguous United States Using VNIR Spectra","authors":["Paul A. Parker","Bruno Sans\u00f3"],"raw_abstract":"The Rapid Carbon Assessment, conducted by the U.S. Department of Agriculture,\nwas implemented in order to obtain a representative sample of soil organic\ncarbon across the contiguous United States. In conjunction with a statistical\nmodel, the dataset allows for mapping of soil carbon prediction across the\nU.S., however there are two primary challenges to such an effort. First, there\nexists a large degree of heterogeneity in the data, whereby both the first and\nsecond moments of the data generating process seem to vary both spatially and\nfor different land-use categories. Second, the majority of the sampled\nlocations do not actually have lab measured values for soil organic carbon.\nRather, visible and near-infrared (VNIR) spectra were measured at most\nlocations, which act as a proxy to help predict carbon content. Thus, we\ndevelop a heterogeneous model to analyze this data that allows both the mean\nand the variance to vary as a function of space as well as land-use category,\nwhile incorporating VNIR spectra as covariates. After a cross-validation study\nthat establishes the effectiveness of the model, we construct a complete map of\nsoil organic carbon for the contiguous U.S. along with uncertainty\nquantification.","publication_date":1700506621,"paper_link":"http://arxiv.org/pdf/2311.12020v1","categories":["Statistics"],"abstract":"The Rapid Carbon Assessment, conducted by the U.S. Department of Agriculture, was implemented in order to obtain a representative sample of soil organic carbon across the contiguous United States. In conjunction with a statistical model, the dataset allows for mapping of soil carbon prediction across the U.S., however there are two primary challenges to such an effort. First, there exists a large degree of heterogeneity in the data, whereby both the first and second moments of the data generating process seem to vary both spatially and for different land-use categories. Second, the majority of the sampled locations do not actually have lab measured values for soil organic carbon. Rather, visible and near-infrared (VNIR) spectra were measured at most locations, which act as a proxy to help predict carbon content. Thus, we develop a heterogeneous model to analyze this data that allows both the mean and the variance to vary as a function of space as well as land-use category, while incorporating VNIR spectra as covariates. After a cross-validation study that establishes the effectiveness of the model, we construct a complete map of soil organic carbon for the contiguous U.S. along with uncertainty quantification."}
{"title":"Bayesian Semiparametric Estimation of Heterogeneous Effects in Matched Case-Control Studies with an Application to Alzheimer's Disease and Heat","authors":["Jacob Englert","Stefanie Ebelt","Howard Chang"],"raw_abstract":"Epidemiological approaches for examining human health responses to\nenvironmental exposures in observational studies often control for confounding\nby implementing clever matching schemes and using statistical methods based on\nconditional likelihood. Nonparametric regression models have surged in\npopularity in recent years as a tool for estimating individual-level\nheterogeneous effects, which provide a more detailed picture of the\nexposure-response relationship but can also be aggregated to obtain improved\nmarginal estimates at the population level. In this work we incorporate\nBayesian additive regression trees (BART) into the conditional logistic\nregression model to identify heterogeneous effects of environmental exposures\nin a case-crossover design. Conditional logistic BART (CL-BART) utilizes\nreversible jump Markov chain Monte Carlo to bypass the conditional conjugacy\nrequirement of the original BART algorithm. Our work is motivated by the\ngrowing interest in identifying subpopulations more vulnerable to environmental\nexposures. We apply CL-BART to a study of the impact of heatwaves on people\nwith Alzheimer's Disease in California and effect modification by other chronic\nconditions. Through this application, we also describe strategies to examine\nheterogeneous odds ratios through variable importance, partial dependence, and\nlower-dimensional summaries. CL-BART is available in the clbart R package.","publication_date":1700506486,"paper_link":"http://arxiv.org/pdf/2311.12016v1","categories":["Statistics"],"abstract":"Epidemiological approaches for examining human health responses to environmental exposures in observational studies often control for confounding by implementing clever matching schemes and using statistical methods based on conditional likelihood. Nonparametric regression models have surged in popularity in recent years as a tool for estimating individual-level heterogeneous effects, which provide a more detailed picture of the exposure-response relationship but can also be aggregated to obtain improved marginal estimates at the population level. In this work we incorporate Bayesian additive regression trees (BART) into the conditional logistic regression model to identify heterogeneous effects of environmental exposures in a case-crossover design. Conditional logistic BART (CL-BART) utilizes reversible jump Markov chain Monte Carlo to bypass the conditional conjugacy requirement of the original BART algorithm. Our work is motivated by the growing interest in identifying subpopulations more vulnerable to environmental exposures. We apply CL-BART to a study of the impact of heatwaves on people with Alzheimer's Disease in California and effect modification by other chronic conditions. Through this application, we also describe strategies to examine heterogeneous odds ratios through variable importance, partial dependence, and lower-dimensional summaries. CL-BART is available in the clbart R package."}
{"title":"Categorizing the Visual Environment and Analyzing the Visual Attention of Dogs","authors":["Shreyas Sundara Raman","Madeline H. Pelgrim","Daphna Buchsbaum","Thomas Serre"],"raw_abstract":"Dogs have a unique evolutionary relationship with humans and serve many\nimportant roles e.g. search and rescue, blind assistance, emotional support.\nHowever, few datasets exist to categorize visual features and objects available\nto dogs, as well as how dogs direct their visual attention within their\nenvironment. We collect and study a dataset with over 11,698 gazes to\ncategorize the objects available to be gazed at by 11 dogs in everyday outdoor\nenvironments i.e. a walk around a college campus and urban area. We explore the\navailability of these object categories and the visual attention of dogs over\nthese categories using a head mounted eye tracking apparatus. A small portion\n(approx. 600 images or < 20% of total dataset) of the collected data is used to\nfine tune a MaskRCNN for the novel image domain to segment objects present in\nthe scene, enabling further statistical analysis on the visual gaze tendencies\nof dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to end\nmodel for automatically classifying the visual fixations of dogs. The fine\ntuned MaskRCNN performs far better than chance. There are few individual\ndifferences between the 11 dogs and we observe greater visual fixations on\nbuses, plants, pavement, and construction equipment. This work takes a step\ntowards understanding visual behavior of dogs and their interaction with the\nphysical world.","publication_date":1700504478,"paper_link":"http://arxiv.org/pdf/2311.11988v1","categories":["Statistics"],"abstract":"Dogs have a unique evolutionary relationship with humans and serve many important roles e.g. search and rescue, blind assistance, emotional support. However, few datasets exist to categorize visual features and objects available to dogs, as well as how dogs direct their visual attention within their environment. We collect and study a dataset with over 11,698 gazes to categorize the objects available to be gazed at by 11 dogs in everyday outdoor environments i.e. a walk around a college campus and urban area. We explore the availability of these object categories and the visual attention of dogs over these categories using a head mounted eye tracking apparatus. A small portion (approx. 600 images or < 20% of total dataset) of the collected data is used to fine tune a MaskRCNN for the novel image domain to segment objects present in the scene, enabling further statistical analysis on the visual gaze tendencies of dogs. The MaskRCNN, with eye tracking apparatus, serves as an end to end model for automatically classifying the visual fixations of dogs. The fine tuned MaskRCNN performs far better than chance. There are few individual differences between the 11 dogs and we observe greater visual fixations on buses, plants, pavement, and construction equipment. This work takes a step towards understanding visual behavior of dogs and their interaction with the physical world."}
{"title":"SA-Med2D-20M Dataset: Segment Anything in 2D Medical Imaging with 20 Million masks","authors":["Jin Ye","Junlong Cheng","Jianpin Chen","Zhongying Deng","Tianbin Li","Haoyu Wang","Yanzhou Su","Ziyan Huang","Jilong Chen","Lei Jiang","Hui Sun","Min Zhu","Shaoting Zhang","Junjun He","Yu Qiao"],"raw_abstract":"Segment Anything Model (SAM) has achieved impressive results for natural\nimage segmentation with input prompts such as points and bounding boxes. Its\nsuccess largely owes to massive labeled training data. However, directly\napplying SAM to medical image segmentation cannot perform well because SAM\nlacks medical knowledge -- it does not use medical images for training. To\nincorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a\nlarge-scale segmentation dataset of 2D medical images built upon numerous\npublic and private datasets. It consists of 4.6 million 2D medical images and\n19.7 million corresponding masks, covering almost the whole body and showing\nsignificant diversity. This paper describes all the datasets collected in\nSA-Med2D-20M and details how to process these datasets. Furthermore,\ncomprehensive statistics of SA-Med2D-20M are presented to facilitate the better\nuse of our dataset, which can help the researchers build medical vision\nfoundation models or apply their models to downstream medical applications. We\nhope that the large scale and diversity of SA-Med2D-20M can be leveraged to\ndevelop medical artificial intelligence for enhancing diagnosis, medical image\nanalysis, knowledge sharing, and education. The data with the redistribution\nlicense is publicly available at https://github.com/OpenGVLab/SAM-Med2D.","publication_date":1700503143,"paper_link":"http://arxiv.org/pdf/2311.11969v1","categories":["Electrical Engineering and Systems Science"],"abstract":"Segment Anything Model (SAM) has achieved impressive results for natural image segmentation with input prompts such as points and bounding boxes. Its success largely owes to massive labeled training data. However, directly applying SAM to medical image segmentation cannot perform well because SAM lacks medical knowledge -- it does not use medical images for training. To incorporate medical knowledge into SAM, we introduce SA-Med2D-20M, a large-scale segmentation dataset of 2D medical images built upon numerous public and private datasets. It consists of 4.6 million 2D medical images and 19.7 million corresponding masks, covering almost the whole body and showing significant diversity. This paper describes all the datasets collected in SA-Med2D-20M and details how to process these datasets. Furthermore, comprehensive statistics of SA-Med2D-20M are presented to facilitate the better use of our dataset, which can help the researchers build medical vision foundation models or apply their models to downstream medical applications. We hope that the large scale and diversity of SA-Med2D-20M can be leveraged to develop medical artificial intelligence for enhancing diagnosis, medical image analysis, knowledge sharing, and education. The data with the redistribution license is publicly available at https://github.com/OpenGVLab/SAM-Med2D."}
{"title":"Point Processes and the Statistics of Cellular Neighborhoods in Simple Multicellular Organisms","authors":["Anand Srinivasan","Steph Hohn","Raymond E. Goldstein"],"raw_abstract":"Recent work on several distinct multicellular organisms has revealed a\nhitherto unknown type of biological noise; rather than a highly regular\narrangement, cellular neighborhood volumes, obtained by Voronoi tessellations\nof the cell locations, are broadly distributed and consistent with gamma\ndistributions. We propose an explanation for those observations in the case of\nthe alga $Volvox$, whose somatic cells are embedded in an extracellular matrix\n(ECM) exported by cells. We show that the simplest models for stochastic ECM\ngeneration in one and two dimensions are point processes whose Voronoi\ntesselations are demonstrably governed by gamma distributions. These results\nhighlight a universal consequence of intrinsic biological noise on the\narchitecture of certain tissues.","publication_date":1700500953,"paper_link":"http://arxiv.org/pdf/2311.11939v1","categories":["Quantitative Biology","Physics"],"abstract":"Recent work on several distinct multicellular organisms has revealed a hitherto unknown type of biological noise; rather than a highly regular arrangement, cellular neighborhood volumes, obtained by Voronoi tessellations of the cell locations, are broadly distributed and consistent with gamma distributions. We propose an explanation for those observations in the case of the alga __FORMULA__, whose somatic cells are embedded in an extracellular matrix (ECM) exported by cells. We show that the simplest models for stochastic ECM generation in one and two dimensions are point processes whose Voronoi tesselations are demonstrably governed by gamma distributions. These results highlight a universal consequence of intrinsic biological noise on the architecture of certain tissues."}
{"title":"Interleaving Distances, Monoidal Actions and 2-Categories","authors":["Patrick K. McFaddin","Tom Needham"],"raw_abstract":"Interleaving distances are used widely in Topological Data Analysis (TDA) as\na tool for comparing topological signatures of datasets. The theory of\ninterleaving distances has been extended through various category-theoretic\nconstructions, enabling its usage beyond standard constructions of TDA, while\nclarifying certain observed stability phenomena by unifying them under a common\nframework. Inspired by metrics used in the field of statistical shape analysis,\nwhich are based on minimizing energy functions over group actions, we define\nthree new types of increasingly general interleaving distances. Our\nconstructions use ideas from the theories of monoidal actions and 2-categories.\nWe show that these distances naturally extend the category with a flow\nframework of de Silva, Munch and Stefanou and the locally persistent category\nframework of Scoccola, and we provide a general stability result. Along the\nway, we give examples of distances that fit into our framework which connect to\nideas from differential geometry, geometric shape analysis, statistical TDA and\nmultiparameter persistent homology.","publication_date":1700500789,"paper_link":"http://arxiv.org/pdf/2311.11936v1","categories":["Mathematics"],"abstract":"Interleaving distances are used widely in Topological Data Analysis (TDA) as a tool for comparing topological signatures of datasets. The theory of interleaving distances has been extended through various category-theoretic constructions, enabling its usage beyond standard constructions of TDA, while clarifying certain observed stability phenomena by unifying them under a common framework. Inspired by metrics used in the field of statistical shape analysis, which are based on minimizing energy functions over group actions, we define three new types of increasingly general interleaving distances. Our constructions use ideas from the theories of monoidal actions and 2-categories. We show that these distances naturally extend the category with a flow framework of de Silva, Munch and Stefanou and the locally persistent category framework of Scoccola, and we provide a general stability result. Along the way, we give examples of distances that fit into our framework which connect to ideas from differential geometry, geometric shape analysis, statistical TDA and multiparameter persistent homology."}
{"title":"Estimation of entropy-regularized optimal transport maps between non-compactly supported measures","authors":["Matthew Werenski","James M. Murphy","Shuchin Aeron"],"raw_abstract":"This paper addresses the problem of estimating entropy-regularized optimal\ntransport (EOT) maps with squared-Euclidean cost between source and target\nmeasures that are subGaussian. In the case that the target measure is compactly\nsupported or strongly log-concave, we show that for a recently proposed\nin-sample estimator, the expected squared $L^2$-error decays at least as fast\nas $O(n^{-1/3})$ where $n$ is the sample size. For the general subGaussian case\nwe show that the expected $L^1$-error decays at least as fast as $O(n^{-1/6})$,\nand in both cases we have polynomial dependence on the regularization\nparameter. While these results are suboptimal compared to known results in the\ncase of compactness of both the source and target measures (squared $L^2$-error\nconverging at a rate $O(n^{-1})$) and for when the source is subGaussian while\nthe target is compactly supported (squared $L^2$-error converging at a rate\n$O(n^{-1/2})$), their importance lie in eliminating the compact support\nrequirements. The proof technique makes use of a bias-variance decomposition\nwhere the variance is controlled using standard concentration of measure\nresults and the bias is handled by T1-transport inequalities along with sample\ncomplexity results in estimation of EOT cost under subGaussian assumptions. Our\nexperimental results point to a looseness in controlling the variance terms and\nwe conclude by posing several open problems.","publication_date":1700500701,"paper_link":"http://arxiv.org/pdf/2311.11934v1","categories":["Mathematics","Statistics"],"abstract":"This paper addresses the problem of estimating entropy-regularized optimal transport (EOT) maps with squared-Euclidean cost between source and target measures that are subGaussian. In the case that the target measure is compactly supported or strongly log-concave, we show that for a recently proposed in-sample estimator, the expected squared __FORMULA__-error decays at least as fast as __FORMULA__ where __FORMULA__ is the sample size. For the general subGaussian case we show that the expected __FORMULA__-error decays at least as fast as __FORMULA__, and in both cases we have polynomial dependence on the regularization parameter. While these results are suboptimal compared to known results in the case of compactness of both the source and target measures (squared __FORMULA__-error converging at a rate __FORMULA__) and for when the source is subGaussian while the target is compactly supported (squared __FORMULA__-error converging at a rate __FORMULA__), their importance lie in eliminating the compact support requirements. The proof technique makes use of a bias-variance decomposition where the variance is controlled using standard concentration of measure results and the bias is handled by T1-transport inequalities along with sample complexity results in estimation of EOT cost under subGaussian assumptions. Our experimental results point to a looseness in controlling the variance terms and we conclude by posing several open problems."}
{"title":"Evaluating the Surrogate Index as a Decision-Making Tool Using 200 A/B Tests at Netflix","authors":["Vickie Zhang","Michael Zhao","Anh Le","Nathan Kallus"],"raw_abstract":"Surrogate index approaches have recently become a popular method of\nestimating longer-term impact from shorter-term outcomes. In this paper, we\nleverage 1098 test arms from 200 A/B tests at Netflix to empirically\ninvestigate to what degree would decisions made using a surrogate index\nutilizing 14 days of data would align with those made using direct measurement\nof day 63 treatment effects. Focusing specifically on linear \"auto-surrogate\"\nmodels that utilize the shorter-term observations of the long-term outcome of\ninterest, we find that the statistical inferences that we would draw from using\nthe surrogate index are ~95% consistent with those from directly measuring the\nlong-term treatment effect. Moreover, when we restrict ourselves to the set of\ntests that would be \"launched\" (i.e. positive and statistically significant)\nbased on the 63-day directly measured treatment effects, we find that relying\ninstead on the surrogate index achieves 79% and 65% recall.","publication_date":1700499558,"paper_link":"http://arxiv.org/pdf/2311.11922v1","categories":["Statistics"],"abstract":"Surrogate index approaches have recently become a popular method of estimating longer-term impact from shorter-term outcomes. In this paper, we leverage 1098 test arms from 200 A/B tests at Netflix to empirically investigate to what degree would decisions made using a surrogate index utilizing 14 days of data would align with those made using direct measurement of day 63 treatment effects. Focusing specifically on linear \"auto-surrogate\" models that utilize the shorter-term observations of the long-term outcome of interest, we find that the statistical inferences that we would draw from using the surrogate index are ~95% consistent with those from directly measuring the long-term treatment effect. Moreover, when we restrict ourselves to the set of tests that would be \"launched\" (i.e. positive and statistically significant) based on the 63-day directly measured treatment effects, we find that relying instead on the surrogate index achieves 79% and 65% recall."}
{"title":"Measuring and Mitigating Biases in Motor Insurance Pricing","authors":["Mulah Moriah","Franck Vermet","Arthur Charpentier"],"raw_abstract":"The non-life insurance sector operates within a highly competitive and\ntightly regulated framework, confronting a pivotal juncture in the formulation\nof pricing strategies. Insurers are compelled to harness a range of statistical\nmethodologies and available data to construct optimal pricing structures that\nalign with the overarching corporate strategy while accommodating the dynamics\nof market competition. Given the fundamental societal role played by insurance,\npremium rates are subject to rigorous scrutiny by regulatory authorities. These\nrates must conform to principles of transparency, explainability, and ethical\nconsiderations. Consequently, the act of pricing transcends mere statistical\ncalculations and carries the weight of strategic and societal factors. These\nmultifaceted concerns may drive insurers to establish equitable premiums,\ntaking into account various variables. For instance, regulations mandate the\nprovision of equitable premiums, considering factors such as policyholder\ngender or mutualist group dynamics in accordance with respective corporate\nstrategies. Age-based premium fairness is also mandated. In certain insurance\ndomains, variables such as the presence of serious illnesses or disabilities\nare emerging as new dimensions for evaluating fairness. Regardless of the\nmotivating factor prompting an insurer to adopt fairer pricing strategies for a\nspecific variable, the insurer must possess the capability to define, measure,\nand ultimately mitigate any ethical biases inherent in its pricing practices\nwhile upholding standards of consistency and performance. This study seeks to\nprovide a comprehensive set of tools for these endeavors and assess their\neffectiveness through practical application in the context of automobile\ninsurance.","publication_date":1700498088,"paper_link":"http://arxiv.org/pdf/2311.11900v1","categories":["Statistics"],"abstract":"The non-life insurance sector operates within a highly competitive and tightly regulated framework, confronting a pivotal juncture in the formulation of pricing strategies. Insurers are compelled to harness a range of statistical methodologies and available data to construct optimal pricing structures that align with the overarching corporate strategy while accommodating the dynamics of market competition. Given the fundamental societal role played by insurance, premium rates are subject to rigorous scrutiny by regulatory authorities. These rates must conform to principles of transparency, explainability, and ethical considerations. Consequently, the act of pricing transcends mere statistical calculations and carries the weight of strategic and societal factors. These multifaceted concerns may drive insurers to establish equitable premiums, taking into account various variables. For instance, regulations mandate the provision of equitable premiums, considering factors such as policyholder gender or mutualist group dynamics in accordance with respective corporate strategies. Age-based premium fairness is also mandated. In certain insurance domains, variables such as the presence of serious illnesses or disabilities are emerging as new dimensions for evaluating fairness. Regardless of the motivating factor prompting an insurer to adopt fairer pricing strategies for a specific variable, the insurer must possess the capability to define, measure, and ultimately mitigate any ethical biases inherent in its pricing practices while upholding standards of consistency and performance. This study seeks to provide a comprehensive set of tools for these endeavors and assess their effectiveness through practical application in the context of automobile insurance."}
{"title":"Diffusion with two resetting points","authors":["Pedro Juli\u00e1n Salgado","Leonardo Dagdug","Denis Boyer"],"raw_abstract":"We study the problem of a target search by a Brownian particle subject to\nstochastic resetting to a pair of sites. The mean search time is minimized by\nan optimal resetting rate which does not vary smoothly, in contrast with the\nwell-known single site case, but exhibits a discontinuous transition as the\nposition of one resetting site is varied, while keeping the initial position of\nthe particle fixed. The discontinuity vanishes at a \\lq\\lq liquid-gas\" critical\npoint in the positions space. This critical point exists provided that the\nrelative weight of the further site is comprised in the interval\n$[2.9028...,8.5603...]$. This setup can be mapped onto an intermittent search\nproblem with switching diffusion coefficients and represents a minimal model\nfor the study of distributed resetting.","publication_date":1700497943,"paper_link":"http://arxiv.org/pdf/2311.11897v1","categories":["Quantitative Biology","Physics"],"abstract":"We study the problem of a target search by a Brownian particle subject to stochastic resetting to a pair of sites. The mean search time is minimized by an optimal resetting rate which does not vary smoothly, in contrast with the well-known single site case, but exhibits a discontinuous transition as the position of one resetting site is varied, while keeping the initial position of the particle fixed. The discontinuity vanishes at a \\lq\\lq liquid-gas\" critical point in the positions space. This critical point exists provided that the relative weight of the further site is comprised in the interval __FORMULA__. This setup can be mapped onto an intermittent search problem with switching diffusion coefficients and represents a minimal model for the study of distributed resetting."}
{"title":"Optimal escapes in active matter","authors":["Luca Angelani"],"raw_abstract":"The out-of-equilibrium character of active particles, responsible for\naccumulation at boundaries in confining domains, determines not-trivial effects\nwhen considering escape processes. Non-monotonous behavior of exit times with\nrespect to tumbling rate (inverse of mean persistent time) appears, as a\nconsequence of the competing processes of exploring the bulk and accumulate at\nboundaries. By using both 1D analytical results and 2D numerical simulations of\nrun-and-tumble particles with different behaviours at boundaries, we scrutinize\nthis very general phenomenon of active matter, evidencing the role of\naccumulation at walls for the existence of optimal tumbling rates for fast\nescapes.","publication_date":1700496673,"paper_link":"http://arxiv.org/pdf/2311.11873v1","categories":["Physics"],"abstract":"The out-of-equilibrium character of active particles, responsible for accumulation at boundaries in confining domains, determines not-trivial effects when considering escape processes. Non-monotonous behavior of exit times with respect to tumbling rate (inverse of mean persistent time) appears, as a consequence of the competing processes of exploring the bulk and accumulate at boundaries. By using both 1D analytical results and 2D numerical simulations of run-and-tumble particles with different behaviours at boundaries, we scrutinize this very general phenomenon of active matter, evidencing the role of accumulation at walls for the existence of optimal tumbling rates for fast escapes."}
{"title":"Statistical Prediction of Peaks Over a Threshold","authors":["S. A. Padoan","Stefano Rizzelli"],"raw_abstract":"In many applied fields it is desired to make predictions with the aim of\nassessing the plausibility of more severe events than those already recorded to\nsafeguard against calamities that have not yet occurred. This problem can be\nanalysed using extreme value theory. We consider the popular peaks over a\nthreshold method and show that the generalised Pareto approximation of the true\npredictive densities of both a future unobservable excess or peak random\nvariable can be very accurate. We propose both a frequentist and a Bayesian\napproach for the estimation of such predictive densities. We show the\nasymptotic accuracy of the corresponding estimators and, more importantly,\nprove that the resulting predictive inference is asymptotically reliable. We\nshow the utility of the proposed predictive tools analysing extreme\ntemperatures in Milan in Italy.","publication_date":1700494881,"paper_link":"http://arxiv.org/pdf/2311.11852v1","categories":["Statistics"],"abstract":"In many applied fields it is desired to make predictions with the aim of assessing the plausibility of more severe events than those already recorded to safeguard against calamities that have not yet occurred. This problem can be analysed using extreme value theory. We consider the popular peaks over a threshold method and show that the generalised Pareto approximation of the true predictive densities of both a future unobservable excess or peak random variable can be very accurate. We propose both a frequentist and a Bayesian approach for the estimation of such predictive densities. We show the asymptotic accuracy of the corresponding estimators and, more importantly, prove that the resulting predictive inference is asymptotically reliable. We show the utility of the proposed predictive tools analysing extreme temperatures in Milan in Italy."}
{"title":"An implementation of nDGP gravity in Pinocchio","authors":["Yanling Song","Bin Hu","Chengzong Ruan","Chiara Moretti","Pierluigi Monaco"],"raw_abstract":"In this paper we investigate dark matter structure formation in the normal\nbranch of the Dvali-Gabadadze-Porrati (nDGP) model using the PINOCCHIO\nalgorithm. We first present 2nd order Lagrangian perturbation theory for the\nnDGP model, which shows that the 1st- and 2nd-order growth functions in nDGP\nare larger than those in {\\Lambda}CDM. We then examine the dynamics of\nellipsoidal collapse in nDGP, which is accelerated compared to {\\Lambda}CDM due\nto enhanced gravitational interactions. Running the nDGP-PINOCCHIO code with a\nbox size of 512 Mpc/h and 1024*1024*1024 particles, we analyze the statistical\nproperties of the output halo catalogs, including the halo power spectrum and\nhalo mass function. The calibrated PINOCCHIO halo power spectrum agrees with\nN-body simulations within 5% in the comoving wavenumber range k < 0.3 (h/Mpc)\nat redshift z = 0. The agreement is extended to smaller scales for higher\nredshifts. For the cumulative halo mass function, the agreement between N-body\nand PINOCCHIO is also within the simulation scatter.","publication_date":1700493399,"paper_link":"http://arxiv.org/pdf/2311.11840v1","categories":["Physics"],"abstract":"In this paper we investigate dark matter structure formation in the normal branch of the Dvali-Gabadadze-Porrati (nDGP) model using the PINOCCHIO algorithm. We first present 2nd order Lagrangian perturbation theory for the nDGP model, which shows that the 1st- and 2nd-order growth functions in nDGP are larger than those in {\\Lambda}CDM. We then examine the dynamics of ellipsoidal collapse in nDGP, which is accelerated compared to {\\Lambda}CDM due to enhanced gravitational interactions. Running the nDGP-PINOCCHIO code with a box size of 512 Mpc/h and 1024*1024*1024 particles, we analyze the statistical properties of the output halo catalogs, including the halo power spectrum and halo mass function. The calibrated PINOCCHIO halo power spectrum agrees with N-body simulations within 5% in the comoving wavenumber range k < 0.3 (h/Mpc) at redshift z = 0. The agreement is extended to smaller scales for higher redshifts. For the cumulative halo mass function, the agreement between N-body and PINOCCHIO is also within the simulation scatter."}
{"title":"Congressional Districting: \"Rocks-Pebbles-Sand\"","authors":["Jimmy Risk","Jennifer Switkes","Ann Zhang"],"raw_abstract":"As a case study into an algorithmic approach to congressional districting,\nNorth Carolina provides a lot to explore. Statistical modeling has called into\nquestion whether recent North Carolina district plans are unbiased. In\nparticular, the literature suggests that the district plan used in the 2016\nU.S. House of Representatives election yields outlier results that are\nstatistically unlikely to be obtained without the application of bias.\nTherefore, methods for creating strong and fair district plans are needed.\nInformed by previous districting models and algorithms, we build a model and\nalgorithm to produce an ensemble of viable Congressional district plans. Our\nwork contributes a ``Rocks-Pebbles-Sand'' concept and procedure facilitating\nreasonable population equity with a small overall number of county splits among\ndistricts. Additionally, our methodology minimizes the initial need for\ngranular, precinct-level data, thereby reducing the risk of covert\ngerrymandering. This case study indicates plausibility of an approach built\nupon an easy-to-understand intuition.","publication_date":1700492997,"paper_link":"http://arxiv.org/pdf/2311.11834v1","categories":["Mathematics"],"abstract":"As a case study into an algorithmic approach to congressional districting, North Carolina provides a lot to explore. Statistical modeling has called into question whether recent North Carolina district plans are unbiased. In particular, the literature suggests that the district plan used in the 2016 U.S. House of Representatives election yields outlier results that are statistically unlikely to be obtained without the application of bias. Therefore, methods for creating strong and fair district plans are needed. Informed by previous districting models and algorithms, we build a model and algorithm to produce an ensemble of viable Congressional district plans. Our work contributes a ``Rocks-Pebbles-Sand'' concept and procedure facilitating reasonable population equity with a small overall number of county splits among districts. Additionally, our methodology minimizes the initial need for granular, precinct-level data, thereby reducing the risk of covert gerrymandering. This case study indicates plausibility of an approach built upon an easy-to-understand intuition."}
{"title":"Operator Learning for Continuous Spatial-Temporal Model with A Hybrid Optimization Scheme","authors":["Chuanqi Chen","Jin-Long Wu"],"raw_abstract":"Partial differential equations are often used in the spatial-temporal\nmodeling of complex dynamical systems in many engineering applications. In this\nwork, we build on the recent progress of operator learning and present a\ndata-driven modeling framework that is continuous in both space and time. A key\nfeature of the proposed model is the resolution-invariance with respect to both\nspatial and temporal discretizations. To improve the long-term performance of\nthe calibrated model, we further propose a hybrid optimization scheme that\nleverages both gradient-based and derivative-free optimization methods and\nefficiently trains on both short-term time series and long-term statistics. We\ninvestigate the performance of the spatial-temporal continuous learning\nframework with three numerical examples, including the viscous Burgers'\nequation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation.\nThe results confirm the resolution-invariance of the proposed modeling\nframework and also demonstrate stable long-term simulations with only\nshort-term time series data. In addition, we show that the proposed model can\nbetter predict long-term statistics via the hybrid optimization scheme with a\ncombined use of short-term and long-term data.","publication_date":1700490678,"paper_link":"http://arxiv.org/pdf/2311.11798v1","categories":["Statistics"],"abstract":"Partial differential equations are often used in the spatial-temporal modeling of complex dynamical systems in many engineering applications. In this work, we build on the recent progress of operator learning and present a data-driven modeling framework that is continuous in both space and time. A key feature of the proposed model is the resolution-invariance with respect to both spatial and temporal discretizations. To improve the long-term performance of the calibrated model, we further propose a hybrid optimization scheme that leverages both gradient-based and derivative-free optimization methods and efficiently trains on both short-term time series and long-term statistics. We investigate the performance of the spatial-temporal continuous learning framework with three numerical examples, including the viscous Burgers' equation, the Navier-Stokes equations, and the Kuramoto-Sivashinsky equation. The results confirm the resolution-invariance of the proposed modeling framework and also demonstrate stable long-term simulations with only short-term time series data. In addition, we show that the proposed model can better predict long-term statistics via the hybrid optimization scheme with a combined use of short-term and long-term data."}
{"title":"Remarks on Statistical mechanics of a moving system","authors":["Jinwu Ye"],"raw_abstract":"In the realm of statistical mechanics, it has been established that there is\nno distinction between the micro-canonical and canonical ensembles in the\nthermodynamic limit. However, this paradigm may alter when addressing\nstatistical mechanics in the context of a moving sample with a velocity $ v $.\nOur investigation reveals significant disparities between the two ensembles\nwhen considering relativistic effects up to the order of $ (v/c)^2 $. While the\ntemperature remains the same in the former, it experiences an increase in the\nlatter. If the system undergoes a finite-temperature phase transition, the\ncritical temperature decreases in the co-moving frame of the latter ensemble.\nThe implications of these findings on the thermodynamic zeroth to the third\nlaws and the eigenstate thermalization hypothesis are analysed. The potential\nfor the experimental detection of these novel effects in condensed matter\nsystems are discussed.","publication_date":1700488437,"paper_link":"http://arxiv.org/pdf/2311.11767v1","categories":["Physics"],"abstract":"In the realm of statistical mechanics, it has been established that there is no distinction between the micro-canonical and canonical ensembles in the thermodynamic limit. However, this paradigm may alter when addressing statistical mechanics in the context of a moving sample with a velocity __FORMULA__. Our investigation reveals significant disparities between the two ensembles when considering relativistic effects up to the order of __FORMULA__. While the temperature remains the same in the former, it experiences an increase in the latter. If the system undergoes a finite-temperature phase transition, the critical temperature decreases in the co-moving frame of the latter ensemble. The implications of these findings on the thermodynamic zeroth to the third laws and the eigenstate thermalization hypothesis are analysed. The potential for the experimental detection of these novel effects in condensed matter systems are discussed."}
{"title":"Probing primordial black holes at high redshift with future gravitational wave detector","authors":["Paolo Marcoccia","Germano Nardini","Mauro Pieroni"],"raw_abstract":"We analyze the detection prospects for potential Primordial Black Hole Binary\n(PBHB) populations buried in the Stellar-Origin Black Hole Binary (SOBHB)\npopulation inferred by the LVK collaboration. We consider different PBHB\npopulation scenarios and several future Gravitational Wave (GW) detectors. To\nseparate the PBHB component from the SOBHB one, we exploit the prediction that\nthe PBHB merger rate does not decline as fast as the SOBHB one at high\nredshift. However, only a tiny fraction of PBHB events may be resolved\nindividually, and the sub-threshold events may yield an undetectable Stochastic\nGW Background (SGWB). For this reason, we determine the statistical\nsignificance of the PBHB contributions in the number of resolvable events seen\nin future Earth-based detectors and the SGWB measured at LISA. We find that the\nsynergy between these probes will consistently help assess whether or not a\nsizeable PBHB population is present.","publication_date":1700487603,"paper_link":"http://arxiv.org/pdf/2311.11760v1","categories":["Physics"],"abstract":"We analyze the detection prospects for potential Primordial Black Hole Binary (PBHB) populations buried in the Stellar-Origin Black Hole Binary (SOBHB) population inferred by the LVK collaboration. We consider different PBHB population scenarios and several future Gravitational Wave (GW) detectors. To separate the PBHB component from the SOBHB one, we exploit the prediction that the PBHB merger rate does not decline as fast as the SOBHB one at high redshift. However, only a tiny fraction of PBHB events may be resolved individually, and the sub-threshold events may yield an undetectable Stochastic GW Background (SGWB). For this reason, we determine the statistical significance of the PBHB contributions in the number of resolvable events seen in future Earth-based detectors and the SGWB measured at LISA. We find that the synergy between these probes will consistently help assess whether or not a sizeable PBHB population is present."}
{"title":"Measurement of (anti)alpha production in central Pb-Pb collisions at $\\sqrt{s_{\\rm NN}}$ = 5.02 TeV","authors":["ALICE Collaboration"],"raw_abstract":"In this letter, measurements of (anti)alpha production in central (0$-$10%)\nPb$-$Pb collisions at a center-of-mass energy per nucleon$-$nucleon pair of\n$\\sqrt{s_{\\rm NN}}$ = 5.02 TeV are presented, including the first measurement\nof an antialpha transverse-momentum spectrum. Owing to its large mass,\n(anti)alpha production yields and transverse-momentum spectra are of particular\ninterest because they provide a stringent test of particle production models.\nThe averaged antialpha and alpha spectrum is included into a common blast-wave\nfit with lighter particles, indicating that the (anti)alpha also participates\nin the collective expansion of the medium created in the collision. A\nblast-wave fit including only protons, (anti)alpha, and other light nuclei\nresults in a similar flow velocity as the fit that includes all particles. A\nsimilar flow velocity, but a significantly larger kinetic freeze-out\ntemperature is obtained when only protons and light nuclei are included in the\nfit. The coalescence parameter $B_4$ is well described by calculations from a\nstatistical hadronization model but significantly underestimated by\ncalculations assuming nucleus formation via coalescence of nucleons. Similarly,\nthe (anti)alpha-to-proton ratio is well described by the statistical\nhadronization model. On the other hand, coalescence calculations including\napproaches with different implementations of the (anti)alpha substructure tend\nto underestimate the data.","publication_date":1700487300,"paper_link":"http://arxiv.org/pdf/2311.11758v1","categories":["Physics"],"abstract":"In this letter, measurements of (anti)alpha production in central (0__FORMULA__10%) Pb__FORMULA__Pb collisions at a center-of-mass energy per nucleon__FORMULA__nucleon pair of __FORMULA__ = 5.02 TeV are presented, including the first measurement of an antialpha transverse-momentum spectrum. Owing to its large mass, (anti)alpha production yields and transverse-momentum spectra are of particular interest because they provide a stringent test of particle production models. The averaged antialpha and alpha spectrum is included into a common blast-wave fit with lighter particles, indicating that the (anti)alpha also participates in the collective expansion of the medium created in the collision. A blast-wave fit including only protons, (anti)alpha, and other light nuclei results in a similar flow velocity as the fit that includes all particles. A similar flow velocity, but a significantly larger kinetic freeze-out temperature is obtained when only protons and light nuclei are included in the fit. The coalescence parameter __FORMULA__ is well described by calculations from a statistical hadronization model but significantly underestimated by calculations assuming nucleus formation via coalescence of nucleons. Similarly, the (anti)alpha-to-proton ratio is well described by the statistical hadronization model. On the other hand, coalescence calculations including approaches with different implementations of the (anti)alpha substructure tend to underestimate the data."}
{"title":"Nanoswimmers in a ratchet potential: Effects of a transverse rocking force","authors":["Mykola Tasinkevych","Artem Ryabov"],"raw_abstract":"We study the dynamics of a chemical nanoswimmer in a ratchet potential, which\nis periodically rocked in the transverse direction. As a result of the\nmechanochemical coupling, the self-propulsion velocity becomes force-dependent\nand particle trajectories are rectified in the direction of the ratchet\nmodulation. The magnitude and direction of the nanoswimmer mean velocity depend\nupon both the rocking amplitude and the frequency. Remarkably, for frequencies\nlarger than the inverse correlation time of the rotational diffusion, the\nvelocity exhibits oscillatory behaviour as a function of the amplitude and the\nfrequency with multiple reversals of the sign. These findings suggest that\nmechanochemical coupling can be utilized for controlling the motion of\nchemically active particles at the nanoscale.","publication_date":1700485024,"paper_link":"http://arxiv.org/pdf/2311.11735v1","categories":["Physics"],"abstract":"We study the dynamics of a chemical nanoswimmer in a ratchet potential, which is periodically rocked in the transverse direction. As a result of the mechanochemical coupling, the self-propulsion velocity becomes force-dependent and particle trajectories are rectified in the direction of the ratchet modulation. The magnitude and direction of the nanoswimmer mean velocity depend upon both the rocking amplitude and the frequency. Remarkably, for frequencies larger than the inverse correlation time of the rotational diffusion, the velocity exhibits oscillatory behaviour as a function of the amplitude and the frequency with multiple reversals of the sign. These findings suggest that mechanochemical coupling can be utilized for controlling the motion of chemically active particles at the nanoscale."}
{"title":"Private and Secure Post-Quantum Verifiable Random Function with NIZK Proof and Ring-LWE Encryption in Blockchain","authors":["Bong Gon Kim","Dennis Wong","Yoon Seok Yang"],"raw_abstract":"We present a secure and private blockchain-based Verifiable Random Function\n(VRF) scheme addressing some limitations of classical VRF constructions. Given\nthe imminent quantum computing adversarial scenario, conventional cryptographic\nmethods face vulnerabilities. To enhance our VRF's secure randomness, we adopt\npost-quantum Ring-LWE encryption for synthesizing pseudo-random sequences.\nConsidering computational costs and resultant on-chain gas costs, we suggest a\nbifurcated architecture for VRF design, optimizing interactions between\non-chain and off-chain. Our approach employs a secure ring signature supported\nby NIZK proof and a delegated key generation method, inspired by the\nChaum-Pedersen equality proof and the Fiat-Shamir Heuristic. Our VRF scheme\nintegrates multi-party computation (MPC) with blockchain-based decentralized\nidentifiers (DID), ensuring both security and randomness. We elucidate the\nsecurity and privacy aspects of our VRF scheme, analyzing temporal and spatial\ncomplexities. We also approximate the entropy of the VRF scheme and detail its\nimplementation in a Solidity contract. Also, we delineate a method for\nvalidating the VRF's proof, matching for the contexts requiring both randomness\nand verification. Conclusively, using the NIST SP800-22 of the statistical\nrandomness test suite, our results exhibit a 98.86% pass rate over 11 test\ncases, with an average p-value of 0.5459 from 176 total tests.","publication_date":1700485010,"paper_link":"http://arxiv.org/pdf/2311.11734v1","categories":["Statistics"],"abstract":"We present a secure and private blockchain-based Verifiable Random Function (VRF) scheme addressing some limitations of classical VRF constructions. Given the imminent quantum computing adversarial scenario, conventional cryptographic methods face vulnerabilities. To enhance our VRF's secure randomness, we adopt post-quantum Ring-LWE encryption for synthesizing pseudo-random sequences. Considering computational costs and resultant on-chain gas costs, we suggest a bifurcated architecture for VRF design, optimizing interactions between on-chain and off-chain. Our approach employs a secure ring signature supported by NIZK proof and a delegated key generation method, inspired by the Chaum-Pedersen equality proof and the Fiat-Shamir Heuristic. Our VRF scheme integrates multi-party computation (MPC) with blockchain-based decentralized identifiers (DID), ensuring both security and randomness. We elucidate the security and privacy aspects of our VRF scheme, analyzing temporal and spatial complexities. We also approximate the entropy of the VRF scheme and detail its implementation in a Solidity contract. Also, we delineate a method for validating the VRF's proof, matching for the contexts requiring both randomness and verification. Conclusively, using the NIST SP800-22 of the statistical randomness test suite, our results exhibit a 98.86% pass rate over 11 test cases, with an average p-value of 0.5459 from 176 total tests."}
{"title":"Mixing properties for multivariate Hawkes processes","authors":["Ousmane Boly","Felix Cheysson","Thi Hien Nguyen"],"raw_abstract":"Properties of strong mixing have been established for the stationary linear\nHawkes process in the univariate case, and can serve as a basis for statistical\napplications. In this paper, we provide the technical arguments needed to\nextend the proof to the multivariate case. We illustrate these properties by\nestablishing a functional central limit theorem for multivariate Hawkes\nprocesses.","publication_date":1700484628,"paper_link":"http://arxiv.org/pdf/2311.11730v1","categories":["Mathematics","Statistics"],"abstract":"Properties of strong mixing have been established for the stationary linear Hawkes process in the univariate case, and can serve as a basis for statistical applications. In this paper, we provide the technical arguments needed to extend the proof to the multivariate case. We illustrate these properties by establishing a functional central limit theorem for multivariate Hawkes processes."}
{"title":"When correlations exceed system size: finite-size scaling in free boundary conditions above the upper critical dimension","authors":["Yulian Honchar","Bertrand Berche","Yurij Holovatch","Ralph Kenna"],"raw_abstract":"We progress finite-size scaling in systems with free boundary conditions\nabove their upper critical dimension, where in the thermodynamic limit critical\nscaling is described by mean-field theory. Recent works show that the\ncorrelation length is not bound by the system's physical size, a belief that\nlong held sway. Instead, two scaling regimes can be observed -- at the critical\nand pseudo-critical temperatures. We demonstrate that both are manifest for\nfree boundaries. We use numerical simulations of the $d=5$ Ising model to\nanalyse the magnetization, susceptibility, magnetization Fourier modes and the\npartition function zeros. While some of the response functions hide the dual\nfinite-size scaling, the precision enabled by the analysis of Lee-Yang zeros\nallows this be brought to the fore. In particular, finite-size scaling of\nleading zeros at the pseudocritical point confirms recent predictions coming\nfrom correlations exceeding system size. This paper is dedicated to Jaroslav\nIlnytskyi on the occasion of his 60th birthday.","publication_date":1700483854,"paper_link":"http://arxiv.org/pdf/2311.11721v1","categories":["Physics"],"abstract":"We progress finite-size scaling in systems with free boundary conditions above their upper critical dimension, where in the thermodynamic limit critical scaling is described by mean-field theory. Recent works show that the correlation length is not bound by the system's physical size, a belief that long held sway. Instead, two scaling regimes can be observed -- at the critical and pseudo-critical temperatures. We demonstrate that both are manifest for free boundaries. We use numerical simulations of the __FORMULA__ Ising model to analyse the magnetization, susceptibility, magnetization Fourier modes and the partition function zeros. While some of the response functions hide the dual finite-size scaling, the precision enabled by the analysis of Lee-Yang zeros allows this be brought to the fore. In particular, finite-size scaling of leading zeros at the pseudocritical point confirms recent predictions coming from correlations exceeding system size. This paper is dedicated to Jaroslav Ilnytskyi on the occasion of his 60th birthday."}
{"title":"Quenched disorder and instability control dynamic fracture in three dimensions","authors":["Yuri Lubomirsky","Eran Bouchbinder"],"raw_abstract":"Materials failure in 3D still poses basic challenges. We study 3D brittle\ncrack dynamics using a phase-field approach, where Gaussian quenched disorder\nin the fracture energy is incorporated. Disorder is characterized by a\ncorrelation length $R$ and strength $\\sigma$. We find that the mean crack\nvelocity $v$ is bounded by a limiting velocity, which is smaller than the\nhomogeneous material's prediction and decreases with $\\sigma$. It emerges from\na dynamic renormalization of the fracture energy with increasing crack driving\nforce $G$, resembling a critical point, due to an interplay between a 2D\nbranching instability and disorder. At small $G$, the probability of localized\nbranching on a scale $R$ is super-exponentially small. With increasing $G$ this\nprobability quickly increases, leading to misty fracture surfaces, yet the\nassociated extra dissipation remains small. As $G$ is further increased,\nbranching-related lengthscales become dynamic and persistently increase,\nleading to hackle-like structures and to a macroscopic contribution to the\nfracture surface. The latter dynamically renormalizes the actual fracture\nenergy until eventually any increase in $G$ is balanced by extra fracture\nsurface, with no accompanying increase in $v$. Finally, branching width reaches\nthe system's thickness such that 2D symmetry is statistically restored. Our\nfindings are consistent with a broad range of experimental observations.","publication_date":1700480691,"paper_link":"http://arxiv.org/pdf/2311.11692v1","categories":["Physics"],"abstract":"Materials failure in 3D still poses basic challenges. We study 3D brittle crack dynamics using a phase-field approach, where Gaussian quenched disorder in the fracture energy is incorporated. Disorder is characterized by a correlation length __FORMULA__ and strength __FORMULA__. We find that the mean crack velocity __FORMULA__ is bounded by a limiting velocity, which is smaller than the homogeneous material's prediction and decreases with __FORMULA__. It emerges from a dynamic renormalization of the fracture energy with increasing crack driving force __FORMULA__, resembling a critical point, due to an interplay between a 2D branching instability and disorder. At small __FORMULA__, the probability of localized branching on a scale __FORMULA__ is super-exponentially small. With increasing __FORMULA__ this probability quickly increases, leading to misty fracture surfaces, yet the associated extra dissipation remains small. As __FORMULA__ is further increased, branching-related lengthscales become dynamic and persistently increase, leading to hackle-like structures and to a macroscopic contribution to the fracture surface. The latter dynamically renormalizes the actual fracture energy until eventually any increase in __FORMULA__ is balanced by extra fracture surface, with no accompanying increase in __FORMULA__. Finally, branching width reaches the system's thickness such that 2D symmetry is statistically restored. Our findings are consistent with a broad range of experimental observations."}
{"title":"Nonequilibrium protection effect and spatial localization of noise-induced fluctuations under gas flow scattering on partially penetrable obstacle","authors":["S. P. Lukyanets","O. V. Kliushnichenko"],"raw_abstract":"We consider a nonequilibrium transition that leads to the formation of\nnonlinear steady-state structures due to the gas flow scattering on a partially\npenetrable obstacle. The resulting nonequilibrium steady state (NESS)\ncorresponds to a two-domain gas structure attained at certain critical\nparameters. We use a simple mean-field model of the driven lattice gas with\nring topology to demonstrate that this transition is accompanied by the\nemergence of local invariants related to a complex composed of the obstacle and\nits nearest gas surrounding, which we refer to as obstacle edges. These\ninvariants are independent of the main system parameters and behave as local\nfirst integrals, at least qualitatively. As a result, the complex becomes\ninsensitive to the noise of external driving field within the overcritical\ndomain. The emerged invariants describe the conservation of the number of\nparticles inside the obstacle and strong temporal synchronization or\ncorrelation of gas states at obstacle edges. Such synchronization guarantees\nthe equality to zero of the total edge current at any time. The robustness\nagainst external drive fluctuations is shown to be accompanied by strong\nspatial localization of induced gas fluctuations near the domain wall\nseparating the depleted and dense gas phases. Such a behavior can be associated\nwith nonequilibrium protection effect and synchronization of edges. The\ntransition rates between different NESSs are shown to be different. The\nrelaxation rates from one NESS to another take complex and real values in the\nsub- and overcritical regimes, respectively. The mechanism of these transitions\nis governed by the generation of shock waves at the back side of the obstacle.\nIn the subcritical regime, these solitary waves are generated sequentially many\ntimes, while only a single excitation is sufficient to rearrange the system\nstate in the overcritical regime.","publication_date":1700477339,"paper_link":"http://arxiv.org/pdf/2311.11658v1","categories":["Physics"],"abstract":"We consider a nonequilibrium transition that leads to the formation of nonlinear steady-state structures due to the gas flow scattering on a partially penetrable obstacle. The resulting nonequilibrium steady state (NESS) corresponds to a two-domain gas structure attained at certain critical parameters. We use a simple mean-field model of the driven lattice gas with ring topology to demonstrate that this transition is accompanied by the emergence of local invariants related to a complex composed of the obstacle and its nearest gas surrounding, which we refer to as obstacle edges. These invariants are independent of the main system parameters and behave as local first integrals, at least qualitatively. As a result, the complex becomes insensitive to the noise of external driving field within the overcritical domain. The emerged invariants describe the conservation of the number of particles inside the obstacle and strong temporal synchronization or correlation of gas states at obstacle edges. Such synchronization guarantees the equality to zero of the total edge current at any time. The robustness against external drive fluctuations is shown to be accompanied by strong spatial localization of induced gas fluctuations near the domain wall separating the depleted and dense gas phases. Such a behavior can be associated with nonequilibrium protection effect and synchronization of edges. The transition rates between different NESSs are shown to be different. The relaxation rates from one NESS to another take complex and real values in the sub- and overcritical regimes, respectively. The mechanism of these transitions is governed by the generation of shock waves at the back side of the obstacle. In the subcritical regime, these solitary waves are generated sequentially many times, while only a single excitation is sufficient to rearrange the system state in the overcritical regime."}
{"title":"Minimax Two-Stage Gradient Boosting for Parameter Estimation","authors":["Braghadeesh Lakshminarayanan","Cristian R. Rojas"],"raw_abstract":"Parameter estimation is an important sub-field in statistics and system\nidentification. Various methods for parameter estimation have been proposed in\nthe literature, among which the Two-Stage (TS) approach is particularly\npromising, due to its ease of implementation and reliable estimates. Among the\ndifferent statistical frameworks used to derive TS estimators, the min-max\nframework is attractive due to its mild dependence on prior knowledge about the\nparameters to be estimated. However, the existing implementation of the minimax\nTS approach has currently limited applicability, due to its heavy computational\nload. In this paper, we overcome this difficulty by using a gradient boosting\nmachine (GBM) in the second stage of TS approach. We call the resulting\nalgorithm the Two-Stage Gradient Boosting Machine (TSGBM) estimator. Finally,\nwe test our proposed TSGBM estimator on several numerical examples including\nmodels of dynamical systems.","publication_date":1700477319,"paper_link":"http://arxiv.org/pdf/2311.11657v1","categories":["Statistics"],"abstract":"Parameter estimation is an important sub-field in statistics and system identification. Various methods for parameter estimation have been proposed in the literature, among which the Two-Stage (TS) approach is particularly promising, due to its ease of implementation and reliable estimates. Among the different statistical frameworks used to derive TS estimators, the min-max framework is attractive due to its mild dependence on prior knowledge about the parameters to be estimated. However, the existing implementation of the minimax TS approach has currently limited applicability, due to its heavy computational load. In this paper, we overcome this difficulty by using a gradient boosting machine (GBM) in the second stage of TS approach. We call the resulting algorithm the Two-Stage Gradient Boosting Machine (TSGBM) estimator. Finally, we test our proposed TSGBM estimator on several numerical examples including models of dynamical systems."}
{"title":"Functional relative error regression under left truncation and right censoring","authors":["Adel Boucetta","Zohra Guessoum","Elias Ould-Said"],"raw_abstract":"The nonparametric estimators built by minimizing the mean squared relative\nerror are gaining in popularity for their robustness in the presence of\noutliers in comparison to the Nadaraya Watson estimators. In this paper we\nbuild a relative error regression function estimator in the case of a\nfunctional explanatory variable and a left truncated and right censored scalar\nvariable. The pointwise and uniform convergence of the estimator is proved and\nits performance is assessed by a numerical study in particularly the robustness\nwhich is highlighted using the influence function as a measure of robustness.","publication_date":1700471550,"paper_link":"http://arxiv.org/pdf/2311.11622v1","categories":["Mathematics","Statistics"],"abstract":"The nonparametric estimators built by minimizing the mean squared relative error are gaining in popularity for their robustness in the presence of outliers in comparison to the Nadaraya Watson estimators. In this paper we build a relative error regression function estimator in the case of a functional explanatory variable and a left truncated and right censored scalar variable. The pointwise and uniform convergence of the estimator is proved and its performance is assessed by a numerical study in particularly the robustness which is highlighted using the influence function as a measure of robustness."}
{"title":"Testing multivariate normality by testing independence","authors":["Povilas Daniu\u0161is"],"raw_abstract":"We propose a simple multivariate normality test based on Kac-Bernstein's\ncharacterization, which can be conducted by utilising existing statistical\nindependence tests for sums and differences of data samples. We also perform\nits empirical investigation, which reveals that for high-dimensional data, the\nproposed approach may be more efficient than the alternative ones. The\naccompanying code repository is provided at \\url{https://shorturl.at/rtuy5}.","publication_date":1700464792,"paper_link":"http://arxiv.org/pdf/2311.11575v1","categories":["Statistics"],"abstract":"We propose a simple multivariate normality test based on Kac-Bernstein's characterization, which can be conducted by utilising existing statistical independence tests for sums and differences of data samples. We also perform its empirical investigation, which reveals that for high-dimensional data, the proposed approach may be more efficient than the alternative ones. The accompanying code repository is provided at https://shorturl.at/rtuy5."}
{"title":"Ground state of the $S$=1/2 pyrochlore Heisenberg antiferromagnet: A quantum spin liquid emergent from dimensional reduction","authors":["Rico Pohle","Youhei Yamaji","Masatoshi Imada"],"raw_abstract":"The quantum antiferromagnet on the pyrochlore lattice offers an archetypal\nfrustrated system, which potentially realizes a quantum spin liquid\ncharacterized by the absence of standard spontaneous symmetry breaking even at\nzero temperature, unusually as an isotropic 3D system. Despite tremendous\nprogress in the literature, however, the nature of the ground state of the\nfully quantum-mechanical spin Hamiltonian on the pyrochlore lattice still\nremains elusive. Here, we show that an unconventional type of quantum spin\nliquid is born out from the pyrochlore system after the self-organized\ndimensional reduction leading to confined states in 2D layers. This conclusion\nis obtained from state-of-the-art variational Monte Carlo (VMC) simulations at\nzero temperature. Quantum spin liquids triggered by the emergent dimensional\nreduction is an unexplored route of the spin-liquid formation. The dimensional\nreduction from 3D to 2D is a consequence of a conventional spontaneous symmetry\nbreaking, while the resultant decoupling of layers enables the emergence of a\n2D quantum spin liquid that is adiabatically disconnected from trivial product\nstates and exhibits strong quantum entanglement. The stabilized quantum spin\nliquid exhibits an algebraic decay of correlations and vanishing excitation gap\nin the thermodynamic limit. The wave-function structure supports the\nfractionalization of the spin into spinons. This spin-liquid ground state\npersists in the presence of spin-orbit interactions, which expands the\npossibilities of realizing quantum spin liquids in real pyrochlore-structured\nmaterials.","publication_date":1700463143,"paper_link":"http://arxiv.org/pdf/2311.11561v1","categories":["Physics"],"abstract":"The quantum antiferromagnet on the pyrochlore lattice offers an archetypal frustrated system, which potentially realizes a quantum spin liquid characterized by the absence of standard spontaneous symmetry breaking even at zero temperature, unusually as an isotropic 3D system. Despite tremendous progress in the literature, however, the nature of the ground state of the fully quantum-mechanical spin Hamiltonian on the pyrochlore lattice still remains elusive. Here, we show that an unconventional type of quantum spin liquid is born out from the pyrochlore system after the self-organized dimensional reduction leading to confined states in 2D layers. This conclusion is obtained from state-of-the-art variational Monte Carlo (VMC) simulations at zero temperature. Quantum spin liquids triggered by the emergent dimensional reduction is an unexplored route of the spin-liquid formation. The dimensional reduction from 3D to 2D is a consequence of a conventional spontaneous symmetry breaking, while the resultant decoupling of layers enables the emergence of a 2D quantum spin liquid that is adiabatically disconnected from trivial product states and exhibits strong quantum entanglement. The stabilized quantum spin liquid exhibits an algebraic decay of correlations and vanishing excitation gap in the thermodynamic limit. The wave-function structure supports the fractionalization of the spin into spinons. This spin-liquid ground state persists in the presence of spin-orbit interactions, which expands the possibilities of realizing quantum spin liquids in real pyrochlore-structured materials."}
{"title":"A Bayesian two-step multiple imputation approach based on mixed models for the missing in EMA data","authors":["Yiheng Wei","Donald Hedeker"],"raw_abstract":"Ecological Momentary Assessments (EMA) capture real-time thoughts and\nbehaviors in natural settings, producing rich longitudinal data for statistical\nand physiological analyses. However, the robustness of these analyses can be\ncompromised by the large amount of missing in EMA data sets. To address this,\nmultiple imputation, a method that replaces missing values with several\nplausible alternatives, has become increasingly popular. In this paper, we\nintroduce a two-step Bayesian multiple imputation framework which leverages the\nconfiguration of mixed models. We adopt the Random Intercept Linear Mixed\nmodel, the Mixed-effect Location Scale model which accounts for subject\nvariance influenced by covariates and random effects, and the Shared Parameter\nLocation Scale Mixed Effect model which links the missing data to the response\nvariable through a random intercept logistic model, to complete the posterior\ndistribution within the framework. In the simulation study and an application\non data from a study on caregivers of dementia patients, we further adapt this\ntwo-step Bayesian multiple imputation strategy to handle simultaneous missing\nvariables in EMA data sets and compare the effectiveness of multiple\nimputations across different mixed models. The analyses highlight the\nadvantages of multiple imputations over single imputations. Furthermore, we\npropose two pivotal considerations in selecting the optimal mixed model for the\ntwo-step imputation: the influence of covariates as well as random effects on\nthe within-variance, and the nature of missing data in relation to the response\nvariable.","publication_date":1700452711,"paper_link":"http://arxiv.org/pdf/2311.11522v1","categories":["Statistics"],"abstract":"Ecological Momentary Assessments (EMA) capture real-time thoughts and behaviors in natural settings, producing rich longitudinal data for statistical and physiological analyses. However, the robustness of these analyses can be compromised by the large amount of missing in EMA data sets. To address this, multiple imputation, a method that replaces missing values with several plausible alternatives, has become increasingly popular. In this paper, we introduce a two-step Bayesian multiple imputation framework which leverages the configuration of mixed models. We adopt the Random Intercept Linear Mixed model, the Mixed-effect Location Scale model which accounts for subject variance influenced by covariates and random effects, and the Shared Parameter Location Scale Mixed Effect model which links the missing data to the response variable through a random intercept logistic model, to complete the posterior distribution within the framework. In the simulation study and an application on data from a study on caregivers of dementia patients, we further adapt this two-step Bayesian multiple imputation strategy to handle simultaneous missing variables in EMA data sets and compare the effectiveness of multiple imputations across different mixed models. The analyses highlight the advantages of multiple imputations over single imputations. Furthermore, we propose two pivotal considerations in selecting the optimal mixed model for the two-step imputation: the influence of covariates as well as random effects on the within-variance, and the nature of missing data in relation to the response variable."}
{"title":"Towards a Post-Market Monitoring Framework for Machine Learning-based Medical Devices: A case study","authors":["Jean Feng","Adarsh Subbaswamy","Alexej Gossmann","Harvineet Singh","Berkman Sahiner","Mi-Ok Kim","Gene Pennello","Nicholas Petrick","Romain Pirracchio","Fan Xia"],"raw_abstract":"After a machine learning (ML)-based system is deployed in clinical practice,\nperformance monitoring is important to ensure the safety and effectiveness of\nthe algorithm over time. The goal of this work is to highlight the complexity\nof designing a monitoring strategy and the need for a systematic framework that\ncompares the multitude of monitoring options. One of the main decisions is\nchoosing between using real-world (observational) versus interventional data.\nAlthough the former is the most convenient source of monitoring data, it\nexhibits well-known biases, such as confounding, selection, and missingness. In\nfact, when the ML algorithm interacts with its environment, the algorithm\nitself may be a primary source of bias. On the other hand, a carefully designed\ninterventional study that randomizes individuals can explicitly eliminate such\nbiases, but the ethics, feasibility, and cost of such an approach must be\ncarefully considered. Beyond the decision of the data source, monitoring\nstrategies vary in the performance criteria they track, the interpretability of\nthe test statistics, the strength of their assumptions, and their speed at\ndetecting performance decay. As a first step towards developing a framework\nthat compares the various monitoring options, we consider a case study of an\nML-based risk prediction algorithm for postoperative nausea and vomiting\n(PONV). Bringing together tools from causal inference and statistical process\ncontrol, we walk through the basic steps of defining candidate monitoring\ncriteria, describing potential sources of bias and the causal model, and\nspecifying and comparing candidate monitoring procedures. We hypothesize that\nthese steps can be applied more generally, as causal inference can address\nother sources of biases as well.","publication_date":1700439316,"paper_link":"http://arxiv.org/pdf/2311.11463v1","categories":["Statistics"],"abstract":"After a machine learning (ML)-based system is deployed in clinical practice, performance monitoring is important to ensure the safety and effectiveness of the algorithm over time. The goal of this work is to highlight the complexity of designing a monitoring strategy and the need for a systematic framework that compares the multitude of monitoring options. One of the main decisions is choosing between using real-world (observational) versus interventional data. Although the former is the most convenient source of monitoring data, it exhibits well-known biases, such as confounding, selection, and missingness. In fact, when the ML algorithm interacts with its environment, the algorithm itself may be a primary source of bias. On the other hand, a carefully designed interventional study that randomizes individuals can explicitly eliminate such biases, but the ethics, feasibility, and cost of such an approach must be carefully considered. Beyond the decision of the data source, monitoring strategies vary in the performance criteria they track, the interpretability of the test statistics, the strength of their assumptions, and their speed at detecting performance decay. As a first step towards developing a framework that compares the various monitoring options, we consider a case study of an ML-based risk prediction algorithm for postoperative nausea and vomiting (PONV). Bringing together tools from causal inference and statistical process control, we walk through the basic steps of defining candidate monitoring criteria, describing potential sources of bias and the causal model, and specifying and comparing candidate monitoring procedures. We hypothesize that these steps can be applied more generally, as causal inference can address other sources of biases as well."}
{"title":"Data efficient protein backmapping with backbone-to-side chain transformers","authors":["Shriram Chennakesavalu","Grant M. Rotskoff"],"raw_abstract":"Excitement at the prospect of using data-driven generative models to sample\nconfigurational ensembles of biomolecular systems stems from the extraordinary\nsuccess of these models on a diverse set of high-dimensional sampling tasks.\nUnlike image generation or even the closely related problem of protein\nstructure prediction, there are not currently data sources with sufficient\nbreadth to parameterize generative models for conformational ensembles. To\nenable discovery, a fundamentally different approach to building generative\nmodels is required: models should be able to propose rare, albeit physical,\nconformations that may not arise in even the largest data sets. Here we\nintroduce a modular strategy to generate conformations based on ``backmapping''\nfrom a fixed protein backbone that 1) maintains conformational diversity of the\nside chains and 2) couples the side chain fluctuations using global information\nabout the protein conformation. Our model combines simple statistical models of\nside chain conformations based on rotamer libraries with the now ubiquitous\ntransformer architecture to sample with atomistic accuracy. Together, these\ningredients provide a strategy for rapid data acquistion and hence a crucial\ningredient for scalable physical simulation with generative neural networks.","publication_date":1700437813,"paper_link":"http://arxiv.org/pdf/2311.11459v1","categories":["Physics"],"abstract":"Excitement at the prospect of using data-driven generative models to sample configurational ensembles of biomolecular systems stems from the extraordinary success of these models on a diverse set of high-dimensional sampling tasks. Unlike image generation or even the closely related problem of protein structure prediction, there are not currently data sources with sufficient breadth to parameterize generative models for conformational ensembles. To enable discovery, a fundamentally different approach to building generative models is required: models should be able to propose rare, albeit physical, conformations that may not arise in even the largest data sets. Here we introduce a modular strategy to generate conformations based on ``backmapping'' from a fixed protein backbone that 1) maintains conformational diversity of the side chains and 2) couples the side chain fluctuations using global information about the protein conformation. Our model combines simple statistical models of side chain conformations based on rotamer libraries with the now ubiquitous transformer architecture to sample with atomistic accuracy. Together, these ingredients provide a strategy for rapid data acquistion and hence a crucial ingredient for scalable physical simulation with generative neural networks."}
{"title":"Binary Stars in the New Millennium","authors":["Xuefei Chen","Zhengwei Liu","Zhanwen Han"],"raw_abstract":"Binary stars are as common as single stars. Binary stars are of immense\nimportance to astrophysicists because that they allow us to determine the\nmasses of the stars independent of their distances. They are the cornerstone of\nthe understanding of stellar evolutionary theory and play an essential role in\ncosmic distance measurement, galactic evolution, nucleosynthesis and the\nformation of important objects such as cataclysmic variable stars, X-ray\nbinaries, Type Ia supernovae, and gravitational wave-producing double compact\nobjects. In this article, we review the significant theoretical and\nobservational progresses in addressing binary stars in the new millennium.\nIncreasing large survey projects have led to the discovery of enormous numbers\nof binary stars, which enables us to conduct statistical studies of binary\npopulations, and therefore provide unprecedented insight into the stellar and\nbinary evolution physics. Meanwhile, the rapid development of theoretical\nconcepts and numerical approaches for binary evolution have made a substantial\nprogress on the alleviation of some long-standing binary-related problems such\nas the stability of mass transfer and common envelope evolution. Nevertheless,\nit remains a challenge to have a full understanding of fundamental problems of\nstellar and binary astrophysics. The upcoming massive survey projects and\nincreasingly sophisticated computational methods will lead to future progress.","publication_date":1700436651,"paper_link":"http://arxiv.org/pdf/2311.11454v1","categories":["Physics"],"abstract":"Binary stars are as common as single stars. Binary stars are of immense importance to astrophysicists because that they allow us to determine the masses of the stars independent of their distances. They are the cornerstone of the understanding of stellar evolutionary theory and play an essential role in cosmic distance measurement, galactic evolution, nucleosynthesis and the formation of important objects such as cataclysmic variable stars, X-ray binaries, Type Ia supernovae, and gravitational wave-producing double compact objects. In this article, we review the significant theoretical and observational progresses in addressing binary stars in the new millennium. Increasing large survey projects have led to the discovery of enormous numbers of binary stars, which enables us to conduct statistical studies of binary populations, and therefore provide unprecedented insight into the stellar and binary evolution physics. Meanwhile, the rapid development of theoretical concepts and numerical approaches for binary evolution have made a substantial progress on the alleviation of some long-standing binary-related problems such as the stability of mass transfer and common envelope evolution. Nevertheless, it remains a challenge to have a full understanding of fundamental problems of stellar and binary astrophysics. The upcoming massive survey projects and increasingly sophisticated computational methods will lead to future progress."}
{"title":"Duality of Bures and Shape Distances with Implications for Comparing Neural Representations","authors":["Sarah E. Harvey","Brett W. Larsen","Alex H. Williams"],"raw_abstract":"A multitude of (dis)similarity measures between neural network\nrepresentations have been proposed, resulting in a fragmented research\nlandscape. Most of these measures fall into one of two categories.\n  First, measures such as linear regression, canonical correlations analysis\n(CCA), and shape distances, all learn explicit mappings between neural units to\nquantify similarity while accounting for expected invariances. Second, measures\nsuch as representational similarity analysis (RSA), centered kernel alignment\n(CKA), and normalized Bures similarity (NBS) all quantify similarity in summary\nstatistics, such as stimulus-by-stimulus kernel matrices, which are already\ninvariant to expected symmetries. Here, we take steps towards unifying these\ntwo broad categories of methods by observing that the cosine of the Riemannian\nshape distance (from category 1) is equal to NBS (from category 2). We explore\nhow this connection leads to new interpretations of shape distances and NBS,\nand draw contrasts of these measures with CKA, a popular similarity measure in\nthe deep learning literature.","publication_date":1700432229,"paper_link":"http://arxiv.org/pdf/2311.11436v1","categories":["Statistics"],"abstract":"A multitude of (dis)similarity measures between neural network representations have been proposed, resulting in a fragmented research landscape. Most of these measures fall into one of two categories.   First, measures such as linear regression, canonical correlations analysis (CCA), and shape distances, all learn explicit mappings between neural units to quantify similarity while accounting for expected invariances. Second, measures such as representational similarity analysis (RSA), centered kernel alignment (CKA), and normalized Bures similarity (NBS) all quantify similarity in summary statistics, such as stimulus-by-stimulus kernel matrices, which are already invariant to expected symmetries. Here, we take steps towards unifying these two broad categories of methods by observing that the cosine of the Riemannian shape distance (from category 1) is equal to NBS (from category 2). We explore how this connection leads to new interpretations of shape distances and NBS, and draw contrasts of these measures with CKA, a popular similarity measure in the deep learning literature."}
{"title":"Refining Blecher and Knopfmacher's Integer Partition Fixed Points","authors":["Brian Hopkins"],"raw_abstract":"Recently, Blecher and Knopfmacher explored the notion of fixed points in\ninteger partitions. Here, we distinguish partitions with a fixed point by which\nvalue is fixed and analyze the resulting triangle of integers. In particular,\nwe confirm various identities for diagonal sums, row sums, and antidiagonal\nsums (which are finite for this triangle) and establish a four-term recurrence\nfor triangle entries analogous to Pascal's lemma for the triangle of binomial\ncoefficients. The partition statistics crank and mex arise. All proofs are\ncombinatorial.","publication_date":1700431027,"paper_link":"http://arxiv.org/pdf/2311.11433v1","categories":["Mathematics"],"abstract":"Recently, Blecher and Knopfmacher explored the notion of fixed points in integer partitions. Here, we distinguish partitions with a fixed point by which value is fixed and analyze the resulting triangle of integers. In particular, we confirm various identities for diagonal sums, row sums, and antidiagonal sums (which are finite for this triangle) and establish a four-term recurrence for triangle entries analogous to Pascal's lemma for the triangle of binomial coefficients. The partition statistics crank and mex arise. All proofs are combinatorial."}
{"title":"Secondary-Structure Phase Formation for Semifelxible Polymers by Bifurcation in Hyperphase Space","authors":["Dilimulati Aierken","Michael Bachmann"],"raw_abstract":"Canonical analysis has long been the primary analysis method for studies of\nphase transitions. However, this approach is not sensitive enough if transition\nsignals are too close in temperature space. The recently introduced generalized\nmicrocanonical inflection-point analysis method not only enables the systematic\nidentification and classification of transitions in systems of any size, but it\ncan also distinguish transitions that standard canonical analysis cannot\nresolve. By applying this method to a generic coarse-grained model for\nsemiflexible polymers, we identify a mixed structural phase dominated by\nsecondary structures such as hairpins and loops that originates from a\nbifurcation in the hyperspace spanned by inverse temperature and bending\nstiffness. This intermediate phase, which is embraced by the well-known\nrandom-coil and toroidal phases, is testimony to the necessity of balancing\nentropic variability and energetic stability in functional macromolecules under\nphysiological conditions.","publication_date":1700419231,"paper_link":"http://arxiv.org/pdf/2311.11395v1","categories":["Physics"],"abstract":"Canonical analysis has long been the primary analysis method for studies of phase transitions. However, this approach is not sensitive enough if transition signals are too close in temperature space. The recently introduced generalized microcanonical inflection-point analysis method not only enables the systematic identification and classification of transitions in systems of any size, but it can also distinguish transitions that standard canonical analysis cannot resolve. By applying this method to a generic coarse-grained model for semiflexible polymers, we identify a mixed structural phase dominated by secondary structures such as hairpins and loops that originates from a bifurcation in the hyperspace spanned by inverse temperature and bending stiffness. This intermediate phase, which is embraced by the well-known random-coil and toroidal phases, is testimony to the necessity of balancing entropic variability and energetic stability in functional macromolecules under physiological conditions."}
{"title":"Inspecting Explainability of Transformer Models with Additional Statistical Information","authors":["Hoang C. Nguyen","Haeil Lee","Junmo Kim"],"raw_abstract":"Transformer becomes more popular in the vision domain in recent years so\nthere is a need for finding an effective way to interpret the Transformer model\nby visualizing it. In recent work, Chefer et al. can visualize the Transformer\non vision and multi-modal tasks effectively by combining attention layers to\nshow the importance of each image patch. However, when applying to other\nvariants of Transformer such as the Swin Transformer, this method can not focus\non the predicted object. Our method, by considering the statistics of tokens in\nlayer normalization layers, shows a great ability to interpret the\nexplainability of Swin Transformer and ViT.","publication_date":1700414570,"paper_link":"http://arxiv.org/pdf/2311.11378v1","categories":["Statistics"],"abstract":"Transformer becomes more popular in the vision domain in recent years so there is a need for finding an effective way to interpret the Transformer model by visualizing it. In recent work, Chefer et al. can visualize the Transformer on vision and multi-modal tasks effectively by combining attention layers to show the importance of each image patch. However, when applying to other variants of Transformer such as the Swin Transformer, this method can not focus on the predicted object. Our method, by considering the statistics of tokens in layer normalization layers, shows a great ability to interpret the explainability of Swin Transformer and ViT."}
{"title":"Rapidity scan approach for net-baryon cumulants with a statistical thermal model","authors":["Jianing Li","Lipei Du","Shuzhe Shi"],"raw_abstract":"Utilizing rapidity-dependent measurements to map the QCD phase diagram\nprovides a complementary approach to traditional beam energy-dependent\nmeasurements around midrapidity. The changing nature of thermodynamic\nproperties of QCD matter along the beam axis in heavy-ion collisions at low\ncollision energies both motivate and pose challenges for this method. In this\nstudy, we derive the analytical cumulant-generating function for subsystems\nwithin distinct rapidity windows, while accounting for global net-baryon charge\nconservation of the full system. Rapidity-dependent net-baryon cumulants are\nthen calculated for a system exhibiting inhomogeneity along the beam axis, and\ntheir sensitivity to finite acceptances through changing rapidity bin widths is\nexplored. We highlight the non-trivial behaviors exhibited by these cumulants,\nunderscoring their importance in establishing a non-critical baseline for\ninterpreting net-proton cumulants in the search for the QCD critical point.\nFinally, we discuss the implications of the rapidity scan for mapping the QCD\nphase diagram within the current context.","publication_date":1700412803,"paper_link":"http://arxiv.org/pdf/2311.11374v1","categories":["Physics"],"abstract":"Utilizing rapidity-dependent measurements to map the QCD phase diagram provides a complementary approach to traditional beam energy-dependent measurements around midrapidity. The changing nature of thermodynamic properties of QCD matter along the beam axis in heavy-ion collisions at low collision energies both motivate and pose challenges for this method. In this study, we derive the analytical cumulant-generating function for subsystems within distinct rapidity windows, while accounting for global net-baryon charge conservation of the full system. Rapidity-dependent net-baryon cumulants are then calculated for a system exhibiting inhomogeneity along the beam axis, and their sensitivity to finite acceptances through changing rapidity bin widths is explored. We highlight the non-trivial behaviors exhibited by these cumulants, underscoring their importance in establishing a non-critical baseline for interpreting net-proton cumulants in the search for the QCD critical point. Finally, we discuss the implications of the rapidity scan for mapping the QCD phase diagram within the current context."}
{"title":"Using Causal Threads to Explain Changes in a Dynamic System","authors":["Robert B. Allen"],"raw_abstract":"We explore developing rich semantic models of systems. Specifically, we\nconsider structured causal explanations about state changes in those systems.\nEssentially, we are developing process-based dynamic knowledge graphs. As an\nexample, we construct a model of the causal threads for geological changes\nproposed by the Snowball Earth theory. Further, we describe an early prototype\nof a graphical interface to present the explanations. Unlike statistical\napproaches to summarization and explanation such as Large Language Models\n(LLMs), our approach of direct representation can be inspected and verified\ndirectly.","publication_date":1700404326,"paper_link":"http://arxiv.org/pdf/2311.11334v1","categories":["Statistics"],"abstract":"We explore developing rich semantic models of systems. Specifically, we consider structured causal explanations about state changes in those systems. Essentially, we are developing process-based dynamic knowledge graphs. As an example, we construct a model of the causal threads for geological changes proposed by the Snowball Earth theory. Further, we describe an early prototype of a graphical interface to present the explanations. Unlike statistical approaches to summarization and explanation such as Large Language Models (LLMs), our approach of direct representation can be inspected and verified directly."}
{"title":"Observational constraints for cubic gravity theory based on third order contractions of the Riemann tensor","authors":["Mihai Marciu","Dana Maria Ioan","Mihai Dragomir"],"raw_abstract":"The paper studies different observational features in the case of a specific\ncubic gravity theory, based on third order contractions of the Riemann tensor.\nConsidering viable cosmic chronometers data, baryon acoustic oscillations, and\nsupernovae, we analyze the viability of such a theoretical model, obtaining\nspecific constraints for different parameters in the current scenario. It is\nshown that the present extension of the $\\Lambda$CDM cosmological model is\ncompatible with recent data sets. The results indicate that the dark energy\nequation of state is exhibiting a phantom regime in the near past in the case\nof the best fitted values, a behavior which is in agreement with various\nphenomenological studies.","publication_date":1700391987,"paper_link":"http://arxiv.org/pdf/2311.11297v1","categories":["Physics"],"abstract":"The paper studies different observational features in the case of a specific cubic gravity theory, based on third order contractions of the Riemann tensor. Considering viable cosmic chronometers data, baryon acoustic oscillations, and supernovae, we analyze the viability of such a theoretical model, obtaining specific constraints for different parameters in the current scenario. It is shown that the present extension of the __FORMULA__CDM cosmological model is compatible with recent data sets. The results indicate that the dark energy equation of state is exhibiting a phantom regime in the near past in the case of the best fitted values, a behavior which is in agreement with various phenomenological studies."}
{"title":"Jeffreys-prior penalty for high-dimensional logistic regression: A conjecture about aggregate bias","authors":["Ioannis Kosmidis","Patrick Zietkiewicz"],"raw_abstract":"Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized\nlikelihood estimator in logistic regression has asymptotic bias decreasing with\nthe square of the number of observations when the number of parameters is\nfixed, which is an order faster than the typical rate from maximum likelihood.\nThe widespread use of that estimator in applied work is supported by the\nresults in Kosmidis and Firth (2021, Biometrika), who show that it takes finite\nvalues, even in cases where the maximum likelihood estimate does not exist.\nKosmidis and Firth (2021, Biometrika) also provide empirical evidence that the\nestimator has good bias properties in high-dimensional settings where the\nnumber of parameters grows asymptotically linearly but slower than the number\nof observations. We design and carry out a large-scale computer experiment\ncovering a wide range of such high-dimensional settings and produce strong\nempirical evidence for a simple rescaling of the maximum Jeffreys' prior\npenalized likelihood estimator that delivers high accuracy in signal recovery\nin the presence of an intercept parameter. The rescaled estimator is effective\neven in cases where estimates from maximum likelihood and other recently\nproposed corrective methods based on approximate message passing do not exist.","publication_date":1700389800,"paper_link":"http://arxiv.org/pdf/2311.11290v1","categories":["Mathematics","Statistics"],"abstract":"Firth (1993, Biometrika) shows that the maximum Jeffreys' prior penalized likelihood estimator in logistic regression has asymptotic bias decreasing with the square of the number of observations when the number of parameters is fixed, which is an order faster than the typical rate from maximum likelihood. The widespread use of that estimator in applied work is supported by the results in Kosmidis and Firth (2021, Biometrika), who show that it takes finite values, even in cases where the maximum likelihood estimate does not exist. Kosmidis and Firth (2021, Biometrika) also provide empirical evidence that the estimator has good bias properties in high-dimensional settings where the number of parameters grows asymptotically linearly but slower than the number of observations. We design and carry out a large-scale computer experiment covering a wide range of such high-dimensional settings and produce strong empirical evidence for a simple rescaling of the maximum Jeffreys' prior penalized likelihood estimator that delivers high accuracy in signal recovery in the presence of an intercept parameter. The rescaled estimator is effective even in cases where estimates from maximum likelihood and other recently proposed corrective methods based on approximate message passing do not exist."}
{"title":"On the stability of the filtration functions for weakly dependent data with applications to structural break detection","authors":["Johannes Krebs","Daniel Rademacher"],"raw_abstract":"In this paper, we study the stability of commonly used filtration functions\nin topological data analysis under small pertubations of the underlying\nnonrandom point cloud. Relying on these stability results, we then develop a\ntest procedure to detect and determine structural breaks in a sequence of\ntopological data objects obtained from weakly dependent data. The proposed\nmethod applies for instance to statistics of persistence diagrams of\n$\\mathbb{R}^d$-valued Bernoulli shift systems under the \\v{C}ech or\nVietoris-Rips filtration.","publication_date":1700379743,"paper_link":"http://arxiv.org/pdf/2311.11259v1","categories":["Mathematics","Statistics"],"abstract":"In this paper, we study the stability of commonly used filtration functions in topological data analysis under small pertubations of the underlying nonrandom point cloud. Relying on these stability results, we then develop a test procedure to detect and determine structural breaks in a sequence of topological data objects obtained from weakly dependent data. The proposed method applies for instance to statistics of persistence diagrams of __FORMULA__-valued Bernoulli shift systems under the Cech or Vietoris-Rips filtration."}
{"title":"Generation of squeezed high-order harmonics","authors":["Matan Even Tzur","Michael Birk","Alexey Gorlach","Ido Kaminer","Michael Krueger","Oren Cohen"],"raw_abstract":"For decades, most research on high harmonic generation (HHG) considered\nmatter as quantum but light as classical, leaving the quantum-optical nature of\nthe harmonics an open question. Here we explore the quantum properties of high\nharmonics. We derive a formula for the quantum state of the high harmonics,\nwhen driven by arbitrary quantum light states, and then explore specific cases\nof experimental relevance. Specifically, for a moderately squeezed pump, HHG\ndriven by squeezed coherent light results in squeezed high harmonics. Harmonic\nsqueezing is optimized by syncing ionization times with the pump's squeezing\nphase. Beyond this regime, as pump squeezing is increased, the harmonics\ninitially acquire squeezed thermal photon statistics, and then occupy an\nintricate quantum state which strongly depends on the semi-classical nonlinear\nresponse function of the interacting system. Our results pave the way for the\ngeneration of squeezed extreme-ultraviolet ultrashort pulses, and, more\ngenerally, quantum frequency conversion into previously inaccessible spectral\nranges, which may enable ultrasensitive attosecond metrology.","publication_date":1700378942,"paper_link":"http://arxiv.org/pdf/2311.11257v1","categories":["Physics"],"abstract":"For decades, most research on high harmonic generation (HHG) considered matter as quantum but light as classical, leaving the quantum-optical nature of the harmonics an open question. Here we explore the quantum properties of high harmonics. We derive a formula for the quantum state of the high harmonics, when driven by arbitrary quantum light states, and then explore specific cases of experimental relevance. Specifically, for a moderately squeezed pump, HHG driven by squeezed coherent light results in squeezed high harmonics. Harmonic squeezing is optimized by syncing ionization times with the pump's squeezing phase. Beyond this regime, as pump squeezing is increased, the harmonics initially acquire squeezed thermal photon statistics, and then occupy an intricate quantum state which strongly depends on the semi-classical nonlinear response function of the interacting system. Our results pave the way for the generation of squeezed extreme-ultraviolet ultrashort pulses, and, more generally, quantum frequency conversion into previously inaccessible spectral ranges, which may enable ultrasensitive attosecond metrology."}
{"title":"BOIS: Bayesian Optimization of Interconnected Systems","authors":["Leonardo D. Gonz\u00e1lez","Victor M. Zavala"],"raw_abstract":"Bayesian optimization (BO) has proven to be an effective paradigm for the\nglobal optimization of expensive-to-sample systems. One of the main advantages\nof BO is its use of Gaussian processes (GPs) to characterize model uncertainty\nwhich can be leveraged to guide the learning and search process. However, BO\ntypically treats systems as black-boxes and this limits the ability to exploit\nstructural knowledge (e.g., physics and sparse interconnections). Composite\nfunctions of the form $f(x, y(x))$, wherein GP modeling is shifted from the\nperformance function $f$ to an intermediate function $y$, offer an avenue for\nexploiting structural knowledge. However, the use of composite functions in a\nBO framework is complicated by the need to generate a probability density for\n$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is\nnonlinear it is not possible to obtain a closed-form expression). Previous work\nhas handled this issue using sampling techniques; these are easy to implement\nand flexible but are computationally intensive. In this work, we introduce a\nnew paradigm which allows for the efficient use of composite functions in BO;\nthis uses adaptive linearizations of $f$ to obtain closed-form expressions for\nthe statistical moments of the composite function. We show that this simple\napproach (which we call BOIS) enables the exploitation of structural knowledge,\nsuch as that arising in interconnected systems as well as systems that embed\nmultiple GP models and combinations of physics and GP models. Using a chemical\nprocess optimization case study, we benchmark the effectiveness of BOIS against\nstandard BO and sampling approaches. Our results indicate that BOIS achieves\nperformance gains and accurately captures the statistics of composite\nfunctions.","publication_date":1700376253,"paper_link":"http://arxiv.org/pdf/2311.11254v2","categories":["Statistics"],"abstract":"Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search process. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form __FORMULA__, wherein GP modeling is shifted from the performance function __FORMULA__ to an intermediate function __FORMULA__, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for __FORMULA__ from the Gaussian density of __FORMULA__ calculated by the GP (e.g., when __FORMULA__ is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of __FORMULA__ to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions."}
{"title":"Generalized Linear Models via the Lasso: To Scale or Not to Scale?","authors":["Anant Mathur","Sarat Moka","Zdravko Botev"],"raw_abstract":"The Lasso regression is a popular regularization method for feature selection\nin statistics. Prior to computing the Lasso estimator in both linear and\ngeneralized linear models, it is common to conduct a preliminary rescaling of\nthe feature matrix to ensure that all the features are standardized. Without\nthis standardization, it is argued, the Lasso estimate will unfortunately\ndepend on the units used to measure the features. We propose a new type of\niterative rescaling of the features in the context of generalized linear\nmodels. Whilst existing Lasso algorithms perform a single scaling as a\npreprocessing step, the proposed rescaling is applied iteratively throughout\nthe Lasso computation until convergence. We provide numerical examples, with\nboth real and simulated data, illustrating that the proposed iterative\nrescaling can significantly improve the statistical performance of the Lasso\nestimator without incurring any significant additional computational cost.","publication_date":1700372762,"paper_link":"http://arxiv.org/pdf/2311.11236v1","categories":["Statistics"],"abstract":"The Lasso regression is a popular regularization method for feature selection in statistics. Prior to computing the Lasso estimator in both linear and generalized linear models, it is common to conduct a preliminary rescaling of the feature matrix to ensure that all the features are standardized. Without this standardization, it is argued, the Lasso estimate will unfortunately depend on the units used to measure the features. We propose a new type of iterative rescaling of the features in the context of generalized linear models. Whilst existing Lasso algorithms perform a single scaling as a preprocessing step, the proposed rescaling is applied iteratively throughout the Lasso computation until convergence. We provide numerical examples, with both real and simulated data, illustrating that the proposed iterative rescaling can significantly improve the statistical performance of the Lasso estimator without incurring any significant additional computational cost."}
{"title":"Can We Utilize Pre-trained Language Models within Causal Discovery Algorithms?","authors":["Chanhui Lee","Juhyeon Kim","Yongjun Jeong","Juhyun Lyu","Junghee Kim","Sangmin Lee","Sangjun Han","Hyeokjun Choe","Soyeon Park","Woohyung Lim","Sungbin Lim","Sanghack Lee"],"raw_abstract":"Scaling laws have allowed Pre-trained Language Models (PLMs) into the field\nof causal reasoning. Causal reasoning of PLM relies solely on text-based\ndescriptions, in contrast to causal discovery which aims to determine the\ncausal relationships between variables utilizing data. Recently, there has been\ncurrent research regarding a method that mimics causal discovery by aggregating\nthe outcomes of repetitive causal reasoning, achieved through specifically\ndesigned prompts. It highlights the usefulness of PLMs in discovering cause and\neffect, which is often limited by a lack of data, especially when dealing with\nmultiple variables. Conversely, the characteristics of PLMs which are that PLMs\ndo not analyze data and they are highly dependent on prompt design leads to a\ncrucial limitation for directly using PLMs in causal discovery. Accordingly,\nPLM-based causal reasoning deeply depends on the prompt design and carries out\nthe risk of overconfidence and false predictions in determining causal\nrelationships. In this paper, we empirically demonstrate the aforementioned\nlimitations of PLM-based causal reasoning through experiments on\nphysics-inspired synthetic data. Then, we propose a new framework that\nintegrates prior knowledge obtained from PLM with a causal discovery algorithm.\nThis is accomplished by initializing an adjacency matrix for causal discovery\nand incorporating regularization using prior knowledge. Our proposed framework\nnot only demonstrates improved performance through the integration of PLM and\ncausal discovery but also suggests how to leverage PLM-extracted prior\nknowledge with existing causal discovery algorithms.","publication_date":1700364690,"paper_link":"http://arxiv.org/pdf/2311.11212v1","categories":["Statistics"],"abstract":"Scaling laws have allowed Pre-trained Language Models (PLMs) into the field of causal reasoning. Causal reasoning of PLM relies solely on text-based descriptions, in contrast to causal discovery which aims to determine the causal relationships between variables utilizing data. Recently, there has been current research regarding a method that mimics causal discovery by aggregating the outcomes of repetitive causal reasoning, achieved through specifically designed prompts. It highlights the usefulness of PLMs in discovering cause and effect, which is often limited by a lack of data, especially when dealing with multiple variables. Conversely, the characteristics of PLMs which are that PLMs do not analyze data and they are highly dependent on prompt design leads to a crucial limitation for directly using PLMs in causal discovery. Accordingly, PLM-based causal reasoning deeply depends on the prompt design and carries out the risk of overconfidence and false predictions in determining causal relationships. In this paper, we empirically demonstrate the aforementioned limitations of PLM-based causal reasoning through experiments on physics-inspired synthetic data. Then, we propose a new framework that integrates prior knowledge obtained from PLM with a causal discovery algorithm. This is accomplished by initializing an adjacency matrix for causal discovery and incorporating regularization using prior knowledge. Our proposed framework not only demonstrates improved performance through the integration of PLM and causal discovery but also suggests how to leverage PLM-extracted prior knowledge with existing causal discovery algorithms."}
{"title":"Scale-free networks: improved inference","authors":["Nixon Jerez-Lillo","Francisco A. Rodrigues","Pedro L. Ramos"],"raw_abstract":"The power-law distribution plays a crucial role in complex networks as well\nas various applied sciences. Investigating whether the degree distribution of a\nnetwork follows a power-law distribution is an important concern. The commonly\nused inferential methods for estimating the model parameters often yield biased\nestimates, which can lead to the rejection of the hypothesis that a model\nconforms to a power-law. In this paper, we discuss improved methods that\nutilize Bayesian inference to obtain accurate estimates and precise credibility\nintervals. The inferential methods are derived for both continuous and discrete\ndistributions. These methods reveal that objective Bayesian approaches return\nnearly unbiased estimates for the parameters of both models. Notably, in the\ncontinuous case, we identify an explicit posterior distribution. This work\nenhances the power of goodness-of-fit tests, enabling us to accurately discern\nwhether a network or any other dataset adheres to a power-law distribution. We\napply the proposed approach to fit degree distributions for more than 5,000\nsynthetic networks and over 3,000 real networks. The results indicate that our\nmethod is more suitable in practice, as it yields a frequency of acceptance\nclose to the specified nominal level.","publication_date":1700360776,"paper_link":"http://arxiv.org/pdf/2311.11200v1","categories":["Physics"],"abstract":"The power-law distribution plays a crucial role in complex networks as well as various applied sciences. Investigating whether the degree distribution of a network follows a power-law distribution is an important concern. The commonly used inferential methods for estimating the model parameters often yield biased estimates, which can lead to the rejection of the hypothesis that a model conforms to a power-law. In this paper, we discuss improved methods that utilize Bayesian inference to obtain accurate estimates and precise credibility intervals. The inferential methods are derived for both continuous and discrete distributions. These methods reveal that objective Bayesian approaches return nearly unbiased estimates for the parameters of both models. Notably, in the continuous case, we identify an explicit posterior distribution. This work enhances the power of goodness-of-fit tests, enabling us to accurately discern whether a network or any other dataset adheres to a power-law distribution. We apply the proposed approach to fit degree distributions for more than 5,000 synthetic networks and over 3,000 real networks. The results indicate that our method is more suitable in practice, as it yields a frequency of acceptance close to the specified nominal level."}
{"title":"LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation","authors":["S\u00e9bastien Henry","John A. Christian"],"raw_abstract":"Triangulation algorithms often aim to minimize the reprojection ($L_2$)\nerror, but this only provides the maximum likelihood estimate when there are no\nerrors in the camera parameters or camera poses. Although recent advancements\nhave yielded techniques to estimate camera parameters accounting for 3D point\nuncertainties, most structure from motion (SfM) pipelines still use older\ntriangulation algorithms. This work leverages recent discoveries to provide a\nfast, scalable, and statistically optimal way to triangulate called LOSTU.\nResults show that LOSTU consistently produces lower 3D reconstruction errors\nthan conventional $L_2$ triangulation methods -- often allowing LOSTU to\nsuccessfully triangulate more points. Moreover, in addition to providing a\nbetter 3D reconstruction, LOSTU can be substantially faster than\nLevenberg-Marquardt (or similar) optimization schemes.","publication_date":1700342824,"paper_link":"http://arxiv.org/pdf/2311.11171v1","categories":["Statistics"],"abstract":"Triangulation algorithms often aim to minimize the reprojection (__FORMULA__) error, but this only provides the maximum likelihood estimate when there are no errors in the camera parameters or camera poses. Although recent advancements have yielded techniques to estimate camera parameters accounting for 3D point uncertainties, most structure from motion (SfM) pipelines still use older triangulation algorithms. This work leverages recent discoveries to provide a fast, scalable, and statistically optimal way to triangulate called LOSTU. Results show that LOSTU consistently produces lower 3D reconstruction errors than conventional __FORMULA__ triangulation methods -- often allowing LOSTU to successfully triangulate more points. Moreover, in addition to providing a better 3D reconstruction, LOSTU can be substantially faster than Levenberg-Marquardt (or similar) optimization schemes."}
{"title":"On the Hardness of Learning to Stabilize Linear Systems","authors":["Xiong Zeng","Zexiang Liu","Zhe Du","Necmiye Ozay","Mario Sznaier"],"raw_abstract":"Inspired by the work of Tsiamis et al. \\cite{tsiamis2022learning}, in this\npaper we study the statistical hardness of learning to stabilize linear\ntime-invariant systems. Hardness is measured by the number of samples required\nto achieve a learning task with a given probability. The work in\n\\cite{tsiamis2022learning} shows that there exist system classes that are hard\nto learn to stabilize with the core reason being the hardness of\nidentification. Here we present a class of systems that can be easy to\nidentify, thanks to a non-degenerate noise process that excites all modes, but\nthe sample complexity of stabilization still increases exponentially with the\nsystem dimension. We tie this result to the hardness of co-stabilizability for\nthis class of systems using ideas from robust control.","publication_date":1700336096,"paper_link":"http://arxiv.org/pdf/2311.11151v1","categories":["Statistics","Electrical Engineering and Systems Science"],"abstract":"Inspired by the work of Tsiamis et al. tsiamis2022learning, in this paper we study the statistical hardness of learning to stabilize linear time-invariant systems. Hardness is measured by the number of samples required to achieve a learning task with a given probability. The work in tsiamis2022learning shows that there exist system classes that are hard to learn to stabilize with the core reason being the hardness of identification. Here we present a class of systems that can be easy to identify, thanks to a non-degenerate noise process that excites all modes, but the sample complexity of stabilization still increases exponentially with the system dimension. We tie this result to the hardness of co-stabilizability for this class of systems using ideas from robust control."}
{"title":"Small and simple systems that favor the arrow of time","authors":["Ralph V. Chamberlin"],"raw_abstract":"The 2nd law of thermodynamics yields an irreversible increase in entropy\nuntil thermal equilibrium is achieved. This irreversible increase is often\nassumed to require large and complex systems to emerge from the reversible\nmicroscopic laws of physics. We test this assumption using simulations and\ntheory of a 1D ring of N Ising spins coupled to an explicit heat bath of N\nEinstein oscillators. The exact entropy of the spins and bath can be calculated\nfor any N, with dynamics that is readily altered from reversible to\nirreversible. We find thermal equilibrium behavior in the thermodynamic limit,\nand in systems as small as N=2, but only if the microscopic dynamics is\nintrinsically irreversible.","publication_date":1700335531,"paper_link":"http://arxiv.org/pdf/2311.11149v1","categories":["Physics"],"abstract":"The 2nd law of thermodynamics yields an irreversible increase in entropy until thermal equilibrium is achieved. This irreversible increase is often assumed to require large and complex systems to emerge from the reversible microscopic laws of physics. We test this assumption using simulations and theory of a 1D ring of N Ising spins coupled to an explicit heat bath of N Einstein oscillators. The exact entropy of the spins and bath can be calculated for any N, with dynamics that is readily altered from reversible to irreversible. We find thermal equilibrium behavior in the thermodynamic limit, and in systems as small as N=2, but only if the microscopic dynamics is intrinsically irreversible."}
{"title":"DSCom: A Data-Driven Self-Adaptive Community-Based Framework for Influence Maximization in Social Networks","authors":["Yuxin Zuo","Haojia Sun","Yongyi Hu","Jianxiong Guo","Xiaofeng Gao"],"raw_abstract":"Influence maximization aims to find a subset of seeds that maximize the\ninfluence spread under a given budget. In this paper, we mainly address the\ndata-driven version of this problem, where the diffusion model is not given but\nneeds to be inferred from the history cascades. Several previous works have\naddressed this topic in a statistical way and provided efficient algorithms\nwith theoretical guarantee. However, in their settings, though the diffusion\nparameters are inferred, they still need users to preset the diffusion model,\nwhich can be an intractable problem in real-world practices. In this paper, we\nreformulate the problem on the attributed network and leverage the node\nattributes to estimate the closeness between the connected nodes. Specifically,\nwe propose a machine learning-based framework, named DSCom, to address this\nproblem in a heuristic way. Under this framework, we first infer the users'\nrelationship from the diffusion dataset through attention mechanism and then\nleverage spectral clustering to overcome the influence overlap problem in the\nlack of exact diffusion formula. Compared to the previous theoretical works, we\ncarefully designed empirical experiments with parameterized diffusion models\nbased on real-world social networks, which prove the efficiency and\neffectiveness of our algorithm.","publication_date":1700316223,"paper_link":"http://arxiv.org/pdf/2311.11080v1","categories":["Statistics"],"abstract":"Influence maximization aims to find a subset of seeds that maximize the influence spread under a given budget. In this paper, we mainly address the data-driven version of this problem, where the diffusion model is not given but needs to be inferred from the history cascades. Several previous works have addressed this topic in a statistical way and provided efficient algorithms with theoretical guarantee. However, in their settings, though the diffusion parameters are inferred, they still need users to preset the diffusion model, which can be an intractable problem in real-world practices. In this paper, we reformulate the problem on the attributed network and leverage the node attributes to estimate the closeness between the connected nodes. Specifically, we propose a machine learning-based framework, named DSCom, to address this problem in a heuristic way. Under this framework, we first infer the users' relationship from the diffusion dataset through attention mechanism and then leverage spectral clustering to overcome the influence overlap problem in the lack of exact diffusion formula. Compared to the previous theoretical works, we carefully designed empirical experiments with parameterized diffusion models based on real-world social networks, which prove the efficiency and effectiveness of our algorithm."}
{"title":"A unified fused Lasso approach for sparse and blocky feature selection in regression and classification","authors":["Xiaofei Wu","Rongmei Liang","Zhimin Zhang","Zhenyu Cui"],"raw_abstract":"In many applications, sparse and blocky coefficients often occur in\nregression and classification problems. The fused Lasso was designed to recover\nthese sparse structured features especially when the design matrix encounters\nthe situation of ultrahigh dimension. Quantile loss is well known as a robust\nloss function in regression and classification. In this paper, we combine\nquantile loss and fused Lasso penalty together to produce quantile fused Lasso\nwhich can achieve sparse and blocky feature selection in both regression and\nclassification. Interestingly, our proposed model has the unified optimization\nformula for regression and classification. For ultrahigh dimensional collected\ndata, we derive multi-block linearized alternating direction method of\nmultipliers (LADMM) to deal with it. Moreover, we prove convergence and derive\nconvergence rates of the proposed LADMM algorithm through an elegant method.\nNote that the algorithm can be easily extended to solve many existing fused\nLasso models. Finally, we present some numerical results for several synthetic\nand real world examples, which illustrate the robustness, scalability, and\naccuracy of the proposed method.","publication_date":1700313732,"paper_link":"http://arxiv.org/pdf/2311.11068v1","categories":["Mathematics","Statistics"],"abstract":"In many applications, sparse and blocky coefficients often occur in regression and classification problems. The fused Lasso was designed to recover these sparse structured features especially when the design matrix encounters the situation of ultrahigh dimension. Quantile loss is well known as a robust loss function in regression and classification. In this paper, we combine quantile loss and fused Lasso penalty together to produce quantile fused Lasso which can achieve sparse and blocky feature selection in both regression and classification. Interestingly, our proposed model has the unified optimization formula for regression and classification. For ultrahigh dimensional collected data, we derive multi-block linearized alternating direction method of multipliers (LADMM) to deal with it. Moreover, we prove convergence and derive convergence rates of the proposed LADMM algorithm through an elegant method. Note that the algorithm can be easily extended to solve many existing fused Lasso models. Finally, we present some numerical results for several synthetic and real world examples, which illustrate the robustness, scalability, and accuracy of the proposed method."}
{"title":"Exotic Symmetry Breaking Properties of Self-Dual Fracton Spin Models","authors":["Giovanni Canossa","Lode Pollet","Miguel A. Martin-Delgado","Hao Song","Ke Liu"],"raw_abstract":"Fracton codes host unconventional topological states of matter and are\npromising for fault-tolerant quantum computation due to their large coding\nspace and strong resilience against decoherence and noise. In this work, we\ninvestigate the ground-state properties and phase transitions of two\nprototypical self-dual fracton spin models -- the tetrahedral Ising model and\nthe fractal Ising model -- which correspond to error-correction procedures for\nthe representative fracton codes of type-I and type-II, the checkerboard code\nand the Haah's code, respectively, in the error-free limit. They are endowed\nwith exotic symmetry-breaking properties that contrast sharply with the\nspontaneous breaking of global symmetries and deconfinement transition of gauge\ntheories. To show these unconventional behaviors, which are associated with\nsub-dimensional symmetries, we construct and analyze the order parameters,\ncorrelators, and symmetry generators for both models. Notably, the tetrahedral\nIsing model acquires an extended semi-local ordering moment, while the fractal\nIsing model fits into a polynomial ring representation and leads to a fractal\norder parameter. Numerical studies combined with analytical tools show that\nboth models experience a strong first-order phase transition with an anomalous\n$L^{-(D-1)}$ scaling, despite the fractal symmetry of the latter. Our work\nprovides new understanding of sub-dimensional symmetry breaking and makes an\nimportant step for studying quantum-error-correction properties of the\ncheckerboard and Haah's codes.","publication_date":1700313134,"paper_link":"http://arxiv.org/pdf/2311.11066v1","categories":["Physics"],"abstract":"Fracton codes host unconventional topological states of matter and are promising for fault-tolerant quantum computation due to their large coding space and strong resilience against decoherence and noise. In this work, we investigate the ground-state properties and phase transitions of two prototypical self-dual fracton spin models -- the tetrahedral Ising model and the fractal Ising model -- which correspond to error-correction procedures for the representative fracton codes of type-I and type-II, the checkerboard code and the Haah's code, respectively, in the error-free limit. They are endowed with exotic symmetry-breaking properties that contrast sharply with the spontaneous breaking of global symmetries and deconfinement transition of gauge theories. To show these unconventional behaviors, which are associated with sub-dimensional symmetries, we construct and analyze the order parameters, correlators, and symmetry generators for both models. Notably, the tetrahedral Ising model acquires an extended semi-local ordering moment, while the fractal Ising model fits into a polynomial ring representation and leads to a fractal order parameter. Numerical studies combined with analytical tools show that both models experience a strong first-order phase transition with an anomalous __FORMULA__ scaling, despite the fractal symmetry of the latter. Our work provides new understanding of sub-dimensional symmetry breaking and makes an important step for studying quantum-error-correction properties of the checkerboard and Haah's codes."}
{"title":"Modern extreme value statistics for Utopian extremes","authors":["Jordan Richards","Noura Alotaibi","Daniela Cisneros","Yan Gong","Matheus B. Guerrero","Paolo Redondo","Xuanjie Shao"],"raw_abstract":"Capturing the extremal behaviour of data often requires bespoke marginal and\ndependence models which are grounded in rigorous asymptotic theory, and hence\nprovide reliable extrapolation into the upper tails of the data-generating\ndistribution. We present a modern toolbox of four methodological frameworks,\nmotivated by modern extreme value theory, that can be used to accurately\nestimate extreme exceedance probabilities or the corresponding level in either\na univariate or multivariate setting. Our frameworks were used to facilitate\nthe winning contribution of Team Yalla to the data competition organised for\nthe 13th International Conference on Extreme Value Analysis (EVA2023). This\ncompetition comprised seven teams competing across four separate\nsub-challenges, with each requiring the modelling of data simulated from known,\nyet highly complex, statistical distributions, and extrapolation far beyond the\nrange of the available samples in order to predict probabilities of extreme\nevents. Data were constructed to be representative of real environmental data,\nsampled from the fantasy country of \"Utopia\".","publication_date":1700310471,"paper_link":"http://arxiv.org/pdf/2311.11054v1","categories":["Statistics"],"abstract":"Capturing the extremal behaviour of data often requires bespoke marginal and dependence models which are grounded in rigorous asymptotic theory, and hence provide reliable extrapolation into the upper tails of the data-generating distribution. We present a modern toolbox of four methodological frameworks, motivated by modern extreme value theory, that can be used to accurately estimate extreme exceedance probabilities or the corresponding level in either a univariate or multivariate setting. Our frameworks were used to facilitate the winning contribution of Team Yalla to the data competition organised for the 13th International Conference on Extreme Value Analysis (EVA2023). This competition comprised seven teams competing across four separate sub-challenges, with each requiring the modelling of data simulated from known, yet highly complex, statistical distributions, and extrapolation far beyond the range of the available samples in order to predict probabilities of extreme events. Data were constructed to be representative of real environmental data, sampled from the fantasy country of \"Utopia\"."}
{"title":"The temperature dependent Boltzmann equation beyond local equilibrium assumption","authors":["Zheng-Chuan Wang"],"raw_abstract":"In this manuscript, we present a temperature dependent Boltzmann equation for\nthe particles transport through a environmental reservoir, where the\ntemperature refers to the equilibrium temperature of reservoir, a new damping\nforce and a inverse damping relaxation time are derived based on the classical\nBoltzmann equation, which have obvious influence on the external force and the\nrelaxation time of transport particles. For comparison, we also define a\nnon-equilibrium temperature for the transport particle by its distribution\nfunction out of equilibrium, which is different from the equilibrium\ntemperature of reservoir. There exist heat transfer between the transport\nparticle and the reservoir, because the whole transport particles are in\nnon-equilibrium state. Finally, we illustrate them by an example of\none-dimensional transport procedure, the damping force and the non-equilibrium\ntemperature defined by us are shown numerically.","publication_date":1700303702,"paper_link":"http://arxiv.org/pdf/2311.11028v1","categories":["Physics"],"abstract":"In this manuscript, we present a temperature dependent Boltzmann equation for the particles transport through a environmental reservoir, where the temperature refers to the equilibrium temperature of reservoir, a new damping force and a inverse damping relaxation time are derived based on the classical Boltzmann equation, which have obvious influence on the external force and the relaxation time of transport particles. For comparison, we also define a non-equilibrium temperature for the transport particle by its distribution function out of equilibrium, which is different from the equilibrium temperature of reservoir. There exist heat transfer between the transport particle and the reservoir, because the whole transport particles are in non-equilibrium state. Finally, we illustrate them by an example of one-dimensional transport procedure, the damping force and the non-equilibrium temperature defined by us are shown numerically."}
{"title":"Lyman-alpha at Cosmic Noon I: Ly-alpha Spectral Type Selection of z ~ 2-3 Lyman Break Galaxies with Broadband Imaging","authors":["Garry Foran","Jeff Cooke","Naveen Reddy","Charles Steidle","Alice Shapley"],"raw_abstract":"High-redshift Lyman break galaxies (LBGs) are efficiently selected in deep\nimages using as few as three broadband filters, and have been shown to have\nmultiple intrinsic and small- to large-scale environmental properties related\nto Lyman-alpha. In this paper we demonstrate a statistical relationship between\nnet Lyman-alpha equivalent width (net Lya EW) and the optical broadband\nphotometric properties of LBGs at z~2. We show that LBGs with the strongest net\nLya EW in absorption (aLBGs) and strongest net Lya EW in emission (eLBGs)\nseparate into overlapping but discrete distributions in $(U_n-R)$ colour and\n$R$-band magnitude space, and use this segregation behaviour to determine\nphotometric criteria by which sub-samples with a desired Lya spectral type can\nbe selected using data from as few as three broadband optical filters. We\npropose application of our result to current and future large-area and all-sky\nphotometric surveys that will select hundreds of millions of LBGs across many\nhundreds to thousands of Mpc, and for which spectroscopic follow-up to obtain\nLya spectral information is prohibitive. To this end, we use spectrophotometry\nof composite spectra derived from a sample of 798 LBGs divided into quartiles\non the basis of net Lya EW to calculate criteria for the selection of Lya\nabsorbing and Lya emitting populations of z~3 LBGs using $ugri$ broadband\nphotometric data from the Vera Rubin Observatory Legacy Survey of Space and\nTime (LSST).","publication_date":1700289198,"paper_link":"http://arxiv.org/pdf/2311.10985v1","categories":["Physics"],"abstract":"High-redshift Lyman break galaxies (LBGs) are efficiently selected in deep images using as few as three broadband filters, and have been shown to have multiple intrinsic and small- to large-scale environmental properties related to Lyman-alpha. In this paper we demonstrate a statistical relationship between net Lyman-alpha equivalent width (net Lya EW) and the optical broadband photometric properties of LBGs at z~2. We show that LBGs with the strongest net Lya EW in absorption (aLBGs) and strongest net Lya EW in emission (eLBGs) separate into overlapping but discrete distributions in __FORMULA__ colour and __FORMULA__-band magnitude space, and use this segregation behaviour to determine photometric criteria by which sub-samples with a desired Lya spectral type can be selected using data from as few as three broadband optical filters. We propose application of our result to current and future large-area and all-sky photometric surveys that will select hundreds of millions of LBGs across many hundreds to thousands of Mpc, and for which spectroscopic follow-up to obtain Lya spectral information is prohibitive. To this end, we use spectrophotometry of composite spectra derived from a sample of 798 LBGs divided into quartiles on the basis of net Lya EW to calculate criteria for the selection of Lya absorbing and Lya emitting populations of z~3 LBGs using __FORMULA__ broadband photometric data from the Vera Rubin Observatory Legacy Survey of Space and Time (LSST)."}
{"title":"Asymptotic distributions of the average clustering coefficient and its variant","authors":["Mingao Yuan","Xiaofeng Zhao"],"raw_abstract":"In network data analysis, summary statistics of a network can provide us with\nmeaningful insight into the structure of the network. The average clustering\ncoefficient is one of the most popular and widely used network statistics. In\nthis paper, we investigate the asymptotic distributions of the average\nclustering coefficient and its variant of a heterogeneous Erd\\\"{o}s-R\\'{e}nyi\nrandom graph. We show that the standardized average clustering coefficient\nconverges in distribution to the standard normal distribution. Interestingly,\nthe variance of the average clustering coefficient exhibits a phase transition\nphenomenon. The sum of weighted triangles is a variant of the average\nclustering coefficient. It is recently introduced to detect geometry in a\nnetwork. We also derive the asymptotic distribution of the sum weighted\ntriangles, which does not exhibit a phase transition phenomenon as the average\nclustering coefficient. This result signifies the difference between the two\nsummary statistics.","publication_date":1700287442,"paper_link":"http://arxiv.org/pdf/2311.10979v1","categories":["Mathematics","Statistics","Physics"],"abstract":"In network data analysis, summary statistics of a network can provide us with meaningful insight into the structure of the network. The average clustering coefficient is one of the most popular and widely used network statistics. In this paper, we investigate the asymptotic distributions of the average clustering coefficient and its variant of a heterogeneous Erd\\\"{o}s-R\\'{e}nyi random graph. We show that the standardized average clustering coefficient converges in distribution to the standard normal distribution. Interestingly, the variance of the average clustering coefficient exhibits a phase transition phenomenon. The sum of weighted triangles is a variant of the average clustering coefficient. It is recently introduced to detect geometry in a network. We also derive the asymptotic distribution of the sum weighted triangles, which does not exhibit a phase transition phenomenon as the average clustering coefficient. This result signifies the difference between the two summary statistics."}
{"title":"2d Quantum Breakdown Model with Krylov Subspace Many-Body Localization","authors":["Xinyu Liu","Biao Lian"],"raw_abstract":"We propose a two-dimensional (2d) quantum breakdown model of hardcore bosons\ninteracting with disordered spins, which resembles particles incident into\nsupersaturated vapor. The model exhibits a strong fragmentation into Krylov\nsubspaces in each symmetry sector. The Hamiltonian in each Krylov subspace maps\nto a single-particle problem in a Cayley tree-like graph, which enters a 2d\nmany-body localization (MBL) phase beyond certain disorder strength $W_*$, as\nindicated by Poisson level spacing statistics and entanglement entropy growing\nas $\\log t$ with time $t$. Our theoretical arguments suggest $W_*$ is finite or\nzero for boson number $N_b\\lesssim L^\\gamma/\\log L$ ($1/2\\le \\gamma \\le 1$) as\nsystem size $L\\rightarrow\\infty$. At zero disorder, the model also exhibits\nfully or partially solvable features, such as degenerate quantum scar states.","publication_date":1700282006,"paper_link":"http://arxiv.org/pdf/2311.10968v1","categories":["Physics"],"abstract":"We propose a two-dimensional (2d) quantum breakdown model of hardcore bosons interacting with disordered spins, which resembles particles incident into supersaturated vapor. The model exhibits a strong fragmentation into Krylov subspaces in each symmetry sector. The Hamiltonian in each Krylov subspace maps to a single-particle problem in a Cayley tree-like graph, which enters a 2d many-body localization (MBL) phase beyond certain disorder strength __FORMULA__, as indicated by Poisson level spacing statistics and entanglement entropy growing as __FORMULA__ with time __FORMULA__. Our theoretical arguments suggest __FORMULA__ is finite or zero for boson number __FORMULA__ (__FORMULA__) as system size __FORMULA__. At zero disorder, the model also exhibits fully or partially solvable features, such as degenerate quantum scar states."}
{"title":"Strong law of large numbers for the generalized Fr\u00e9chet means with random minimizing domains","authors":["Jaesung Park","Sungkyu Jung"],"raw_abstract":"This paper introduces a novel extension of Fr\\'{e}chet means, called\n\\textit{generalized Fr\\'{e}chet means} as a comprehensive framework for\ncharacterizing features in probability distributions in general topological\nspaces. The generalized Fr\\'{e}chet means are defined as minimizers of a\nsuitably defined cost function. The framework encompasses various extensions of\nFr\\'{e}chet means in the literature. The most distinctive difference of the new\nframework from the previous works is that we allow the domain of minimization\nof the empirical means be random and different from that of the population\nmeans. This expands the applicability of the Fr\\'{e}chet mean framework to\ndiverse statistical scenarios, including dimension reduction for\nmanifold-valued data.","publication_date":1700278731,"paper_link":"http://arxiv.org/pdf/2311.10958v1","categories":["Mathematics","Statistics"],"abstract":"This paper introduces a novel extension of Fr\\'{e}chet means, called generalized Fr\\'{echet means} as a comprehensive framework for characterizing features in probability distributions in general topological spaces. The generalized Fr\\'{e}chet means are defined as minimizers of a suitably defined cost function. The framework encompasses various extensions of Fr\\'{e}chet means in the literature. The most distinctive difference of the new framework from the previous works is that we allow the domain of minimization of the empirical means be random and different from that of the population means. This expands the applicability of the Fr\\'{e}chet mean framework to diverse statistical scenarios, including dimension reduction for manifold-valued data."}
{"title":"Dazed & Confused: A Large-Scale Real-World User Study of reCAPTCHAv2","authors":["Andrew Searles","Renascence Tarafder Prapty","Gene Tsudik"],"raw_abstract":"Since about 2003, captchas have been widely used as a barrier against bots,\nwhile simultaneously annoying great multitudes of users worldwide. As their use\ngrew, techniques to defeat or bypass captchas kept improving, while captchas\nthemselves evolved in terms of sophistication and diversity, becoming\nincreasingly difficult to solve for both bots and humans. Given this\nlong-standing and still-ongoing arms race, it is important to investigate\nusability, solving performance, and user perceptions of modern captchas. In\nthis work, we do so via a large-scale (over 3, 600 distinct users) 13-month\nreal-world user study and post-study survey. The study, conducted at a large\npublic university, was based on a live account creation and password recovery\nservice with currently prevalent captcha type: reCAPTCHAv2.\n  Results show that, with more attempts, users improve in solving checkbox\nchallenges. For website developers and user study designers, results indicate\nthat the website context directly influences (with statistically significant\ndifferences) solving time between password recovery and account creation. We\nconsider the impact of participants' major and education level, showing that\ncertain majors exhibit better performance, while, in general, education level\nhas a direct impact on solving time. Unsurprisingly, we discover that\nparticipants find image challenges to be annoying, while checkbox challenges\nare perceived as easy. We also show that, rated via System Usability Scale\n(SUS), image tasks are viewed as \"OK\", while checkbox tasks are viewed as\n\"good\". We explore the cost and security of reCAPTCHAv2 and conclude that it\nhas an immense cost and no security. Overall, we believe that this study's\nresults prompt a natural conclusion: reCAPTCHAv2 and similar reCAPTCHA\ntechnology should be deprecated.","publication_date":1700264328,"paper_link":"http://arxiv.org/pdf/2311.10911v2","categories":["Statistics"],"abstract":"Since about 2003, captchas have been widely used as a barrier against bots, while simultaneously annoying great multitudes of users worldwide. As their use grew, techniques to defeat or bypass captchas kept improving, while captchas themselves evolved in terms of sophistication and diversity, becoming increasingly difficult to solve for both bots and humans. Given this long-standing and still-ongoing arms race, it is important to investigate usability, solving performance, and user perceptions of modern captchas. In this work, we do so via a large-scale (over 3, 600 distinct users) 13-month real-world user study and post-study survey. The study, conducted at a large public university, was based on a live account creation and password recovery service with currently prevalent captcha type: reCAPTCHAv2.   Results show that, with more attempts, users improve in solving checkbox challenges. For website developers and user study designers, results indicate that the website context directly influences (with statistically significant differences) solving time between password recovery and account creation. We consider the impact of participants' major and education level, showing that certain majors exhibit better performance, while, in general, education level has a direct impact on solving time. Unsurprisingly, we discover that participants find image challenges to be annoying, while checkbox challenges are perceived as easy. We also show that, rated via System Usability Scale (SUS), image tasks are viewed as \"OK\", while checkbox tasks are viewed as \"good\". We explore the cost and security of reCAPTCHAv2 and conclude that it has an immense cost and no security. Overall, we believe that this study's results prompt a natural conclusion: reCAPTCHAv2 and similar reCAPTCHA technology should be deprecated."}
{"title":"Closely-Spaced Object Classification Using MuyGPyS","authors":["Kerianne Pruett","Nathan McNaughton","Michael Schneider"],"raw_abstract":"Accurately detecting rendezvous and proximity operations (RPO) is crucial for\nunderstanding how objects are behaving in the space domain. However, detecting\nclosely-spaced objects (CSO) is challenging for ground-based optical space\ndomain awareness (SDA) algorithms as two objects close together along the\nline-of-sight can appear blended as a single object within the point-spread\nfunction (PSF) of the optical system. Traditional machine learning methods can\nbe useful for differentiating between singular objects and closely-spaced\nobjects, but many methods require large training sample sizes or high\nsignal-to-noise conditions. The quality and quantity of realistic data make\nprobabilistic classification methods a superior approach, as they are better\nsuited to handle these data inadequacies. We present CSO classification results\nusing the Gaussian process python package, MuyGPyS, and examine classification\naccuracy as a function of angular separation and magnitude difference between\nthe simulated satellites. This orbit-independent analysis is done on highly\naccurate simulated SDA images that emulate realistic ground-based\ncommercial-of-the-shelf (COTS) optical sensor observations of CSOs. We find\nthat MuyGPyS outperforms traditional machine learning methods, especially under\nmore challenging circumstances.","publication_date":1700261566,"paper_link":"http://arxiv.org/pdf/2311.10904v1","categories":["Physics"],"abstract":"Accurately detecting rendezvous and proximity operations (RPO) is crucial for understanding how objects are behaving in the space domain. However, detecting closely-spaced objects (CSO) is challenging for ground-based optical space domain awareness (SDA) algorithms as two objects close together along the line-of-sight can appear blended as a single object within the point-spread function (PSF) of the optical system. Traditional machine learning methods can be useful for differentiating between singular objects and closely-spaced objects, but many methods require large training sample sizes or high signal-to-noise conditions. The quality and quantity of realistic data make probabilistic classification methods a superior approach, as they are better suited to handle these data inadequacies. We present CSO classification results using the Gaussian process python package, MuyGPyS, and examine classification accuracy as a function of angular separation and magnitude difference between the simulated satellites. This orbit-independent analysis is done on highly accurate simulated SDA images that emulate realistic ground-based commercial-of-the-shelf (COTS) optical sensor observations of CSOs. We find that MuyGPyS outperforms traditional machine learning methods, especially under more challenging circumstances."}
{"title":"A powerful rank-based correction to multiple testing under positive dependency","authors":["Alexander Timans","Christoph-Nikolas Straehle","Kaspar Sakmann","Eric Nalisnick"],"raw_abstract":"We develop a novel multiple hypothesis testing correction with family-wise\nerror rate (FWER) control that efficiently exploits positive dependencies\nbetween potentially correlated statistical hypothesis tests. Our proposed\nalgorithm $\\texttt{max-rank}$ is conceptually straight-forward, relying on the\nuse of a $\\max$-operator in the rank domain of computed test statistics. We\ncompare our approach to the frequently employed Bonferroni correction,\ntheoretically and empirically demonstrating its superiority over Bonferroni in\nthe case of existing positive dependency, and its equivalence otherwise. Our\nadvantage over Bonferroni increases as the number of tests rises, and we\nmaintain high statistical power whilst ensuring FWER control. We specifically\nframe our algorithm in the context of parallel permutation testing, a scenario\nthat arises in our primary application of conformal prediction, a recently\npopularized approach for quantifying uncertainty in complex predictive\nsettings.","publication_date":1700261062,"paper_link":"http://arxiv.org/pdf/2311.10900v1","categories":["Mathematics","Statistics"],"abstract":"We develop a novel multiple hypothesis testing correction with family-wise error rate (FWER) control that efficiently exploits positive dependencies between potentially correlated statistical hypothesis tests. Our proposed algorithm __FORMULA__ is conceptually straight-forward, relying on the use of a __FORMULA__-operator in the rank domain of computed test statistics. We compare our approach to the frequently employed Bonferroni correction, theoretically and empirically demonstrating its superiority over Bonferroni in the case of existing positive dependency, and its equivalence otherwise. Our advantage over Bonferroni increases as the number of tests rises, and we maintain high statistical power whilst ensuring FWER control. We specifically frame our algorithm in the context of parallel permutation testing, a scenario that arises in our primary application of conformal prediction, a recently popularized approach for quantifying uncertainty in complex predictive settings."}
{"title":"Virtual trajectories for I-24 MOTION: data and tools","authors":["Junyi Ji","Yanbing Wang","Derek Gloudemans","Gergely Zach\u00e1r","William Barbour","Daniel B. Work"],"raw_abstract":"This article introduces a new virtual trajectory dataset derived from the\nI-24 MOTION INCEPTION v1.0.0 dataset to address challenges in analyzing large\nbut noisy trajectory datasets. Building on the concept of virtual trajectories,\nwe provide a Python implementation to generate virtual trajectories from large\nraw datasets that are typically challenging to process due to their size. We\ndemonstrate the practical utility of these trajectories in assessing speed\nvariability and travel times across different lanes within the INCEPTION\ndataset. The virtual trajectory dataset opens future research on traffic waves\nand their impact on energy.","publication_date":1700259182,"paper_link":"http://arxiv.org/pdf/2311.10888v1","categories":["Physics","Electrical Engineering and Systems Science"],"abstract":"This article introduces a new virtual trajectory dataset derived from the I-24 MOTION INCEPTION v1.0.0 dataset to address challenges in analyzing large but noisy trajectory datasets. Building on the concept of virtual trajectories, we provide a Python implementation to generate virtual trajectories from large raw datasets that are typically challenging to process due to their size. We demonstrate the practical utility of these trajectories in assessing speed variability and travel times across different lanes within the INCEPTION dataset. The virtual trajectory dataset opens future research on traffic waves and their impact on energy."}
{"title":"Exploration of hadronization through heavy flavor production at the future Electron-Ion Collider","authors":["Xuan Li"],"raw_abstract":"The future Electron-Ion Collider will utilize high-luminosity high-energy\nelectron+proton ($e+p$) and electron+nucleus ($e+A$) collisions to solve\nseveral fundamental questions in the high energy nuclear physics field. Heavy\nflavor products play an important role in constraining the initial-state\nnucleon/nucleus parton distribution functions especially in the high and low\nBjorken-x ($x_{BJ}$) region and exploring the final-state parton propagation\nand hadronization processes under different nuclear medium conditions. Latest\nsimulation studies of heavy flavor hadron and jet measurements with the EIC\nproject detector conceptual design will be discussed. The projected statistical\naccuracy of heavy flavor jet and heavy flavor hadron inside jet measurements in\ncomparison with latest theoretical calculations will be presented.","publication_date":1700256724,"paper_link":"http://arxiv.org/pdf/2311.10875v1","categories":["Physics"],"abstract":"The future Electron-Ion Collider will utilize high-luminosity high-energy electron+proton (__FORMULA__) and electron+nucleus (__FORMULA__) collisions to solve several fundamental questions in the high energy nuclear physics field. Heavy flavor products play an important role in constraining the initial-state nucleon/nucleus parton distribution functions especially in the high and low Bjorken-x (__FORMULA__) region and exploring the final-state parton propagation and hadronization processes under different nuclear medium conditions. Latest simulation studies of heavy flavor hadron and jet measurements with the EIC project detector conceptual design will be discussed. The projected statistical accuracy of heavy flavor jet and heavy flavor hadron inside jet measurements in comparison with latest theoretical calculations will be presented."}
{"title":"Statistics dependent spectral properties of random arrays of particles","authors":["Romil Audhkhasi","Maksym Zhelyeznyakov","Steven Brunton","Arka Majumdar"],"raw_abstract":"The ability to tailor the spectral response of photonic devices is paramount\nto the advancement of a broad range of applications. The vast design space\noffered by disordered optical media provide enhanced functionality for spectral\ntailoring, although it is challenging to map the spectral properties of such\ncomplex systems to their structural attributes. In this work, we investigate\ncorrelations between the statistics underlying the structure of random arrays\nof particles and their spectral properties. We consider 1D and 2D arrays of\ndielectric nanorods suspended in vacuum and numerically study their optical\nscattering properties in the visible wavelength range. We show that the\nscattering cross section of a random particle array is primarily governed by\nits configuration statistics and is independent of its exact instantiation or\nthe number of its constituent particles. We further exploit the strong\ncorrelations between the statistics and spectral properties of random particle\narrays to predict their spectral response. By using a semi-analytical nearest\nneighbor coupling model, we produce accurate qualitative estimates of the\nspectral responses of both one and two-dimensional random arrays for different\nconfiguration statistics. The results presented in this manuscript will open\nnew avenues for optimizing large-scale random systems to achieve enhanced\noptical functionalities for a wide variety of applications.","publication_date":1700256516,"paper_link":"http://arxiv.org/pdf/2311.10874v1","categories":["Physics"],"abstract":"The ability to tailor the spectral response of photonic devices is paramount to the advancement of a broad range of applications. The vast design space offered by disordered optical media provide enhanced functionality for spectral tailoring, although it is challenging to map the spectral properties of such complex systems to their structural attributes. In this work, we investigate correlations between the statistics underlying the structure of random arrays of particles and their spectral properties. We consider 1D and 2D arrays of dielectric nanorods suspended in vacuum and numerically study their optical scattering properties in the visible wavelength range. We show that the scattering cross section of a random particle array is primarily governed by its configuration statistics and is independent of its exact instantiation or the number of its constituent particles. We further exploit the strong correlations between the statistics and spectral properties of random particle arrays to predict their spectral response. By using a semi-analytical nearest neighbor coupling model, we produce accurate qualitative estimates of the spectral responses of both one and two-dimensional random arrays for different configuration statistics. The results presented in this manuscript will open new avenues for optimizing large-scale random systems to achieve enhanced optical functionalities for a wide variety of applications."}
{"title":"Scaling laws of failure dynamics on complex networks","authors":["G. P\u00e1l","Zs. Danku","A. Batool","V. K\u00e1d\u00e1r","N. Yoshioka","N. Ito","G. \u00d3dor","F. Kun"],"raw_abstract":"The topology of the network of load transmitting connections plays an\nessential role in the cascading failure dynamics of complex systems driven by\nthe redistribution of load after local breakdown events. In particular, as the\nnetwork structure is gradually tuned from regular to completely random a\ntransition occurs from the localized to mean field behavior of failure\nspreading. Based on finite size scaling in the fiber bundle model of failure\nphenomena, here we demonstrate that outside the localized regime, the load\nbearing capacity and damage tolerance on the macro-scale, and the statistics of\nclusters of failed nodes on the micro-scale obey scaling laws with exponents\nwhich depend on the topology of the load transmission network and on the degree\nof disorder of the strength of nodes. Most notably, we show that the spatial\nstructure of damage governs the emergence of the localized to mean field\ntransition: as the network gets gradually randomized failed clusters formed on\nlocally regular patches merge through long range links generating a percolation\nlike transition which reduces the load concentration on the network. The\nresults may help to design network structures with an improved robustness\nagainst cascading failure.","publication_date":1700251932,"paper_link":"http://arxiv.org/pdf/2311.10850v1","categories":["Physics"],"abstract":"The topology of the network of load transmitting connections plays an essential role in the cascading failure dynamics of complex systems driven by the redistribution of load after local breakdown events. In particular, as the network structure is gradually tuned from regular to completely random a transition occurs from the localized to mean field behavior of failure spreading. Based on finite size scaling in the fiber bundle model of failure phenomena, here we demonstrate that outside the localized regime, the load bearing capacity and damage tolerance on the macro-scale, and the statistics of clusters of failed nodes on the micro-scale obey scaling laws with exponents which depend on the topology of the load transmission network and on the degree of disorder of the strength of nodes. Most notably, we show that the spatial structure of damage governs the emergence of the localized to mean field transition: as the network gets gradually randomized failed clusters formed on locally regular patches merge through long range links generating a percolation like transition which reduces the load concentration on the network. The results may help to design network structures with an improved robustness against cascading failure."}
{"title":"High precision accelerator for our hybrid model of the redshift space power spectrum","authors":["M. Icaza-Lizaola","Yong-Seon Song","Minji Oh","Yi Zheng"],"raw_abstract":"Upcoming Large Scale Structure surveys aim to achieve an unprecedented level\nof precision in measuring galaxy clustering. However, accurately modeling these\nstatistics may require theoretical templates that go beyond second-order\nperturbation theory, especially for achieving precision at smaller scales. In\nour previous work, we introduced a hybrid model for the redshift space power\nspectrum of galaxies. This model combines second-order templates with N-body\nsimulations to capture the influence of scale-independent parameters on the\ngalaxy power spectrum. However, the impact of scale-dependent parameters was\naddressed by precomputing a set of input statistics derived from\ncomputationally expensive N-body simulations. As a result, exploring the\nscale-dependent parameter space was not feasible in this approach. To address\nthis challenge, we present an accelerated methodology that utilizes Gaussian\nprocesses, a machine learning technique, to emulate these input statistics. Our\nemulators exhibit remarkable accuracy, achieving reliable results with just 13\nN-body simulations for training. We reproduce all necessary input statistics\nfor a set of test simulations with an error of approximately 0.1 per cent in\nthe parameter space within $5\\sigma$ of the Planck predictions, specifically\nfor scales around $k > 0.1$ $h$Mpc$^{-1}$. Following the training of our\nemulators, we can predict all inputs for our hybrid model in approximately\n0.2,seconds at a specified redshift. Given that performing 13 N-body\nsimulations is a manageable task, our present methodology enables us to\nconstruct efficient and highly accurate models of the galaxy power spectra\nwithin a manageable time frame.","publication_date":1700247827,"paper_link":"http://arxiv.org/pdf/2311.10823v1","categories":["Physics"],"abstract":"Upcoming Large Scale Structure surveys aim to achieve an unprecedented level of precision in measuring galaxy clustering. However, accurately modeling these statistics may require theoretical templates that go beyond second-order perturbation theory, especially for achieving precision at smaller scales. In our previous work, we introduced a hybrid model for the redshift space power spectrum of galaxies. This model combines second-order templates with N-body simulations to capture the influence of scale-independent parameters on the galaxy power spectrum. However, the impact of scale-dependent parameters was addressed by precomputing a set of input statistics derived from computationally expensive N-body simulations. As a result, exploring the scale-dependent parameter space was not feasible in this approach. To address this challenge, we present an accelerated methodology that utilizes Gaussian processes, a machine learning technique, to emulate these input statistics. Our emulators exhibit remarkable accuracy, achieving reliable results with just 13 N-body simulations for training. We reproduce all necessary input statistics for a set of test simulations with an error of approximately 0.1 per cent in the parameter space within __FORMULA__ of the Planck predictions, specifically for scales around __FORMULA__ __FORMULA__Mpc__FORMULA__. Following the training of our emulators, we can predict all inputs for our hybrid model in approximately 0.2,seconds at a specified redshift. Given that performing 13 N-body simulations is a manageable task, our present methodology enables us to construct efficient and highly accurate models of the galaxy power spectra within a manageable time frame."}
{"title":"Eigenstate entanglement entropy in the integrable spin-\\texorpdfstring{$\\frac{1}{2}$}{1/2} XYZ model","authors":["Rafa\u0142 \u015awi\u0119tek","Maksymilian Kliczkowski","Lev Vidmar","Marcos Rigol"],"raw_abstract":"We study the average and the standard deviation of the entanglement entropy\nof highly excited eigenstates of the integrable interacting spin-$\\frac{1}{2}$\nXYZ chain away from and at special lines with $U(1)$ symmetry and\nsupersymmetry. We universally find that the average eigenstate entanglement\nentropy exhibits a volume-law coefficient that is smaller than that of\nquantum-chaotic interacting models. At the supersymmetric point, we resolve the\neffect that degeneracies have on the computed averages. We further find that\nthe normalized standard deviation of the eigenstate entanglement entropy decays\npolynomially with increasing system size, which we contrast to the exponential\ndecay in quantum-chaotic interacting models. Our results provide state-of-the\nart numerical evidence that integrability in spin-$\\frac{1}{2}$ chains reduces\nthe average, and increases the standard deviation, of the entanglement entropy\nof highly excited energy eigenstates when compared to those in quantum-chaotic\ninteracting models.","publication_date":1700247609,"paper_link":"http://arxiv.org/pdf/2311.10819v1","categories":["Physics"],"abstract":"We study the average and the standard deviation of the entanglement entropy of highly excited eigenstates of the integrable interacting spin-__FORMULA__ XYZ chain away from and at special lines with __FORMULA__ symmetry and supersymmetry. We universally find that the average eigenstate entanglement entropy exhibits a volume-law coefficient that is smaller than that of quantum-chaotic interacting models. At the supersymmetric point, we resolve the effect that degeneracies have on the computed averages. We further find that the normalized standard deviation of the eigenstate entanglement entropy decays polynomially with increasing system size, which we contrast to the exponential decay in quantum-chaotic interacting models. Our results provide state-of-the art numerical evidence that integrability in spin-__FORMULA__ chains reduces the average, and increases the standard deviation, of the entanglement entropy of highly excited energy eigenstates when compared to those in quantum-chaotic interacting models."}
{"title":"Mixing properties of nonstationary multivariate count processes","authors":["Zinsou Max Debaly","Michael H. Neumann","Lionel Truquet"],"raw_abstract":"We prove absolute regularity ($\\beta$-mixing) for nonstationary and\nmultivariate versions of two popular classes of integer-valued processes. We\nshow how this result can be used to prove asymptotic normality of a least\nsquares estimator of an involved model parameter.","publication_date":1700245529,"paper_link":"http://arxiv.org/pdf/2311.10692v1","categories":["Mathematics","Statistics"],"abstract":"We prove absolute regularity (__FORMULA__-mixing) for nonstationary and multivariate versions of two popular classes of integer-valued processes. We show how this result can be used to prove asymptotic normality of a least squares estimator of an involved model parameter."}
{"title":"A sliding window-based algorithm for faster transformation of time series into complex networks","authors":["R. Carmona-Cabezas","J. Gomez-Gomez","E. Gutierrez de Rave","F. J. Jimenez-Hornero"],"raw_abstract":"A new alternative method to approximate the Visibility Graph (VG) of a time\nseries has been introduced here. It exploits the fact that most of the nodes in\nthe resulting network are not connected to those that are far away from them.\nThis means that the adjacency matrix is almost empty, and its nonzero values\nare close to the main diagonal. This new method is called Sliding Visibility\nGraph (SVG). Numerical tests have been performed for several time series,\nshowing a time efficiency that scales linearly with the size of the series\n[O(N)], in contrast to the original VG that does so quadratically [O(N2)]. This\nfact is noticeably convenient when dealing with very large time series. The\nresults obtained from the SVG of the studied time series have been compared to\nthe exact values of the original VG. As expected, the SVG outcomes converge\nvery rapidly to the desired ones, especially for random and stochastic series.\nAlso, this method can be extended to the analysis of time series that evolve in\nreal time, since it does not require the entire dataset to perform the analysis\nbut a shorter segment of it. The length segment can remain constant, making\npossible a simple analysis as the series evolves in time.","publication_date":1700244988,"paper_link":"http://arxiv.org/pdf/2311.10688v1","categories":["Physics"],"abstract":"A new alternative method to approximate the Visibility Graph (VG) of a time series has been introduced here. It exploits the fact that most of the nodes in the resulting network are not connected to those that are far away from them. This means that the adjacency matrix is almost empty, and its nonzero values are close to the main diagonal. This new method is called Sliding Visibility Graph (SVG). Numerical tests have been performed for several time series, showing a time efficiency that scales linearly with the size of the series [O(N)], in contrast to the original VG that does so quadratically [O(N2)]. This fact is noticeably convenient when dealing with very large time series. The results obtained from the SVG of the studied time series have been compared to the exact values of the original VG. As expected, the SVG outcomes converge very rapidly to the desired ones, especially for random and stochastic series. Also, this method can be extended to the analysis of time series that evolve in real time, since it does not require the entire dataset to perform the analysis but a shorter segment of it. The length segment can remain constant, making possible a simple analysis as the series evolves in time."}
{"title":"Target-based Distributionally Robust Minimum Spanning Tree Problem","authors":["Yang Xu","Lianmin Zhang"],"raw_abstract":"Due to its broad applications in practice, the minimum spanning tree problem\nand its all kinds of variations have been studied extensively during the last\ndecades, for which a host of efficient exact and heuristic algorithms have been\nproposed. Meanwhile, motivated by realistic applications, the minimum spanning\ntree problem in stochastic network has attracted considerable attention of\nresearchers, with respect to which stochastic and robust spanning tree models\nand related algorithms have been continuingly developed. However, all of them\nwould be either too restricted by the types of the edge weight random variables\nor computationally intractable, especially in large-scale networks. In this\npaper, we introduce a target-based distributionally robust optimization\nframework to solve the minimum spanning tree problem in stochastic graphs where\nthe probability distribution function of the edge weight is unknown but some\nstatistical information could be utilized to prevent the optimal solution from\nbeing too conservative. We propose two exact algorithms to solve it, based on\nBenders decomposition framework and a modified classical greedy algorithm of\nMST problem (Prim algorithm),respectively. Compared with the NP-hard stochastic\nand robust spanning tree problems,The proposed target-based distributionally\nrobust minimum spanning tree problem enjoys more satisfactory algorithmic\naspect and robustness, when faced with uncertainty in input data.","publication_date":1700242987,"paper_link":"http://arxiv.org/pdf/2311.10670v1","categories":["Mathematics"],"abstract":"Due to its broad applications in practice, the minimum spanning tree problem and its all kinds of variations have been studied extensively during the last decades, for which a host of efficient exact and heuristic algorithms have been proposed. Meanwhile, motivated by realistic applications, the minimum spanning tree problem in stochastic network has attracted considerable attention of researchers, with respect to which stochastic and robust spanning tree models and related algorithms have been continuingly developed. However, all of them would be either too restricted by the types of the edge weight random variables or computationally intractable, especially in large-scale networks. In this paper, we introduce a target-based distributionally robust optimization framework to solve the minimum spanning tree problem in stochastic graphs where the probability distribution function of the edge weight is unknown but some statistical information could be utilized to prevent the optimal solution from being too conservative. We propose two exact algorithms to solve it, based on Benders decomposition framework and a modified classical greedy algorithm of MST problem (Prim algorithm),respectively. Compared with the NP-hard stochastic and robust spanning tree problems,The proposed target-based distributionally robust minimum spanning tree problem enjoys more satisfactory algorithmic aspect and robustness, when faced with uncertainty in input data."}
{"title":"A BRAIN study to tackle image analysis with artificial intelligence in the ALMA 2030 era","authors":["Fabrizia Guglielmetti","Michele Delli Veneri","Ivano Baronchelli","Carmen Blanco","Andrea Dosi","Torsten En\u00dflin","Vishal Johnson","Giuseppe Longo","Jakob Roth","Felix Stoehr","\u0141ukasz Tychoniec","Eric Villard"],"raw_abstract":"An ESO internal ALMA development study, BRAIN, is addressing the ill-posed\ninverse problem of synthesis image analysis employing astrostatistics and\nastroinformatics. These emerging fields of research offer interdisciplinary\napproaches at the intersection of observational astronomy, statistics,\nalgorithm development, and data science. In this study, we provide evidence of\nthe benefits of employing these approaches to ALMA imaging for operational and\nscientific purposes. We show the potential of two techniques, RESOLVE and\nDeepFocus, applied to ALMA calibrated science data. Significant advantages are\nprovided with the prospect to improve the quality and completeness of the data\nproducts stored in the science archive and overall processing time for\noperations. Both approaches evidence the logical pathway to address the\nincoming revolution in data rates dictated by the planned electronic upgrades.\nMoreover, we bring to the community additional products through a new package,\nALMASim, to promote advancements in these fields, providing a refined ALMA\nsimulator usable by a large community for training and/or testing new\nalgorithms.","publication_date":1700241728,"paper_link":"http://arxiv.org/pdf/2311.10657v1","categories":["Physics"],"abstract":"An ESO internal ALMA development study, BRAIN, is addressing the ill-posed inverse problem of synthesis image analysis employing astrostatistics and astroinformatics. These emerging fields of research offer interdisciplinary approaches at the intersection of observational astronomy, statistics, algorithm development, and data science. In this study, we provide evidence of the benefits of employing these approaches to ALMA imaging for operational and scientific purposes. We show the potential of two techniques, RESOLVE and DeepFocus, applied to ALMA calibrated science data. Significant advantages are provided with the prospect to improve the quality and completeness of the data products stored in the science archive and overall processing time for operations. Both approaches evidence the logical pathway to address the incoming revolution in data rates dictated by the planned electronic upgrades. Moreover, we bring to the community additional products through a new package, ALMASim, to promote advancements in these fields, providing a refined ALMA simulator usable by a large community for training and/or testing new algorithms."}
{"title":"Critical ultrasonic propagation in magnetic fields","authors":["A. Pawlak"],"raw_abstract":"Effect of an external magnetic field on the critical sound attenuation and\nvelocity of the longitudinal wave is studied in ferromagnets. We derive a\nparametric model that incorporates a crossover from the asymptotic critical\nbehavior to the Landau-Ginzburg regular behavior far away from the critical\npoint. The dynamics is based on the time dependent Ginzburg-Landau model with\nnon conserved order parameter (model A). The variations of the sound\nattenuation coefficient and velocity have been obtained for arbitrary values of\nthe magnetic field and reduced temperature. The scaling functions are given\nwithin the renormalization group formalism at one-loop order. Using MnP as an\nexample, we show that such parametric crossover model yields an accurate\ndescription of ultrasonic data in a large region of temperatures and magnetic\nfields around the critical point.","publication_date":1700241442,"paper_link":"http://arxiv.org/pdf/2311.10654v1","categories":["Physics"],"abstract":"Effect of an external magnetic field on the critical sound attenuation and velocity of the longitudinal wave is studied in ferromagnets. We derive a parametric model that incorporates a crossover from the asymptotic critical behavior to the Landau-Ginzburg regular behavior far away from the critical point. The dynamics is based on the time dependent Ginzburg-Landau model with non conserved order parameter (model A). The variations of the sound attenuation coefficient and velocity have been obtained for arbitrary values of the magnetic field and reduced temperature. The scaling functions are given within the renormalization group formalism at one-loop order. Using MnP as an example, we show that such parametric crossover model yields an accurate description of ultrasonic data in a large region of temperatures and magnetic fields around the critical point."}
